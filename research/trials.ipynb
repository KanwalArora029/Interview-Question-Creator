{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Set up the OpenAI API Key in the environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/ML_book.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the number of pages page in pdf\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christopher M. Bishop\n",
      "Pattern Recognition and\n",
      "Machine Learning\n",
      "Christopher M. Bishop F.R.Eng.\n",
      "Assistant Director\n",
      "Microsoft Research Ltd\n",
      "Cambridge CB3 0FB, U.K.\n",
      "cmbishop@microsoft.com\n",
      "http://research.microsoft.com/ /H11011cmbishop\n",
      "Series Editors\n",
      "Michael Jordan\n",
      "Department of Computer\n",
      "Science and Department\n",
      "of Statistics\n",
      "University of California,\n",
      "Berkeley\n",
      "Berkeley, CA 94720\n",
      "USAProfessor Jon Kleinberg\n",
      "Department of Computer\n",
      "Science\n",
      "Cornell University\n",
      "Ithaca, NY 14853\n",
      "USABernhard Scho ¨lkopf\n",
      "Max Planck Institute for\n",
      "Biological Cybernetics\n",
      "Spemannstrasse 38\n",
      "72076 Tu ¨bingen\n",
      "Germany\n",
      "Library of Congress Control Number: 2006922522\n",
      "ISBN-10: 0-387-31073-8\n",
      "ISBN-13: 978-0387-31073-2\n",
      "Printed on acid-free paper.\n",
      "©2006 Springer Science +Business Media, LLC\n",
      "All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher\n",
      "(Springer Science +Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection\n",
      "with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation,\n",
      "computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.\n",
      "The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such,\n",
      "is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\n",
      "Printed in Singapore. (KYO)\n",
      "987654321\n",
      "springer.comThis book is dedicated to my family:\n",
      "Jenna, Mark, and Hugh\n",
      "Total eclipse of the sun, Antalya, Turkey, 29 March 2006.Preface\n",
      "Pattern recognition has its origins in engineering, whereas machine learning grew\n",
      "out of computer science. However, these activities can be viewed as two facets of\n",
      "the same ﬁeld, and together they have undergone substantial development over the\n",
      "past ten years. In particular, Bayesian methods have grown from a specialist niche to\n",
      "become mainstream, while graphical models have emerged as a general framework\n",
      "for describing and applying probabilistic models. Also, the practical applicability ofBayesian methods has been greatly enhanced through the development of a range of\n",
      "approximate inference algorithms such as variational Bayes and expectation propa-\n",
      "gation. Similarly, new models based on kernels have had signiﬁcant impact on bothalgorithms and applications.\n",
      "This new textbook reﬂects these recent developments while providing a compre-\n",
      "hensive introduction to the ﬁelds of pattern recognition and machine learning. It isaimed at advanced undergraduates or ﬁrst year PhD students, as well as researchers\n",
      "and practitioners, and assumes no previous knowledge of pattern recognition or ma-\n",
      "chine learning concepts. Knowledge of multivariate calculus and basic linear algebrais required, and some familiarity with probabilities would be helpful though not es-\n",
      "sential as the book includes a self-contained introduction to basic probability theory.\n",
      "Because this book has broad scope, it is impossible to provide a complete list of\n",
      "references, and in particular no attempt has been made to provide accurate historical\n",
      "attribution of ideas. Instead, the aim has been to give references that offer greater\n",
      "detail than is possible here and that hopefully provide entry points into what, in some\n",
      "cases, is a very extensive literature. For this reason, the references are often to more\n",
      "recent textbooks and review articles rather than to original sources.\n",
      "The book is supported by a great deal of additional material, including lecture\n",
      "slides as well as the complete set of ﬁgures used in the book, and the reader is\n",
      "encouraged to visit the book web site for the latest information:\n",
      "http://research.microsoft.com/ ∼cmbishop/PRML\n",
      "viiviii PREFACE\n",
      "Exercises\n",
      "The exercises that appear at the end of every chapter form an important com-\n",
      "ponent of the book. Each exercise has been carefully chosen to reinforce concepts\n",
      "explained in the text or to develop and generalize them in signiﬁcant ways, and each\n",
      "is graded according to difﬁculty ranging from (⋆), which denotes a simple exercise\n",
      "taking a few minutes to complete, through to (⋆⋆⋆ ), which denotes a signiﬁcantly\n",
      "more complex exercise.\n",
      "It has been difﬁcult to know to what extent these solutions should be made\n",
      "widely available. Those engaged in self study will ﬁnd worked solutions very ben-\n",
      "eﬁcial, whereas many course tutors request that solutions be available only via thepublisher so that the exercises may be used in class. In order to try to meet these\n",
      "conﬂicting requirements, those exercises that help amplify key points in the text, or\n",
      "that ﬁll in important details, have solutions that are available as a PDF ﬁle from thebook web site. Such exercises are denoted by\n",
      "www . Solutions for the remaining\n",
      "exercises are available to course tutors by contacting the publisher (contact details\n",
      "are given on the book web site). Readers are strongly encouraged to work throughthe exercises unaided, and to turn to the solutions only as required.\n",
      "Although this book focuses on concepts and principles, in a taught course the\n",
      "students should ideally have the opportunity to experiment with some of the keyalgorithms using appropriate data sets. A companion volume (Bishop and Nabney,\n",
      "2008) will deal with practical aspects of pattern recognition and machine learning,\n",
      "and will be accompanied by Matlab software implementing most of the algorithmsdiscussed in this book.\n",
      "Acknowledgements\n",
      "First of all I would like to express my sincere thanks to Markus Svens ´en who\n",
      "has provided immense help with preparation of ﬁgures and with the typesetting of\n",
      "the book in L ATEX. His assistance has been invaluable.\n",
      "I am very grateful to Microsoft Research for providing a highly stimulating re-\n",
      "search environment and for giving me the freedom to write this book (the views and\n",
      "opinions expressed in this book, however, are my own and are therefore not neces-sarily the same as those of Microsoft or its afﬁliates).\n",
      "Springer has provided excellent support throughout the ﬁnal stages of prepara-\n",
      "tion of this book, and I would like to thank my commissioning editor John Kimmelfor his support and professionalism, as well as Joseph Piliero for his help in design-\n",
      "ing the cover and the text format and MaryAnn Brickner for her numerous contribu-\n",
      "tions during the production phase. The inspiration for the cover design came from adiscussion with Antonio Criminisi.\n",
      "I also wish to thank Oxford University Press for permission to reproduce ex-\n",
      "cerpts from an earlier textbook, Neural Networks for Pattern Recognition (Bishop,\n",
      "1995a). The images of the Mark 1 perceptron and of Frank Rosenblatt are repro-\n",
      "duced with the permission of Arvin Calspan Advanced Technology Center. I wouldalso like to thank Asela Gunawardana for plotting the spectrogram in Figure 13.1,\n",
      "and Bernhard Sch ¨olkopf for permission to use his kernel PCA code to plot Fig-\n",
      "ure 12.17.PREFACE ix\n",
      "Many people have helped by proofreading draft material and providing com-\n",
      "ments and suggestions, including Shivani Agarwal, C ´edric Archambeau, Arik Azran,\n",
      "Andrew Blake, Hakan Cevikalp, Michael Fourman, Brendan Frey, Zoubin Ghahra-\n",
      "mani, Thore Graepel, Katherine Heller, Ralf Herbrich, Geoffrey Hinton, Adam Jo-\n",
      "hansen, Matthew Johnson, Michael Jordan, Eva Kalyvianaki, Anitha Kannan, JuliaLasserre, David Liu, Tom Minka, Ian Nabney, Tonatiuh Pena, Y uan Qi, Sam Roweis,\n",
      "Balaji Sanjiya, Toby Sharp, Ana Costa e Silva, David Spiegelhalter, Jay Stokes, Tara\n",
      "Symeonides, Martin Szummer, Marshall Tappen, Ilkay Ulusoy, Chris Williams, JohnWinn, and Andrew Zisserman.\n",
      "Finally, I would like to thank my wife Jenna who has been hugely supportive\n",
      "throughout the several years it has taken to write this book.\n",
      "Chris Bishop\n",
      "Cambridge\n",
      "February 2006Mathematical notation\n",
      "I have tried to keep the mathematical content of the book to the minimum neces-\n",
      "sary to achieve a proper understanding of the ﬁeld. However, this minimum level is\n",
      "nonzero, and it should be emphasized that a good grasp of calculus, linear algebra,\n",
      "and probability theory is essential for a clear understanding of modern pattern recog-\n",
      "nition and machine learning techniques. Nevertheless, the emphasis in this book is\n",
      "on conveying the underlying concepts rather than on mathematical rigour.\n",
      "I have tried to use a consistent notation throughout the book, although at times\n",
      "this means departing from some of the conventions used in the corresponding re-\n",
      "search literature. V ectors are denoted by lower case bold Roman letters such asx, and all vectors are assumed to be column vectors. A superscript Tdenotes the\n",
      "transpose of a matrix or vector, so that x\n",
      "Twill be a row vector. Uppercase bold\n",
      "roman letters, such as M, denote matrices. The notation (w1,...,w M)denotes a\n",
      "row vector with Melements, while the corresponding column vector is written as\n",
      "w=(w1,...,w M)T.\n",
      "The notation [a, b]is used to denote the closed interval from atob, that is the\n",
      "interval including the values aandbthemselves, while (a, b)denotes the correspond-\n",
      "ing open interval, that is the interval excluding aandb. Similarly, [a, b)denotes an\n",
      "interval that includes abut excludes b. For the most part, however, there will be\n",
      "little need to dwell on such reﬁnements as whether the end points of an interval are\n",
      "included or not.\n",
      "TheM×Midentity matrix (also known as the unit matrix) is denoted IM,\n",
      "which will be abbreviated to Iwhere there is no ambiguity about it dimensionality.\n",
      "It has elements Iijthat equal 1ifi=jand0ifi̸=j.\n",
      "A functional is denoted f[y]where y(x)is some function. The concept of a\n",
      "functional is discussed in Appendix D.\n",
      "The notation g(x)=O(f(x))denotes that |f(x)/g(x)|is bounded as x→∞ .\n",
      "For instance if g(x)=3x2+2 , theng(x)=O(x2).\n",
      "The expectation of a function f(x, y)with respect to a random variable xis de-\n",
      "noted by Ex[f(x, y)]. In situations where there is no ambiguity as to which variable\n",
      "is being averaged over, this will be simpliﬁed by omitting the sufﬁx, for instance\n",
      "xixii MATHEMATICAL NOTATION\n",
      "E[x]. If the distribution of xis conditioned on another variable z, then the corre-\n",
      "sponding conditional expectation will be written Ex[f(x)|z]. Similarly, the variance\n",
      "is denoted var[f(x)], and for vector variables the covariance is written cov[x,y].W e\n",
      "shall also use cov[x]as a shorthand notation for cov[x,x]. The concepts of expecta-\n",
      "tions and covariances are introduced in Section 1.2.2.\n",
      "If we have Nvaluesx1,...,xNof aD-dimensional vector x=(x1,...,x D)T,\n",
      "we can combine the observations into a data matrix Xin which the nthrow of X\n",
      "corresponds to the row vector xT\n",
      "n. Thus the n, i element of Xcorresponds to the\n",
      "ithelement of the nthobservation xn. For the case of one-dimensional variables we\n",
      "shall denote such a matrix by x, which is a column vector whose nthelement is xn.\n",
      "Note that x(which has dimensionality N) uses a different typeface to distinguish it\n",
      "fromx(which has dimensionality D).Contents\n",
      "Preface vii\n",
      "Mathematical notation xiContents xiii\n",
      "1 Introduction 1\n",
      "1.1 Example: Polynomial Curve Fitting . . . . ............. 4\n",
      "1.2 Probability Theory . . ........................ 1 2\n",
      "1.2.1 Probability densities . .................... 1 7\n",
      "1.2.2 Expectations and covariances ................ 1 9\n",
      "1.2.3 Bayesian probabilities .................... 2 1\n",
      "1.2.4 The Gaussian distribution . . . . . ............. 2 4\n",
      "1.2.5 Curve ﬁtting re-visited .................... 2 8\n",
      "1.2.6 Bayesian curve ﬁtting .................... 3 0\n",
      "1.3 Model Selection . . . ........................ 3 2\n",
      "1.4 The Curse of Dimensionality . .................... 3 3\n",
      "1.5 Decision Theory . . . ........................ 3 8\n",
      "1.5.1 Minimizing the misclassiﬁcation rate . . . ......... 3 9\n",
      "1.5.2 Minimizing the expected loss . . .............. 4 1\n",
      "1.5.3 The reject option . . . .................... 4 2\n",
      "1.5.4 Inference and decision .................... 4 2\n",
      "1.5.5 Loss functions for regression . . . . ............. 4 6\n",
      "1.6 Information Theory . . ........................ 4 8\n",
      "1.6.1 Relative entropy and mutual information . ......... 5 5\n",
      "Exercises . . . ............................... 5 8\n",
      "xiiixiv CONTENTS\n",
      "2 Probability Distributions 67\n",
      "2.1 Binary V ariables . . . ........................ 6 8\n",
      "2.1.1 The beta distribution . .................... 7 1\n",
      "2.2 Multinomial V ariables ........................ 7 4\n",
      "2.2.1 The Dirichlet distribution . . . . . . ............. 7 6\n",
      "2.3 The Gaussian Distribution . . .................... 7 8\n",
      "2.3.1 Conditional Gaussian distributions . ............. 8 5\n",
      "2.3.2 Marginal Gaussian distributions . . ............. 8 8\n",
      "2.3.3 Bayes’ theorem for Gaussian variables . . . ......... 9 0\n",
      "2.3.4 Maximum likelihood for the Gaussian . . . ......... 9 3\n",
      "2.3.5 Sequential estimation . .................... 9 4\n",
      "2.3.6 Bayesian inference for the Gaussian ............. 9 7\n",
      "2.3.7 Student’s t-distribution .................... 1 0 2\n",
      "2.3.8 Periodic variables . . . .................... 1 0 5\n",
      "2.3.9 Mixtures of Gaussians .................... 1 1 0\n",
      "2.4 The Exponential Family . . . .................... 1 1 3\n",
      "2.4.1 Maximum likelihood and sufﬁcient statistics . . . . . . . . 116\n",
      "2.4.2 Conjugate priors . . . .................... 1 1 7\n",
      "2.4.3 Noninformative priors .................... 1 1 7\n",
      "2.5 Nonparametric Methods . . . .................... 1 2 0\n",
      "2.5.1 Kernel density estimators . . . . . . ............. 1 2 2\n",
      "2.5.2 Nearest-neighbour methods . . . . ............. 1 2 4\n",
      "Exercises . . . ............................... 1 2 7\n",
      "3 Linear Models for Regression 137\n",
      "3.1 Linear Basis Function Models .................... 1 3 8\n",
      "3.1.1 Maximum likelihood and least squares . . . ......... 1 4 0\n",
      "3.1.2 Geometry of least squares . . . . . ............. 1 4 3\n",
      "3.1.3 Sequential learning . . .................... 1 4 3\n",
      "3.1.4 Regularized least squares . . . . . . ............. 1 4 4\n",
      "3.1.5 Multiple outputs . . . .................... 1 4 6\n",
      "3.2 The Bias-V ariance Decomposition . . . . . ............. 1 4 7\n",
      "3.3 Bayesian Linear Regression . .................... 1 5 2\n",
      "3.3.1 Parameter distribution .................... 1 5 2\n",
      "3.3.2 Predictive distribution .................... 1 5 6\n",
      "3.3.3 Equivalent kernel . . . .................... 1 5 9\n",
      "3.4 Bayesian Model Comparison . .................... 1 6 1\n",
      "3.5 The Evidence Approximation .................... 1 6 5\n",
      "3.5.1 Evaluation of the evidence function ............. 1 6 6\n",
      "3.5.2 Maximizing the evidence function . ............. 1 6 8\n",
      "3.5.3 Effective number of parameters . . ............. 1 7 0\n",
      "3.6 Limitations of Fixed Basis Functions . . . ............. 1 7 2\n",
      "Exercises . . . ............................... 1 7 3CONTENTS xv\n",
      "4 Linear Models for Classiﬁcation 179\n",
      "4.1 Discriminant Functions ........................ 1 8 1\n",
      "4.1.1 Two classes . . ........................ 1 8 1\n",
      "4.1.2 Multiple classes ........................ 1 8 2\n",
      "4.1.3 Least squares for classiﬁcation ................ 1 8 4\n",
      "4.1.4 Fisher’s linear discriminant . . . . . ............. 1 8 6\n",
      "4.1.5 Relation to least squares . . . . . . ............. 1 8 9\n",
      "4.1.6 Fisher’s discriminant for multiple classes . ......... 1 9 1\n",
      "4.1.7 The perceptron algorithm . . . . . . ............. 1 9 2\n",
      "4.2 Probabilistic Generative Models . . . ................ 1 9 6\n",
      "4.2.1 Continuous inputs . . .................... 1 9 8\n",
      "4.2.2 Maximum likelihood solution . . . ............. 2 0 0\n",
      "4.2.3 Discrete features . . . .................... 2 0 2\n",
      "4.2.4 Exponential family . . .................... 2 0 2\n",
      "4.3 Probabilistic Discriminative Models . . . . ............. 2 0 3\n",
      "4.3.1 Fixed basis functions . .................... 2 0 4\n",
      "4.3.2 Logistic regression . . .................... 2 0 5\n",
      "4.3.3 Iterative reweighted least squares .............. 2 0 7\n",
      "4.3.4 Multiclass logistic regression . . . .............. 2 0 9\n",
      "4.3.5 Probit regression . . . .................... 2 1 0\n",
      "4.3.6 Canonical link functions . . . . . . ............. 2 1 2\n",
      "4.4 The Laplace Approximation . .................... 2 1 3\n",
      "4.4.1 Model comparison and BIC . . . .............. 2 1 6\n",
      "4.5 Bayesian Logistic Regression .................... 2 1 7\n",
      "4.5.1 Laplace approximation .................... 2 1 7\n",
      "4.5.2 Predictive distribution .................... 2 1 8\n",
      "Exercises . . . ............................... 2 2 0\n",
      "5 Neural Networks 225\n",
      "5.1 Feed-forward Network Functions . . . . . ............. 2 2 7\n",
      "5.1.1 Weight-space symmetries . . . . .............. 2 3 1\n",
      "5.2 Network Training . . . ........................ 2 3 2\n",
      "5.2.1 Parameter optimization .................... 2 3 6\n",
      "5.2.2 Local quadratic approximation . . . ............. 2 3 7\n",
      "5.2.3 Use of gradient information . . . .............. 2 3 9\n",
      "5.2.4 Gradient descent optimization . . . ............. 2 4 0\n",
      "5.3 Error Backpropagation ........................ 2 4 1\n",
      "5.3.1 Evaluation of error-function derivatives . . ......... 2 4 2\n",
      "5.3.2 A simple example ...................... 2 4 5\n",
      "5.3.3 Efﬁciency of backpropagation . . . ............. 2 4 6\n",
      "5.3.4 The Jacobian matrix . .................... 2 4 7\n",
      "5.4 The Hessian Matrix . . ........................ 2 4 9\n",
      "5.4.1 Diagonal approximation . . . . . .............. 2 5 0\n",
      "5.4.2 Outer product approximation . ................ 2 5 1\n",
      "5.4.3 Inverse Hessian ........................ 2 5 2xvi CONTENTS\n",
      "5.4.4 Finite difference s....................... 2 5 2\n",
      "5.4.5 Exact evaluation of the Hessian . . ............. 2 5 3\n",
      "5.4.6 Fast multiplication by the Hessian . ............. 2 5 4\n",
      "5.5 Regularization in Neural Networks . . . . ............. 2 5 6\n",
      "5.5.1 Consistent Gaussian priors . . . . .............. 2 5 7\n",
      "5.5.2 Early stopping ........................ 2 5 9\n",
      "5.5.3 Invariances . . ........................ 2 6 1\n",
      "5.5.4 Tangent propagation . .................... 2 6 3\n",
      "5.5.5 Training with transformed data . . . ............. 2 6 5\n",
      "5.5.6 Convolutional networks . . . . . . ............. 2 6 7\n",
      "5.5.7 Soft weight sharing . . .................... 2 6 9\n",
      "5.6 Mixture Density Networks . . .................... 2 7 2\n",
      "5.7 Bayesian Neural Networks . . .................... 2 7 7\n",
      "5.7.1 Posterior parameter distribution . . ............. 2 7 8\n",
      "5.7.2 Hyperparameter optimization . . . ............. 2 8 0\n",
      "5.7.3 Bayesian neural networks for classiﬁcation ......... 2 8 1\n",
      "Exercises . . . ............................... 2 8 4\n",
      "6 Kernel Methods 291\n",
      "6.1 Dual Representations . ........................ 2 9 3\n",
      "6.2 Constructing Kernels . ........................ 2 9 4\n",
      "6.3 Radial Basis Function Networks . . . ................ 2 9 9\n",
      "6.3.1 Nadaraya-Watson model . . . . . . ............. 3 0 1\n",
      "6.4 Gaussian Processes . . ........................ 3 0 3\n",
      "6.4.1 Linear regression revisited . . . . . ............. 3 0 4\n",
      "6.4.2 Gaussian processes for regression . ............. 3 0 6\n",
      "6.4.3 Learning the hyperparameters . . . ............. 3 1 1\n",
      "6.4.4 Automatic relevance determination ............. 3 1 2\n",
      "6.4.5 Gaussian processes for classiﬁcation ............. 3 1 3\n",
      "6.4.6 Laplace approximation .................... 3 1 5\n",
      "6.4.7 Connection to neural networks . . . ............. 3 1 9\n",
      "Exercises . . . ............................... 3 2 0\n",
      "7 Sparse Kernel Machines 325\n",
      "7.1 Maximum Margin Classiﬁers .................... 3 2 6\n",
      "7.1.1 Overlapping class distributions . . .............. 3 3 1\n",
      "7.1.2 Relation to logistic regression . . .............. 3 3 6\n",
      "7.1.3 Multiclass SVMs . . . .................... 3 3 8\n",
      "7.1.4 SVMs for regression . .................... 3 3 9\n",
      "7.1.5 Computational learning theory . . . ............. 3 4 4\n",
      "7.2 Relevance V ector Machines . .................... 3 4 5\n",
      "7.2.1 RVM for regression . . .................... 3 4 5\n",
      "7.2.2 Analysis of sparsity . . .................... 3 4 9\n",
      "7.2.3 RVM for classiﬁcation .................... 3 5 3\n",
      "Exercises . . . ............................... 3 5 7CONTENTS xvii\n",
      "8 Graphical Models 359\n",
      "8.1 Bayesian Networks . . ........................ 3 6 0\n",
      "8.1.1 Example: Polynomial regression . . ............. 3 6 2\n",
      "8.1.2 Generative models . . .................... 3 6 5\n",
      "8.1.3 Discrete variables . . . .................... 3 6 6\n",
      "8.1.4 Linear-Gaussian models . . . . . . ............. 3 7 0\n",
      "8.2 Conditional Independence . . .................... 3 7 2\n",
      "8.2.1 Three example graphs .................... 3 7 3\n",
      "8.2.2 D-separation . ........................ 3 7 8\n",
      "8.3 Markov Random Fields . . . .................... 3 8 3\n",
      "8.3.1 Conditional independence properties ............. 3 8 3\n",
      "8.3.2 Factorization properties . . . . . . ............. 3 8 4\n",
      "8.3.3 Illustration: Image de-noising . . . ............. 3 8 7\n",
      "8.3.4 Relation to directed graphs . . . . . ............. 3 9 0\n",
      "8.4 Inference in Graphical Models .................... 3 9 3\n",
      "8.4.1 Inference on a chain . .................... 3 9 4\n",
      "8.4.2 Trees ............................. 3 9 8\n",
      "8.4.3 Factor graphs . ........................ 3 9 9\n",
      "8.4.4 The sum-product algorithm . . . . .............. 4 0 2\n",
      "8.4.5 The max-sum algorithm . . . . . . ............. 4 1 1\n",
      "8.4.6 Exact inference in general graphs . ............. 4 1 6\n",
      "8.4.7 Loopy belief propagation . . . . . . ............. 4 1 7\n",
      "8.4.8 Learning the graph structure . . . .............. 4 1 8\n",
      "Exercises . . . ............................... 4 1 8\n",
      "9 Mixture Models and EM 423\n",
      "9.1 K-means Clustering . ........................ 4 2 4\n",
      "9.1.1 Image segmentation and compression . . . ......... 4 2 8\n",
      "9.2 Mixtures of Gaussians ........................ 4 3 0\n",
      "9.2.1 Maximum likelihood . .................... 4 3 2\n",
      "9.2.2 EM for Gaussian mixtures . . . . . ............. 4 3 5\n",
      "9.3 An Alternative View of EM . .................... 4 3 9\n",
      "9.3.1 Gaussian mixtures revisited . . . .............. 4 4 1\n",
      "9.3.2 Relation to K-means . .................... 4 4 3\n",
      "9.3.3 Mixtures of Bernoulli distributions . ............. 4 4 4\n",
      "9.3.4 EM for Bayesian linear regression . ............. 4 4 8\n",
      "9.4 The EM Algorithm in General .................... 4 5 0\n",
      "Exercises . . . ............................... 4 5 5\n",
      "10 Approximate Inference 461\n",
      "10.1 V ariational Inference . ........................ 4 6 2\n",
      "10.1.1 Factorized distributions .................... 4 6 4\n",
      "10.1.2 Properties of factorized approximations . . ......... 4 6 6\n",
      "10.1.3 Example: The univariate Gaussian . ............. 4 7 0\n",
      "10.1.4 Model comparison . . .................... 4 7 3\n",
      "10.2 Illustration: V ariational Mixture of Gaussians . . . ......... 4 7 4xviii CONTENTS\n",
      "10.2.1 V ariational distribution .................... 4 7 5\n",
      "10.2.2 V ariational lower bound . . . . . . ............. 4 8 1\n",
      "10.2.3 Predictive density . . . .................... 4 8 2\n",
      "10.2.4 Determining the number of components . . ......... 4 8 3\n",
      "10.2.5 Induced factorizations .................... 4 8 5\n",
      "10.3 V ariational Linear Regression .................... 4 8 6\n",
      "10.3.1 V ariational distribution .................... 4 8 6\n",
      "10.3.2 Predictive distribution .................... 4 8 8\n",
      "10.3.3 Lower bound . ........................ 4 8 9\n",
      "10.4 Exponential Family Distributions . . . . . ............. 4 9 0\n",
      "10.4.1 V ariational message passing . . . .............. 4 9 1\n",
      "10.5 Local V ariational Methods . . .................... 4 9 3\n",
      "10.6 V ariational Logistic Regression . . . ................ 4 9 8\n",
      "10.6.1 V ariational posterior distribution . . ............. 4 9 8\n",
      "10.6.2 Optimizing the variational parameters . . . ......... 5 0 0\n",
      "10.6.3 Inference of hyperparameters ................ 5 0 2\n",
      "10.7 Expectation Propagation . . . .................... 5 0 5\n",
      "10.7.1 Example: The clutter problem . . . ............. 5 1 1\n",
      "10.7.2 Expectation propagation on graphs . ............. 5 1 3\n",
      "Exercises . . . ............................... 5 1 7\n",
      "11 Sampling Methods 523\n",
      "11.1 Basic Sampling Algorithms . .................... 5 2 6\n",
      "11.1.1 Standard distributions .................... 5 2 6\n",
      "11.1.2 Rejection sampling . . .................... 5 2 8\n",
      "11.1.3 Adaptive rejection sampling . . . . ............. 5 3 0\n",
      "11.1.4 Importance sampling . .................... 5 3 2\n",
      "11.1.5 Sampling-importance-resampling . ............. 5 3 4\n",
      "11.1.6 Sampling and the EM algorithm . . ............. 5 3 6\n",
      "11.2 Markov Chain Monte Carlo . .................... 5 3 7\n",
      "11.2.1 Markov chains ........................ 5 3 9\n",
      "11.2.2 The Metropolis-Hastings algorithm ............. 5 4 1\n",
      "11.3 Gibbs Sampling . . . ........................ 5 4 2\n",
      "11.4 Slice Sampling . . . . ........................ 5 4 6\n",
      "11.5 The Hybrid Monte Carlo Algorithm . . . . ............. 5 4 8\n",
      "11.5.1 Dynamical systems . . .................... 5 4 8\n",
      "11.5.2 Hybrid Monte Carlo . .................... 5 5 2\n",
      "11.6 Estimating the Partition Function . . . . . ............. 5 5 4\n",
      "Exercises . . . ............................... 5 5 6\n",
      "12 Continuous Latent Variables 559\n",
      "12.1 Principal Component Analysis .................... 5 6 1\n",
      "12.1.1 Maximum variance formulation . . ............. 5 6 1\n",
      "12.1.2 Minimum-error formulation . . . . ............. 5 6 3\n",
      "12.1.3 Applications of PCA . .................... 5 6 5\n",
      "12.1.4 PCA for high-dimensional data . . ............. 5 6 9CONTENTS xix\n",
      "12.2 Probabilistic PCA . . ........................ 5 7 0\n",
      "12.2.1 Maximum likelihood PCA . . . . . ............. 5 7 4\n",
      "12.2.2 EM algorithm for PCA .................... 5 7 7\n",
      "12.2.3 Bayesian PCA ........................ 5 8 0\n",
      "12.2.4 Factor analysis ........................ 5 8 3\n",
      "12.3 Kernel PCA .............................. 5 8 6\n",
      "12.4 Nonlinear Latent V ariable Models . . . . . ............. 5 9 1\n",
      "12.4.1 Independent component analysis . . ............. 5 9 1\n",
      "12.4.2 Autoassociative neural networks . . ............. 5 9 2\n",
      "12.4.3 Modelling nonlinear manifolds . . . ............. 5 9 5\n",
      "Exercises . . . ............................... 5 9 9\n",
      "13 Sequential Data 605\n",
      "13.1 Markov Models . . . . ........................ 6 0 7\n",
      "13.2 Hidden Markov Models . . . .................... 6 1 0\n",
      "13.2.1 Maximum likelihood for the HMM ............. 6 1 5\n",
      "13.2.2 The forward-backward algorithm . ............. 6 1 8\n",
      "13.2.3 The sum-product algorithm for the HMM . ......... 6 2 5\n",
      "13.2.4 Scaling factors ........................ 6 2 7\n",
      "13.2.5 The Viterbi algorithm . .................... 6 2 9\n",
      "13.2.6 Extensions of the hidden Markov model . . ......... 6 3 1\n",
      "13.3 Linear Dynamical Systems . . .................... 6 3 5\n",
      "13.3.1 Inference in LDS . . . .................... 6 3 8\n",
      "13.3.2 Learning in LDS . . . .................... 6 4 2\n",
      "13.3.3 Extensions of LDS . . .................... 6 4 4\n",
      "13.3.4 Particle ﬁlters . ........................ 6 4 5\n",
      "Exercises . . . ............................... 6 4 6\n",
      "14 Combining Models 653\n",
      "14.1 Bayesian Model Averaging . . .................... 6 5 4\n",
      "14.2 Committees . . . . . . ........................ 6 5 5\n",
      "14.3 Boosting ............................... 6 5 7\n",
      "14.3.1 Minimizing exponential error . . . ............. 6 5 9\n",
      "14.3.2 Error functions for boosting . . . .............. 6 6 1\n",
      "14.4 Tree-based Models . . ........................ 6 6 3\n",
      "14.5 Conditional Mixture Models . .................... 6 6 6\n",
      "14.5.1 Mixtures of linear regression models ............. 6 6 7\n",
      "14.5.2 Mixtures of logistic models . . . . ............. 6 7 0\n",
      "14.5.3 Mixtures of experts . . .................... 6 7 2\n",
      "Exercises . . . ............................... 6 7 4\n",
      "Appendix A Data Sets 677Appendix B Probability Distributions 685\n",
      "Appendix C Properties of Matrices 695xx CONTENTS\n",
      "Appendix D Calculus of Variations 703\n",
      "Appendix E Lagrange Multipliers 707\n",
      "References 711Index 7291\n",
      "Introduction\n",
      "The problem of searching for patterns in data is a fundamental one and has a long and\n",
      "successful history. For instance, the extensive astronomical observations of TychoBrahe in the 16\n",
      "thcentury allowed Johannes Kepler to discover the empirical laws of\n",
      "planetary motion, which in turn provided a springboard for the development of clas-\n",
      "sical mechanics. Similarly, the discovery of regularities in atomic spectra played akey role in the development and veriﬁcation of quantum physics in the early twenti-\n",
      "eth century. The ﬁeld of pattern recognition is concerned with the automatic discov-\n",
      "ery of regularities in data through the use of computer algorithms and with the use ofthese regularities to take actions such as classifying the data into different categories.\n",
      "Consider the example of recognizing handwritten digits, illustrated in Figure 1.1.\n",
      "Each digit corresponds to a 28×28pixel image and so can be represented by a vector\n",
      "xcomprising 784 real numbers. The goal is to build a machine that will take such a\n",
      "vectorxas input and that will produce the identity of the digit 0,...,9as the output.\n",
      "This is a nontrivial problem due to the wide variability of handwriting. It could be\n",
      "12 1. INTRODUCTION\n",
      "Figure 1.1 Examples of hand-written dig-\n",
      "its taken from US zip codes.\n",
      "tackled using handcrafted rules or heuristics for distinguishing the digits based on\n",
      "the shapes of the strokes, but in practice such an approach leads to a proliferation of\n",
      "rules and of exceptions to the rules and so on, and invariably gives poor results.\n",
      "Far better results can be obtained by adopting a machine learning approach in\n",
      "which a large set of Ndigits{x1,...,xN}called a training set is used to tune the\n",
      "parameters of an adaptive model. The categories of the digits in the training setare known in advance, typically by inspecting them individually and hand-labelling\n",
      "them. We can express the category of a digit using target vector t, which represents\n",
      "the identity of the corresponding digit. Suitable techniques for representing cate-gories in terms of vectors will be discussed later. Note that there is one such target\n",
      "vectortfor each digit image x.\n",
      "The result of running the machine learning algorithm can be expressed as a\n",
      "function y(x)which takes a new digit image xas input and that generates an output\n",
      "vectory, encoded in the same way as the target vectors. The precise form of the\n",
      "function y(x)is determined during the training phase, also known as the learning\n",
      "phase, on the basis of the training data. Once the model is trained it can then de-\n",
      "termine the identity of new digit images, which are said to comprise a test set . The\n",
      "ability to categorize correctly new examples that differ from those used for train-\n",
      "ing is known as generalization . In practical applications, the variability of the input\n",
      "vectors will be such that the training data can comprise only a tiny fraction of allpossible input vectors, and so generalization is a central goal in pattern recognition.\n",
      "For most practical applications, the original input variables are typically prepro-\n",
      "cessed to transform them into some new space of variables where, it is hoped, the\n",
      "pattern recognition problem will be easier to solve. For instance, in the digit recogni-\n",
      "tion problem, the images of the digits are typically translated and scaled so that each\n",
      "digit is contained within a box of a ﬁxed size. This greatly reduces the variabilitywithin each digit class, because the location and scale of all the digits are now the\n",
      "same, which makes it much easier for a subsequent pattern recognition algorithm\n",
      "to distinguish between the different classes. This pre-processing stage is sometimesalso called feature extraction . Note that new test data must be pre-processed using\n",
      "the same steps as the training data.\n",
      "Pre-processing might also be performed in order to speed up computation. For\n",
      "example, if the goal is real-time face detection in a high-resolution video stream,\n",
      "the computer must handle huge numbers of pixels per second, and presenting thesedirectly to a complex pattern recognition algorithm may be computationally infeasi-\n",
      "ble. Instead, the aim is to ﬁnd useful features that are fast to compute, and yet that1. INTRODUCTION 3\n",
      "also preserve useful discriminatory information enabling faces to be distinguished\n",
      "from non-faces. These features are then used as the inputs to the pattern recognitionalgorithm. For instance, the average value of the image intensity over a rectangular\n",
      "subregion can be evaluated extremely efﬁciently (Viola and Jones, 2004), and a set of\n",
      "such features can prove very effective in fast face detection. Because the number ofsuch features is smaller than the number of pixels, this kind of pre-processing repre-\n",
      "sents a form of dimensionality reduction. Care must be taken during pre-processing\n",
      "because often information is discarded, and if this information is important to thesolution of the problem then the overall accuracy of the system can suffer.\n",
      "Applications in which the training data comprises examples of the input vectors\n",
      "along with their corresponding target vectors are known as supervised learning prob-\n",
      "lems. Cases such as the digit recognition example, in which the aim is to assign each\n",
      "input vector to one of a ﬁnite number of discrete categories, are called classiﬁcation\n",
      "problems. If the desired output consists of one or more continuous variables, then\n",
      "the task is called regression . An example of a regression problem would be the pre-\n",
      "diction of the yield in a chemical manufacturing process in which the inputs consistof the concentrations of reactants, the temperature, and the pressure.\n",
      "In other pattern recognition problems, the training data consists of a set of input\n",
      "vectors xwithout any corresponding target values. The goal in such unsupervised\n",
      "learning problems may be to discover groups of similar examples within the data,\n",
      "where it is called clustering , or to determine the distribution of data within the input\n",
      "space, known as density estimation , or to project the data from a high-dimensional\n",
      "space down to two or three dimensions for the purpose of visualization .\n",
      "Finally, the technique of reinforcement learning (Sutton and Barto, 1998) is con-\n",
      "cerned with the problem of ﬁnding suitable actions to take in a given situation inorder to maximize a reward. Here the learning algorithm is not given examples of\n",
      "optimal outputs, in contrast to supervised learning, but must instead discover them\n",
      "by a process of trial and error. Typically there is a sequence of states and actions in\n",
      "which the learning algorithm is interacting with its environment. In many cases, the\n",
      "current action not only affects the immediate reward but also has an impact on the re-ward at all subsequent time steps. For example, by using appropriate reinforcement\n",
      "learning techniques a neural network can learn to play the game of backgammon to a\n",
      "high standard (Tesauro, 1994). Here the network must learn to take a board positionas input, along with the result of a dice throw, and produce a strong move as the\n",
      "output. This is done by having the network play against a copy of itself for perhaps a\n",
      "million games. A major challenge is that a game of backgammon can involve dozensof moves, and yet it is only at the end of the game that the reward, in the form of\n",
      "victory, is achieved. The reward must then be attributed appropriately to all of the\n",
      "moves that led to it, even though some moves will have been good ones and othersless so. This is an example of a credit assignment problem. A general feature of re-\n",
      "inforcement learning is the trade-off between exploration , in which the system tries\n",
      "out new kinds of actions to see how effective they are, and exploitation , in which\n",
      "the system makes use of actions that are known to yield a high reward. Too strong\n",
      "a focus on either exploration or exploitation will yield poor results. Reinforcement\n",
      "learning continues to be an active area of machine learning research. However, a4 1. INTRODUCTION\n",
      "Figure 1.2 Plot of a training data set of N=\n",
      "10points, shown as blue circles,\n",
      "each comprising an observation\n",
      "of the input variable xalong with\n",
      "the corresponding target variable\n",
      "t. The green curve shows the\n",
      "function sin(2πx)used to gener-\n",
      "ate the data. Our goal is to pre-\n",
      "dict the value of tfor some new\n",
      "value of x, without knowledge of\n",
      "the green curve.\n",
      "xt\n",
      "0 1−101\n",
      "detailed treatment lies beyond the scope of this book.\n",
      "Although each of these tasks needs its own tools and techniques, many of the\n",
      "key ideas that underpin them are common to all such problems. One of the main\n",
      "goals of this chapter is to introduce, in a relatively informal way, several of the most\n",
      "important of these concepts and to illustrate them using simple examples. Later in\n",
      "the book we shall see these same ideas re-emerge in the context of more sophisti-\n",
      "cated models that are applicable to real-world pattern recognition applications. This\n",
      "chapter also provides a self-contained introduction to three important tools that will\n",
      "be used throughout the book, namely probability theory, decision theory, and infor-\n",
      "mation theory. Although these might sound like daunting topics, they are in fact\n",
      "straightforward, and a clear understanding of them is essential if machine learning\n",
      "techniques are to be used to best effect in practical applications.\n",
      "1.1. Example: Polynomial Curve Fitting\n",
      "We begin by introducing a simple regression problem, which we shall use as a run-\n",
      "ning example throughout this chapter to motivate a number of key concepts. Sup-\n",
      "pose we observe a real-valued input variable xand we wish to use this observation to\n",
      "predict the value of a real-valued target variable t. For the present purposes, it is in-\n",
      "structive to consider an artiﬁcial example using synthetically generated data because\n",
      "we then know the precise process that generated the data for comparison against any\n",
      "learned model. The data for this example is generated from the function sin(2πx)\n",
      "with random noise included in the target values, as described in detail in Appendix A.\n",
      "Now suppose that we are given a training set comprising Nobservations of x,\n",
      "written x≡(x1,...,x N)T, together with corresponding observations of the values\n",
      "oft, denoted t≡(t1,...,t N)T. Figure 1.2 shows a plot of a training set comprising\n",
      "N=1 0 data points. The input data set xin Figure 1.2 was generated by choos-\n",
      "ing values of xn, forn=1,...,N , spaced uniformly in range [0,1], and the target\n",
      "data set twas obtained by ﬁrst computing the corresponding values of the function1.1. Example: Polynomial Curve Fitting 5\n",
      "sin(2πx)and then adding a small level of random noise having a Gaussian distri-\n",
      "bution (the Gaussian distribution is discussed in Section 1.2.4) to each such point inorder to obtain the corresponding value t\n",
      "n. By generating data in this way, we are\n",
      "capturing a property of many real data sets, namely that they possess an underlying\n",
      "regularity, which we wish to learn, but that individual observations are corrupted byrandom noise. This noise might arise from intrinsically stochastic (i.e. random) pro-\n",
      "cesses such as radioactive decay but more typically is due to there being sources of\n",
      "variability that are themselves unobserved.\n",
      "Our goal is to exploit this training set in order to make predictions of the value\n",
      "ˆtof the target variable for some new value ˆxof the input variable. As we shall see\n",
      "later, this involves implicitly trying to discover the underlying function sin(2πx).\n",
      "This is intrinsically a difﬁcult problem as we have to generalize from a ﬁnite data\n",
      "set. Furthermore the observed data are corrupted with noise, and so for a given ˆx\n",
      "there is uncertainty as to the appropriate value for ˆt. Probability theory, discussed\n",
      "in Section 1.2, provides a framework for expressing such uncertainty in a precise\n",
      "and quantitative manner, and decision theory, discussed in Section 1.5, allows us toexploit this probabilistic representation in order to make predictions that are optimal\n",
      "according to appropriate criteria.\n",
      "For the moment, however, we shall proceed rather informally and consider a\n",
      "simple approach based on curve ﬁtting. In particular, we shall ﬁt the data using a\n",
      "polynomial function of the form\n",
      "y(x,w)=w\n",
      "0+w1x+w2x2+...+wMxM=M∑\n",
      "j=0wjxj(1.1)\n",
      "where Mis the order of the polynomial, and xjdenotes xraised to the power of j.\n",
      "The polynomial coefﬁcients w0,...,w Mare collectively denoted by the vector w.\n",
      "Note that, although the polynomial function y(x,w)is a nonlinear function of x,i t\n",
      "is a linear function of the coefﬁcients w. Functions, such as the polynomial, which\n",
      "are linear in the unknown parameters have important properties and are called linear\n",
      "models and will be discussed extensively in Chapters 3 and 4.\n",
      "The values of the coefﬁcients will be determined by ﬁtting the polynomial to the\n",
      "training data. This can be done by minimizing an error function that measures the\n",
      "misﬁt between the function y(x,w), for any given value of w, and the training set\n",
      "data points. One simple choice of error function, which is widely used, is given by\n",
      "the sum of the squares of the errors between the predictions y(xn,w)for each data\n",
      "pointxnand the corresponding target values tn, so that we minimize\n",
      "E(w)=1\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2(1.2)\n",
      "where the factor of 1/2is included for later convenience. We shall discuss the mo-\n",
      "tivation for this choice of error function later in this chapter. For the moment wesimply note that it is a nonnegative quantity that would be zero if, and only if, the6 1. INTRODUCTION\n",
      "Figure 1.3 The error function (1.2) corre-\n",
      "sponds to (one half of) the sum of\n",
      "the squares of the displacements\n",
      "(shown by the vertical green bars)\n",
      "of each data point from the function\n",
      "y(x,w).t\n",
      "xy(xn,w)tn\n",
      "xn\n",
      "function y(x,w)were to pass exactly through each training data point. The geomet-\n",
      "rical interpretation of the sum-of-squares error function is illustrated in Figure 1.3.\n",
      "We can solve the curve ﬁtting problem by choosing the value of wfor which\n",
      "E(w)is as small as possible. Because the error function is a quadratic function of\n",
      "the coefﬁcients w, its derivatives with respect to the coefﬁcients will be linear in the\n",
      "elements of w, and so the minimization of the error function has a unique solution,\n",
      "denoted by w⋆, which can be found in closed form. The resulting polynomial is Exercise 1.1\n",
      "given by the function y(x,w⋆).\n",
      "There remains the problem of choosing the order Mof the polynomial, and as\n",
      "we shall see this will turn out to be an example of an important concept called model\n",
      "comparison ormodel selection . In Figure 1.4, we show four examples of the results\n",
      "of ﬁtting polynomials having orders M=0,1,3,a n d9to the data set shown in\n",
      "Figure 1.2.\n",
      "We notice that the constant ( M=0 ) and ﬁrst order ( M=1 ) polynomials\n",
      "give rather poor ﬁts to the data and consequently rather poor representations of the\n",
      "function sin(2πx). The third order ( M=3 ) polynomial seems to give the best ﬁt\n",
      "to the function sin(2πx)of the examples shown in Figure 1.4. When we go to a\n",
      "much higher order polynomial ( M=9 ), we obtain an excellent ﬁt to the training\n",
      "data. In fact, the polynomial passes exactly through each data point and E(w⋆)=0 .\n",
      "However, the ﬁtted curve oscillates wildly and gives a very poor representation of\n",
      "the function sin(2πx). This latter behaviour is known as over-ﬁtting .\n",
      "As we have noted earlier, the goal is to achieve good generalization by making\n",
      "accurate predictions for new data. We can obtain some quantitative insight into the\n",
      "dependence of the generalization performance on Mby considering a separate test\n",
      "set comprising 100 data points generated using exactly the same procedure used\n",
      "to generate the training set points but with new choices for the random noise values\n",
      "included in the target values. For each choice of M, we can then evaluate the residual\n",
      "value of E(w⋆)given by (1.2) for the training data, and we can also evaluate E(w⋆)\n",
      "for the test data set. It is sometimes more convenient to use the root-mean-square1.1. Example: Polynomial Curve Fitting 7\n",
      "xtM=0\n",
      "0 1−101\n",
      "xtM=1\n",
      "0 1−101\n",
      "xtM=3\n",
      "0 1−101\n",
      "xtM=9\n",
      "0 1−101\n",
      "Figure 1.4 Plots of polynomials having various orders M, shown as red curves, ﬁtted to the data set shown in\n",
      "Figure 1.2.\n",
      "(RMS) error deﬁned by\n",
      "ERMS=√\n",
      "2E(w⋆)/N (1.3)\n",
      "in which the division by Nallows us to compare different sizes of data sets on\n",
      "an equal footing, and the square root ensures that ERMS is measured on the same\n",
      "scale (and in the same units) as the target variable t. Graphs of the training and\n",
      "test set RMS errors are shown, for various values of M, in Figure 1.5. The test\n",
      "set error is a measure of how well we are doing in predicting the values of tfor\n",
      "new data observations of x. We note from Figure 1.5 that small values of Mgive\n",
      "relatively large values of the test set error, and this can be attributed to the fact that\n",
      "the corresponding polynomials are rather inﬂexible and are incapable of capturing\n",
      "the oscillations in the function sin(2πx). V alues of Min the range 3⩽M⩽8\n",
      "give small values for the test set error, and these also give reasonable representations\n",
      "of the generating function sin(2πx), as can be seen, for the case of M=3 , from\n",
      "Figure 1.4.8 1. INTRODUCTION\n",
      "Figure 1.5 Graphs of the root-mean-square\n",
      "error, deﬁned by (1.3), evaluated\n",
      "on the training set and on an inde-\n",
      "pendent test set for various values\n",
      "ofM.\n",
      "MERMS\n",
      "0 3 6 900.51\n",
      "Training\n",
      "Test\n",
      "ForM=9 , the training set error goes to zero, as we might expect because\n",
      "this polynomial contains 10degrees of freedom corresponding to the 10coefﬁcients\n",
      "w0,...,w 9, and so can be tuned exactly to the 10data points in the training set.\n",
      "However, the test set error has become very large and, as we saw in Figure 1.4, the\n",
      "corresponding function y(x,w⋆)exhibits wild oscillations.\n",
      "This may seem paradoxical because a polynomial of given order contains all\n",
      "lower order polynomials as special cases. The M=9 polynomial is therefore capa-\n",
      "ble of generating results at least as good as the M=3 polynomial. Furthermore, we\n",
      "might suppose that the best predictor of new data would be the function sin(2πx)\n",
      "from which the data was generated (and we shall see later that this is indeed the\n",
      "case). We know that a power series expansion of the function sin(2πx)contains\n",
      "terms of all orders, so we might expect that results should improve monotonically as\n",
      "we increase M.\n",
      "We can gain some insight into the problem by examining the values of the co-\n",
      "efﬁcients w⋆obtained from polynomials of various order, as shown in Table 1.1.\n",
      "We see that, as Mincreases, the magnitude of the coefﬁcients typically gets larger.\n",
      "In particular for the M=9 polynomial, the coefﬁcients have become ﬁnely tuned\n",
      "to the data by developing large positive and negative values so that the correspond-\n",
      "Table 1.1 Table of the coefﬁcients w⋆for\n",
      "polynomials of various order.\n",
      "Observe how the typical mag-\n",
      "nitude of the coefﬁcients in-\n",
      "creases dramatically as the or-\n",
      "der of the polynomial increases.M=0 M=1 M=6 M=9\n",
      "w⋆\n",
      "0 0.19 0.82 0.31 0.35\n",
      "w⋆\n",
      "1 -1.27 7.99 232.37\n",
      "w⋆\n",
      "2 -25.43 -5321.83\n",
      "w⋆\n",
      "3 17.37 48568.31\n",
      "w⋆\n",
      "4 -231639.30\n",
      "w⋆\n",
      "5 640042.26\n",
      "w⋆\n",
      "6 -1061800.52\n",
      "w⋆\n",
      "7 1042400.18\n",
      "w⋆\n",
      "8 -557682.99\n",
      "w⋆\n",
      "9 125201.431.1. Example: Polynomial Curve Fitting 9\n",
      "xtN=1 5\n",
      "0 1−101\n",
      "xtN= 100\n",
      "0 1−101\n",
      "Figure 1.6 Plots of the solutions obtained by minimizing the sum-of-squares error function using the M=9\n",
      "polynomial for N=1 5 data points (left plot) and N= 100 data points (right plot). We see that increasing the\n",
      "size of the data set reduces the over-ﬁtting problem.\n",
      "ing polynomial function matches each of the data points exactly, but between data\n",
      "points (particularly near the ends of the range) the function exhibits the large oscilla-\n",
      "tions observed in Figure 1.4. Intuitively, what is happening is that the more ﬂexible\n",
      "polynomials with larger values of Mare becoming increasingly tuned to the random\n",
      "noise on the target values.\n",
      "It is also interesting to examine the behaviour of a given model as the size of the\n",
      "data set is varied, as shown in Figure 1.6. We see that, for a given model complexity,\n",
      "the over-ﬁtting problem become less severe as the size of the data set increases.\n",
      "Another way to say this is that the larger the data set, the more complex (in other\n",
      "words more ﬂexible) the model that we can afford to ﬁt to the data. One rough\n",
      "heuristic that is sometimes advocated is that the number of data points should be\n",
      "no less than some multiple (say 5 or 10) of the number of adaptive parameters in\n",
      "the model. However, as we shall see in Chapter 3, the number of parameters is not\n",
      "necessarily the most appropriate measure of model complexity.\n",
      "Also, there is something rather unsatisfying about having to limit the number of\n",
      "parameters in a model according to the size of the available training set. It would\n",
      "seem more reasonable to choose the complexity of the model according to the com-\n",
      "plexity of the problem being solved. We shall see that the least squares approach\n",
      "to ﬁnding the model parameters represents a speciﬁc case of maximum likelihood\n",
      "(discussed in Section 1.2.5), and that the over-ﬁtting problem can be understood as\n",
      "a general property of maximum likelihood. By adopting a Bayesian approach, the Section 3.4\n",
      "over-ﬁtting problem can be avoided. We shall see that there is no difﬁculty from\n",
      "a Bayesian perspective in employing models for which the number of parameters\n",
      "greatly exceeds the number of data points. Indeed, in a Bayesian model the effective\n",
      "number of parameters adapts automatically to the size of the data set.\n",
      "For the moment, however, it is instructive to continue with the current approach\n",
      "and to consider how in practice we can apply it to data sets of limited size where we10 1. INTRODUCTION\n",
      "xtlnλ=−18\n",
      "0 1−101\n",
      "xtlnλ=0\n",
      "0 1−101\n",
      "Figure 1.7 Plots of M=9polynomials ﬁtted to the data set shown in Figure 1.2 using the regularized error\n",
      "function (1.4) for two values of the regularization parameter λcorresponding to lnλ=−18andlnλ=0. The\n",
      "case of no regularizer, i.e., λ=0, corresponding to lnλ=−∞, is shown at the bottom right of Figure 1.4.\n",
      "may wish to use relatively complex and ﬂexible models. One technique that is often\n",
      "used to control the over-ﬁtting phenomenon in such cases is that of regularization ,\n",
      "which involves adding a penalty term to the error function (1.2) in order to discourage\n",
      "the coefﬁcients from reaching large values. The simplest such penalty term takes the\n",
      "form of a sum of squares of all of the coefﬁcients, leading to a modiﬁed error function\n",
      "of the form\n",
      "˜E(w)=1\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2+λ\n",
      "2∥w∥2(1.4)\n",
      "where∥w∥2≡wTw=w2\n",
      "0+w2\n",
      "1+...+w2\n",
      "M, and the coefﬁcient λgoverns the rel-\n",
      "ative importance of the regularization term compared with the sum-of-squares error\n",
      "term. Note that often the coefﬁcient w0is omitted from the regularizer because its\n",
      "inclusion causes the results to depend on the choice of origin for the target variable\n",
      "(Hastie et al. , 2001), or it may be included but with its own regularization coefﬁcient\n",
      "(we shall discuss this topic in more detail in Section 5.5.1). Again, the error function\n",
      "in (1.4) can be minimized exactly in closed form. Techniques such as this are known Exercise 1.2\n",
      "in the statistics literature as shrinkage methods because they reduce the value of the\n",
      "coefﬁcients. The particular case of a quadratic regularizer is called ridge regres-\n",
      "sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is\n",
      "known as weight decay .\n",
      "Figure 1.7 shows the results of ﬁtting the polynomial of order M=9 to the\n",
      "same data set as before but now using the regularized error function given by (1.4).\n",
      "We see that, for a value of lnλ=−18, the over-ﬁtting has been suppressed and we\n",
      "now obtain a much closer representation of the underlying function sin(2πx). If,\n",
      "however, we use too large a value for λthen we again obtain a poor ﬁt, as shown in\n",
      "Figure 1.7 for lnλ=0 . The corresponding coefﬁcients from the ﬁtted polynomials\n",
      "are given in Table 1.2, showing that regularization has the desired effect of reducing1.1. Example: Polynomial Curve Fitting 11\n",
      "Table 1.2 Table of the coefﬁcients w⋆forM=\n",
      "9polynomials with various values for\n",
      "the regularization parameter λ. Note\n",
      "thatlnλ=−∞ corresponds to a\n",
      "model with no regularization, i.e., to\n",
      "the graph at the bottom right in Fig-\n",
      "ure 1.4. We see that, as the value of\n",
      "λincreases, the typical magnitude of\n",
      "the coefﬁcients gets smaller.lnλ=−∞ lnλ=−18 ln λ=0\n",
      "w⋆\n",
      "0 0.35 0.35 0.13\n",
      "w⋆\n",
      "1 232.37 4.74 -0.05\n",
      "w⋆\n",
      "2 -5321.83 -0.77 -0.06\n",
      "w⋆\n",
      "3 48568.31 -31.97 -0.05\n",
      "w⋆\n",
      "4 -231639.30 -3.89 -0.03\n",
      "w⋆\n",
      "5 640042.26 55.28 -0.02\n",
      "w⋆\n",
      "6 -1061800.52 41.32 -0.01\n",
      "w⋆\n",
      "7 1042400.18 -45.95 -0.00\n",
      "w⋆\n",
      "8 -557682.99 -91.53 0.00\n",
      "w⋆\n",
      "9 125201.43 72.68 0.01\n",
      "the magnitude of the coefﬁcients.\n",
      "The impact of the regularization term on the generalization error can be seen by\n",
      "plotting the value of the RMS error (1.3) for both training and test sets against lnλ,\n",
      "as shown in Figure 1.8. We see that in effect λnow controls the effective complexity\n",
      "of the model and hence determines the degree of over-ﬁtting.\n",
      "The issue of model complexity is an important one and will be discussed at\n",
      "length in Section 1.3. Here we simply note that, if we were trying to solve a practical\n",
      "application using this approach of minimizing an error function, we would have to\n",
      "ﬁnd a way to determine a suitable value for the model complexity. The results above\n",
      "suggest a simple way of achieving this, namely by taking the available data and\n",
      "partitioning it into a training set, used to determine the coefﬁcients w, and a separate\n",
      "validation set, also called a hold-out set, used to optimize the model complexity\n",
      "(either Morλ). In many cases, however, this will prove to be too wasteful of\n",
      "valuable training data, and we have to seek more sophisticated approaches. Section 1.3\n",
      "So far our discussion of polynomial curve ﬁtting has appealed largely to in-\n",
      "tuition. We now seek a more principled approach to solving problems in pattern\n",
      "recognition by turning to a discussion of probability theory. As well as providing the\n",
      "foundation for nearly all of the subsequent developments in this book, it will also\n",
      "Figure 1.8 Graph of the root-mean-square er-\n",
      "ror (1.3) versus lnλfor the M=9\n",
      "polynomial.\n",
      "ERMS\n",
      "lnλ−35 −30 −25 −2000.51\n",
      "Training\n",
      "Test12 1. INTRODUCTION\n",
      "give us some important insights into the concepts we have introduced in the con-\n",
      "text of polynomial curve ﬁtting and will allow us to extend these to more complexsituations.\n",
      "1.2. Probability Theory\n",
      "A key concept in the ﬁeld of pattern recognition is that of uncertainty. It arises boththrough noise on measurements, as well as through the ﬁnite size of data sets. Prob-ability theory provides a consistent framework for the quantiﬁcation and manipula-\n",
      "tion of uncertainty and forms one of the central foundations for pattern recognition.\n",
      "When combined with decision theory, discussed in Section 1.5, it allows us to makeoptimal predictions given all the information available to us, even though that infor-\n",
      "mation may be incomplete or ambiguous.\n",
      "We will introduce the basic concepts of probability theory by considering a sim-\n",
      "ple example. Imagine we have two boxes, one red and one blue, and in the red box\n",
      "we have 2 apples and 6 oranges, and in the blue box we have 3 apples and 1 orange.\n",
      "This is illustrated in Figure 1.9. Now suppose we randomly pick one of the boxesand from that box we randomly select an item of fruit, and having observed which\n",
      "sort of fruit it is we replace it in the box from which it came. We could imagine\n",
      "repeating this process many times. Let us suppose that in so doing we pick the red\n",
      "box 40% of the time and we pick the blue box 60% of the time, and that when we\n",
      "remove an item of fruit from a box we are equally likely to select any of the piecesof fruit in the box.\n",
      "In this example, the identity of the box that will be chosen is a random variable,\n",
      "which we shall denote by B. This random variable can take one of two possible\n",
      "values, namely r(corresponding to the red box) or b(corresponding to the blue\n",
      "box). Similarly, the identity of the fruit is also a random variable and will be denoted\n",
      "byF. It can take either of the values a(for apple) or o(for orange).\n",
      "To begin with, we shall deﬁne the probability of an event to be the fraction\n",
      "of times that event occurs out of the total number of trials, in the limit that the total\n",
      "number of trials goes to inﬁnity. Thus the probability of selecting the red box is 4/10\n",
      "Figure 1.9 We use a simple example of two\n",
      "coloured boxes each containing fruit(apples shown in green and or-anges shown in orange) to intro-duce the basic ideas of probability.1.2. Probability Theory 13\n",
      "Figure 1.10 We can derive the sum and product rules of probability by\n",
      "considering two random variables, X, which takes the values {xi}where\n",
      "i=1,...,M , and Y, which takes the values {yj}where j=1,...,L .\n",
      "In this illustration we have M=5 andL=3. If we consider a total\n",
      "number Nof instances of these variables, then we denote the number\n",
      "of instances where X=xiandY=yjbynij, which is the number of\n",
      "points in the corresponding cell of the array. The number of points in\n",
      "column i, corresponding to X=xi, is denoted by ci, and the number of\n",
      "points in row j, corresponding to Y=yj, is denoted by rj.}}ci\n",
      "rj yj\n",
      "xinij\n",
      "and the probability of selecting the blue box is 6/10. We write these probabilities\n",
      "asp(B=r)=4/10andp(B=b)=6/10. Note that, by deﬁnition, probabilities\n",
      "must lie in the interval [0,1]. Also, if the events are mutually exclusive and if they\n",
      "include all possible outcomes (for instance, in this example the box must be either\n",
      "red or blue), then we see that the probabilities for those events must sum to one.\n",
      "We can now ask questions such as: “what is the overall probability that the se-\n",
      "lection procedure will pick an apple?”, or “given that we have chosen an orange,\n",
      "what is the probability that the box we chose was the blue one?”. We can answer\n",
      "questions such as these, and indeed much more complex questions associated with\n",
      "problems in pattern recognition, once we have equipped ourselves with the two el-\n",
      "ementary rules of probability, known as the sum rule and the product rule . Having\n",
      "obtained these rules, we shall then return to our boxes of fruit example.\n",
      "In order to derive the rules of probability, consider the slightly more general ex-\n",
      "ample shown in Figure 1.10 involving two random variables XandY(which could\n",
      "for instance be the Box and Fruit variables considered above). We shall suppose that\n",
      "Xcan take any of the values xiwhere i=1,...,M , andYcan take the values yj\n",
      "where j=1,...,L . Consider a total of Ntrials in which we sample both of the\n",
      "variables XandY, and let the number of such trials in which X=xiandY=yj\n",
      "benij. Also, let the number of trials in which Xtakes the value xi(irrespective\n",
      "of the value that Ytakes) be denoted by ci, and similarly let the number of trials in\n",
      "which Ytakes the value yjbe denoted by rj.\n",
      "The probability that Xwill take the value xiandYwill take the value yjis\n",
      "written p(X=xi,Y=yj)and is called the joint probability of X=xiand\n",
      "Y=yj. It is given by the number of points falling in the cell i,jas a fraction of the\n",
      "total number of points, and hence\n",
      "p(X=xi,Y=yj)=nij\n",
      "N. (1.5)\n",
      "Here we are implicitly considering the limit N→∞ . Similarly, the probability that\n",
      "Xtakes the value xiirrespective of the value of Yis written as p(X=xi)and is\n",
      "given by the fraction of the total number of points that fall in column i, so that\n",
      "p(X=xi)=ci\n",
      "N. (1.6)\n",
      "Because the number of instances in column iin Figure 1.10 is just the sum of the\n",
      "number of instances in each cell of that column, we have ci=∑\n",
      "jnijand therefore,14 1. INTRODUCTION\n",
      "from (1.5) and (1.6), we have\n",
      "p(X=xi)=L∑\n",
      "j=1p(X=xi,Y=yj) (1.7)\n",
      "which is the sum rule of probability. Note that p(X=xi)is sometimes called the\n",
      "marginal probability, because it is obtained by marginalizing, or summing out, the\n",
      "other variables (in this case Y).\n",
      "If we consider only those instances for which X=xi, then the fraction of\n",
      "such instances for which Y=yjis written p(Y=yj|X=xi)and is called the\n",
      "conditional probability of Y=yjgiven X=xi. It is obtained by ﬁnding the\n",
      "fraction of those points in column ithat fall in cell i,jand hence is given by\n",
      "p(Y=yj|X=xi)=nij\n",
      "ci. (1.8)\n",
      "From (1.5), (1.6), and (1.8), we can then derive the following relationship\n",
      "p(X=xi,Y=yj)=nij\n",
      "N=nij\n",
      "ci·ci\n",
      "N\n",
      "=p(Y=yj|X=xi)p(X=xi) (1.9)\n",
      "which is the product rule of probability.\n",
      "So far we have been quite careful to make a distinction between a random vari-\n",
      "able, such as the box Bin the fruit example, and the values that the random variable\n",
      "can take, for example rif the box were the red one. Thus the probability that Btakes\n",
      "the value ris denoted p(B=r). Although this helps to avoid ambiguity, it leads\n",
      "to a rather cumbersome notation, and in many cases there will be no need for such\n",
      "pedantry. Instead, we may simply write p(B)to denote a distribution over the ran-\n",
      "dom variable B,o rp(r)to denote the distribution evaluated for the particular value\n",
      "r, provided that the interpretation is clear from the context.\n",
      "With this more compact notation, we can write the two fundamental rules of\n",
      "probability theory in the following form.\n",
      "The Rules of Probability\n",
      "sum rule p(X)=∑\n",
      "Yp(X,Y) (1.10)\n",
      "product rule p(X,Y)=p(Y|X)p(X). (1.11)\n",
      "Herep(X,Y)is a joint probability and is verbalized as “the probability of Xand\n",
      "Y”. Similarly, the quantity p(Y|X)is a conditional probability and is verbalized as\n",
      "“the probability of YgivenX”, whereas the quantity p(X)is a marginal probability1.2. Probability Theory 15\n",
      "and is simply “the probability of X”. These two simple rules form the basis for all\n",
      "of the probabilistic machinery that we use throughout this book.\n",
      "From the product rule, together with the symmetry property p(X,Y)=p(Y,X),\n",
      "we immediately obtain the following relationship between conditional probabilities\n",
      "p(Y|X)=p(X|Y)p(Y)\n",
      "p(X)(1.12)\n",
      "which is called Bayes’ theorem and which plays a central role in pattern recognition\n",
      "and machine learning. Using the sum rule, the denominator in Bayes’ theorem can\n",
      "be expressed in terms of the quantities appearing in the numerator\n",
      "p(X)=∑\n",
      "Yp(X|Y)p(Y). (1.13)\n",
      "We can view the denominator in Bayes’ theorem as being the normalization constant\n",
      "required to ensure that the sum of the conditional probability on the left-hand side of\n",
      "(1.12) over all values of Yequals one.\n",
      "In Figure 1.11, we show a simple example involving a joint distribution over two\n",
      "variables to illustrate the concept of marginal and conditional distributions. Here\n",
      "a ﬁnite sample of N=6 0 data points has been drawn from the joint distribution\n",
      "and is shown in the top left. In the top right is a histogram of the fractions of data\n",
      "points having each of the two values of Y. From the deﬁnition of probability, these\n",
      "fractions would equal the corresponding probabilities p(Y)in the limit N→∞ .W e\n",
      "can view the histogram as a simple way to model a probability distribution given only\n",
      "a ﬁnite number of points drawn from that distribution. Modelling distributions from\n",
      "data lies at the heart of statistical pattern recognition and will be explored in greatdetail in this book. The remaining two plots in Figure 1.11 show the corresponding\n",
      "histogram estimates of p(X)andp(X|Y=1 ) .\n",
      "Let us now return to our example involving boxes of fruit. For the moment, we\n",
      "shall once again be explicit about distinguishing between the random variables and\n",
      "their instantiations. We have seen that the probabilities of selecting either the red or\n",
      "the blue boxes are given by\n",
      "p(B=r)=4 /10 (1.14)\n",
      "p(B=b)=6 /10 (1.15)\n",
      "respectively. Note that these satisfy p(B=r)+p(B=b)=1 .\n",
      "Now suppose that we pick a box at random, and it turns out to be the blue box.\n",
      "Then the probability of selecting an apple is just the fraction of apples in the blue\n",
      "box which is 3/4, and so p(F=a|\n",
      "B=b)=3/4. In fact, we can write out all four\n",
      "conditional probabilities for the type of fruit, given the selected box\n",
      "p(F=a|B=r)=1 /4 (1.16)\n",
      "p(F=o|B=r)=3 /4 (1.17)\n",
      "p(F=a|B=b)=3 /4 (1.18)\n",
      "p(F=o|B=b)=1 /4. (1.19)16 1. INTRODUCTION\n",
      "p(X,Y)\n",
      "XY=2\n",
      "Y=1p(Y)\n",
      "p(X)\n",
      "X Xp(X|Y=1 )\n",
      "Figure 1.11 An illustration of a distribution over two variables, X, which takes 9possible values, and Y, which\n",
      "takes two possible values. The top left ﬁgure shows a sample of 60points drawn from a joint probability distri-\n",
      "bution over these variables. The remaining ﬁgures show histogram estimates of the marginal distributions p(X)\n",
      "andp(Y), as well as the conditional distribution p(X|Y=1 ) corresponding to the bottom row in the top left\n",
      "ﬁgure.\n",
      "Again, note that these probabilities are normalized so that\n",
      "p(F=a|B=r)+p(F=o|B=r)=1 (1.20)\n",
      "and similarly\n",
      "p(F=a|B=b)+p(F=o|B=b)=1. (1.21)\n",
      "We can now use the sum and product rules of probability to evaluate the overall\n",
      "probability of choosing an apple\n",
      "p(F=a)= p(F=a|B=r)p(B=r)+p(F=a|B=b)p(B=b)\n",
      "=1\n",
      "4×4\n",
      "10+3\n",
      "4×6\n",
      "10=11\n",
      "20(1.22)\n",
      "from which it follows, using the sum rule, that p(F=o)=1−11/20 = 9 /20.1.2. Probability Theory 17\n",
      "Suppose instead we are told that a piece of fruit has been selected and it is an\n",
      "orange, and we would like to know which box it came from. This requires thatwe evaluate the probability distribution over boxes conditioned on the identity of\n",
      "the fruit, whereas the probabilities in (1.16)–(1.19) give the probability distribution\n",
      "over the fruit conditioned on the identity of the box. We can solve the problem ofreversing the conditional probability by using Bayes’ theorem to give\n",
      "p(B=r|F=o)=p(F=o|B=r)p(B=r)\n",
      "p(F=o)=3\n",
      "4×4\n",
      "10×20\n",
      "9=2\n",
      "3. (1.23)\n",
      "From the sum rule, it then follows that p(B=b|F=o)=1−2/3=1/3.\n",
      "We can provide an important interpretation of Bayes’ theorem as follows. If\n",
      "we had been asked which box had been chosen before being told the identity ofthe selected item of fruit, then the most complete information we have available is\n",
      "provided by the probability p(B). We call this the prior probability because it is the\n",
      "probability available before we observe the identity of the fruit. Once we are told that\n",
      "the fruit is an orange, we can then use Bayes’ theorem to compute the probability\n",
      "p(B|F), which we shall call the posterior probability because it is the probability\n",
      "obtained after we have observed F. Note that in this example, the prior probability\n",
      "of selecting the red box was 4/10, so that we were more likely to select the blue box\n",
      "than the red one. However, once we have observed that the piece of selected fruit is\n",
      "an orange, we ﬁnd that the posterior probability of the red box is now 2/3, so that\n",
      "it is now more likely that the box we selected was in fact the red one. This result\n",
      "accords with our intuition, as the proportion of oranges is much higher in the red box\n",
      "than it is in the blue box, and so the observation that the fruit was an orange providessigniﬁcant evidence favouring the red box. In fact, the evidence is sufﬁciently strong\n",
      "that it outweighs the prior and makes it more likely that the red box was chosen\n",
      "rather than the blue one.\n",
      "Finally, we note that if the joint distribution of two variables factorizes into the\n",
      "product of the marginals, so that p(X,Y)=p(X)p(Y), then XandYare said to\n",
      "beindependent . From the product rule, we see that p(Y|X)=p(Y), and so the\n",
      "conditional distribution of Ygiven\n",
      "Xis indeed independent of the value of X.F o r\n",
      "instance, in our boxes of fruit example, if each box contained the same fraction ofapples and oranges, then p(F|B)=P(F), so that the probability of selecting, say,\n",
      "an apple is independent of which box is chosen.\n",
      "1.2.1 Probability densities\n",
      "As well as considering probabilities deﬁned over discrete sets of events, we\n",
      "also wish to consider probabilities with respect to continuous variables. We shall\n",
      "limit ourselves to a relatively informal discussion. If the probability of a real-valued\n",
      "variable xfalling in the interval (x, x+δx)is given by p(x)δxforδx→0, then\n",
      "p(x)is called the probability density overx. This is illustrated in Figure 1.12. The\n",
      "probability that xwill lie in an interval (a, b)is then given by\n",
      "p(x∈(a, b)) =∫b\n",
      "ap(x)dx. (1.24)18 1. INTRODUCTION\n",
      "Figure 1.12 The concept of probability for\n",
      "discrete variables can be ex-\n",
      "tended to that of a probability\n",
      "density p(x)over a continuous\n",
      "variable xand is such that the\n",
      "probability of xlying in the inter-\n",
      "val(x, x+δx)is given by p(x)δx\n",
      "forδx→0. The probability\n",
      "density can be expressed as the\n",
      "derivative of a cumulative distri-\n",
      "bution function P(x).\n",
      "x δxp(x)P(x)\n",
      "Because probabilities are nonnegative, and because the value of xmust lie some-\n",
      "where on the real axis, the probability density p(x)must satisfy the two conditions\n",
      "p(x)⩾0 (1.25)∫∞\n",
      "−∞p(x)dx=1. (1.26)\n",
      "Under a nonlinear change of variable, a probability density transforms differently\n",
      "from a simple function, due to the Jacobian factor. For instance, if we consider\n",
      "a change of variables x=g(y), then a function f(x)becomes˜f(y)=f(g(y)).\n",
      "Now consider a probability density px(x)that corresponds to a density py(y)with\n",
      "respect to the new variable y, where the sufﬁces denote the fact that px(x)andpy(y)\n",
      "are different densities. Observations falling in the range (x, x+δx)will, for small\n",
      "values of δx, be transformed into the range (y,y+δy)where px(x)δx≃py(y)δy,\n",
      "and hence\n",
      "py(y)= px(x)⏐⏐⏐⏐dx\n",
      "dy⏐⏐⏐⏐\n",
      "=px(g(y))|g′(y)|. (1.27)\n",
      "One consequence of this property is that the concept of the maximum of a probability\n",
      "density is dependent on the choice of variable. Exercise 1.4\n",
      "The probability that xlies in the interval (−∞,z)is given by the cumulative\n",
      "distribution function deﬁned by\n",
      "P(z)=∫z\n",
      "−∞p(x)dx (1.28)\n",
      "which satisﬁes P′(x)=p(x), as shown in Figure 1.12.\n",
      "If we have several continuous variables x1,...,x D, denoted collectively by the\n",
      "vectorx, then we can deﬁne a joint probability density p(x)=p(x1,...,x D)such1.2. Probability Theory 19\n",
      "that the probability of xfalling in an inﬁnitesimal volume δxcontaining the point x\n",
      "is given by p(x)δx. This multivariate probability density must satisfy\n",
      "p(x)⩾0 (1.29)∫\n",
      "p(x)dx=1 (1.30)\n",
      "in which the integral is taken over the whole of xspace. We can also consider joint\n",
      "probability distributions over a combination of discrete and continuous variables.\n",
      "Note that if x is a discrete variable, then p(x)is sometimes called a probability\n",
      "mass function because it can be regarded as a set of ‘probability masses’ concentrated\n",
      "at the allowed values of x.\n",
      "The sum and product rules of probability, as well as Bayes’ theorem, apply\n",
      "equally to the case of probability densities, or to combinations of discrete and con-\n",
      "tinuous variables. For instance, if xandyare two real variables, then the sum and\n",
      "product rules take the form\n",
      "p(x)=∫\n",
      "p(x, y)dy (1.31)\n",
      "p(x, y)= p(y|x)p(x). (1.32)\n",
      "A formal justiﬁcation of the sum and product rules for continuous variables (Feller,\n",
      "1966) requires a branch of mathematics called measure theory and lies outside the\n",
      "scope of this book. Its validity can be seen informally, however, by dividing eachreal variable into intervals of width ∆and considering the discrete probability dis-\n",
      "tribution over these intervals. Taking the limit ∆→0then turns sums into integrals\n",
      "and gives the desired result.\n",
      "1.2.2 Expectations and covariances\n",
      "One of the most important operations involving probabilities is that of ﬁnding\n",
      "weighted averages of functions. The average value of some function f(x)under a\n",
      "probability distribution p(x)is called the expectation off(x)and will be denoted by\n",
      "E[f]. For a discrete distribution, it is given by\n",
      "E[f]=∑\n",
      "xp(x)f(x) (1.33)\n",
      "so that the average is weighted by the relative probabilities of the different values\n",
      "ofx. In the case of continuous variables, expectations are expressed in terms of an\n",
      "integration with respect to the corresponding probability density\n",
      "E[f]=∫\n",
      "p(x)f(x)dx. (1.34)\n",
      "In either case, if we are given a ﬁnite number Nof points drawn from the probability\n",
      "distribution or probability density, then the expectation can be approximated as a20 1. INTRODUCTION\n",
      "ﬁnite sum over these points\n",
      "E[f]≃1\n",
      "NN∑\n",
      "n=1f(xn). (1.35)\n",
      "We shall make extensive use of this result when we discuss sampling methods in\n",
      "Chapter 11. The approximation in (1.35) becomes exact in the limit N→∞ .\n",
      "Sometimes we will be considering expectations of functions of several variables,\n",
      "in which case we can use a subscript to indicate which variable is being averaged\n",
      "over, so that for instance\n",
      "Ex[f(x, y)] (1.36)\n",
      "denotes the average of the function f(x, y)with respect to the distribution of x. Note\n",
      "that Ex[f(x, y)]will be a function of y.\n",
      "We can also consider a conditional expectation with respect to a conditional\n",
      "distribution, so that\n",
      "Ex[f|y]=∑\n",
      "xp(x|y)f(x) (1.37)\n",
      "with an analogous deﬁnition for continuous variables.\n",
      "The variance off(x)is deﬁned by\n",
      "var[f]= E[\n",
      "(f(x)−E[f(x)])2]\n",
      "(1.38)\n",
      "and provides a measure of how much variability there is in f(x)around its mean\n",
      "value E[f(x)]. Expanding out the square, we see that the variance can also be written\n",
      "in terms of the expectations of f(x)andf(x)2Exercise 1.5\n",
      "var[f]= E[f(x)2]−E[f(x)]2. (1.39)\n",
      "In particular, we can consider the variance of the variable xitself, which is given by\n",
      "var[x]= E[x2]−E[x]2. (1.40)\n",
      "For two random variables xandy, the covariance is deﬁned by\n",
      "cov[x, y]= Ex,y[{x−E[x]}{y−E[y]}]\n",
      "= Ex,y[xy]−E[x]E[y] (1.41)\n",
      "which expresses the extent to which xandyvary together. If xandyare indepen-\n",
      "dent, then their covariance vanishes. Exercise 1.6\n",
      "In the case of two vectors of random variables xandy, the covariance is a matrix\n",
      "cov[x,y]= Ex,y[\n",
      "{x−E[x]}{yT−E[yT]}]\n",
      "= Ex,y[xyT]−E[x]E[yT]. (1.42)\n",
      "If we consider the covariance of the components of a vector xwith each other, then\n",
      "we use a slightly simpler notation cov[x]≡cov[x,x].1.2. Probability Theory 21\n",
      "1.2.3 Bayesian probabilities\n",
      "So far in this chapter, we have viewed probabilities in terms of the frequencies\n",
      "of random, repeatable events. We shall refer to this as the classical orfrequentist\n",
      "interpretation of probability. Now we turn to the more general Bayesian view, in\n",
      "which probabilities provide a quantiﬁcation of uncertainty.\n",
      "Consider an uncertain event, for example whether the moon was once in its own\n",
      "orbit around the sun, or whether the Arctic ice cap will have disappeared by the end\n",
      "of the century. These are not events that can be repeated numerous times in order\n",
      "to deﬁne a notion of probability as we did earlier in the context of boxes of fruit.\n",
      "Nevertheless, we will generally have some idea, for example, of how quickly wethink the polar ice is melting. If we now obtain fresh evidence, for instance from a\n",
      "new Earth observation satellite gathering novel forms of diagnostic information, we\n",
      "may revise our opinion on the rate of ice loss. Our assessment of such matters willaffect the actions we take, for instance the extent to which we endeavour to reduce\n",
      "the emission of greenhouse gasses. In such circumstances, we would like to be able\n",
      "to quantify our expression of uncertainty and make precise revisions of uncertainty inthe light of new evidence, as well as subsequently to be able to take optimal actions\n",
      "or decisions as a consequence. This can all be achieved through the elegant, and very\n",
      "general, Bayesian interpretation of probability.\n",
      "The use of probability to represent uncertainty, however, is not an ad-hoc choice,\n",
      "but is inevitable if we are to respect common sense while making rational coherent\n",
      "inferences. For instance, Cox (1946) showed that if numerical values are used torepresent degrees of belief, then a simple set of axioms encoding common sense\n",
      "properties of such beliefs leads uniquely to a set of rules for manipulating degrees of\n",
      "belief that are equivalent to the sum and product rules of probability. This providedthe ﬁrst rigorous proof that probability theory could be regarded as an extension of\n",
      "Boolean logic to situations involving uncertainty (Jaynes, 2003). Numerous other\n",
      "authors have proposed different sets of properties or axioms that such measures of\n",
      "uncertainty should satisfy (Ramsey, 1931; Good, 1950; Savage, 1961; deFinetti,\n",
      "1970; Lindley, 1982). In each case, the resulting numerical quantities behave pre-cisely according to the rules of probability. It is therefore natural to refer to these\n",
      "quantities as (Bayesian) probabilities.\n",
      "In the ﬁeld of pattern recognition, too, it is helpful to have a more general no-\n",
      "Thomas Bayes\n",
      "1701–1761\n",
      "Thomas Bayes was born in Tun-\n",
      "bridge Wells and was a clergymanas well as an amateur scientist anda mathematician. He studied logicand theology at Edinburgh Univer-sity and was elected Fellow of the\n",
      "Royal Society in 1742. During the 18\n",
      "thcentury, is-\n",
      "sues regarding probability arose in connection withgambling and with the new concept of insurance. One\n",
      "particularly important problem concerned so-called in-verse probability. A solution was proposed by ThomasBayes in his paper ‘Essay towards solving a problemin the doctrine of chances’, which was published in1764, some three years after his death, in the\n",
      "Philo-\n",
      "sophical Transactions of the Royal Society . In fact,\n",
      "Bayes only formulated his theory for the case of a uni-form prior, and it was Pierre-Simon Laplace who inde-pendently rediscovered the theory in general form andwho demonstrated its broad applicability.22 1. INTRODUCTION\n",
      "tion of probability. Consider the example of polynomial curve ﬁtting discussed in\n",
      "Section 1.1. It seems reasonable to apply the frequentist notion of probability to therandom values of the observed variables t\n",
      "n. However, we would like to address and\n",
      "quantify the uncertainty that surrounds the appropriate choice for the model param-\n",
      "etersw. We shall see that, from a Bayesian perspective, we can use the machinery\n",
      "of probability theory to describe the uncertainty in model parameters such as w,o r\n",
      "indeed in the choice of model itself.\n",
      "Bayes’ theorem now acquires a new signiﬁcance. Recall that in the boxes of fruit\n",
      "example, the observation of the identity of the fruit provided relevant information\n",
      "that altered the probability that the chosen box was the red one. In that example,\n",
      "Bayes’ theorem was used to convert a prior probability into a posterior probability\n",
      "by incorporating the evidence provided by the observed data. As we shall see in\n",
      "detail later, we can adopt a similar approach when making inferences about quantitiessuch as the parameters win the polynomial curve ﬁtting example. We capture our\n",
      "assumptions about w, before observing the data, in the form of a prior probability\n",
      "distribution p(w). The effect of the observed data D={t\n",
      "1,...,t N}is expressed\n",
      "through the conditional probability p(D|w), and we shall see later, in Section 1.2.5,\n",
      "how this can be represented explicitly. Bayes’ theorem, which takes the form\n",
      "p(w|D)=p(D|w)p(w)\n",
      "p(D)(1.43)\n",
      "then allows us to evaluate the uncertainty in wafter we have observed Din the form\n",
      "of the posterior probability p(w|D).\n",
      "The quantity p(D|w)on the right-hand side of Bayes’ theorem is evaluated for\n",
      "the observed data set Dand can be viewed as a function of the parameter vector\n",
      "w, in which case it is called the likelihood function . It expresses how probable the\n",
      "observed data set is for different settings of the parameter vector w. Note that the\n",
      "likelihood is not a probability distribution over w, and its integral with respect to w\n",
      "does not (necessarily) equal one.\n",
      "Given this deﬁnition of likelihood, we can state Bayes’ theorem in words\n",
      "posterior ∝likelihood ×prior (1.44)\n",
      "where all of these quantities are viewed as functions of w. The denominator in\n",
      "(1.43) is the normalization constant, which ensures that the posterior distributionon the left-hand side is a valid probability density and integrates to one. Indeed,\n",
      "integrating both sides of (1.43) with respect to w, we can express the denominator\n",
      "in Bayes’ theorem in terms of the prior distribution and the likelihood function\n",
      "p(D)=∫\n",
      "p(D|w)p(w)dw. (1.45)\n",
      "In both the Bayesian and frequentist paradigms, the likelihood function p(D|w)\n",
      "plays a central role. However, the manner in which it is used is fundamentally dif-ferent in the two approaches. In a frequentist setting, wis considered to be a ﬁxed\n",
      "parameter, whose value is determined by some form of ‘estimator’, and error bars1.2. Probability Theory 23\n",
      "on this estimate are obtained by considering the distribution of possible data sets D.\n",
      "By contrast, from the Bayesian viewpoint there is only a single data set D(namely\n",
      "the one that is actually observed), and the uncertainty in the parameters is expressed\n",
      "through a probability distribution over w.\n",
      "A widely used frequentist estimator is maximum likelihood , in which wis set\n",
      "to the value that maximizes the likelihood function p(D|w). This corresponds to\n",
      "choosing the value of wfor which the probability of the observed data set is maxi-\n",
      "mized. In the machine learning literature, the negative log of the likelihood functionis called an error function . Because the negative logarithm is a monotonically de-\n",
      "creasing function, maximizing the likelihood is equivalent to minimizing the error.\n",
      "One approach to determining frequentist error bars is the bootstrap (Efron, 1979;\n",
      "Hastie et al. , 2001), in which multiple data sets are created as follows. Suppose our\n",
      "original data set consists of Ndata points X={x\n",
      "1,...,xN}. We can create a new\n",
      "data set XBby drawing Npoints at random from X, with replacement, so that some\n",
      "points in Xmay be replicated in XB, whereas other points in Xmay be absent from\n",
      "XB. This process can be repeated Ltimes to generate Ldata sets each of size Nand\n",
      "each obtained by sampling from the original data set X. The statistical accuracy of\n",
      "parameter estimates can then be evaluated by looking at the variability of predictions\n",
      "between the different bootstrap data sets.\n",
      "One advantage of the Bayesian viewpoint is that the inclusion of prior knowl-\n",
      "edge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three\n",
      "times and lands heads each time. A classical maximum likelihood estimate of theprobability of landing heads would give 1, implying that all future tosses will land Section 2.1\n",
      "heads! By contrast, a Bayesian approach with any reasonable prior will lead to a\n",
      "much less extreme conclusion.\n",
      "There has been much controversy and debate associated with the relative mer-\n",
      "its of the frequentist and Bayesian paradigms, which have not been helped by the\n",
      "fact that there is no unique frequentist, or even Bayesian, viewpoint. For instance,\n",
      "one common criticism of the Bayesian approach is that the prior distribution is of-\n",
      "ten selected on the basis of mathematical convenience rather than as a reﬂection ofany prior beliefs. Even the subjective nature of the conclusions through their de-\n",
      "pendence on the choice of prior is seen by some as a source of difﬁculty. Reducing\n",
      "the dependence on the prior is one motivation for so-called noninformative priors. Section 2.4.3\n",
      "However, these lead to difﬁculties when comparing different models, and indeed\n",
      "Bayesian methods based on poor choices of prior can give poor results with high\n",
      "conﬁdence. Frequentist evaluation methods offer some protection from such prob-lems, and techniques such as cross-validation remain useful in areas such as model Section 1.3\n",
      "comparison.\n",
      "This book places a strong emphasis on the Bayesian viewpoint, reﬂecting the\n",
      "huge growth in the practical importance of Bayesian methods in the past few years,\n",
      "while also discussing useful frequentist concepts as required.\n",
      "Although the Bayesian framework has its origins in the 18\n",
      "thcentury, the prac-\n",
      "tical application of Bayesian methods was for a long time severely limited by the\n",
      "difﬁculties in carrying through the full Bayesian procedure, particularly the need to\n",
      "marginalize (sum or integrate) over the whole of parameter space, which, as we shall24 1. INTRODUCTION\n",
      "see, is required in order to make predictions or to compare different models. The\n",
      "development of sampling methods, such as Markov chain Monte Carlo (discussed inChapter 11) along with dramatic improvements in the speed and memory capacity\n",
      "of computers, opened the door to the practical use of Bayesian techniques in an im-\n",
      "pressive range of problem domains. Monte Carlo methods are very ﬂexible and canbe applied to a wide range of models. However, they are computationally intensive\n",
      "and have mainly been used for small-scale problems.\n",
      "More recently, highly efﬁcient deterministic approximation schemes such as\n",
      "variational Bayes and expectation propagation (discussed in Chapter 10) have been\n",
      "developed. These offer a complementary alternative to sampling methods and have\n",
      "allowed Bayesian techniques to be used in large-scale applications (Blei et al. , 2003).\n",
      "1.2.4 The Gaussian distribution\n",
      "We shall devote the whole of Chapter 2 to a study of various probability dis-\n",
      "tributions and their key properties. It is convenient, however, to introduce here one\n",
      "of the most important probability distributions for continuous variables, called the\n",
      "normal orGaussian distribution. We shall make extensive use of this distribution in\n",
      "the remainder of this chapter and indeed throughout much of the book.\n",
      "For the case of a single real-valued variable x, the Gaussian distribution is de-\n",
      "ﬁned by\n",
      "N(\n",
      "x|µ, σ2)\n",
      "=1\n",
      "(2πσ2)1/2exp{\n",
      "−1\n",
      "2σ2(x−µ)2}\n",
      "(1.46)\n",
      "which is governed by two parameters: µ, called the mean , andσ2, called the vari-\n",
      "ance . The square root of the variance, given by σ, is called the standard deviation ,\n",
      "and the reciprocal of the variance, written as β=1/σ2, is called the precision .W e\n",
      "shall see the motivation for these terms shortly. Figure 1.13 shows a plot of the\n",
      "Gaussian distribution.\n",
      "From the form of (1.46) we see that the Gaussian distribution satisﬁes\n",
      "N(x|µ, σ2)>0. (1.47)\n",
      "Also it is straightforward to show that the Gaussian is normalized, so that Exercise 1.7\n",
      "Pierre-Simon Laplace\n",
      "1749–1827\n",
      "It is said that Laplace was seri-\n",
      "ously lacking in modesty and at onepoint declared himself to be thebest mathematician in France at thetime, a claim that was arguably true.As well as being proliﬁc in mathe-\n",
      "matics, he also made numerous contributions to as-tronomy, including the nebular hypothesis by which theearth is thought to have formed from the condensa-\n",
      "tion and cooling of a large rotating disk of gas anddust. In 1812 he published the ﬁrst edition of\n",
      "Th´eorie\n",
      "Analytique des Probabilit ´es, in which Laplace states\n",
      "that “probability theory is nothing but common sensereduced to calculation”. This work included a discus-sion of the inverse probability calculation (later termedBayes’ theorem by Poincar ´e), which he used to solve\n",
      "problems in life expectancy, jurisprudence, planetarymasses, triangulation, and error estimation.1.2. Probability Theory 25\n",
      "Figure 1.13 Plot of the univariate Gaussian\n",
      "showing the mean µand the\n",
      "standard deviation σ.N(x|µ, σ2)\n",
      "x2σ\n",
      "µ\n",
      "∫∞\n",
      "−∞N(\n",
      "x|µ, σ2)\n",
      "dx=1. (1.48)\n",
      "Thus (1.46) satisﬁes the two requirements for a valid probability density.\n",
      "We can readily ﬁnd expectations of functions of xunder the Gaussian distribu-\n",
      "tion. In particular, the average value of xis given by Exercise 1.8\n",
      "E[x]=∫∞\n",
      "−∞N(\n",
      "x|µ, σ2)\n",
      "xdx=µ. (1.49)\n",
      "Because the parameter µrepresents the average value of xunder the distribution, it\n",
      "is referred to as the mean. Similarly, for the second order moment\n",
      "E[x2]=∫∞\n",
      "−∞N(\n",
      "x|µ, σ2)\n",
      "x2dx=µ2+σ2. (1.50)\n",
      "From (1.49) and (1.50), it follows that the variance of xis given by\n",
      "var[x]= E[x2]−E[x]2=σ2(1.51)\n",
      "and hence σ2is referred to as the variance parameter. The maximum of a distribution\n",
      "is known as its mode. For a Gaussian, the mode coincides with the mean. Exercise 1.9\n",
      "We are also interested in the Gaussian distribution deﬁned over a D-dimensional\n",
      "vectorxof continuous variables, which is given by\n",
      "N(x|µ,Σ)=1\n",
      "(2π)D/21\n",
      "|Σ|1/2exp{\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)}\n",
      "(1.52)\n",
      "where the D-dimensional vector µis called the mean, the D×DmatrixΣis called\n",
      "the covariance, and |Σ|denotes the determinant of Σ. We shall make use of the\n",
      "multivariate Gaussian distribution brieﬂy in this chapter, although its properties will\n",
      "be studied in detail in Section 2.3.26 1. INTRODUCTION\n",
      "Figure 1.14 Illustration of the likelihood function for\n",
      "a Gaussian distribution, shown by the\n",
      "red curve. Here the black points de-\n",
      "note a data set of values {xn}, and\n",
      "the likelihood function given by (1.53)\n",
      "corresponds to the product of the blue\n",
      "values. Maximizing the likelihood in-\n",
      "volves adjusting the mean and vari-\n",
      "ance of the Gaussian so as to maxi-\n",
      "mize this product.\n",
      "xp(x)\n",
      "xnN(xn|µ, σ2)\n",
      "Now suppose that we have a data set of observations x=(x1,...,x N)T, rep-\n",
      "resenting Nobservations of the scalar variable x. Note that we are using the type-\n",
      "face xto distinguish this from a single observation of the vector-valued variable\n",
      "(x1,...,x D)T, which we denote by x. We shall suppose that the observations are\n",
      "drawn independently from a Gaussian distribution whose mean µand variance σ2\n",
      "are unknown, and we would like to determine these parameters from the data set.\n",
      "Data points that are drawn independently from the same distribution are said to be\n",
      "independent and identically distributed , which is often abbreviated to i.i.d. We have\n",
      "seen that the joint probability of two independent events is given by the product of\n",
      "the marginal probabilities for each event separately. Because our data set xis i.i.d.,\n",
      "we can therefore write the probability of the data set, given µandσ2, in the form\n",
      "p(x|µ, σ2)=N∏\n",
      "n=1N(\n",
      "xn|µ, σ2)\n",
      ". (1.53)\n",
      "When viewed as a function of µandσ2, this is the likelihood function for the Gaus-\n",
      "sian and is interpreted diagrammatically in Figure 1.14.\n",
      "One common criterion for determining the parameters in a probability distribu-\n",
      "tion using an observed data set is to ﬁnd the parameter values that maximize the\n",
      "likelihood function. This might seem like a strange criterion because, from our fore-\n",
      "going discussion of probability theory, it would seem more natural to maximize the\n",
      "probability of the parameters given the data, not the probability of the data given the\n",
      "parameters. In fact, these two criteria are related, as we shall discuss in the context\n",
      "of curve ﬁtting. Section 1.2.5\n",
      "For the moment, however, we shall determine values for the unknown parame-\n",
      "tersµandσ2in the Gaussian by maximizing the likelihood function (1.53). In prac-\n",
      "tice, it is more convenient to maximize the log of the likelihood function. Because\n",
      "the logarithm is a monotonically increasing function of its argument, maximization\n",
      "of the log of a function is equivalent to maximization of the function itself. Taking\n",
      "the log not only simpliﬁes the subsequent mathematical analysis, but it also helps\n",
      "numerically because the product of a large number of small probabilities can easily\n",
      "underﬂow the numerical precision of the computer, and this is resolved by computing\n",
      "instead the sum of the log probabilities. From (1.46) and (1.53), the log likelihood1.2. Probability Theory 27\n",
      "function can be written in the form\n",
      "lnp(\n",
      "x|µ, σ2)\n",
      "=−1\n",
      "2σ2N∑\n",
      "n=1(xn−µ)2−N\n",
      "2lnσ2−N\n",
      "2ln(2π). (1.54)\n",
      "Maximizing (1.54) with respect to µ, we obtain the maximum likelihood solution\n",
      "given by Exercise 1.11\n",
      "µML=1\n",
      "NN∑\n",
      "n=1xn (1.55)\n",
      "which is the sample mean , i.e., the mean of the observed values {xn}. Similarly,\n",
      "maximizing (1.54) with respect to σ2, we obtain the maximum likelihood solution\n",
      "for the variance in the form\n",
      "σ2\n",
      "ML=1\n",
      "NN∑\n",
      "n=1(xn−µML)2(1.56)\n",
      "which is the sample variance measured with respect to the sample mean µML. Note\n",
      "that we are performing a joint maximization of (1.54) with respect to µandσ2,b u t\n",
      "in the case of the Gaussian distribution the solution for µdecouples from that for σ2\n",
      "so that we can ﬁrst evaluate (1.55) and then subsequently use this result to evaluate\n",
      "(1.56).\n",
      "Later in this chapter, and also in subsequent chapters, we shall highlight the sig-\n",
      "niﬁcant limitations of the maximum likelihood approach. Here we give an indication\n",
      "of the problem in the context of our solutions for the maximum likelihood param-\n",
      "eter settings for the univariate Gaussian distribution. In particular, we shall show\n",
      "that the maximum likelihood approach systematically underestimates the varianceof the distribution. This is an example of a phenomenon called bias and is related\n",
      "to the problem of over-ﬁtting encountered in the context of polynomial curve ﬁtting. Section 1.1\n",
      "We ﬁrst note that the maximum likelihood solutions µ\n",
      "MLandσ2\n",
      "MLare functions of\n",
      "the data set values x1,...,x N. Consider the expectations of these quantities with\n",
      "respect to the data set values, which themselves come from a Gaussian distribution\n",
      "with parameters µandσ2. It is straightforward to show that Exercise 1.12\n",
      "E[µML]= µ (1.57)\n",
      "E[σ2\n",
      "ML]=(N−1\n",
      "N)\n",
      "σ2(1.58)\n",
      "so that on average the maximum likelihood estimate will obtain the correct mean but\n",
      "will underestimate the true variance by a factor (N−1)/N. The intuition behind\n",
      "this result is given by Figure 1.15.\n",
      "From (1.58) it follows that the following estimate for the variance parameter is\n",
      "unbiased\n",
      "˜σ2=N\n",
      "N−1σ2\n",
      "ML=1\n",
      "N−1N∑\n",
      "n=1(xn−µML)2. (1.59)28 1. INTRODUCTION\n",
      "Figure 1.15 Illustration of how bias arises in using max-\n",
      "imum likelihood to determine the variance\n",
      "of a Gaussian. The green curve shows\n",
      "the true Gaussian distribution from which\n",
      "data is generated, and the three red curves\n",
      "show the Gaussian distributions obtained\n",
      "by ﬁtting to three data sets, each consist-\n",
      "ing of two data points shown in blue, us-\n",
      "ing the maximum likelihood results (1.55)\n",
      "and (1.56). Averaged across the three data\n",
      "sets, the mean is correct, but the variance\n",
      "is systematically under-estimated because\n",
      "it is measured relative to the sample mean\n",
      "and not relative to the true mean.(a)\n",
      "(b)\n",
      "(c)\n",
      "In Section 10.1.3, we shall see how this result arises automatically when we adopt a\n",
      "Bayesian approach.\n",
      "Note that the bias of the maximum likelihood solution becomes less signiﬁcant\n",
      "as the number Nof data points increases, and in the limit N→∞ the maximum\n",
      "likelihood solution for the variance equals the true variance of the distribution that\n",
      "generated the data. In practice, for anything other than small N, this bias will not\n",
      "prove to be a serious problem. However, throughout this book we shall be interested\n",
      "in more complex models with many parameters, for which the bias problems asso-\n",
      "ciated with maximum likelihood will be much more severe. In fact, as we shall see,\n",
      "the issue of bias in maximum likelihood lies at the root of the over-ﬁtting problem\n",
      "that we encountered earlier in the context of polynomial curve ﬁtting.\n",
      "1.2.5 Curve ﬁtting re-visited\n",
      "We have seen how the problem of polynomial curve ﬁtting can be expressed in\n",
      "terms of error minimization. Here we return to the curve ﬁtting example and view it Section 1.1\n",
      "from a probabilistic perspective, thereby gaining some insights into error functions\n",
      "and regularization, as well as taking us towards a full Bayesian treatment.\n",
      "The goal in the curve ﬁtting problem is to be able to make predictions for the\n",
      "target variable tgiven some new value of the input variable xon the basis of a set of\n",
      "training data comprising Ninput values x=(x1,...,x N)Tand their corresponding\n",
      "target values t=(t1,...,t N)T. We can express our uncertainty over the value of\n",
      "the target variable using a probability distribution. For this purpose, we shall assume\n",
      "that, given the value of x, the corresponding value of thas a Gaussian distribution\n",
      "with a mean equal to the value y(x,w)of the polynomial curve given by (1.1). Thus\n",
      "we have\n",
      "p(t|x,w,β)=N(\n",
      "t|y(x,w),β−1)\n",
      "(1.60)\n",
      "where, for consistency with the notation in later chapters, we have deﬁned a preci-\n",
      "sion parameter βcorresponding to the inverse variance of the distribution. This is\n",
      "illustrated schematically in Figure 1.16.1.2. Probability Theory 29\n",
      "Figure 1.16 Schematic illustration of a Gaus-\n",
      "sian conditional distribution for tgiven xgiven by\n",
      "(1.60), in which the mean is given by the polyno-\n",
      "mial function y(x,w), and the precision is given\n",
      "by the parameter β, which is related to the vari-\n",
      "ance by β−1=σ2.t\n",
      "x x02σ y(x0,w)y(x,w)\n",
      "p(t|x0,w,β)\n",
      "We now use the training data {x,t}to determine the values of the unknown\n",
      "parameters wandβby maximum likelihood. If the data are assumed to be drawn\n",
      "independently from the distribution (1.60), then the likelihood function is given by\n",
      "p(t|x,w,β)=N∏\n",
      "n=1N(\n",
      "tn|y(xn,w),β−1)\n",
      ". (1.61)\n",
      "As we did in the case of the simple Gaussian distribution earlier, it is convenient to\n",
      "maximize the logarithm of the likelihood function. Substituting for the form of the\n",
      "Gaussian distribution, given by (1.46), we obtain the log likelihood function in the\n",
      "form\n",
      "lnp(t|x,w,β)=−β\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2+N\n",
      "2lnβ−N\n",
      "2ln(2π). (1.62)\n",
      "Consider ﬁrst the determination of the maximum likelihood solution for the polyno-\n",
      "mial coefﬁcients, which will be denoted by wML. These are determined by maxi-\n",
      "mizing (1.62) with respect to w. For this purpose, we can omit the last two terms\n",
      "on the right-hand side of (1.62) because they do not depend on w. Also, we note\n",
      "that scaling the log likelihood by a positive constant coefﬁcient does not alter the\n",
      "location of the maximum with respect to w, and so we can replace the coefﬁcient\n",
      "β/2with1/2. Finally, instead of maximizing the log likelihood, we can equivalently\n",
      "minimize the negative log likelihood. We therefore see that maximizing likelihood is\n",
      "equivalent, so far as determining wis concerned, to minimizing the sum-of-squares\n",
      "error function deﬁned by (1.2). Thus the sum-of-squares error function has arisen as\n",
      "a consequence of maximizing likelihood under the assumption of a Gaussian noise\n",
      "distribution.\n",
      "We can also use maximum likelihood to determine the precision parameter βof\n",
      "the Gaussian conditional distribution. Maximizing (1.62) with respect to βgives\n",
      "1\n",
      "βML=1\n",
      "NN∑\n",
      "n=1{y(xn,wML)−tn}2. (1.63)30 1. INTRODUCTION\n",
      "Again we can ﬁrst determine the parameter vector wMLgoverning the mean and sub-\n",
      "sequently use this to ﬁnd the precision βMLas was the case for the simple Gaussian\n",
      "distribution. Section 1.2.4\n",
      "Having determined the parameters wandβ, we can now make predictions for\n",
      "new values of x. Because we now have a probabilistic model, these are expressed\n",
      "in terms of the predictive distribution that gives the probability distribution over t,\n",
      "rather than simply a point estimate, and is obtained by substituting the maximum\n",
      "likelihood parameters into (1.60) to give\n",
      "p(t|x,wML,βML)=N(\n",
      "t|y(x,wML),β−1\n",
      "ML)\n",
      ". (1.64)\n",
      "Now let us take a step towards a more Bayesian approach and introduce a prior\n",
      "distribution over the polynomial coefﬁcients w. For simplicity, let us consider a\n",
      "Gaussian distribution of the form\n",
      "p(w|α)=N(w|0,α−1I)=(α\n",
      "2π)(M+1)/2\n",
      "exp{\n",
      "−α\n",
      "2wTw}\n",
      "(1.65)\n",
      "where αis the precision of the distribution, and M+1 is the total number of elements\n",
      "in the vector wfor an Mthorder polynomial. V ariables such as α, which control\n",
      "the distribution of model parameters, are called hyperparameters . Using Bayes’\n",
      "theorem, the posterior distribution for wis proportional to the product of the prior\n",
      "distribution and the likelihood function\n",
      "p(w|x,t,α,β)∝p(t|x,w,β)p(w|α). (1.66)\n",
      "We can now determine wby ﬁnding the most probable value of wgiven the data,\n",
      "in other words by maximizing the posterior distribution. This technique is called\n",
      "maximum posterior , or simply MAP . Taking the negative logarithm of (1.66) and\n",
      "combining with (1.62) and (1.65), we ﬁnd that the maximum of the posterior isgiven by the minimum of\n",
      "β\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2+α\n",
      "2wTw. (1.67)\n",
      "Thus we see that maximizing the posterior distribution is equivalent to minimizing\n",
      "the regularized sum-of-squares error function encountered earlier in the form (1.4),with a regularization parameter given by λ=α/β .\n",
      "1.2.6 Bayesian curve ﬁtting\n",
      "Although we have included a prior distribution p(w|α), we are so far still mak-\n",
      "ing a point estimate of wand so this does not yet amount to a Bayesian treatment. In\n",
      "a fully Bayesian approach, we should consistently apply the sum and product rules\n",
      "of probability, which requires, as we shall see shortly, that we integrate over all val-ues of w. Such marginalizations lie at the heart of Bayesian methods for pattern\n",
      "recognition.1.2. Probability Theory 31\n",
      "In the curve ﬁtting problem, we are given the training data xand t, along with\n",
      "a new test point x, and our goal is to predict the value of t. We therefore wish\n",
      "to evaluate the predictive distribution p(t|x,x,t). Here we shall assume that the\n",
      "parameters αandβare ﬁxed and known in advance (in later chapters we shall discuss\n",
      "how such parameters can be inferred from data in a Bayesian setting).\n",
      "A Bayesian treatment simply corresponds to a consistent application of the sum\n",
      "and product rules of probability, which allow the predictive distribution to be written\n",
      "in the form\n",
      "p(t|x,x,t)=∫\n",
      "p(t|x,w)p(w|x,t)dw. (1.68)\n",
      "Herep(t|x,w)is given by (1.60), and we have omitted the dependence on αand\n",
      "βto simplify the notation. Here p(w|x,t)is the posterior distribution over param-\n",
      "eters, and can be found by normalizing the right-hand side of (1.66). We shall seein Section 3.3 that, for problems such as the curve-ﬁtting example, this posterior\n",
      "distribution is a Gaussian and can be evaluated analytically. Similarly, the integra-\n",
      "tion in (1.68) can also be performed analytically with the result that the predictive\n",
      "distribution is given by a Gaussian of the form\n",
      "p(t|x,x,t)=N(\n",
      "t|m(x),s2(x))\n",
      "(1.69)\n",
      "where the mean and variance are given by\n",
      "m(x)= βφ(x)TSN∑\n",
      "n=1φ(xn)tn (1.70)\n",
      "s2(x)= β−1+φ(x)TSφ(x). (1.71)\n",
      "Here the matrix Sis given by\n",
      "S−1=αI+βN∑\n",
      "n=1φ(xn)φ(x)T(1.72)\n",
      "whereIis the unit matrix, and we have deﬁned the vector φ(x)with elements\n",
      "φi(x)=xifori=0,...,M .\n",
      "We see that the variance, as well as the mean, of the predictive distribution in\n",
      "(1.69) is dependent on x. The ﬁrst term in (1.71) represents the uncertainty in the\n",
      "predicted value of tdue to the noise on the target variables and was expressed already\n",
      "in the maximum likelihood predictive distribution (1.64) through β−1\n",
      "ML. However, the\n",
      "second term arises from the uncertainty in the parameters wand is a consequence\n",
      "of the Bayesian treatment. The predictive distribution for the synthetic sinusoidalregression problem is illustrated in Figure 1.17.32 1. INTRODUCTION\n",
      "Figure 1.17 The predictive distribution result-\n",
      "ing from a Bayesian treatment of\n",
      "polynomial curve ﬁtting using an\n",
      "M=9 polynomial, with the ﬁxed\n",
      "parameters α=5×10−3andβ=\n",
      "11.1(corresponding to the known\n",
      "noise variance), in which the red\n",
      "curve denotes the mean of the\n",
      "predictive distribution and the red\n",
      "region corresponds to ±1stan-\n",
      "dard deviation around the mean.\n",
      "xt\n",
      "0 1−101\n",
      "1.3. Model Selection\n",
      "In our example of polynomial curve ﬁtting using least squares, we saw that there was\n",
      "an optimal order of polynomial that gave the best generalization. The order of the\n",
      "polynomial controls the number of free parameters in the model and thereby governs\n",
      "the model complexity. With regularized least squares, the regularization coefﬁcient\n",
      "λalso controls the effective complexity of the model, whereas for more complex\n",
      "models, such as mixture distributions or neural networks there may be multiple pa-\n",
      "rameters governing complexity. In a practical application, we need to determine\n",
      "the values of such parameters, and the principal objective in doing so is usually to\n",
      "achieve the best predictive performance on new data. Furthermore, as well as ﬁnd-\n",
      "ing the appropriate values for complexity parameters within a given model, we may\n",
      "wish to consider a range of different types of model in order to ﬁnd the best one for\n",
      "our particular application.\n",
      "We have already seen that, in the maximum likelihood approach, the perfor-\n",
      "mance on the training set is not a good indicator of predictive performance on un-\n",
      "seen data due to the problem of over-ﬁtting. If data is plentiful, then one approach is\n",
      "simply to use some of the available data to train a range of models, or a given model\n",
      "with a range of values for its complexity parameters, and then to compare them on\n",
      "independent data, sometimes called a validation set , and select the one having the\n",
      "best predictive performance. If the model design is iterated many times using a lim-\n",
      "ited size data set, then some over-ﬁtting to the validation data can occur and so it may\n",
      "be necessary to keep aside a third test set on which the performance of the selected\n",
      "model is ﬁnally evaluated.\n",
      "In many applications, however, the supply of data for training and testing will be\n",
      "limited, and in order to build good models, we wish to use as much of the available\n",
      "data as possible for training. However, if the validation set is small, it will give a\n",
      "relatively noisy estimate of predictive performance. One solution to this dilemma is\n",
      "to use cross-validation , which is illustrated in Figure 1.18. This allows a proportion\n",
      "(S−1)/Sof the available data to be used for training while making use of all of the1.4. The Curse of Dimensionality 33\n",
      "Figure 1.18 The technique of S-fold cross-validation, illus-\n",
      "trated here for the case of S=4, involves tak-\n",
      "ing the available data and partitioning it into S\n",
      "groups (in the simplest case these are of equal\n",
      "size). Then S−1of the groups are used to train\n",
      "a set of models that are then evaluated on the re-\n",
      "maining group. This procedure is then repeated\n",
      "for all Spossible choices for the held-out group,\n",
      "indicated here by the red blocks, and the perfor-\n",
      "mance scores from the Sruns are then averaged.run 1\n",
      "run 2\n",
      "run 3\n",
      "run 4\n",
      "data to assess performance. When data is particularly scarce, it may be appropriate\n",
      "to consider the case S=N, where Nis the total number of data points, which gives\n",
      "the leave-one-out technique.\n",
      "One major drawback of cross-validation is that the number of training runs that\n",
      "must be performed is increased by a factor of S, and this can prove problematic for\n",
      "models in which the training is itself computationally expensive. A further problem\n",
      "with techniques such as cross-validation that use separate data to assess performance\n",
      "is that we might have multiple complexity parameters for a single model (for in-\n",
      "stance, there might be several regularization parameters). Exploring combinations\n",
      "of settings for such parameters could, in the worst case, require a number of training\n",
      "runs that is exponential in the number of parameters. Clearly, we need a better ap-\n",
      "proach. Ideally, this should rely only on the training data and should allow multiple\n",
      "hyperparameters and model types to be compared in a single training run. We there-\n",
      "fore need to ﬁnd a measure of performance which depends only on the training data\n",
      "and which does not suffer from bias due to over-ﬁtting.\n",
      "Historically various ‘information criteria’ have been proposed that attempt to\n",
      "correct for the bias of maximum likelihood by the addition of a penalty term to\n",
      "compensate for the over-ﬁtting of more complex models. For example, the Akaike\n",
      "information criterion , or AIC (Akaike, 1974), chooses the model for which the quan-\n",
      "tity\n",
      "lnp(D|wML)−M (1.73)\n",
      "is largest. Here p(D|wML)is the best-ﬁt log likelihood, and Mis the number of\n",
      "adjustable parameters in the model. A variant of this quantity, called the Bayesian\n",
      "information criterion ,o r BIC , will be discussed in Section 4.4.1. Such criteria do\n",
      "not take account of the uncertainty in the model parameters, however, and in practice\n",
      "they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully\n",
      "Bayesian approach where we shall see how complexity penalties arise in a natural\n",
      "and principled way.\n",
      "1.4. The Curse of Dimensionality\n",
      "In the polynomial curve ﬁtting example we had just one input variable x. For prac-\n",
      "tical applications of pattern recognition, however, we will have to deal with spaces34 1. INTRODUCTION\n",
      "Figure 1.19 Scatter plot of the oil ﬂow data\n",
      "for input variables x6andx7,i n\n",
      "which red denotes the ‘homoge-\n",
      "nous’ class, green denotes the\n",
      "‘annular’ class, and blue denotes\n",
      "the ‘laminar’ class. Our goal is\n",
      "to classify the new test point de-\n",
      "noted by ‘ ×’.\n",
      "x6x7\n",
      "0 0.25 0.5 0.75 100.511.52\n",
      "of high dimensionality comprising many input variables. As we now discuss, this\n",
      "poses some serious challenges and is an important factor inﬂuencing the design of\n",
      "pattern recognition techniques.\n",
      "In order to illustrate the problem we consider a synthetically generated data set\n",
      "representing measurements taken from a pipeline containing a mixture of oil, wa-\n",
      "ter, and gas (Bishop and James, 1993). These three materials can be present in one\n",
      "of three different geometrical conﬁgurations known as ‘homogenous’, ‘annular’, and\n",
      "‘laminar’, and the fractions of the three materials can also vary. Each data point com-\n",
      "prises a 12-dimensional input vector consisting of measurements taken with gamma\n",
      "ray densitometers that measure the attenuation of gamma rays passing along nar-\n",
      "row beams through the pipe. This data set is described in detail in Appendix A.\n",
      "Figure 1.19 shows 100 points from this data set on a plot showing two of the mea-\n",
      "surements x6andx7(the remaining ten input values are ignored for the purposes of\n",
      "this illustration). Each data point is labelled according to which of the three geomet-\n",
      "rical classes it belongs to, and our goal is to use this data as a training set in order to\n",
      "be able to classify a new observation (x6,x7), such as the one denoted by the cross\n",
      "in Figure 1.19. We observe that the cross is surrounded by numerous red points, and\n",
      "so we might suppose that it belongs to the red class. However, there are also plenty\n",
      "of green points nearby, so we might think that it could instead belong to the green\n",
      "class. It seems unlikely that it belongs to the blue class. The intuition here is that the\n",
      "identity of the cross should be determined more strongly by nearby points from the\n",
      "training set and less strongly by more distant points. In fact, this intuition turns out\n",
      "to be reasonable and will be discussed more fully in later chapters.\n",
      "How can we turn this intuition into a learning algorithm? One very simple ap-\n",
      "proach would be to divide the input space into regular cells, as indicated in Fig-\n",
      "ure 1.20. When we are given a test point and we wish to predict its class, we ﬁrst\n",
      "decide which cell it belongs to, and we then ﬁnd all of the training data points that1.4. The Curse of Dimensionality 35\n",
      "Figure 1.20 Illustration of a simple approach\n",
      "to the solution of a classiﬁcation\n",
      "problem in which the input space\n",
      "is divided into cells and any new\n",
      "test point is assigned to the class\n",
      "that has a majority number of rep-\n",
      "resentatives in the same cell as\n",
      "the test point. As we shall see\n",
      "shortly, this simplistic approach\n",
      "has some severe shortcomings.\n",
      "x6x7\n",
      "0 0.25 0.5 0.75 100.511.52\n",
      "fall in the same cell. The identity of the test point is predicted as being the same\n",
      "as the class having the largest number of training points in the same cell as the test\n",
      "point (with ties being broken at random).\n",
      "There are numerous problems with this naive approach, but one of the most se-\n",
      "vere becomes apparent when we consider its extension to problems having larger\n",
      "numbers of input variables, corresponding to input spaces of higher dimensionality.\n",
      "The origin of the problem is illustrated in Figure 1.21, which shows that, if we divide\n",
      "a region of a space into regular cells, then the number of such cells grows exponen-\n",
      "tially with the dimensionality of the space. The problem with an exponentially large\n",
      "number of cells is that we would need an exponentially large quantity of training data\n",
      "in order to ensure that the cells are not empty. Clearly, we have no hope of applying\n",
      "such a technique in a space of more than a few variables, and so we need to ﬁnd a\n",
      "more sophisticated approach.\n",
      "We can gain further insight into the problems of high-dimensional spaces by\n",
      "returning to the example of polynomial curve ﬁtting and considering how we would Section 1.1\n",
      "Figure 1.21 Illustration of the\n",
      "curse of dimensionality, showing\n",
      "how the number of regions of a\n",
      "regular grid grows exponentially\n",
      "with the dimensionality Dof the\n",
      "space. For clarity, only a subset of\n",
      "the cubical regions are shown for\n",
      "D=3.\n",
      "x1\n",
      "D=1x1x2\n",
      "D=2x1x2\n",
      "x3\n",
      "D=336 1. INTRODUCTION\n",
      "extend this approach to deal with input spaces having several variables. If we have\n",
      "Dinput variables, then a general polynomial with coefﬁcients up to order 3would\n",
      "take the form\n",
      "y(x,w)=w0+D∑\n",
      "i=1wixi+D∑\n",
      "i=1D∑\n",
      "j=1wijxixj+D∑\n",
      "i=1D∑\n",
      "j=1D∑\n",
      "k=1wijkxixjxk.(1.74)\n",
      "AsDincreases, so the number of independent coefﬁcients (not all of the coefﬁcients\n",
      "are independent due to interchange symmetries amongst the xvariables) grows pro-\n",
      "portionally to D3. In practice, to capture complex dependencies in the data, we may\n",
      "need to use a higher-order polynomial. For a polynomial of order M, the growth in\n",
      "the number of coefﬁcients is like DM. Although this is now a power law growth, Exercise 1.16\n",
      "rather than an exponential growth, it still points to the method becoming rapidlyunwieldy and of limited practical utility.\n",
      "Our geometrical intuitions, formed through a life spent in a space of three di-\n",
      "mensions, can fail badly when we consider spaces of higher dimensionality. As asimple example, consider a sphere of radius r=1 in a space of Ddimensions, and\n",
      "ask what is the fraction of the volume of the sphere that lies between radius r=1−ϵ\n",
      "andr=1 . We can evaluate this fraction by noting that the volume of a sphere of\n",
      "radius rinDdimensions must scale as r\n",
      "D, and so we write\n",
      "VD(r)=KDrD(1.75)\n",
      "where the constant KDdepends only on D. Thus the required fraction is given by Exercise 1.18\n",
      "VD(1)−VD(1−ϵ)\n",
      "VD(1)=1−(1−ϵ)D(1.76)\n",
      "which is plotted as a function of ϵfor various values of Din Figure 1.22. We see\n",
      "that, for large D, this fraction tends to 1even for small values of ϵ. Thus, in spaces\n",
      "of high dimensionality, most of the volume of a sphere is concentrated in a thin shell\n",
      "near the surface!\n",
      "As a further example, of direct relevance to pattern recognition, consider the\n",
      "behaviour of a Gaussian distribution in a high-dimensional space. If we transform\n",
      "from Cartesian to polar coordinates, and then integrate out the directional variables,we obtain an expression for the density p(r)as a function of radius rfrom the origin. Exercise 1.20\n",
      "Thusp(r)δris the probability mass inside a thin shell of thickness δrlocated at\n",
      "radius r. This distribution is plotted, for various values of D, in Figure 1.23, and we\n",
      "see that for large Dthe probability mass of the Gaussian is concentrated in a thin\n",
      "shell.\n",
      "The severe difﬁculty that can arise in spaces of many dimensions is sometimes\n",
      "called the curse of dimensionality (Bellman, 1961). In this book, we shall make ex-\n",
      "tensive use of illustrative examples involving input spaces of one or two dimensions,because this makes it particularly easy to illustrate the techniques graphically. The\n",
      "reader should be warned, however, that not all intuitions developed in spaces of low\n",
      "dimensionality will generalize to spaces of many dimensions.1.4. The Curse of Dimensionality 37\n",
      "Figure 1.22 Plot of the fraction of the volume of\n",
      "a sphere lying in the range r=1−ϵ\n",
      "tor=1 for various values of the\n",
      "dimensionality D.\n",
      "ϵvolume fractionD=1D=2D=5D=2 0\n",
      "0 0.2 0.4 0.6 0.8 100.20.40.60.81\n",
      "Although the curse of dimensionality certainly raises important issues for pat-\n",
      "tern recognition applications, it does not prevent us from ﬁnding effective techniques\n",
      "applicable to high-dimensional spaces. The reasons for this are twofold. First, real\n",
      "data will often be conﬁned to a region of the space having lower effective dimension-\n",
      "ality, and in particular the directions over which important variations in the target\n",
      "variables occur may be so conﬁned. Second, real data will typically exhibit some\n",
      "smoothness properties (at least locally) so that for the most part small changes in the\n",
      "input variables will produce small changes in the target variables, and so we can ex-\n",
      "ploit local interpolation-like techniques to allow us to make predictions of the target\n",
      "variables for new values of the input variables. Successful pattern recognition tech-\n",
      "niques exploit one or both of these properties. Consider, for example, an application\n",
      "in manufacturing in which images are captured of identical planar objects on a con-\n",
      "veyor belt, in which the goal is to determine their orientation. Each image is a point\n",
      "Figure 1.23 Plot of the probability density with\n",
      "respect to radius rof a Gaus-\n",
      "sian distribution for various values\n",
      "of the dimensionality D.I n a\n",
      "high-dimensional space, most of the\n",
      "probability mass of a Gaussian is lo-\n",
      "cated within a thin shell at a speciﬁc\n",
      "radius.D=1\n",
      "D=2\n",
      "D=2 0\n",
      "rp(r)\n",
      "0 2 401238 1. INTRODUCTION\n",
      "in a high-dimensional space whose dimensionality is determined by the number of\n",
      "pixels. Because the objects can occur at different positions within the image andin different orientations, there are three degrees of freedom of variability between\n",
      "images, and a set of images will live on a three dimensional manifold embedded\n",
      "within the high-dimensional space. Due to the complex relationships between theobject position or orientation and the pixel intensities, this manifold will be highly\n",
      "nonlinear. If the goal is to learn a model that can take an input image and output the\n",
      "orientation of the object irrespective of its position, then there is only one degree offreedom of variability within the manifold that is signiﬁcant.\n",
      "1.5. Decision Theory\n",
      "We have seen in Section 1.2 how probability theory provides us with a consistentmathematical framework for quantifying and manipulating uncertainty. Here we\n",
      "turn to a discussion of decision theory that, when combined with probability theory,\n",
      "allows us to make optimal decisions in situations involving uncertainty such as thoseencountered in pattern recognition.\n",
      "Suppose we have an input vector xtogether with a corresponding vector tof\n",
      "target variables, and our goal is to predict tgiven a new value for x. For regression\n",
      "problems, twill comprise continuous variables, whereas for classiﬁcation problems\n",
      "twill represent class labels. The joint probability distribution p(x,t)provides a\n",
      "complete summary of the uncertainty associated with these variables. Determinationofp(x,t)from a set of training data is an example of inference and is typically a\n",
      "very difﬁcult problem whose solution forms the subject of much of this book. In\n",
      "a practical application, however, we must often make a speciﬁc prediction for the\n",
      "value of t, or more generally take a speciﬁc action based on our understanding of the\n",
      "valuestis likely to take, and this aspect is the subject of decision theory.\n",
      "Consider, for example, a medical diagnosis problem in which we have taken an\n",
      "X-ray image of a patient, and we wish to determine whether the patient has cancer\n",
      "or not. In this case, the input vector xis the set of pixel intensities in the image,\n",
      "and output variable twill represent the presence of cancer, which we denote by the\n",
      "classC\n",
      "1, or the absence of cancer, which we denote by the class C2. We might, for\n",
      "instance, choose tto be a binary variable such that t=0 corresponds to class C1and\n",
      "t=1 corresponds to class C2. We shall see later that this choice of label values is\n",
      "particularly convenient for probabilistic models. The general inference problem then\n",
      "involves determining the joint distribution p(x,Ck), or equivalently p(x,t), which\n",
      "gives us the most complete probabilistic description of the situation. Although this\n",
      "can be a very useful and informative quantity, in the end we must decide either to\n",
      "give treatment to the patient or not, and we would like this choice to be optimalin some appropriate sense (Duda and Hart, 1973). This is the decision step, and\n",
      "it is the subject of decision theory to tell us how to make optimal decisions given\n",
      "the appropriate probabilities. We shall see that the decision stage is generally very\n",
      "simple, even trivial, once we have solved the inference problem.\n",
      "Here we give an introduction to the key ideas of decision theory as required for1.5. Decision Theory 39\n",
      "the rest of the book. Further background, as well as more detailed accounts, can be\n",
      "found in Berger (1985) and Bather (2000).\n",
      "Before giving a more detailed analysis, let us ﬁrst consider informally how we\n",
      "might expect probabilities to play a role in making decisions. When we obtain the\n",
      "X-ray image xfor a new patient, our goal is to decide which of the two classes to\n",
      "assign to the image. We are interested in the probabilities of the two classes given\n",
      "the image, which are given by p(Ck|x). Using Bayes’ theorem, these probabilities\n",
      "can be expressed in the form\n",
      "p(Ck|x)=p(x|Ck)p(Ck)\n",
      "p(x). (1.77)\n",
      "Note that any of the quantities appearing in Bayes’ theorem can be obtained from\n",
      "the joint distribution p(x,Ck)by either marginalizing or conditioning with respect to\n",
      "the appropriate variables. We can now interpret p(Ck)as the prior probability for the\n",
      "classCk, andp(Ck|x)as the corresponding posterior probability. Thus p(C1)repre-\n",
      "sents the probability that a person has cancer, before we take the X-ray measurement.\n",
      "Similarly, p(C1|x)is the corresponding probability, revised using Bayes’ theorem in\n",
      "light of the information contained in the X-ray. If our aim is to minimize the chance\n",
      "of assigning xto the wrong class, then intuitively we would choose the class having\n",
      "the higher posterior probability. We now show that this intuition is correct, and wealso discuss more general criteria for making decisions.\n",
      "1.5.1 Minimizing the misclassiﬁcation rate\n",
      "Suppose that our goal is simply to make as few misclassiﬁcations as possible.\n",
      "We need a rule that assigns each value of xto one of the available classes. Such a\n",
      "rule will divide the input space into regions Rkcalled decision regions , one for each\n",
      "class, such that all points in Rkare assigned to class Ck. The boundaries between\n",
      "decision regions are called decision boundaries ordecision surfaces . Note that each\n",
      "decision region need not be contiguous but could comprise some number of disjoint\n",
      "regions. We shall encounter examples of decision boundaries and decision regions inlater chapters. In order to ﬁnd the optimal decision rule, consider ﬁrst of all the case\n",
      "of two classes, as in the cancer problem for instance. A mistake occurs when an input\n",
      "vector belonging to class C\n",
      "1is assigned to class C2or vice versa. The probability of\n",
      "this occurring is given by\n",
      "p(mistake) = p(x∈R1,C2)+p(x∈R2,C1)\n",
      "=∫\n",
      "R1p(x,C2)dx+∫\n",
      "R2p(x,C1)dx. (1.78)\n",
      "We are free to choose the decision rule that assigns each point xto one of the two\n",
      "classes. Clearly to minimize p(mistake) we should arrange that each xis assigned to\n",
      "whichever class has the smaller value of the integrand in (1.78). Thus, if p(x,C1)>\n",
      "p(x,C2)for a given value of x, then we should assign that xto class C1. From the\n",
      "product rule of probability we have p(x,Ck)=p(Ck|x)p(x). Because the factor\n",
      "p(x)is common to both terms, we can restate this result as saying that the minimum40 1. INTRODUCTION\n",
      "R1 R2x0ˆx\n",
      "p(x,C1)\n",
      "p(x,C2)\n",
      "x\n",
      "Figure 1.24 Schematic illustration of the joint probabilities p(x,Ck)for each of two classes plotted\n",
      "against x, together with the decision boundary x=bx. Values of x⩾bxare classiﬁed as\n",
      "classC2and hence belong to decision region R2, whereas points x< bxare classiﬁed\n",
      "asC1and belong to R1. Errors arise from the blue, green, and red regions, so that for\n",
      "x< bxthe errors are due to points from class C2being misclassiﬁed as C1(represented by\n",
      "the sum of the red and green regions), and conversely for points in the region x⩾bxthe\n",
      "errors are due to points from class C1being misclassiﬁed as C2(represented by the blue\n",
      "region). As we vary the location bxof the decision boundary, the combined areas of the\n",
      "blue and green regions remains constant, whereas the size of the red region varies. The\n",
      "optimal choice for bxis where the curves for p(x,C1)andp(x,C2)cross, corresponding to\n",
      "bx=x0, because in this case the red region disappears. This is equivalent to the minimum\n",
      "misclassiﬁcation rate decision rule, which assigns each value of xto the class having the\n",
      "higher posterior probability p(Ck|x).\n",
      "probability of making a mistake is obtained if each value of xis assigned to the class\n",
      "for which the posterior probability p(Ck|x)is largest. This result is illustrated for\n",
      "two classes, and a single input variable x, in Figure 1.24.\n",
      "For the more general case of Kclasses, it is slightly easier to maximize the\n",
      "probability of being correct, which is given by\n",
      "p(correct) =K∑\n",
      "k=1p(x∈R k,Ck)\n",
      "=K∑\n",
      "k=1∫\n",
      "Rkp(x,Ck)dx (1.79)\n",
      "which is maximized when the regions Rkare chosen such that each xis assigned\n",
      "to the class for which p(x,Ck)is largest. Again, using the product rule p(x,Ck)=\n",
      "p(Ck|x)p(x), and noting that the factor of p(x)is common to all terms, we see\n",
      "that each xshould be assigned to the class having the largest posterior probability\n",
      "p(Ck|x).1.5. Decision Theory 41\n",
      "Figure 1.25 An example of a loss matrix with ele-\n",
      "ments Lkjfor the cancer treatment problem. The rows\n",
      "correspond to the true class, whereas the columns cor-\n",
      "respond to the assignment of class made by our deci-sion criterion.(cancer normal\n",
      "cancer 0 1000\n",
      "normal 10)\n",
      "1.5.2 Minimizing the expected loss\n",
      "For many applications, our objective will be more complex than simply mini-\n",
      "mizing the number of misclassiﬁcations. Let us consider again the medical diagnosisproblem. We note that, if a patient who does not have cancer is incorrectly diagnosed\n",
      "as having cancer, the consequences may be some patient distress plus the need for\n",
      "further investigations. Conversely, if a patient with cancer is diagnosed as healthy,\n",
      "the result may be premature death due to lack of treatment. Thus the consequences\n",
      "of these two types of mistake can be dramatically different. It would clearly be betterto make fewer mistakes of the second kind, even if this was at the expense of making\n",
      "more mistakes of the ﬁrst kind.\n",
      "We can formalize such issues through the introduction of a loss function , also\n",
      "called a cost function , which is a single, overall measure of loss incurred in taking\n",
      "any of the available decisions or actions. Our goal is then to minimize the total loss\n",
      "incurred. Note that some authors consider instead a utility function , whose value\n",
      "they aim to maximize. These are equivalent concepts if we take the utility to be\n",
      "simply the negative of the loss, and throughout this text we shall use the loss function\n",
      "convention. Suppose that, for a new value of x, the true class is C\n",
      "kand that we assign\n",
      "xto class Cj(where jmay or may not be equal to k). In so doing, we incur some\n",
      "level of loss that we denote by Lkj, which we can view as the k,j element of a loss\n",
      "matrix . For instance, in our cancer example, we might have a loss matrix of the form\n",
      "shown in Figure 1.25. This particular loss matrix says that there is no loss incurred\n",
      "if the correct decision is made, there is a loss of 1if a healthy patient is diagnosed as\n",
      "having cancer, whereas there is a loss of 1000 if a patient having cancer is diagnosed\n",
      "as healthy.\n",
      "The optimal solution is the one which minimizes the loss function. However,\n",
      "the loss function depends on the true class, which is unknown. For a given input\n",
      "vectorx, our uncertainty in the true class is expressed through the joint probability\n",
      "distribution p(x,Ck)and so we seek instead to minimize the average loss, where the\n",
      "average is computed with respect to this distribution, which is given by\n",
      "E[L]=∑\n",
      "k∑\n",
      "j∫\n",
      "RjLkjp(x,Ck)dx. (1.80)\n",
      "Eachxcan be assigned independently to one of the decision regions Rj. Our goal\n",
      "is to choose the regions Rjin order to minimize the expected loss (1.80), which\n",
      "implies that for each xwe should minimize∑\n",
      "kLkjp(x,Ck). As before, we can use\n",
      "the product rule p(x,Ck)=p(Ck|x)p(x)to eliminate the common factor of p(x).\n",
      "Thus the decision rule that minimizes the expected loss is the one that assigns each42 1. INTRODUCTION\n",
      "Figure 1.26 Illustration of the reject option. Inputs\n",
      "xsuch that the larger of the two poste-\n",
      "rior probabilities is less than or equal to\n",
      "some threshold θwill be rejected.\n",
      "xp(C1|x) p(C2|x)\n",
      "0.01.0\n",
      "θ\n",
      "reject region\n",
      "newxto the class jfor which the quantity\n",
      "∑\n",
      "kLkjp(Ck|x) (1.81)\n",
      "is a minimum. This is clearly trivial to do, once we know the posterior class proba-\n",
      "bilities p(Ck|x).\n",
      "1.5.3 The reject option\n",
      "We have seen that classiﬁcation errors arise from the regions of input space\n",
      "where the largest of the posterior probabilities p(Ck|x)is signiﬁcantly less than unity,\n",
      "or equivalently where the joint distributions p(x,Ck)have comparable values. These\n",
      "are the regions where we are relatively uncertain about class membership. In some\n",
      "applications, it will be appropriate to avoid making decisions on the difﬁcult cases\n",
      "in anticipation of a lower error rate on those examples for which a classiﬁcation de-\n",
      "cision is made. This is known as the reject option . For example, in our hypothetical\n",
      "medical illustration, it may be appropriate to use an automatic system to classify\n",
      "those X-ray images for which there is little doubt as to the correct class, while leav-\n",
      "ing a human expert to classify the more ambiguous cases. We can achieve this by\n",
      "introducing a threshold θand rejecting those inputs xfor which the largest of the\n",
      "posterior probabilities p(Ck|x)is less than or equal to θ. This is illustrated for the\n",
      "case of two classes, and a single continuous input variable x, in Figure 1.26. Note\n",
      "that setting θ=1 will ensure that all examples are rejected, whereas if there are K\n",
      "classes then setting θ<1/K will ensure that no examples are rejected. Thus the\n",
      "fraction of examples that get rejected is controlled by the value of θ.\n",
      "We can easily extend the reject criterion to minimize the expected loss, when\n",
      "a loss matrix is given, taking account of the loss incurred when a reject decision is\n",
      "made. Exercise 1.24\n",
      "1.5.4 Inference and decision\n",
      "We have broken the classiﬁcation problem down into two separate stages, the\n",
      "inference stage in which we use training data to learn a model for p(Ck|x), and the1.5. Decision Theory 43\n",
      "subsequent decision stage in which we use these posterior probabilities to make op-\n",
      "timal class assignments. An alternative possibility would be to solve both problemstogether and simply learn a function that maps inputs xdirectly into decisions. Such\n",
      "a function is called a discriminant function .\n",
      "In fact, we can identify three distinct approaches to solving decision problems,\n",
      "all of which have been used in practical applications. These are given, in decreasing\n",
      "order of complexity, by:\n",
      "(a) First solve the inference problem of determining the class-conditional densities\n",
      "p(x|C\n",
      "k)for each class Ckindividually. Also separately infer the prior class\n",
      "probabilities p(Ck). Then use Bayes’ theorem in the form\n",
      "p(Ck|x)=p(x|Ck)p(Ck)\n",
      "p(x)(1.82)\n",
      "to ﬁnd the posterior class probabilities p(Ck|x). As usual, the denominator\n",
      "in Bayes’ theorem can be found in terms of the quantities appearing in the\n",
      "numerator, because\n",
      "p(x)=∑\n",
      "kp(x|Ck)p(Ck). (1.83)\n",
      "Equivalently, we can model the joint distribution p(x,Ck)directly and then\n",
      "normalize to obtain the posterior probabilities. Having found the posterior\n",
      "probabilities, we use decision theory to determine class membership for each\n",
      "new input x. Approaches that explicitly or implicitly model the distribution of\n",
      "inputs as well as outputs are known as generative models , because by sampling\n",
      "from them it is possible to generate synthetic data points in the input space.\n",
      "(b) First solve the inference problem of determining the posterior class probabilities\n",
      "p(Ck|x), and then subsequently use decision theory to assign each new xto\n",
      "one of the classes. Approaches that model the posterior probabilities directly\n",
      "are called discriminative models .\n",
      "(c) Find a function f(x), called a discriminant function, which maps each input x\n",
      "directly onto a class label. For instance, in the case of two-class problems,\n",
      "f(·)might be binary valued and such that f=0 represents class C1andf=1\n",
      "represents class C2. In this case, probabilities play no role.\n",
      "Let us consider the relative merits of these three alternatives. Approach (a) is the\n",
      "most demanding because it involves ﬁnding the joint distribution over both xand\n",
      "Ck. For many applications, xwill have high dimensionality, and consequently we\n",
      "may need a large training set in order to be able to determine the class-conditionaldensities to reasonable accuracy. Note that the class priors p(C\n",
      "k)can often be esti-\n",
      "mated simply from the fractions of the training set data points in each of the classes.\n",
      "One advantage of approach (a), however, is that it also allows the marginal density\n",
      "of data p(x)to be determined from (1.83). This can be useful for detecting new data\n",
      "points that have low probability under the model and for which the predictions may44 1. INTRODUCTION\n",
      "p(x|C1)p(x|C2)\n",
      "xclass densities\n",
      "0 0.2 0.4 0.6 0.8 1012345\n",
      "xp(C1|x) p(C2|x)\n",
      "0 0.2 0.4 0.6 0.8 100.20.40.60.811.2\n",
      "Figure 1.27 Example of the class-conditional densities for two classes having a single input variable x(left\n",
      "plot) together with the corresponding posterior probabilities (right plot). Note that the left-hand mode of the\n",
      "class-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The\n",
      "vertical green line in the right plot shows the decision boundary in xthat gives the minimum misclassiﬁcation\n",
      "rate.\n",
      "be of low accuracy, which is known as outlier detection ornovelty detection (Bishop,\n",
      "1994; Tarassenko, 1995).\n",
      "However, if we only wish to make classiﬁcation decisions, then it can be waste-\n",
      "ful of computational resources, and excessively demanding of data, to ﬁnd the joint\n",
      "distribution p(x,Ck)when in fact we only really need the posterior probabilities\n",
      "p(Ck|x), which can be obtained directly through approach (b). Indeed, the class-\n",
      "conditional densities may contain a lot of structure that has little effect on the pos-\n",
      "terior probabilities, as illustrated in Figure 1.27. There has been much interest in\n",
      "exploring the relative merits of generative and discriminative approaches to machine\n",
      "learning, and in ﬁnding ways to combine them (Jebara, 2004; Lasserre et al. , 2006).\n",
      "An even simpler approach is (c) in which we use the training data to ﬁnd a\n",
      "discriminant function f(x)that maps each xdirectly onto a class label, thereby\n",
      "combining the inference and decision stages into a single learning problem. In the\n",
      "example of Figure 1.27, this would correspond to ﬁnding the value of xshown by\n",
      "the vertical green line, because this is the decision boundary giving the minimum\n",
      "probability of misclassiﬁcation.\n",
      "With option (c), however, we no longer have access to the posterior probabilities\n",
      "p(Ck|x). There are many powerful reasons for wanting to compute the posterior\n",
      "probabilities, even if we subsequently use them to make decisions. These include:\n",
      "Minimizing risk. Consider a problem in which the elements of the loss matrix are\n",
      "subjected to revision from time to time (such as might occur in a ﬁnancial1.5. Decision Theory 45\n",
      "application). If we know the posterior probabilities, we can trivially revise the\n",
      "minimum risk decision criterion by modifying (1.81) appropriately. If we haveonly a discriminant function, then any change to the loss matrix would require\n",
      "that we return to the training data and solve the classiﬁcation problem afresh.\n",
      "Reject option. Posterior probabilities allow us to determine a rejection criterion that\n",
      "will minimize the misclassiﬁcation rate, or more generally the expected loss,for a given fraction of rejected data points.\n",
      "Compensating for class priors. Consider our medical X-ray problem again, and\n",
      "suppose that we have collected a large number of X-ray images from the gen-\n",
      "eral population for use as training data in order to build an automated screeningsystem. Because cancer is rare amongst the general population, we might ﬁnd\n",
      "that, say, only 1 in every 1,000 examples corresponds to the presence of can-\n",
      "cer. If we used such a data set to train an adaptive model, we could run intosevere difﬁculties due to the small proportion of the cancer class. For instance,\n",
      "a classiﬁer that assigned every point to the normal class would already achieve\n",
      "99.9% accuracy and it would be difﬁcult to avoid this trivial solution. Also,even a large data set will contain very few examples of X-ray images corre-\n",
      "sponding to cancer, and so the learning algorithm will not be exposed to a\n",
      "broad range of examples of such images and hence is not likely to generalizewell. A balanced data set in which we have selected equal numbers of exam-\n",
      "ples from each of the classes would allow us to ﬁnd a more accurate model.\n",
      "However, we then have to compensate for the effects of our modiﬁcations to\n",
      "the training data. Suppose we have used such a modiﬁed data set and found\n",
      "models for the posterior probabilities. From Bayes’ theorem (1.82), we see thatthe posterior probabilities are proportional to the prior probabilities, which we\n",
      "can interpret as the fractions of points in each class. We can therefore simply\n",
      "take the posterior probabilities obtained from our artiﬁcially balanced data setand ﬁrst divide by the class fractions in that data set and then multiply by the\n",
      "class fractions in the population to which we wish to apply the model. Finally,\n",
      "we need to normalize to ensure that the new posterior probabilities sum to one.Note that this procedure cannot be applied if we have learned a discriminant\n",
      "function directly instead of determining posterior probabilities.\n",
      "Combining models. For complex applications, we may wish to break the problem\n",
      "into a number of smaller subproblems each of which can be tackled by a sep-arate module. For example, in our hypothetical medical diagnosis problem,\n",
      "we may have information available from, say, blood tests as well as X-ray im-\n",
      "ages. Rather than combine all of this heterogeneous information into one hugeinput space, it may be more effective to build one system to interpret the X-\n",
      "ray images and a different one to interpret the blood data. As long as each of\n",
      "the two models gives posterior probabilities for the classes, we can combinethe outputs systematically using the rules of probability. One simple way to\n",
      "do this is to assume that, for each class separately, the distributions of inputs\n",
      "for the X-ray images, denoted by x\n",
      "I, and the blood data, denoted by xB,a r e46 1. INTRODUCTION\n",
      "independent, so that\n",
      "p(xI,xB|Ck)=p(xI|Ck)p(xB|Ck). (1.84)\n",
      "This is an example of conditional independence property, because the indepen- Section 8.2\n",
      "dence holds when the distribution is conditioned on the class Ck. The posterior\n",
      "probability, given both the X-ray and blood data, is then given by\n",
      "p(Ck|xI,xB)∝p(xI,xB|Ck)p(Ck)\n",
      "∝p(xI|Ck)p(xB|Ck)p(Ck)\n",
      "∝p(Ck|xI)p(Ck|xB)\n",
      "p(Ck)(1.85)\n",
      "Thus we need the class prior probabilities p(Ck), which we can easily estimate\n",
      "from the fractions of data points in each class, and then we need to normalize\n",
      "the resulting posterior probabilities so they sum to one. The particular condi-tional independence assumption (1.84) is an example of the naive Bayes model . Section 8.2.2\n",
      "Note that the joint marginal distribution p(x\n",
      "I,xB)will typically not factorize\n",
      "under this model. We shall see in later chapters how to construct models forcombining data that do not require the conditional independence assumption\n",
      "(1.84).\n",
      "1.5.5 Loss functions for regression\n",
      "So far, we have discussed decision theory in the context of classiﬁcation prob-\n",
      "lems. We now turn to the case of regression problems, such as the curve ﬁtting\n",
      "example discussed earlier. The decision stage consists of choosing a speciﬁc esti- Section 1.1\n",
      "matey(x)of the value of tfor each input x. Suppose that in doing so, we incur a\n",
      "lossL(t, y(x)). The average, or expected, loss is then given by\n",
      "E[L]=∫∫\n",
      "L(t, y(x))p(x,t)dxdt. (1.86)\n",
      "A common choice of loss function in regression problems is the squared loss given\n",
      "byL(t, y(x)) ={y(x)−t}2. In this case, the expected loss can be written\n",
      "E[L]=∫∫\n",
      "{y(x)−t}2p(x,t)dxdt. (1.87)\n",
      "Our goal is to choose y(x)so as to minimize E[L]. If we assume a completely\n",
      "ﬂexible function y(x), we can do this formally using the calculus of variations to Appendix D\n",
      "give\n",
      "δE[L]\n",
      "δy(x)=2∫\n",
      "{y(x)−t}p(x,t)dt=0. (1.88)\n",
      "Solving for y(x), and using the sum and product rules of probability, we obtain\n",
      "y(x)=∫\n",
      "tp(x,t)dt\n",
      "p(x)=∫\n",
      "tp(t|x)dt=Et[t|x] (1.89)1.5. Decision Theory 47\n",
      "Figure 1.28 The regression function y(x),\n",
      "which minimizes the expected\n",
      "squared loss, is given by the\n",
      "mean of the conditional distri-\n",
      "bution p(t|x).t\n",
      "x x0y(x0)y(x)\n",
      "p(t|x0)\n",
      "which is the conditional average of tconditioned on xand is known as the regression\n",
      "function . This result is illustrated in Figure 1.28. It can readily be extended to mul-\n",
      "tiple target variables represented by the vector t, in which case the optimal solution\n",
      "is the conditional average y(x)= Et[t|x]. Exercise 1.25\n",
      "We can also derive this result in a slightly different way, which will also shed\n",
      "light on the nature of the regression problem. Armed with the knowledge that the\n",
      "optimal solution is the conditional expectation, we can expand the square term as\n",
      "follows\n",
      "{y(x)−t}2={y(x)−E[t|x]+ E[t|x]−t}2\n",
      "={y(x)−E[t|x]}2+2{y(x)−E[t|x]}{E[t|x]−t}+{E[t|x]−t}2\n",
      "where, to keep the notation uncluttered, we use E[t|x]to denote Et[t|x]. Substituting\n",
      "into the loss function and performing the integral over t, we see that the cross-term\n",
      "vanishes and we obtain an expression for the loss function in the form\n",
      "E[L]=∫\n",
      "{y(x)−E[t|x]}2p(x)dx+∫\n",
      "{E[t|x]−t}2p(x)dx. (1.90)\n",
      "The function y(x)we seek to determine enters only in the ﬁrst term, which will be\n",
      "minimized when y(x)is equal to E[t|x], in which case this term will vanish. This\n",
      "is simply the result that we derived previously and that shows that the optimal least\n",
      "squares predictor is given by the conditional mean. The second term is the variance\n",
      "of the distribution of t, averaged over x. It represents the intrinsic variability of\n",
      "the target data and can be regarded as noise. Because it is independent of y(x),i t\n",
      "represents the irreducible minimum value of the loss function.\n",
      "As with the classiﬁcation problem, we can either determine the appropriate prob-\n",
      "abilities and then use these to make optimal decisions, or we can build models that\n",
      "make decisions directly. Indeed, we can identify three distinct approaches to solving\n",
      "regression problems given, in order of decreasing complexity, by:\n",
      "(a) First solve the inference problem of determining the joint density p(x,t). Then\n",
      "normalize to ﬁnd the conditional density p(t|x), and ﬁnally marginalize to ﬁnd\n",
      "the conditional mean given by (1.89).48 1. INTRODUCTION\n",
      "(b) First solve the inference problem of determining the conditional density p(t|x),\n",
      "and then subsequently marginalize to ﬁnd the conditional mean given by (1.89).\n",
      "(c) Find a regression function y(x)directly from the training data.\n",
      "The relative merits of these three approaches follow the same lines as for classiﬁca-\n",
      "tion problems above.\n",
      "The squared loss is not the only possible choice of loss function for regression.\n",
      "Indeed, there are situations in which squared loss can lead to very poor results and\n",
      "where we need to develop more sophisticated approaches. An important example\n",
      "concerns situations in which the conditional distribution p(t|x)is multimodal, as\n",
      "often arises in the solution of inverse problems. Here we consider brieﬂy one simple Section 5.6\n",
      "generalization of the squared loss, called the Minkowski loss, whose expectation is\n",
      "given by\n",
      "E[Lq]=∫∫\n",
      "|y(x)−t|qp(x,t)dxdt (1.91)\n",
      "which reduces to the expected squared loss for q=2 . The function |y−t|qis\n",
      "plotted against y−tfor various values of qin Figure 1.29. The minimum of E[Lq]\n",
      "is given by the conditional mean for q=2 , the conditional median for q=1 , and\n",
      "the conditional mode for q→0. Exercise 1.27\n",
      "1.6. Information Theory\n",
      "In this chapter, we have discussed a variety of concepts from probability theory and\n",
      "decision theory that will form the foundations for much of the subsequent discussion\n",
      "in this book. We close this chapter by introducing some additional concepts fromthe ﬁeld of information theory, which will also prove useful in our development of\n",
      "pattern recognition and machine learning techniques. Again, we shall focus only on\n",
      "the key concepts, and we refer the reader elsewhere for more detailed discussions(Viterbi and Omura, 1979; Cover and Thomas, 1991; MacKay, 2003) .\n",
      "We begin by considering a discrete random variable xand we ask how much\n",
      "information is received when we observe a speciﬁc value for this variable. Theamount of information can be viewed as the ‘degree of surprise’ on learning the\n",
      "value of x. If we are told that a highly improbable event has just occurred, we will\n",
      "have received more information than if we were told that some very likely eventhas just occurred, and if we knew that the event was certain to happen we would\n",
      "receive no information. Our measure of information content will therefore depend\n",
      "on the probability distribution p(x), and we therefore look for a quantity h(x)that\n",
      "is a monotonic function of the probability p(x)and that expresses the information\n",
      "content. The form of h(·)can be found by noting that if we have two events x\n",
      "andythat are unrelated, then the information gain from observing both of them\n",
      "should be the sum of the information gained from each of them separately, so that\n",
      "h(x, y)=h(x)+h(y). Two unrelated events will be statistically independent and\n",
      "sop(x, y)=p(x)p(y). From these two relationships, it is easily shown that h(x)\n",
      "must be given by the logarithm of p(x)and so we have Exercise 1.281.6. Information Theory 49\n",
      "y−t|y−t|qq=0.3\n",
      "−2 −1 0 1 2012\n",
      "y−t|y−t|qq=1\n",
      "−2 −1 0 1 2012\n",
      "y−t|y−t|qq=2\n",
      "−2 −1 0 1 2012\n",
      "y−t|y−t|qq=1 0\n",
      "−2 −1 0 1 2012\n",
      "Figure 1.29 Plots of the quantity Lq=|y−t|qfor various values of q.\n",
      "h(x)=−log2p(x) (1.92)\n",
      "where the negative sign ensures that information is positive or zero. Note that low\n",
      "probability events xcorrespond to high information content. The choice of basis\n",
      "for the logarithm is arbitrary, and for the moment we shall adopt the convention\n",
      "prevalent in information theory of using logarithms to the base of 2. In this case, as\n",
      "we shall see shortly, the units of h(x)are bits (‘binary digits’).\n",
      "Now suppose that a sender wishes to transmit the value of a random variable to\n",
      "a receiver. The average amount of information that they transmit in the process is\n",
      "obtained by taking the expectation of (1.92) with respect to the distribution p(x)and\n",
      "is given by\n",
      "H[x]=−∑\n",
      "xp(x) log2p(x). (1.93)\n",
      "This important quantity is called the entropy of the random variable x. Note that\n",
      "limp→0plnp=0 and so we shall take p(x)l np(x)=0 whenever we encounter a\n",
      "value for xsuch that p(x)=0 .\n",
      "So far we have given a rather heuristic motivation for the deﬁnition of informa-50 1. INTRODUCTION\n",
      "tion (1.92) and the corresponding entropy (1.93). We now show that these deﬁnitions\n",
      "indeed possess useful properties. Consider a random variable xhaving 8 possible\n",
      "states, each of which is equally likely. In order to communicate the value of xto\n",
      "a receiver, we would need to transmit a message of length 3 bits. Notice that the\n",
      "entropy of this variable is given by\n",
      "H[x]=−8×1\n",
      "8log21\n",
      "8= 3 bits .\n",
      "Now consider an example (Cover and Thomas, 1991) of a variable having 8pos-\n",
      "sible states {a, b, c, d, e, f, g, h }for which the respective probabilities are given by\n",
      "(1\n",
      "2,1\n",
      "4,1\n",
      "8,1\n",
      "16,1\n",
      "64,1\n",
      "64,1\n",
      "64,1\n",
      "64). The entropy in this case is given by\n",
      "H[x]=−1\n",
      "2log21\n",
      "2−1\n",
      "4log21\n",
      "4−1\n",
      "8log21\n",
      "8−1\n",
      "16log21\n",
      "16−4\n",
      "64log21\n",
      "64= 2 bits .\n",
      "We see that the nonuniform distribution has a smaller entropy than the uniform one,\n",
      "and we shall gain some insight into this shortly when we discuss the interpretation of\n",
      "entropy in terms of disorder. For the moment, let us consider how we would transmitthe identity of the variable’s state to a receiver. We could do this, as before, using\n",
      "a 3-bit number. However, we can take advantage of the nonuniform distribution by\n",
      "using shorter codes for the more probable events, at the expense of longer codes forthe less probable events, in the hope of getting a shorter average code length. This\n",
      "can be done by representing the states {a, b, c, d, e, f, g, h }using, for instance, the\n",
      "following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111.The average length of the code that has to be transmitted is then\n",
      "average code length =1\n",
      "2×1+1\n",
      "4×2+1\n",
      "8×3+1\n",
      "16×4+4×1\n",
      "64×6 = 2 bits\n",
      "which again is the same as the entropy of the random variable. Note that shorter code\n",
      "strings cannot be used because it must be possible to disambiguate a concatenationof such strings into its component parts. For instance, 11001110 decodes uniquely\n",
      "into the state sequence c,a,d.\n",
      "This relation between entropy and shortest coding length is a general one. The\n",
      "noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound\n",
      "on the number of bits needed to transmit the state of a random variable.\n",
      "From now on, we shall switch to the use of natural logarithms in deﬁning en-\n",
      "tropy, as this will provide a more convenient link with ideas elsewhere in this book.\n",
      "In this case, the entropy is measured in units of ‘nats’ instead of bits, which differ\n",
      "simply by a factor of ln2 .\n",
      "We have introduced the concept of entropy in terms of the average amount of\n",
      "information needed to specify the state of a random variable. In fact, the concept ofentropy has much earlier origins in physics where it was introduced in the context\n",
      "of equilibrium thermodynamics and later given a deeper interpretation as a measure\n",
      "of disorder through developments in statistical mechanics. We can understand thisalternative view of entropy by considering a set of Nidentical objects that are to be\n",
      "divided amongst a set of bins, such that there are n\n",
      "iobjects in the ithbin. Consider1.6. Information Theory 51\n",
      "the number of different ways of allocating the objects to the bins. There are N\n",
      "ways to choose the ﬁrst object, (N−1)ways to choose the second object, and\n",
      "so on, leading to a total of N!ways to allocate all Nobjects to the bins, where N!\n",
      "(pronounced ‘factorial N’) denotes the product N×(N−1)×···× 2×1. However,\n",
      "we don’t wish to distinguish between rearrangements of objects within each bin. Inthei\n",
      "thbin there are ni!ways of reordering the objects, and so the total number of\n",
      "ways of allocating the Nobjects to the bins is given by\n",
      "W=N!∏\n",
      "ini!(1.94)\n",
      "which is called the multiplicity . The entropy is then deﬁned as the logarithm of the\n",
      "multiplicity scaled by an appropriate constant\n",
      "H=1\n",
      "NlnW=1\n",
      "NlnN!−1\n",
      "N∑\n",
      "ilnni!. (1.95)\n",
      "We now consider the limit N→∞ , in which the fractions ni/N are held ﬁxed, and\n",
      "apply Stirling’s approximation\n",
      "lnN!≃NlnN−N (1.96)\n",
      "which gives\n",
      "H=−lim\n",
      "N→∞∑\n",
      "i(ni\n",
      "N)\n",
      "ln(ni\n",
      "N)\n",
      "=−∑\n",
      "ipilnpi (1.97)\n",
      "where we have used∑\n",
      "ini=N. Here pi= lim N→∞(ni/N)is the probability\n",
      "of an object being assigned to the ithbin. In physics terminology, the speciﬁc ar-\n",
      "rangements of objects in the bins is called a microstate , and the overall distribution\n",
      "of occupation numbers, expressed through the ratios ni/N, is called a macrostate .\n",
      "The multiplicity Wis also known as the weight of the macrostate.\n",
      "We can interpret the bins as the states xiof a discrete random variable X, where\n",
      "p(X=xi)=pi. The entropy of the random variable Xis then\n",
      "H[p]=−∑\n",
      "ip(xi)l np(xi). (1.98)\n",
      "Distributions p(xi)that are sharply peaked around a few values will have a relatively\n",
      "low entropy, whereas those that are spread more evenly across many values will\n",
      "have higher entropy, as illustrated in Figure 1.30. Because 0⩽pi⩽1, the entropy\n",
      "is nonnegative, and it will equal its minimum value of 0 when one of the pi=\n",
      "1and all other pj̸=i=0 . The maximum entropy conﬁguration can be found by\n",
      "maximizing Husing a Lagrange multiplier to enforce the normalization constraint Appendix E\n",
      "on the probabilities. Thus we maximize\n",
      "˜H=−∑\n",
      "ip(xi)l np(xi)+λ(∑\n",
      "ip(xi)−1)\n",
      "(1.99)52 1. INTRODUCTIONprobabilitiesH = 1.77\n",
      "00.250.5\n",
      "probabilitiesH = 3.09\n",
      "00.250.5\n",
      "Figure 1.30 Histograms of two probability distributions over 30bins illustrating the higher value of the entropy\n",
      "Hfor the broader distribution. The largest entropy would arise from a uniform distribution that would give H=\n",
      "−ln(1/30) = 3 .40.\n",
      "from which we ﬁnd that all of the p(xi)are equal and are given by p(xi)=1/M\n",
      "where Mis the total number of states xi. The corresponding value of the entropy\n",
      "is then H=l n M. This result can also be derived from Jensen’s inequality (to be\n",
      "discussed shortly). To verify that the stationary point is indeed a maximum, we can Exercise 1.29\n",
      "evaluate the second derivative of the entropy, which gives\n",
      "∂˜H\n",
      "∂p(xi)∂p(xj)=−Iij1\n",
      "pi(1.100)\n",
      "where Iijare the elements of the identity matrix.\n",
      "We can extend the deﬁnition of entropy to include distributions p(x)over con-\n",
      "tinuous variables xas follows. First divide xinto bins of width ∆. Then, assuming\n",
      "p(x)is continuous, the mean value theorem (Weisstein, 1999) tells us that, for each\n",
      "such bin, there must exist a value xisuch that\n",
      "∫(i+1)∆\n",
      "i∆p(x)dx=p(xi)∆. (1.101)\n",
      "We can now quantize the continuous variable xby assigning any value xto the value\n",
      "xiwhenever xfalls in the ithbin. The probability of observing the value xiis then\n",
      "p(xi)∆. This gives a discrete distribution for which the entropy takes the form\n",
      "H∆=−∑\n",
      "ip(xi)∆ ln( p(xi)∆) = −∑\n",
      "ip(xi)∆ lnp(xi)−ln∆ (1.102)\n",
      "where we have used∑\n",
      "ip(xi)∆ = 1 , which follows from (1.101). We now omit\n",
      "the second term −ln∆ on the right-hand side of (1.102) and then consider the limit1.6. Information Theory 53\n",
      "∆→0. The ﬁrst term on the right-hand side of (1.102) will approach the integral of\n",
      "p(x)l np(x)in this limit so that\n",
      "lim\n",
      "∆→0{∑\n",
      "ip(xi)∆ lnp(xi)}\n",
      "=−∫\n",
      "p(x)l np(x)dx (1.103)\n",
      "where the quantity on the right-hand side is called the differential entropy . We see\n",
      "that the discrete and continuous forms of the entropy differ by a quantity ln∆ , which\n",
      "diverges in the limit ∆→0. This reﬂects the fact that to specify a continuous\n",
      "variable very precisely requires a large number of bits. For a density deﬁned overmultiple continuous variables, denoted collectively by the vector x, the differential\n",
      "entropy is given by\n",
      "H[x]=−∫\n",
      "p(x)l np(x)dx. (1.104)\n",
      "In the case of discrete distributions, we saw that the maximum entropy con-\n",
      "ﬁguration corresponded to an equal distribution of probabilities across the possible\n",
      "states of the variable. Let us now consider the maximum entropy conﬁguration fora continuous variable. In order for this maximum to be well deﬁned, it will be nec-\n",
      "essary to constrain the ﬁrst and second moments of p(x)as well as preserving the\n",
      "normalization constraint. We therefore maximize the differential entropy with the\n",
      "Ludwig Boltzmann\n",
      "1844–1906\n",
      "Ludwig Eduard Boltzmann was an\n",
      "Austrian physicist who created theﬁeld of statistical mechanics. Priorto Boltzmann, the concept of en-tropy was already known fromclassical thermodynamics where it\n",
      "quantiﬁes the fact that when we take energy from asystem, not all of that energy is typically availableto do useful work. Boltzmann showed that the ther-modynamic entropy S, a macroscopic quantity, could\n",
      "be related to the statistical properties at the micro-\n",
      "scopic level. This is expressed through the famous\n",
      "equation S=klnWin which Wrepresents the\n",
      "number of possible microstates in a macrostate, andk≃1.38×10\n",
      "−23(in units of Joules per Kelvin) is\n",
      "known as Boltzmann’s constant. Boltzmann’s ideaswere disputed by many scientists of they day. One dif-ﬁculty they saw arose from the second law of thermo-dynamics, which states that the entropy of a closed\n",
      "system tends to increase with time. By contrast, atthe microscopic level the classical Newtonian equa-tions of physics are reversible, and so they found itdifﬁcult to see how the latter could explain the for-mer. They didn’t fully appreciate Boltzmann’s argu-ments, which were statistical in nature and which con-cluded not that entropy could never decrease overtime but simply that with overwhelming probability itwould generally increase. Boltzmann even had a long-running dispute with the editor of the leading Germanphysics journal who refused to let him refer to atomsand molecules as anything other than convenient the-oretical constructs. The continued attacks on his worklead to bouts of depression, and eventually he com-mitted suicide. Shortly after Boltzmann’s death, newexperiments by Perrin on colloidal suspensions veri-ﬁed his theories and conﬁrmed the value of the Boltz-mann constant. The equation S=klnWis carved on\n",
      "Boltzmann’s tombstone.54 1. INTRODUCTION\n",
      "three constraints\n",
      "∫∞\n",
      "−∞p(x)dx=1 (1.105)\n",
      "∫∞\n",
      "−∞xp(x)dx=µ (1.106)\n",
      "∫∞\n",
      "−∞(x−µ)2p(x)dx=σ2. (1.107)\n",
      "The constrained maximization can be performed using Lagrange multipliers so that Appendix E\n",
      "we maximize the following functional with respect to p(x)\n",
      "−∫∞\n",
      "−∞p(x)l np(x)dx+λ1(∫∞\n",
      "−∞p(x)dx−1)\n",
      "+λ2(∫∞\n",
      "−∞xp(x)dx−µ)\n",
      "+λ3(∫∞\n",
      "−∞(x−µ)2p(x)dx−σ2)\n",
      ".\n",
      "Using the calculus of variations, we set the derivative of this functional to zero giving Appendix D\n",
      "p(x)=e x p{\n",
      "−1+λ1+λ2x+λ3(x−µ)2}\n",
      ". (1.108)\n",
      "The Lagrange multipliers can be found by back substitution of this result into the\n",
      "three constraint equations, leading ﬁnally to the result Exercise 1.34\n",
      "p(x)=1\n",
      "(2πσ2)1/2exp{\n",
      "−(x−µ)2\n",
      "2σ2}\n",
      "(1.109)\n",
      "and so the distribution that maximizes the differential entropy is the Gaussian. Note\n",
      "that we did not constrain the distribution to be nonnegative when we maximized the\n",
      "entropy. However, because the resulting distribution is indeed nonnegative, we seewith hindsight that such a constraint is not necessary.\n",
      "If we evaluate the differential entropy of the Gaussian, we obtain Exercise 1.35\n",
      "H[x]=1\n",
      "2{\n",
      "1+l n ( 2 πσ2)}\n",
      ". (1.110)\n",
      "Thus we see again that the entropy increases as the distribution becomes broader,\n",
      "i.e., as σ2increases. This result also shows that the differential entropy, unlike the\n",
      "discrete entropy, can be negative, because H(x)<0in (1.110) for σ2<1/(2πe).\n",
      "Suppose we have a joint distribution p(x,y)from which we draw pairs of values\n",
      "ofxandy. If a value of xis already known, then the additional information needed\n",
      "to specify the corresponding value of yis given by −lnp(y|x). Thus the average\n",
      "additional information needed to specify ycan be written as\n",
      "H[y|x]=−∫∫\n",
      "p(y,x)l np(y|x)dydx (1.111)1.6. Information Theory 55\n",
      "which is called the conditional entropy ofygivenx. It is easily seen, using the\n",
      "product rule, that the conditional entropy satisﬁes the relation Exercise 1.37\n",
      "H[x,y]=H [y|x]+H [x] (1.112)\n",
      "where H[x,y]is the differential entropy of p(x,y)andH[x]is the differential en-\n",
      "tropy of the marginal distribution p(x). Thus the information needed to describe x\n",
      "andyis given by the sum of the information needed to describe xalone plus the\n",
      "additional information required to specify ygivenx.\n",
      "1.6.1 Relative entropy and mutual information\n",
      "So far in this section, we have introduced a number of concepts from information\n",
      "theory, including the key notion of entropy. We now start to relate these ideas topattern recognition. Consider some unknown distribution p(x), and suppose that\n",
      "we have modelled this using an approximating distribution q(x). I fw eu s e q(x)to\n",
      "construct a coding scheme for the purpose of transmitting values of xto a receiver,\n",
      "then the average additional amount of information (in nats) required to specify the\n",
      "value of x(assuming we choose an efﬁcient coding scheme) as a result of using q(x)\n",
      "instead of the true distribution p(x)is given by\n",
      "KL(p∥q)= −∫\n",
      "p(x)l nq(x)dx−(\n",
      "−∫\n",
      "p(x)l np(x)dx)\n",
      "=−∫\n",
      "p(x)l n{q(x)\n",
      "p(x)}\n",
      "dx. (1.113)\n",
      "This is known as the relative entropy orKullback-Leibler divergence ,o r KL diver-\n",
      "gence (Kullback and Leibler, 1951), between the distributions p(x)andq(x). Note\n",
      "that it is not a symmetrical quantity, that is to say KL(p∥q)̸≡KL(q∥p).\n",
      "We now show that the Kullback-Leibler divergence satisﬁes KL(p∥q)⩾0with\n",
      "equality if, and only if, p(x)=q(x). To do this we ﬁrst introduce the concept of\n",
      "convex functions. A function f(x)is said to be convex if it has the property that\n",
      "every chord lies on or above the function, as shown in Figure 1.31. Any value of x\n",
      "in the interval from x=atox=bcan be written in the form λa+( 1−λ)bwhere\n",
      "0⩽λ⩽1. The corresponding point on the chord is given by λf(a)+( 1−λ)f(b),\n",
      "Claude Shannon\n",
      "1916–2001\n",
      "After graduating from Michigan and\n",
      "MIT, Shannon joined the AT&T BellTelephone laboratories in 1941. Hispaper ‘A Mathematical Theory ofCommunication’ published in the\n",
      "Bell System Technical Journal in\n",
      "1948 laid the foundations for modern information the-ory. This paper introduced the word ‘bit’, and his con-\n",
      "cept that information could be sent as a stream of 1sand 0s paved the way for the communications revo-lution. It is said that von Neumann recommended toShannon that he use the term entropy, not only be-cause of its similarity to the quantity used in physics,but also because “nobody knows what entropy reallyis, so in any discussion you will always have an advan-tage”.56 1. INTRODUCTION\n",
      "Figure 1.31 A convex function f(x)is one for which ev-\n",
      "ery chord (shown in blue) lies on or above\n",
      "the function (shown in red).\n",
      "x a b xλchord\n",
      "xλf(x)\n",
      "and the corresponding value of the function is f(λa+( 1−λ)b). Convexity then\n",
      "implies\n",
      "f(λa+( 1−λ)b)⩽λf(a)+( 1 −λ)f(b). (1.114)\n",
      "This is equivalent to the requirement that the second derivative of the function be\n",
      "everywhere positive. Examples of convex functions are xlnx(forx>0) andx2.A Exercise 1.36\n",
      "function is called strictly convex if the equality is satisﬁed only for λ=0 andλ=1 .\n",
      "If a function has the opposite property, namely that every chord lies on or below the\n",
      "function, it is called concave , with a corresponding deﬁnition for strictly concave .I f\n",
      "a function f(x)is convex, then −f(x)will be concave.\n",
      "Using the technique of proof by induction, we can show from (1.114) that a Exercise 1.38\n",
      "convex function f(x)satisﬁes\n",
      "f(M∑\n",
      "i=1λixi)\n",
      "⩽M∑\n",
      "i=1λif(xi) (1.115)\n",
      "where λi⩾0and∑\n",
      "iλi=1 , for any set of points {xi}. The result (1.115) is\n",
      "known as Jensen’s inequality . If we interpret the λias the probability distribution\n",
      "over a discrete variable xtaking the values {xi}, then (1.115) can be written\n",
      "f(E[x])⩽E[f(x)] (1.116)\n",
      "where E[·]denotes the expectation. For continuous variables, Jensen’s inequality\n",
      "takes the form\n",
      "f(∫\n",
      "xp(x)dx)\n",
      "⩽∫\n",
      "f(x)p(x)dx. (1.117)\n",
      "We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler\n",
      "divergence (1.113) to give\n",
      "KL(p∥q)=−∫\n",
      "p(x)l n{q(x)\n",
      "p(x)}\n",
      "dx⩾−ln∫\n",
      "q(x)dx=0 (1.118)1.6. Information Theory 57\n",
      "where we have used the fact that −lnxis a convex function, together with the nor-\n",
      "malization condition∫\n",
      "q(x)dx=1 . In fact, −lnxis a strictly convex function,\n",
      "so the equality will hold if, and only if, q(x)=p(x)for allx. Thus we can in-\n",
      "terpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two\n",
      "distributions p(x)andq(x).\n",
      "We see that there is an intimate relationship between data compression and den-\n",
      "sity estimation (i.e., the problem of modelling an unknown probability distribution)\n",
      "because the most efﬁcient compression is achieved when we know the true distri-bution. If we use a distribution that is different from the true one, then we must\n",
      "necessarily have a less efﬁcient coding, and on average the additional information\n",
      "that must be transmitted is (at least) equal to the Kullback-Leibler divergence be-\n",
      "tween the two distributions.\n",
      "Suppose that data is being generated from an unknown distribution p(x)that we\n",
      "wish to model. We can try to approximate this distribution using some parametric\n",
      "distribution q(x|θ), governed by a set of adjustable parameters θ, for example a\n",
      "multivariate Gaussian. One way to determine θis to minimize the Kullback-Leibler\n",
      "divergence between p(x)andq(x|θ)with respect to θ. We cannot do this directly\n",
      "because we don’t know p(x). Suppose, however, that we have observed a ﬁnite set\n",
      "of training points x\n",
      "n, forn=1,...,N , drawn from p(x). Then the expectation\n",
      "with respect to p(x)can be approximated by a ﬁnite sum over these points, using\n",
      "(1.35), so that\n",
      "KL(p∥q)≃N∑\n",
      "n=1{−lnq(xn|θ)+l n p(xn)}. (1.119)\n",
      "The second term on the right-hand side of (1.119) is independent of θ, and the ﬁrst\n",
      "term is the negative log likelihood function for θunder the distribution q(x|θ)eval-\n",
      "uated using the training set. Thus we see that minimizing this Kullback-Leiblerdivergence is equivalent to maximizing the likelihood function.\n",
      "Now consider the joint distribution between two sets of variables xandygiven\n",
      "byp(x,y). If the sets of variables are independent, then their joint distribution will\n",
      "factorize into the product of their marginals p(x,y)=p(x)p(y). If the variables are\n",
      "not independent, we can gain some idea of whether they are ‘close’ to being indepen-\n",
      "dent by considering the Kullback-Leibler divergence between the joint distributionand the product of the marginals, given by\n",
      "I[x,y]≡KL(p(x,y)∥p(x)p(y))\n",
      "=−∫∫\n",
      "p(x,y)l n(p(x)p(y)\n",
      "p(x,y))\n",
      "dxdy (1.120)\n",
      "which is called the mutual information between the variables xandy. From the\n",
      "properties of the Kullback-Leibler divergence, we see that I(x,y)⩾0with equal-\n",
      "ity if, and only if, xandyare independent. Using the sum and product rules of\n",
      "probability, we see that the mutual information is related to the conditional entropy\n",
      "through Exercise 1.41\n",
      "I[x,y]=H [x]−H[x|y]=H [y]−H[y|x]. (1.121)58 1. INTRODUCTION\n",
      "Thus we can view the mutual information as the reduction in the uncertainty about x\n",
      "by virtue of being told the value of y(or vice versa). From a Bayesian perspective,\n",
      "we can view p(x)as the prior distribution for xandp(x|y)as the posterior distribu-\n",
      "tion after we have observed new data y. The mutual information therefore represents\n",
      "the reduction in uncertainty about xas a consequence of the new observation y.\n",
      "Exercises\n",
      "1.1 (⋆)www Consider the sum-of-squares error function given by (1.2) in which\n",
      "the function y(x,w)is given by the polynomial (1.1). Show that the coefﬁcients\n",
      "w={wi}that minimize this error function are given by the solution to the following\n",
      "set of linear equations\n",
      "M∑\n",
      "j=0Aijwj=Ti (1.122)\n",
      "where\n",
      "Aij=N∑\n",
      "n=1(xn)i+j,T i=N∑\n",
      "n=1(xn)itn. (1.123)\n",
      "Here a sufﬁx iorjdenotes the index of a component, whereas (x)idenotes xraised\n",
      "to the power of i.\n",
      "1.2 (⋆)Write down the set of coupled linear equations, analogous to (1.122), satisﬁed\n",
      "by the coefﬁcients wiwhich minimize the regularized sum-of-squares error function\n",
      "given by (1.4).\n",
      "1.3 (⋆⋆)Suppose that we have three coloured boxes r(red), b(blue), and g(green).\n",
      "Boxrcontains 3 apples, 4 oranges, and 3 limes, box bcontains 1 apple, 1 orange,\n",
      "and 0 limes, and box gcontains 3 apples, 3 oranges, and 4 limes. If a box is chosen\n",
      "at random with probabilities p(r)=0 .2,p(b)=0 .2,p(g)=0 .6, and a piece of\n",
      "fruit is removed from the box (with equal probability of selecting any of the items in\n",
      "the box), then what is the probability of selecting an apple? If we observe that theselected fruit is in fact an orange, what is the probability that it came from the green\n",
      "box?\n",
      "1.4 (⋆⋆)\n",
      "www Consider a probability density px(x)deﬁned over a continuous vari-\n",
      "ablex, and suppose that we make a nonlinear change of variable using x=g(y),\n",
      "so that the density transforms according to (1.27). By differentiating (1.27), show\n",
      "that the location ˆyof the maximum of the density in yis not in general related to the\n",
      "locationˆxof the maximum of the density over xby the simple functional relation\n",
      "ˆx=g(ˆy)as a consequence of the Jacobian factor. This shows that the maximum\n",
      "of a probability density (in contrast to a simple function) is dependent on the choice\n",
      "of variable. V erify that, in the case of a linear transformation, the location of themaximum transforms in the same way as the variable itself.\n",
      "1.5 (⋆)Using the deﬁnition (1.38) show that var[f(x)]satisﬁes (1.39).Exercises 59\n",
      "1.6 (⋆)Show that if two variables xandyare independent, then their covariance is\n",
      "zero.\n",
      "1.7 (⋆⋆)www In this exercise, we prove the normalization condition (1.48) for the\n",
      "univariate Gaussian. To do this consider, the integral\n",
      "I=∫∞\n",
      "−∞exp(\n",
      "−1\n",
      "2σ2x2)\n",
      "dx (1.124)\n",
      "which we can evaluate by ﬁrst writing its square in the form\n",
      "I2=∫∞\n",
      "−∞∫∞\n",
      "−∞exp(\n",
      "−1\n",
      "2σ2x2−1\n",
      "2σ2y2)\n",
      "dxdy. (1.125)\n",
      "Now make the transformation from Cartesian coordinates (x, y)to polar coordinates\n",
      "(r, θ)and then substitute u=r2. Show that, by performing the integrals over θand\n",
      "u, and then taking the square root of both sides, we obtain\n",
      "I=(\n",
      "2πσ2)1/2. (1.126)\n",
      "Finally, use this result to show that the Gaussian distribution N(x|µ, σ2)is normal-\n",
      "ized.\n",
      "1.8 (⋆⋆)www By using a change of variables, verify that the univariate Gaussian\n",
      "distribution given by (1.46) satisﬁes (1.49). Next, by differentiating both sides of thenormalization condition\n",
      "∫∞\n",
      "−∞N(\n",
      "x|µ, σ2)\n",
      "dx=1 (1.127)\n",
      "with respect to σ2, verify that the Gaussian satisﬁes (1.50). Finally, show that (1.51)\n",
      "holds.\n",
      "1.9 (⋆)www Show that the mode (i.e. the maximum) of the Gaussian distribution\n",
      "(1.46) is given by µ. Similarly, show that the mode of the multivariate Gaussian\n",
      "(1.52) is given by µ.\n",
      "1.10 (⋆)www Suppose that the two variables xandzare statistically independent.\n",
      "Show that the mean and variance of their sum satisﬁes\n",
      "E[x+z]= E[x]+ E[z] (1.128)\n",
      "var[x+z]=v a r [ x]+v a r [ z]. (1.129)\n",
      "1.11 (⋆)By setting the derivatives of the log likelihood function (1.54) with respect to µ\n",
      "andσ2equal to zero, verify the results (1.55) and (1.56).60 1. INTRODUCTION\n",
      "1.12 (⋆⋆)www Using the results (1.49) and (1.50), show that\n",
      "E[xnxm]=µ2+Inmσ2(1.130)\n",
      "where xnandxmdenote data points sampled from a Gaussian distribution with mean\n",
      "µand variance σ2, andInm satisﬁes Inm=1 ifn=mandInm=0 otherwise.\n",
      "Hence prove the results (1.57) and (1.58).\n",
      "1.13 (⋆)Suppose that the variance of a Gaussian is estimated using the result (1.56) but\n",
      "with the maximum likelihood estimate µML replaced with the true value µof the\n",
      "mean. Show that this estimator has the property that its expectation is given by the\n",
      "true variance σ2.\n",
      "1.14 (⋆⋆)Show that an arbitrary square matrix with elements wijcan be written in\n",
      "the form wij=wS\n",
      "ij+wA\n",
      "ijwhere wS\n",
      "ijandwA\n",
      "ijare symmetric and anti-symmetric\n",
      "matrices, respectively, satisfying wS\n",
      "ij=wS\n",
      "jiandwA\n",
      "ij=−wA\n",
      "jifor all iandj.N o w\n",
      "consider the second order term in a higher order polynomial in Ddimensions, given\n",
      "by\n",
      "D∑\n",
      "i=1D∑\n",
      "j=1wijxixj. (1.131)\n",
      "Show that\n",
      "D∑\n",
      "i=1D∑\n",
      "j=1wijxixj=D∑\n",
      "i=1D∑\n",
      "j=1wS\n",
      "ijxixj (1.132)\n",
      "so that the contribution from the anti-symmetric matrix vanishes. We therefore see\n",
      "that, without loss of generality, the matrix of coefﬁcients wijcan be chosen to be\n",
      "symmetric, and so not all of the D2elements of this matrix can be chosen indepen-\n",
      "dently. Show that the number of independent parameters in the matrix wS\n",
      "ijis given\n",
      "byD(D+1 )/2.\n",
      "1.15 (⋆⋆⋆ )www In this exercise and the next, we explore how the number of indepen-\n",
      "dent parameters in a polynomial grows with the order Mof the polynomial and with\n",
      "the dimensionality Dof the input space. We start by writing down the Mthorder\n",
      "term for a polynomial in Ddimensions in the form\n",
      "D∑\n",
      "i1=1D∑\n",
      "i2=1···D∑\n",
      "iM=1wi1i2···iMxi1xi2···xiM. (1.133)\n",
      "The coefﬁcients wi1i2···iMcomprise DMelements, but the number of independent\n",
      "parameters is signiﬁcantly fewer due to the many interchange symmetries of the\n",
      "factor xi1xi2···xiM. Begin by showing that the redundancy in the coefﬁcients can\n",
      "be removed by rewriting this Mthorder term in the form\n",
      "D∑\n",
      "i1=1i1∑\n",
      "i2=1···iM−1∑\n",
      "iM=1˜wi1i2···iMxi1xi2···xiM. (1.134)Exercises 61\n",
      "Note that the precise relationship between the ˜wcoefﬁcients and wcoefﬁcients need\n",
      "not be made explicit. Use this result to show that the number of independent param-\n",
      "etersn(D,M), which appear at order M, satisﬁes the following recursion relation\n",
      "n(D,M)=D∑\n",
      "i=1n(i, M−1). (1.135)\n",
      "Next use proof by induction to show that the following result holds\n",
      "D∑\n",
      "i=1(i+M−2)!\n",
      "(i−1)! (M−1)!=(D+M−1)!\n",
      "(D−1)!M!(1.136)\n",
      "which can be done by ﬁrst proving the result for D=1 and arbitrary Mby making\n",
      "use of the result 0 !=1 , then assuming it is correct for dimension Dand verifying\n",
      "that it is correct for dimension D+1 . Finally, use the two previous results, together\n",
      "with proof by induction, to show\n",
      "n(D,M)=(D+M−1)!\n",
      "(D−1)!M!. (1.137)\n",
      "To do this, ﬁrst show that the result is true for M=2 , and any value of D⩾1,\n",
      "by comparison with the result of Exercise 1.14. Then make use of (1.135), together\n",
      "with (1.136), to show that, if the result holds at order M−1, then it will also hold at\n",
      "orderM\n",
      "1.16 (⋆⋆⋆ )In Exercise 1.15, we proved the result (1.135) for the number of independent\n",
      "parameters in the Mthorder term of a D-dimensional polynomial. We now ﬁnd an\n",
      "expression for the total number N(D,M)of independent parameters in all of the\n",
      "terms up to and including the M6th order. First show that N(D,M)satisﬁes\n",
      "N(D,M)=M∑\n",
      "m=0n(D,m) (1.138)\n",
      "where n(D,m)is the number of independent parameters in the term of order m.\n",
      "Now make use of the result (1.137), together with proof by induction, to show that\n",
      "N(d, M)=(D+M)!\n",
      "D!M!. (1.139)\n",
      "This can be done by ﬁrst proving that the result holds for M=0 and arbitrary\n",
      "D⩾1, then assuming that it holds at order M, and hence showing that it holds at\n",
      "orderM+1 . Finally, make use of Stirling’s approximation in the form\n",
      "n!≃nne−n(1.140)\n",
      "for large nto show that, for D≫M, the quantity N(D,M)grows like DM,\n",
      "and for M≫Dit grows like MD. Consider a cubic ( M=3 ) polynomial in D\n",
      "dimensions, and evaluate numerically the total number of independent parametersfor (i) D=1 0 and (ii) D= 100 , which correspond to typical small-scale and\n",
      "medium-scale machine learning applications.62 1. INTRODUCTION\n",
      "1.17 (⋆⋆)www The gamma function is deﬁned by\n",
      "Γ(x)≡∫∞\n",
      "0ux−1e−udu. (1.141)\n",
      "Using integration by parts, prove the relation Γ(x+1 )= xΓ(x). Show also that\n",
      "Γ(1) = 1 and hence that Γ(x+1 )= x!whenxis an integer.\n",
      "1.18 (⋆⋆)www We can use the result (1.126) to derive an expression for the surface\n",
      "areaSD, and the volume VD, of a sphere of unit radius in Ddimensions. To do this,\n",
      "consider the following result, which is obtained by transforming from Cartesian to\n",
      "polar coordinates\n",
      "D∏\n",
      "i=1∫∞\n",
      "−∞e−x2\n",
      "idxi=SD∫∞\n",
      "0e−r2rD−1dr. (1.142)\n",
      "Using the deﬁnition (1.141) of the Gamma function, together with (1.126), evaluate\n",
      "both sides of this equation, and hence show that\n",
      "SD=2πD/2\n",
      "Γ(D/2). (1.143)\n",
      "Next, by integrating with respect to radius from 0to1, show that the volume of the\n",
      "unit sphere in Ddimensions is given by\n",
      "VD=SD\n",
      "D. (1.144)\n",
      "Finally, use the results Γ(1) = 1 andΓ(3/2) =√π/2to show that (1.143) and\n",
      "(1.144) reduce to the usual expressions for D=2 andD=3 .\n",
      "1.19 (⋆⋆)Consider a sphere of radius ainD-dimensions together with the concentric\n",
      "hypercube of side 2a, so that the sphere touches the hypercube at the centres of each\n",
      "of its sides. By using the results of Exercise 1.18, show that the ratio of the volume\n",
      "of the sphere to the volume of the cube is given by\n",
      "volume of sphere\n",
      "volume of cube=πD/2\n",
      "D2D−1Γ(D/2). (1.145)\n",
      "Now make use of Stirling’s formula in the form\n",
      "Γ(x+1 )≃(2π)1/2e−xxx+1/2(1.146)\n",
      "which is valid for x≫1, to show that, as D→∞ , the ratio (1.145) goes to zero.\n",
      "Show also that the ratio of the distance from the centre of the hypercube to one of\n",
      "the corners, divided by the perpendicular distance to one of the sides, is√\n",
      "D, which\n",
      "therefore goes to ∞asD→∞ . From these results we see that, in a space of high\n",
      "dimensionality, most of the volume of a cube is concentrated in the large number of\n",
      "corners, which themselves become very long ‘spikes’!Exercises 63\n",
      "1.20 (⋆⋆)www In this exercise, we explore the behaviour of the Gaussian distribution\n",
      "in high-dimensional spaces. Consider a Gaussian distribution in Ddimensions given\n",
      "by\n",
      "p(x)=1\n",
      "(2πσ2)D/2exp(\n",
      "−∥x∥2\n",
      "2σ2)\n",
      ". (1.147)\n",
      "We wish to ﬁnd the density with respect to radius in polar coordinates in which the\n",
      "direction variables have been integrated out. To do this, show that the integral of\n",
      "the probability density over a thin shell of radius rand thickness ϵ, where ϵ≪1,i s\n",
      "given by p(r)ϵwhere\n",
      "p(r)=SDrD−1\n",
      "(2πσ2)D/2exp(\n",
      "−r2\n",
      "2σ2)\n",
      "(1.148)\n",
      "where SDis the surface area of a unit sphere in Ddimensions. Show that the function\n",
      "p(r)has a single stationary point located, for large D,a tˆr≃√\n",
      "Dσ. By considering\n",
      "p(ˆr+ϵ)where ϵ≪ˆr, show that for large D,\n",
      "p(ˆr+ϵ)=p(ˆr)e x p(\n",
      "−3ϵ2\n",
      "2σ2)\n",
      "(1.149)\n",
      "which shows that ˆris a maximum of the radial probability density and also that p(r)\n",
      "decays exponentially away from its maximum at ˆrwith length scale σ.W e h a v e\n",
      "already seen that σ≪ˆrfor large D, and so we see that most of the probability\n",
      "mass is concentrated in a thin shell at large radius. Finally, show that the probability\n",
      "density p(x)is larger at the origin than at the radius ˆrby a factor of exp(D/2).\n",
      "We therefore see that most of the probability mass in a high-dimensional Gaussiandistribution is located at a different radius from the region of high probability density.\n",
      "This property of distributions in spaces of high dimensionality will have important\n",
      "consequences when we consider Bayesian inference of model parameters in laterchapters.\n",
      "1.21 (⋆⋆)Consider two nonnegative numbers aandb, and show that, if a⩽b, then\n",
      "a⩽(ab)\n",
      "1/2. Use this result to show that, if the decision regions of a two-class\n",
      "classiﬁcation problem are chosen to minimize the probability of misclassiﬁcation,this probability will satisfy\n",
      "p(mistake) ⩽∫\n",
      "{p(x,C1)p(x,C2)}1/2dx. (1.150)\n",
      "1.22 (⋆)www Given a loss matrix with elements Lkj, the expected risk is minimized\n",
      "if, for each x, we choose the class that minimizes (1.81). V erify that, when the\n",
      "loss matrix is given by Lkj=1−Ikj, where Ikjare the elements of the identity\n",
      "matrix, this reduces to the criterion of choosing the class having the largest posteriorprobability. What is the interpretation of this form of loss matrix?\n",
      "1.23 (⋆)Derive the criterion for minimizing the expected loss when there is a general\n",
      "loss matrix and general prior probabilities for the classes.64 1. INTRODUCTION\n",
      "1.24 (⋆⋆)www Consider a classiﬁcation problem in which the loss incurred when\n",
      "an input vector from class Ckis classiﬁed as belonging to class Cjis given by the\n",
      "loss matrix Lkj, and for which the loss incurred in selecting the reject option is λ.\n",
      "Find the decision criterion that will give the minimum expected loss. V erify that this\n",
      "reduces to the reject criterion discussed in Section 1.5.3 when the loss matrix is givenbyL\n",
      "kj=1−Ikj. What is the relationship between λand the rejection threshold θ?\n",
      "1.25 (⋆)www Consider the generalization of the squared loss function (1.87) for a\n",
      "single target variable tto the case of multiple target variables described by the vector\n",
      "tgiven by\n",
      "E[L(t,y(x))] =∫∫\n",
      "∥y(x)−t∥2p(x,t)dxdt. (1.151)\n",
      "Using the calculus of variations, show that the function y(x)for which this expected\n",
      "loss is minimized is given by y(x)= Et[t|x]. Show that this result reduces to (1.89)\n",
      "for the case of a single target variable t.\n",
      "1.26 (⋆)By expansion of the square in (1.151), derive a result analogous to (1.90) and\n",
      "hence show that the function y(x)that minimizes the expected squared loss for the\n",
      "case of a vector tof target variables is again given by the conditional expectation of\n",
      "t.\n",
      "1.27 (⋆⋆)www Consider the expected loss for regression problems under the Lqloss\n",
      "function given by (1.91). Write down the condition that y(x)must satisfy in order\n",
      "to minimize E[Lq]. Show that, for q=1 , this solution represents the conditional\n",
      "median, i.e., the function y(x)such that the probability mass for t<y (x)is the\n",
      "same as for t⩾y(x). Also show that the minimum expected Lqloss for q→0is\n",
      "given by the conditional mode, i.e., by the function y(x)equal to the value of tthat\n",
      "maximizes p(t|x)for each x.\n",
      "1.28 (⋆)In Section 1.6, we introduced the idea of entropy h(x)as the information gained\n",
      "on observing the value of a random variable xhaving distribution p(x).W e s a w\n",
      "that, for independent variables xandyfor which p(x, y)=p(x)p(y), the entropy\n",
      "functions are additive, so that h(x, y)=h(x)+h(y). In this exercise, we derive the\n",
      "relation between handpin the form of a function h(p). First show that h(p2)=\n",
      "2h(p), and hence by induction that h(pn)=nh(p)where nis a positive integer.\n",
      "Hence show that h(pn/m)=(n/m)h(p)where mis also a positive integer. This\n",
      "implies that h(px)=xh(p)where xis a positive rational number, and hence by\n",
      "continuity when it is a positive real number. Finally, show that this implies h(p)\n",
      "must take the form h(p)∝lnp.\n",
      "1.29 (⋆)www Consider an M-state discrete random variable x, and use Jensen’s in-\n",
      "equality in the form (1.115) to show that the entropy of its distribution p(x)satisﬁes\n",
      "H[x]⩽lnM.\n",
      "1.30 (⋆⋆) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians\n",
      "p(x)=N(x|µ, σ2)andq(x)=N(x|m, s2).Exercises 65\n",
      "Table 1.3 The joint distribution p(x, y)for two binary variables\n",
      "xandyused in Exercise 1.39.y\n",
      "01\n",
      "x0 1/3 1/3\n",
      "1 0 1/3\n",
      "1.31 (⋆⋆)www Consider two variables xandyhaving joint distribution p(x,y). Show\n",
      "that the differential entropy of this pair of variables satisﬁes\n",
      "H[x,y]⩽H[x]+H [y] (1.152)\n",
      "with equality if, and only if, xandyare statistically independent.\n",
      "1.32 (⋆)Consider a vector xof continuous variables with distribution p(x)and corre-\n",
      "sponding entropy H[x]. Suppose that we make a nonsingular linear transformation\n",
      "ofxto obtain a new variable y=Ax. Show that the corresponding entropy is given\n",
      "byH[y]=H [x]+l n|A|where|A|denotes the determinant of A.\n",
      "1.33 (⋆⋆)Suppose that the conditional entropy H[y|x]between two discrete random\n",
      "variables xandyis zero. Show that, for all values of xsuch that p(x)>0, the\n",
      "variable ymust be a function of x, in other words for each xthere is only one value\n",
      "ofysuch that p(y|x)̸=0 .\n",
      "1.34 (⋆⋆)www Use the calculus of variations to show that the stationary point of the\n",
      "functional (1.108) is given by (1.108). Then use the constraints (1.105), (1.106),\n",
      "and (1.107) to eliminate the Lagrange multipliers and hence show that the maximumentropy solution is given by the Gaussian (1.109).\n",
      "1.35 (⋆)\n",
      "www Use the results (1.106) and (1.107) to show that the entropy of the\n",
      "univariate Gaussian (1.109) is given by (1.110).\n",
      "1.36 (⋆)A strictly convex function is deﬁned as one for which every chord lies above\n",
      "the function. Show that this is equivalent to the condition that the second derivativeof the function be positive.\n",
      "1.37 (⋆)Using the deﬁnition (1.111) together with the product rule of probability, prove\n",
      "the result (1.112).\n",
      "1.38 (⋆⋆)\n",
      "www Using proof by induction, show that the inequality (1.114) for convex\n",
      "functions implies the result (1.115).\n",
      "1.39 (⋆⋆⋆ )Consider two binary variables xandyhaving the joint distribution given in\n",
      "Table 1.3.\n",
      "Evaluate the following quantities\n",
      "(a)H[x] (c)H[y|x] (e)H[x, y]\n",
      "(b)H[y] (d)H[x|y] (f)I[x, y].\n",
      "Draw a diagram to show the relationship between these various quantities.66 1. INTRODUCTION\n",
      "1.40 (⋆)By applying Jensen’s inequality (1.115) with f(x)=l n x, show that the arith-\n",
      "metic mean of a set of real numbers is never less than their geometrical mean.\n",
      "1.41 (⋆)www Using the sum and product rules of probability, show that the mutual\n",
      "information I(x,y)satisﬁes the relation (1.121).2\n",
      "Probability\n",
      "Distributions\n",
      "In Chapter 1, we emphasized the central role played by probability theory in the\n",
      "solution of pattern recognition problems. We turn now to an exploration of someparticular examples of probability distributions and their properties. As well as be-\n",
      "ing of great interest in their own right, these distributions can form building blocks\n",
      "for more complex models and will be used extensively throughout the book. Thedistributions introduced in this chapter will also serve another important purpose,\n",
      "namely to provide us with the opportunity to discuss some key statistical concepts,\n",
      "such as Bayesian inference, in the context of simple models before we encounterthem in more complex situations in later chapters.\n",
      "One role for the distributions discussed in this chapter is to model the prob-\n",
      "ability distribution p(x)of a random variable x, given a ﬁnite set x\n",
      "1,...,xNof\n",
      "observations. This problem is known as density estimation . For the purposes of\n",
      "this chapter, we shall assume that the data points are independent and identically\n",
      "distributed. It should be emphasized that the problem of density estimation is fun-\n",
      "6768 2. PROBABILITY DISTRIBUTIONS\n",
      "damentally ill-posed, because there are inﬁnitely many probability distributions that\n",
      "could have given rise to the observed ﬁnite data set. Indeed, any distribution p(x)\n",
      "that is nonzero at each of the data points x1,...,xNis a potential candidate. The\n",
      "issue of choosing an appropriate distribution relates to the problem of model selec-\n",
      "tion that has already been encountered in the context of polynomial curve ﬁtting inChapter 1 and that is a central issue in pattern recognition.\n",
      "We begin by considering the binomial and multinomial distributions for discrete\n",
      "random variables and the Gaussian distribution for continuous random variables.These are speciﬁc examples of parametric distributions, so-called because they are\n",
      "governed by a small number of adaptive parameters, such as the mean and variance in\n",
      "the case of a Gaussian for example. To apply such models to the problem of density\n",
      "estimation, we need a procedure for determining suitable values for the parameters,\n",
      "given an observed data set. In a frequentist treatment, we choose speciﬁc valuesfor the parameters by optimizing some criterion, such as the likelihood function. By\n",
      "contrast, in a Bayesian treatment we introduce prior distributions over the parameters\n",
      "and then use Bayes’ theorem to compute the corresponding posterior distributiongiven the observed data.\n",
      "We shall see that an important role is played by conjugate priors, that lead to\n",
      "posterior distributions having the same functional form as the prior, and that there-fore lead to a greatly simpliﬁed Bayesian analysis. For example, the conjugate prior\n",
      "for the parameters of the multinomial distribution is called the Dirichlet distribution,\n",
      "while the conjugate prior for the mean of a Gaussian is another Gaussian. All of thesedistributions are examples of the exponential family of distributions, which possess\n",
      "a number of important properties, and which will be discussed in some detail.\n",
      "One limitation of the parametric approach is that it assumes a speciﬁc functional\n",
      "form for the distribution, which may turn out to be inappropriate for a particular\n",
      "application. An alternative approach is given by nonparametric density estimation\n",
      "methods in which the form of the distribution typically depends on the size of the data\n",
      "set. Such models still contain parameters, but these control the model complexity\n",
      "rather than the form of the distribution. We end this chapter by considering threenonparametric methods based respectively on histograms, nearest-neighbours, and\n",
      "kernels.\n",
      "2.1. Binary Variables\n",
      "We begin by considering a single binary random variable x∈{0,1}. For example,\n",
      "xmight describe the outcome of ﬂipping a coin, with x=1 representing ‘heads’,\n",
      "andx=0 representing ‘tails’. We can imagine that this is a damaged coin so that\n",
      "the probability of landing heads is not necessarily the same as that of landing tails.\n",
      "The probability of x=1 will be denoted by the parameter µso that\n",
      "p(x=1|µ)=µ (2.1)2.1. Binary Variables 69\n",
      "where 0⩽µ⩽1, from which it follows that p(x=0|µ)=1−µ. The probability\n",
      "distribution over xcan therefore be written in the form\n",
      "Bern(x|µ)=µx(1−µ)1−x(2.2)\n",
      "which is known as the Bernoulli distribution. It is easily veriﬁed that this distribution Exercise 2.1\n",
      "is normalized and that it has mean and variance given by\n",
      "E[x]= µ (2.3)\n",
      "var[x]= µ(1−µ). (2.4)\n",
      "Now suppose we have a data set D={x1,...,x N}of observed values of x.\n",
      "We can construct the likelihood function, which is a function of µ, on the assumption\n",
      "that the observations are drawn independently from p(x|µ), so that\n",
      "p(D|µ)=N∏\n",
      "n=1p(xn|µ)=N∏\n",
      "n=1µxn(1−µ)1−xn. (2.5)\n",
      "In a frequentist setting, we can estimate a value for µby maximizing the likelihood\n",
      "function, or equivalently by maximizing the logarithm of the likelihood. In the case\n",
      "of the Bernoulli distribution, the log likelihood function is given by\n",
      "lnp(D|µ)=N∑\n",
      "n=1lnp(xn|µ)=N∑\n",
      "n=1{xnlnµ+( 1−xn)l n ( 1−µ)}. (2.6)\n",
      "At this point, it is worth noting that the log likelihood function depends on the N\n",
      "observations xnonly through their sum∑\n",
      "nxn. This sum provides an example of a\n",
      "sufﬁcient statistic for the data under this distribution, and we shall study the impor-\n",
      "tant role of sufﬁcient statistics in some detail. If we set the derivative of lnp(D|µ) Section 2.4\n",
      "with respect to µequal to zero, we obtain the maximum likelihood estimator\n",
      "µML=1\n",
      "NN∑\n",
      "n=1xn (2.7)\n",
      "Jacob Bernoulli\n",
      "1654–1705\n",
      "Jacob Bernoulli, also known as\n",
      "Jacques or James Bernoulli, was aSwiss mathematician and was theﬁrst of many in the Bernoulli familyto pursue a career in science andmathematics. Although compelled\n",
      "to study philosophy and theology against his will byhis parents, he travelled extensively after graduatingin order to meet with many of the leading scientists ofhis time, including Boyle and Hooke in England. When\n",
      "he returned to Switzerland, he taught mechanics andbecame Professor of Mathematics at Basel in 1687.Unfortunately, rivalry between Jacob and his youngerbrother Johann turned an initially productive collabora-tion into a bitter and public dispute. Jacob’s most sig-niﬁcant contributions to mathematics appeared in\n",
      "The\n",
      "Art of Conjecture published in 1713, eight years after\n",
      "his death, which deals with topics in probability the-ory including what has become known as the Bernoullidistribution.70 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.1 Histogram plot of the binomial dis-\n",
      "tribution (2.9) as a function of mfor\n",
      "N=1 0 andµ=0.25.\n",
      "m0 1 2 3 4 5 6 7 8 9 1000.10.20.3\n",
      "which is also known as the sample mean . If we denote the number of observations\n",
      "ofx=1 (heads) within this data set by m, then we can write (2.7) in the form\n",
      "µML=m\n",
      "N(2.8)\n",
      "so that the probability of landing heads is given, in this maximum likelihood frame-\n",
      "work, by the fraction of observations of heads in the data set.\n",
      "Now suppose we ﬂip a coin, say, 3 times and happen to observe 3 heads. Then\n",
      "N=m=3 andµML=1 . In this case, the maximum likelihood result would\n",
      "predict that all future observations should give heads. Common sense tells us that\n",
      "this is unreasonable, and in fact this is an extreme example of the over-ﬁtting associ-\n",
      "ated with maximum likelihood. We shall see shortly how to arrive at more sensible\n",
      "conclusions through the introduction of a prior distribution over µ.\n",
      "We can also work out the distribution of the number mof observations of x=1 ,\n",
      "given that the data set has size N. This is called the binomial distribution, and\n",
      "from (2.5) we see that it is proportional to µm(1−µ)N−m. In order to obtain the\n",
      "normalization coefﬁcient we note that out of Ncoin ﬂips, we have to add up all\n",
      "of the possible ways of obtaining mheads, so that the binomial distribution can be\n",
      "written\n",
      "Bin(m|N,µ)=(N\n",
      "m)\n",
      "µm(1−µ)N−m(2.9)\n",
      "where (N\n",
      "m)\n",
      "≡N!\n",
      "(N−m)!m!(2.10)\n",
      "is the number of ways of choosing mobjects out of a total of Nidentical objects. Exercise 2.3\n",
      "Figure 2.1 shows a plot of the binomial distribution for N=1 0 andµ=0.25.\n",
      "The mean and variance of the binomial distribution can be found by using the\n",
      "result of Exercise 1.10, which shows that for independent events the mean of the\n",
      "sum is the sum of the means, and the variance of the sum is the sum of the variances.\n",
      "Because m=x1+...+xN, and for each observation the mean and variance are2.1. Binary Variables 71\n",
      "given by (2.3) and (2.4), respectively, we have\n",
      "E[m]≡N∑\n",
      "m=0mBin(m|N,µ)= Nµ (2.11)\n",
      "var[m]≡N∑\n",
      "m=0(m−E[m])2Bin(m|N,µ)= Nµ(1−µ). (2.12)\n",
      "These results can also be proved directly using calculus. Exercise 2.4\n",
      "2.1.1 The beta distribution\n",
      "We have seen in (2.8) that the maximum likelihood setting for the parameter µ\n",
      "in the Bernoulli distribution, and hence in the binomial distribution, is given by the\n",
      "fraction of the observations in the data set having x=1 . As we have already noted,\n",
      "this can give severely over-ﬁtted results for small data sets. In order to develop a\n",
      "Bayesian treatment for this problem, we need to introduce a prior distribution p(µ)\n",
      "over the parameter µ. Here we consider a form of prior distribution that has a simple\n",
      "interpretation as well as some useful analytical properties. To motivate this prior,\n",
      "we note that the likelihood function takes the form of the product of factors of theformµ\n",
      "x(1−µ)1−x. If we choose a prior to be proportional to powers of µand\n",
      "(1−µ), then the posterior distribution, which is proportional to the product of the\n",
      "prior and the likelihood function, will have the same functional form as the prior.This property is called conjugacy and we will see several examples of it later in this\n",
      "chapter. We therefore choose a prior, called the beta distribution, given by\n",
      "Beta(µ|a, b)=Γ(a+b)\n",
      "Γ(a)Γ(b)µa−1(1−µ)b−1(2.13)\n",
      "where Γ(x)is the gamma function deﬁned by (1.141), and the coefﬁcient in (2.13)\n",
      "ensures that the beta distribution is normalized, so that Exercise 2.5\n",
      "∫1\n",
      "0Beta(µ|a, b)dµ=1. (2.14)\n",
      "The mean and variance of the beta distribution are given by Exercise 2.6\n",
      "E[µ]=a\n",
      "a+b(2.15)\n",
      "var[µ]=ab\n",
      "(a+b)2(a+b+1 ). (2.16)\n",
      "The parameters aandbare often called hyperparameters because they control the\n",
      "distribution of the parameter µ. Figure 2.2 shows plots of the beta distribution for\n",
      "various values of the hyperparameters.\n",
      "The posterior distribution of µis now obtained by multiplying the beta prior\n",
      "(2.13) by the binomial likelihood function (2.9) and normalizing. Keeping only thefactors that depend on µ, we see that this posterior distribution has the form\n",
      "p(µ|m, l, a, b )∝µ\n",
      "m+a−1(1−µ)l+b−1(2.17)72 2. PROBABILITY DISTRIBUTIONS\n",
      "µa=0.1\n",
      "b=0.1\n",
      "0 0.5 10123\n",
      "µa=1\n",
      "b=1\n",
      "0 0.5 10123\n",
      "µa=2\n",
      "b=3\n",
      "0 0.5 10123\n",
      "µa=8\n",
      "b=4\n",
      "0 0.5 10123\n",
      "Figure 2.2 Plots of the beta distribution Beta( µ|a, b)given by (2.13) as a function of µfor various values of the\n",
      "hyperparameters aandb.\n",
      "where l=N−m, and therefore corresponds to the number of ‘tails’ in the coin\n",
      "example. We see that (2.17) has the same functional dependence on µas the prior\n",
      "distribution, reﬂecting the conjugacy properties of the prior with respect to the like-\n",
      "lihood function. Indeed, it is simply another beta distribution, and its normalization\n",
      "coefﬁcient can therefore be obtained by comparison with (2.13) to give\n",
      "p(µ|m, l, a, b )=Γ(m+a+l+b)\n",
      "Γ(m+a)Γ(l+b)µm+a−1(1−µ)l+b−1. (2.18)\n",
      "We see that the effect of observing a data set of mobservations of x=1 and\n",
      "lobservations of x=0 has been to increase the value of abym, and the value of\n",
      "bbyl, in going from the prior distribution to the posterior distribution. This allows\n",
      "us to provide a simple interpretation of the hyperparameters aandbin the prior as\n",
      "aneffective number of observations ofx=1 andx=0 , respectively. Note that\n",
      "aandbneed not be integers. Furthermore, the posterior distribution can act as the\n",
      "prior if we subsequently observe additional data. To see this, we can imagine taking\n",
      "observations one at a time and after each observation updating the current posterior2.1. Binary Variables 73\n",
      "µprior\n",
      "0 0.5 1012\n",
      "µlikelihood function\n",
      "0 0.5 1012\n",
      "µposterior\n",
      "0 0.5 1012\n",
      "Figure 2.3 Illustration of one step of sequential Bayesian inference. The prior is given by a beta distribution\n",
      "with parameters a=2,b=2, and the likelihood function, given by (2.9) with N=m=1, corresponds to a\n",
      "single observation of x=1, so that the posterior is given by a beta distribution with parameters a=3,b=2.\n",
      "distribution by multiplying by the likelihood function for the new observation and\n",
      "then normalizing to obtain the new, revised posterior distribution. At each stage, the\n",
      "posterior is a beta distribution with some total number of (prior and actual) observed\n",
      "values for x=1 andx=0 given by the parameters aandb. Incorporation of an\n",
      "additional observation of x=1 simply corresponds to incrementing the value of a\n",
      "by1, whereas for an observation of x=0 we increment bby1. Figure 2.3 illustrates\n",
      "one step in this process.\n",
      "We see that this sequential approach to learning arises naturally when we adopt\n",
      "a Bayesian viewpoint. It is independent of the choice of prior and of the likelihood\n",
      "function and depends only on the assumption of i.i.d. data. Sequential methods make\n",
      "use of observations one at a time, or in small batches, and then discard them before\n",
      "the next observations are used. They can be used, for example, in real-time learning\n",
      "scenarios where a steady stream of data is arriving, and predictions must be made\n",
      "before all of the data is seen. Because they do not require the whole data set to be\n",
      "stored or loaded into memory, sequential methods are also useful for large data sets.\n",
      "Maximum likelihood methods can also be cast into a sequential framework. Section 2.3.5\n",
      "If our goal is to predict, as best we can, the outcome of the next trial, then we\n",
      "must evaluate the predictive distribution of x, given the observed data set D. From\n",
      "the sum and product rules of probability, this takes the form\n",
      "p(x=1|D)=∫1\n",
      "0p(x=1|µ)p(µ|D)dµ=∫1\n",
      "0µp(µ|D)dµ=E[µ|D].(2.19)\n",
      "Using the result (2.18) for the posterior distribution p(µ|D), together with the result\n",
      "(2.15) for the mean of the beta distribution, we obtain\n",
      "p(x=1|D)=m+a\n",
      "m+a+l+b(2.20)\n",
      "which has a simple interpretation as the total fraction of observations (both real ob-\n",
      "servations and ﬁctitious prior observations) that correspond to x=1 . Note that in\n",
      "the limit of an inﬁnitely large data set m, l→∞ the result (2.20) reduces to the\n",
      "maximum likelihood result (2.8). As we shall see, it is a very general property that\n",
      "the Bayesian and maximum likelihood results will agree in the limit of an inﬁnitely74 2. PROBABILITY DISTRIBUTIONS\n",
      "large data set. For a ﬁnite data set, the posterior mean for µalways lies between the\n",
      "prior mean and the maximum likelihood estimate for µcorresponding to the relative\n",
      "frequencies of events given by (2.7). Exercise 2.7\n",
      "From Figure 2.2, we see that as the number of observations increases, so the\n",
      "posterior distribution becomes more sharply peaked. This can also be seen fromthe result (2.16) for the variance of the beta distribution, in which we see that the\n",
      "variance goes to zero for a→∞ orb→∞ . In fact, we might wonder whether it is\n",
      "a general property of Bayesian learning that, as we observe more and more data, theuncertainty represented by the posterior distribution will steadily decrease.\n",
      "To address this, we can take a frequentist view of Bayesian learning and show\n",
      "that, on average, such a property does indeed hold. Consider a general Bayesian\n",
      "inference problem for a parameter θfor which we have observed a data set D, de-\n",
      "scribed by the joint distribution p(θ,D). The following result Exercise 2.8\n",
      "E\n",
      "θ[θ]= ED[Eθ[θ|D]] (2.21)\n",
      "where\n",
      "Eθ[θ]≡∫\n",
      "p(θ)θdθ (2.22)\n",
      "ED[Eθ[θ|D]]≡∫{∫\n",
      "θp(θ|D)dθ}\n",
      "p(D)dD (2.23)\n",
      "says that the posterior mean of θ, averaged over the distribution generating the data,\n",
      "is equal to the prior mean of θ. Similarly, we can show that\n",
      "varθ[θ]= ED[varθ[θ|D]] + var D[Eθ[θ|D]]. (2.24)\n",
      "The term on the left-hand side of (2.24) is the prior variance of θ. On the right-\n",
      "hand side, the ﬁrst term is the average posterior variance of θ, and the second term\n",
      "measures the variance in the posterior mean of θ. Because this variance is a positive\n",
      "quantity, this result shows that, on average, the posterior variance of θis smaller than\n",
      "the prior variance. The reduction in variance is greater if the variance in the posterior\n",
      "mean is greater. Note, however, that this result only holds on average, and that for a\n",
      "particular observed data set it is possible for the posterior variance to be larger thanthe prior variance.\n",
      "2.2. Multinomial Variables\n",
      "Binary variables can be used to describe quantities that can take one of two possiblevalues. Often, however, we encounter discrete variables that can take on one of K\n",
      "possible mutually exclusive states. Although there are various alternative ways to\n",
      "express such variables, we shall see shortly that a particularly convenient represen-tation is the 1-of-Kscheme in which the variable is represented by a K-dimensional\n",
      "vectorxin which one of the elements x\n",
      "kequals 1, and all remaining elements equal2.2. Multinomial Variables 75\n",
      "0. So, for instance if we have a variable that can take K=6 states and a particular\n",
      "observation of the variable happens to correspond to the state where x3=1 , thenx\n",
      "will be represented by\n",
      "x=( 0,0,1,0,0,0)T. (2.25)\n",
      "Note that such vectors satisfy∑K\n",
      "k=1xk=1 . If we denote the probability of xk=1\n",
      "by the parameter µk, then the distribution of xis given\n",
      "p(x|µ)=K∏\n",
      "k=1µxk\n",
      "k(2.26)\n",
      "where µ=(µ1,...,µ K)T, and the parameters µkare constrained to satisfy µk⩾0\n",
      "and∑\n",
      "kµk=1 , because they represent probabilities. The distribution (2.26) can be\n",
      "regarded as a generalization of the Bernoulli distribution to more than two outcomes.It is easily seen that the distribution is normalized\n",
      "∑\n",
      "xp(x|µ)=K∑\n",
      "k=1µk=1 (2.27)\n",
      "and that\n",
      "E[x|µ]=∑\n",
      "xp(x|µ)x=(µ1,...,µ M)T=µ. (2.28)\n",
      "Now consider a data set DofNindependent observations x1,...,xN. The\n",
      "corresponding likelihood function takes the form\n",
      "p(D|µ)=N∏\n",
      "n=1K∏\n",
      "k=1µxnk\n",
      "k=K∏\n",
      "k=1µ(P\n",
      "nxnk)\n",
      "k=K∏\n",
      "k=1µmk\n",
      "k. (2.29)\n",
      "We see that the likelihood function depends on the Ndata points only through the\n",
      "Kquantities\n",
      "mk=∑\n",
      "nxnk (2.30)\n",
      "which represent the number of observations of xk=1 . These are called the sufﬁcient\n",
      "statistics for this distribution. Section 2.4\n",
      "In order to ﬁnd the maximum likelihood solution for µ, we need to maximize\n",
      "lnp(D|µ)with respect to µktaking account of the constraint that the µkmust sum\n",
      "to one. This can be achieved using a Lagrange multiplier λand maximizing Appendix E\n",
      "K∑\n",
      "k=1mklnµk+λ(K∑\n",
      "k=1µk−1)\n",
      ". (2.31)\n",
      "Setting the derivative of (2.31) with respect to µkto zero, we obtain\n",
      "µk=−mk/λ. (2.32)76 2. PROBABILITY DISTRIBUTIONS\n",
      "We can solve for the Lagrange multiplier λby substituting (2.32) into the constraint∑\n",
      "kµk=1 to give λ=−N. Thus we obtain the maximum likelihood solution in\n",
      "the form\n",
      "µML\n",
      "k=mk\n",
      "N(2.33)\n",
      "which is the fraction of the Nobservations for which xk=1 .\n",
      "We can consider the joint distribution of the quantities m1,...,m K, conditioned\n",
      "on the parameters µand on the total number Nof observations. From (2.29) this\n",
      "takes the form\n",
      "Mult( m1,m2,...,m K|µ,N)=(N\n",
      "m1m2...m K)K∏\n",
      "k=1µmk\n",
      "k(2.34)\n",
      "which is known as the multinomial distribution. The normalization coefﬁcient is the\n",
      "number of ways of partitioning Nobjects into Kgroups of size m1,...,m Kand is\n",
      "given by(N\n",
      "m1m2...m K)\n",
      "=N!\n",
      "m1!m2!...m K!. (2.35)\n",
      "Note that the variables mkare subject to the constraint\n",
      "K∑\n",
      "k=1mk=N. (2.36)\n",
      "2.2.1 The Dirichlet distribution\n",
      "We now introduce a family of prior distributions for the parameters {µk}of\n",
      "the multinomial distribution (2.34). By inspection of the form of the multinomialdistribution, we see that the conjugate prior is given by\n",
      "p(µ|α)∝K∏\n",
      "k=1µαk−1\n",
      "k(2.37)\n",
      "where 0⩽µk⩽1and∑\n",
      "kµk=1 . Here α1,...,α Kare the parameters of the\n",
      "distribution, and αdenotes (α1,...,α K)T. Note that, because of the summation\n",
      "constraint, the distribution over the space of the {µk}is conﬁned to a simplex of\n",
      "dimensionality K−1, as illustrated for K=3 in Figure 2.4.\n",
      "The normalized form for this distribution is by Exercise 2.9\n",
      "Dir(µ|α)=Γ(α0)\n",
      "Γ(α1)···Γ(αK)K∏\n",
      "k=1µαk−1\n",
      "k(2.38)\n",
      "which is called the Dirichlet distribution. Here Γ(x)is the gamma function deﬁned\n",
      "by (1.141) while\n",
      "α0=K∑\n",
      "k=1αk. (2.39)2.2. Multinomial Variables 77\n",
      "Figure 2.4 The Dirichlet distribution over three variables µ1,µ2,µ3\n",
      "is conﬁned to a simplex (a bounded linear manifold) of\n",
      "the form shown, as a consequence of the constraints\n",
      "0⩽µk⩽1andP\n",
      "kµk=1.\n",
      "µ1µ2\n",
      "µ3\n",
      "Plots of the Dirichlet distribution over the simplex, for various settings of the param-\n",
      "etersαk, are shown in Figure 2.5.\n",
      "Multiplying the prior (2.38) by the likelihood function (2.34), we obtain the\n",
      "posterior distribution for the parameters {µk}in the form\n",
      "p(µ|D,α)∝p(D|µ)p(µ|α)∝K∏\n",
      "k=1µαk+mk−1\n",
      "k. (2.40)\n",
      "We see that the posterior distribution again takes the form of a Dirichlet distribution,\n",
      "conﬁrming that the Dirichlet is indeed a conjugate prior for the multinomial. This\n",
      "allows us to determine the normalization coefﬁcient by comparison with (2.38) so\n",
      "that\n",
      "p(µ|D,α)=D i r ( µ|α+m)\n",
      "=Γ(α0+N)\n",
      "Γ(α1+m1)···Γ(αK+mK)K∏\n",
      "k=1µαk+mk−1\n",
      "k(2.41)\n",
      "where we have denoted m=(m1,...,m K)T. As for the case of the binomial\n",
      "distribution with its beta prior, we can interpret the parameters αkof the Dirichlet\n",
      "prior as an effective number of observations of xk=1 .\n",
      "Note that two-state quantities can either be represented as binary variables and\n",
      "Lejeune Dirichlet\n",
      "1805–1859\n",
      "Johann Peter Gustav Lejeune\n",
      "Dirichlet was a modest and re-\n",
      "served mathematician who made\n",
      "contributions in number theory, me-\n",
      "chanics, and astronomy, and who\n",
      "gave the ﬁrst rigorous analysis of\n",
      "Fourier series. His family originated from Richelet\n",
      "in Belgium, and the name Lejeune Dirichlet comesfrom ‘le jeune de Richelet’ (the young person from\n",
      "Richelet). Dirichlet’s ﬁrst paper, which was published\n",
      "in 1825, brought him instant fame. It concerned Fer-\n",
      "mat’s last theorem, which claims that there are no\n",
      "positive integer solutions to xn+yn=znforn>2.\n",
      "Dirichlet gave a partial proof for the case n=5, which\n",
      "was sent to Legendre for review and who in turn com-\n",
      "pleted the proof. Later, Dirichlet gave a complete proof\n",
      "forn=1 4 , although a full proof of Fermat’s last theo-\n",
      "rem for arbitrary nhad to wait until the work of Andrew\n",
      "Wiles in the closing years of the 20thcentury.78 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.5 Plots of the Dirichlet distribution over three vari ables, where the two horizontal axes are coordinates\n",
      "in the plane of the simplex and the vertical axis corresponds to the value of the density . Here {αk}=0.1on the\n",
      "left plot, {αk}=1 in the centre plot, and {αk}=1 0 in the right plot.\n",
      "modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled\n",
      "using the multinomial distribution (2.34) with K=2 .\n",
      "2.3.\n",
      " The Gaussian Distribution\n",
      "The Gaussian, also known as the normal distribution, is a widely used model for the\n",
      "distribution of continuous variables. In the case of a single variable x, the Gaussian\n",
      "distribution can be written in the form\n",
      "N(x|µ,σ2)=1\n",
      "(2πσ2)1/2exp{\n",
      "−1\n",
      "2σ2(x−µ)2}\n",
      "(2.42)\n",
      "where µis the mean and σ2is the variance. For a D-dimensional vector x,t h e\n",
      "multivariate Gaussian distribution takes the form\n",
      "N(x|µ,Σ)=1\n",
      "(2π)D/ 21\n",
      "|Σ|1/2exp{\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)}\n",
      "(2.43)\n",
      "where µis aD-dimensional mean vector, Σis aD×Dcovariance matrix, and |Σ|\n",
      "denotes the determinant of Σ.\n",
      "The Gaussian distribution arises in many different contexts and can be motivated\n",
      "from a variety of different perspectives. For example, we have already seen that for Section 1.6\n",
      "a single real variable, the distribution that maximizes the entropy is the Gaussian.\n",
      "This property applies also to the multivariate Gaussian. Exercise 2.14\n",
      "Another situation in which the Gaussian distribution arises is when we consider\n",
      "the sum of multiple random variables. The central limit theorem (due to Laplace)\n",
      "tells us that, subject to certain mild conditions, the sum of a set of random variables,\n",
      "which is of course itself a random variable, has a distribution that becomes increas-\n",
      "ingly Gaussian as the number of terms in the sum increases (Walker, 1969). We can2.3. The Gaussian Distribution 79\n",
      "N=1\n",
      "0 0.5 10123N=2\n",
      "0 0.5 10123N=1 0\n",
      "0 0.5 10123\n",
      "Figure 2.6 Histogram plots of the mean of Nuniformly distributed numbers for various values of N.W e\n",
      "observe that as Nincreases, the distribution tends towards a Gaussian.\n",
      "illustrate this by considering Nvariables x1,...,x Neach of which has a uniform\n",
      "distribution over the interval [0,1]and then considering the distribution of the mean\n",
      "(x1+···+xN)/N. For large N, this distribution tends to a Gaussian, as illustrated\n",
      "in Figure 2.6. In practice, the convergence to a Gaussian as Nincreases can be\n",
      "very rapid. One consequence of this result is that the binomial distribution (2.9),\n",
      "which is a distribution over mdeﬁned by the sum of Nobservations of the random\n",
      "binary variable x, will tend to a Gaussian as N→∞ (see Figure 2.1 for the case of\n",
      "N=1 0 ).\n",
      "The Gaussian distribution has many important analytical properties, and we shall\n",
      "consider several of these in detail. As a result, this section will be rather more tech-\n",
      "nically involved than some of the earlier sections, and will require familiarity with\n",
      "various matrix identities. However, we strongly encourage the reader to become pro- Appendix C\n",
      "ﬁcient in manipulating Gaussian distributions using the techniques presented here as\n",
      "this will prove invaluable in understanding the more complex models presented in\n",
      "later chapters.\n",
      "We begin by considering the geometrical form of the Gaussian distribution. The\n",
      "Carl Friedrich Gauss\n",
      "1777–1855\n",
      "It is said that when Gauss went\n",
      "to elementary school at age 7, his\n",
      "teacher B ¨uttner, trying to keep the\n",
      "class occupied, asked the pupils to\n",
      "sum the integers from 1 to 100. To\n",
      "the teacher’s amazement, Gauss\n",
      "arrived at the answer in a matter of moments by noting\n",
      "that the sum can be represented as 50 pairs ( 1 + 100 ,\n",
      "2+99 , etc.) each of which added to 101, giving the an-\n",
      "swer 5,050. It is now believed that the problem which\n",
      "was actually set was of the same form but somewhat\n",
      "harder in that the sequence had a larger starting value\n",
      "and a larger increment. Gauss was a German math-ematician and scientist with a reputation for being a\n",
      "hard-working perfectionist. One of his many contribu-\n",
      "tions was to show that least squares can be derived\n",
      "under the assumption of normally distributed errors.\n",
      "He also created an early formulation of non-Euclidean\n",
      "geometry (a self-consistent geometrical theory that vi-\n",
      "olates the axioms of Euclid) but was reluctant to dis-\n",
      "cuss it openly for fear that his reputation might suffer\n",
      "if it were seen that he believed in such a geometry.\n",
      "At one point, Gauss was asked to conduct a geodetic\n",
      "survey of the state of Hanover, which led to his for-\n",
      "mulation of the normal distribution, now also known\n",
      "as the Gaussian. After his death, a study of his di-\n",
      "aries revealed that he had discovered several impor-\n",
      "tant mathematical results years or even decades be-\n",
      "fore they were published by others.80 2. PROBABILITY DISTRIBUTIONS\n",
      "functional dependence of the Gaussian on xis through the quadratic form\n",
      "∆2=(x−µ)TΣ−1(x−µ) (2.44)\n",
      "which appears in the exponent. The quantity ∆is called the Mahalanobis distance\n",
      "fromµtoxand reduces to the Euclidean distance when Σis the identity matrix. The\n",
      "Gaussian distribution will be constant on surfaces in x-space for which this quadratic\n",
      "form is constant.\n",
      "First of all, we note that the matrix Σcan be taken to be symmetric, without\n",
      "loss of generality, because any antisymmetric component would disappear from the\n",
      "exponent. Now consider the eigenvector equation for the covariance matrix Exercise 2.17\n",
      "Σui=λiui (2.45)\n",
      "where i=1,...,D . Because Σis a real, symmetric matrix its eigenvalues will be\n",
      "real, and its eigenvectors can be chosen to form an orthonormal set, so that Exercise 2.18\n",
      "uT\n",
      "iuj=Iij (2.46)\n",
      "where Iijis thei, jelement of the identity matrix and satisﬁes\n",
      "Iij={\n",
      "1,ifi=j\n",
      "0,otherwise.(2.47)\n",
      "The covariance matrix Σcan be expressed as an expansion in terms of its eigenvec-\n",
      "tors in the form Exercise 2.19\n",
      "Σ=D∑\n",
      "i=1λiuiuT\n",
      "i (2.48)\n",
      "and similarly the inverse covariance matrix Σ−1can be expressed as\n",
      "Σ−1=D∑\n",
      "i=11\n",
      "λiuiuT\n",
      "i. (2.49)\n",
      "Substituting (2.49) into (2.44), the quadratic form becomes\n",
      "∆2=D∑\n",
      "i=1y2\n",
      "i\n",
      "λi(2.50)\n",
      "where we have deﬁned\n",
      "yi=uT\n",
      "i(x−µ). (2.51)\n",
      "We can interpret {yi}as a new coordinate system deﬁned by the orthonormal vectors\n",
      "uithat are shifted and rotated with respect to the original xicoordinates. Forming\n",
      "the vector y=(y1,...,y D)T,w eh a v e\n",
      "y=U(x−µ) (2.52)2.3. The Gaussian Distribution 81\n",
      "Figure 2.7 The red curve shows the ellip-\n",
      "tical surface of constant proba-\n",
      "bility density for a Gaussian in\n",
      "a two-dimensional space x=\n",
      "(x1,x2)on which the density\n",
      "isexp(−1/2)of its value at\n",
      "x=µ. The major axes of\n",
      "the ellipse are deﬁned by the\n",
      "eigenvectors uiof the covari-\n",
      "ance matrix, with correspond-\n",
      "ing eigenvalues λi.\n",
      "x1x2\n",
      "λ1/2\n",
      "1λ1/2\n",
      "2y1y2u1u2\n",
      "µ\n",
      "whereUis a matrix whose rows are given by uT\n",
      "i. From (2.46) it follows that Uis\n",
      "anorthogonal matrix, i.e., it satisﬁes UUT=I, and hence also UTU=I, where I Appendix C\n",
      "is the identity matrix.\n",
      "The quadratic form, and hence the Gaussian density, will be constant on surfaces\n",
      "for which (2.51) is constant. If all of the eigenvalues λiare positive, then these\n",
      "surfaces represent ellipsoids, with their centres at µand their axes oriented along ui,\n",
      "and with scaling factors in the directions of the axes given by λ1/2\n",
      "i, as illustrated in\n",
      "Figure 2.7.\n",
      "For the Gaussian distribution to be well deﬁned, it is necessary for all of the\n",
      "eigenvalues λiof the covariance matrix to be strictly positive, otherwise the dis-\n",
      "tribution cannot be properly normalized. A matrix whose eigenvalues are strictly\n",
      "positive is said to be positive deﬁnite . In Chapter 12, we will encounter Gaussian\n",
      "distributions for which one or more of the eigenvalues are zero, in which case the\n",
      "distribution is singular and is conﬁned to a subspace of lower dimensionality. If all\n",
      "of the eigenvalues are nonnegative, then the covariance matrix is said to be positive\n",
      "semideﬁnite .\n",
      "Now consider the form of the Gaussian distribution in the new coordinate system\n",
      "deﬁned by the yi. In going from the xto theycoordinate system, we have a Jacobian\n",
      "matrixJwith elements given by\n",
      "Jij=∂xi\n",
      "∂yj=Uji (2.53)\n",
      "where Ujiare the elements of the matrix UT. Using the orthonormality property of\n",
      "the matrix U, we see that the square of the determinant of the Jacobian matrix is\n",
      "|J|2=⏐⏐UT⏐⏐2=⏐⏐UT⏐⏐|U|=⏐⏐UTU⏐⏐=|I|=1 (2.54)\n",
      "and hence |J|=1. Also, the determinant |Σ|of the covariance matrix can be written82 2. PROBABILITY DISTRIBUTIONS\n",
      "as the product of its eigenvalues, and hence\n",
      "|Σ|1/2=D∏\n",
      "j=1λ1/2\n",
      "j. (2.55)\n",
      "Thus in the yjcoordinate system, the Gaussian distribution takes the form\n",
      "p(y)=p(x)|J|=D∏\n",
      "j=11\n",
      "(2πλj)1/2exp{\n",
      "−y2\n",
      "j\n",
      "2λj}\n",
      "(2.56)\n",
      "which is the product of Dindependent univariate Gaussian distributions. The eigen-\n",
      "vectors therefore deﬁne a new set of shifted and rotated coordinates with respectto which the joint probability distribution factorizes into a product of independent\n",
      "distributions. The integral of the distribution in the ycoordinate system is then\n",
      "∫\n",
      "p(y)dy=D∏\n",
      "j=1∫∞\n",
      "−∞1\n",
      "(2πλj)1/2exp{\n",
      "−y2\n",
      "j\n",
      "2λj}\n",
      "dyj=1 (2.57)\n",
      "where we have used the result (1.48) for the normalization of the univariate Gaussian.\n",
      "This conﬁrms that the multivariate Gaussian (2.43) is indeed normalized.\n",
      "We now look at the moments of the Gaussian distribution and thereby provide an\n",
      "interpretation of the parameters µandΣ. The expectation of xunder the Gaussian\n",
      "distribution is given by\n",
      "E[x]=1\n",
      "(2π)D/21\n",
      "|Σ|1/2∫\n",
      "exp{\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)}\n",
      "xdx\n",
      "=1\n",
      "(2π)D/21\n",
      "|Σ|1/2∫\n",
      "exp{\n",
      "−1\n",
      "2zTΣ−1z}\n",
      "(z+µ)dz (2.58)\n",
      "where we have changed variables using z=x−µ. We now note that the exponent\n",
      "is an even function of the components of zand, because the integrals over these are\n",
      "taken over the range (−∞,∞), the term in zin the factor (z+µ)will vanish by\n",
      "symmetry. Thus\n",
      "E[x]=µ (2.59)\n",
      "and so we refer to µas the mean of the Gaussian distribution.\n",
      "We now consider second order moments of the Gaussian. In the univariate case,\n",
      "we considered the second order moment given by E[x2]. For the multivariate Gaus-\n",
      "sian, there are D2second order moments given by E[xixj], which we can group\n",
      "together to form the matrix E[xxT]. This matrix can be written as\n",
      "E[xxT]=1\n",
      "(2π)D/21\n",
      "|Σ|1/2∫\n",
      "exp{\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)}\n",
      "xxTdx\n",
      "=1\n",
      "(2π)D/21\n",
      "|Σ|1/2∫\n",
      "exp{\n",
      "−1\n",
      "2zTΣ−1z}\n",
      "(z+µ)(z+µ)Tdz2.3. The Gaussian Distribution 83\n",
      "where again we have changed variables using z=x−µ. Note that the cross-terms\n",
      "involving µzTandµTzwill again vanish by symmetry. The term µµTis constant\n",
      "and can be taken outside the integral, which itself is unity because the Gaussian\n",
      "distribution is normalized. Consider the term involving zzT. Again, we can make\n",
      "use of the eigenvector expansion of the covariance matrix given by (2.45), togetherwith the completeness of the set of eigenvectors, to write\n",
      "z=D∑\n",
      "j=1yjuj (2.60)\n",
      "where yj=uT\n",
      "jz, which gives\n",
      "1\n",
      "(2π)D/21\n",
      "|Σ|1/2∫\n",
      "exp{\n",
      "−1\n",
      "2zTΣ−1z}\n",
      "zzTdz\n",
      "=1\n",
      "(2π)D/21\n",
      "|Σ|1/2D∑\n",
      "i=1D∑\n",
      "j=1uiuT\n",
      "j∫\n",
      "exp{\n",
      "−D∑\n",
      "k=1y2\n",
      "k\n",
      "2λk}\n",
      "yiyjdy\n",
      "=D∑\n",
      "i=1uiuT\n",
      "iλi=Σ (2.61)\n",
      "where we have made use of the eigenvector equation (2.45), together with the fact\n",
      "that the integral on the right-hand side of the middle line vanishes by symmetryunless i=j, and in the ﬁnal line we have made use of the results (1.50) and (2.55),\n",
      "together with (2.48). Thus we have\n",
      "E[xx\n",
      "T]=µµT+Σ. (2.62)\n",
      "For single random variables, we subtracted the mean before taking second mo-\n",
      "ments in order to deﬁne a variance. Similarly, in the multivariate case it is again\n",
      "convenient to subtract off the mean, giving rise to the covariance of a random vector\n",
      "xdeﬁned by\n",
      "cov[x]= E[\n",
      "(x−E[x])(x−E[x])T]\n",
      ". (2.63)\n",
      "For the speciﬁc case of a Gaussian distribution, we can make use of E[x]=µ,\n",
      "together with the result (2.62), to give\n",
      "cov[x]=Σ. (2.64)\n",
      "Because the parameter matrix Σgoverns the covariance of xunder the Gaussian\n",
      "distribution, it is called the covariance matrix.\n",
      "Although the Gaussian distribution (2.43) is widely used as a density model, it\n",
      "suffers from some signiﬁcant limitations. Consider the number of free parameters in\n",
      "the distribution. A general symmetric covariance matrix Σwill have D(D+1 )/2\n",
      "independent parameters, and there are another Dindependent parameters in µ,g i v - Exercise 2.21\n",
      "ingD(D+3 )/2parameters in total. For large D, the total number of parameters84 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.8 Contours of constant\n",
      "probability density for a Gaussian\n",
      "distribution in two dimensions in\n",
      "which the covariance matrix is (a) of\n",
      "general form, (b) diagonal, in which\n",
      "the elliptical contours are aligned\n",
      "with the coordinate axes, and (c)\n",
      "proportional to the identity matrix, in\n",
      "which the contours are concentric\n",
      "circles.x1x2\n",
      "(a)x1x2\n",
      "(b)x1x2\n",
      "(c)\n",
      "therefore grows quadratically with D, and the computational task of manipulating\n",
      "and inverting large matrices can become prohibitive. One way to address this prob-\n",
      "lem is to use restricted forms of the covariance matrix. If we consider covariance\n",
      "matrices that are diagonal , so that Σ= diag( σ2\n",
      "i), we then have a total of 2Dinde-\n",
      "pendent parameters in the density model. The corresponding contours of constant\n",
      "density are given by axis-aligned ellipsoids. We could further restrict the covariance\n",
      "matrix to be proportional to the identity matrix, Σ=σ2I, known as an isotropic co-\n",
      "variance, giving D+1independent parameters in the model and spherical surfaces\n",
      "of constant density. The three possibilities of general, diagonal, and isotropic covari-\n",
      "ance matrices are illustrated in Figure 2.8. Unfortunately, whereas such approaches\n",
      "limit the number of degrees of freedom in the distribution and make inversion of the\n",
      "covariance matrix a much faster operation, they also greatly restrict the form of the\n",
      "probability density and limit its ability to capture interesting correlations in the data.\n",
      "A further limitation of the Gaussian distribution is that it is intrinsically uni-\n",
      "modal (i.e., has a single maximum) and so is unable to provide a good approximation\n",
      "to multimodal distributions. Thus the Gaussian distribution can be both too ﬂexible,\n",
      "in the sense of having too many parameters, while also being too limited in the range\n",
      "of distributions that it can adequately represent. We will see later that the introduc-\n",
      "tion of latent variables, also called hidden variables or unobserved variables, allows\n",
      "both of these problems to be addressed. In particular, a rich family of multimodal\n",
      "distributions is obtained by introducing discrete latent variables leading to mixtures\n",
      "of Gaussians, as discussed in Section 2.3.9. Similarly, the introduction of continuous\n",
      "latent variables, as described in Chapter 12, leads to models in which the number of\n",
      "free parameters can be controlled independently of the dimensionality Dof the data\n",
      "space while still allowing the model to capture the dominant correlations in the data\n",
      "set. Indeed, these two approaches can be combined and further extended to derive\n",
      "a very rich set of hierarchical models that can be adapted to a broad range of prac-\n",
      "tical applications. For instance, the Gaussian version of the Markov random ﬁeld , Section 8.3\n",
      "which is widely used as a probabilistic model of images, is a Gaussian distribution\n",
      "over the joint space of pixel intensities but rendered tractable through the imposition\n",
      "of considerable structure reﬂecting the spatial organization of the pixels. Similarly,\n",
      "the linear dynamical system , used to model time series data for applications such Section 13.3\n",
      "as tracking, is also a joint Gaussian distribution over a potentially large number of\n",
      "observed and latent variables and again is tractable due to the structure imposed on\n",
      "the distribution. A powerful framework for expressing the form and properties of2.3. The Gaussian Distribution 85\n",
      "such complex distributions is that of probabilistic graphical models, which will form\n",
      "the subject of Chapter 8.\n",
      "2.3.1 Conditional Gaussian distributions\n",
      "An important property of the multivariate Gaussian distribution is that if two\n",
      "sets of variables are jointly Gaussian, then the conditional distribution of one set\n",
      "conditioned on the other is again Gaussian. Similarly, the marginal distribution of\n",
      "either set is also Gaussian.\n",
      "Consider ﬁrst the case of conditional distributions. Suppose xis aD-dimensional\n",
      "vector with Gaussian distribution N(x|µ,Σ)and that we partition xinto two dis-\n",
      "joint subsets xaandxb. Without loss of generality, we can take xato form the ﬁrst\n",
      "Mcomponents of x, withxbcomprising the remaining D−Mcomponents, so that\n",
      "x=(\n",
      "xa\n",
      "xb)\n",
      ". (2.65)\n",
      "We also deﬁne corresponding partitions of the mean vector µgiven by\n",
      "µ=(\n",
      "µa\n",
      "µb)\n",
      "(2.66)\n",
      "and of the covariance matrix Σgiven by\n",
      "Σ=(\n",
      "ΣaaΣab\n",
      "ΣbaΣbb)\n",
      ". (2.67)\n",
      "Note that the symmetry ΣT=Σof the covariance matrix implies that ΣaaandΣbb\n",
      "are symmetric, while Σba=ΣT\n",
      "ab.\n",
      "In many situations, it will be convenient to work with the inverse of the covari-\n",
      "ance matrix\n",
      "Λ≡Σ−1(2.68)\n",
      "which is known as the precision matrix . In fact, we shall see that some properties\n",
      "of Gaussian distributions are most naturally expressed in terms of the covariance,\n",
      "whereas others take a simpler form when viewed in terms of the precision. We\n",
      "therefore also introduce the partitioned form of the precision matrix\n",
      "Λ=(\n",
      "ΛaaΛab\n",
      "ΛbaΛbb)\n",
      "(2.69)\n",
      "corresponding to the partitioning (2.65) of the vector x. Because the inverse of a\n",
      "symmetric matrix is also symmetric, we see that ΛaaandΛbbare symmetric, while Exercise 2.22\n",
      "ΛT\n",
      "ab=Λba. It should be stressed at this point that, for instance, Λaais not simply\n",
      "given by the inverse of Σaa. In fact, we shall shortly examine the relation between\n",
      "the inverse of a partitioned matrix and the inverses of its partitions.\n",
      "Let us begin by ﬁnding an expression for the conditional distribution p(xa|xb).\n",
      "From the product rule of probability, we see that this conditional distribution can be86 2. PROBABILITY DISTRIBUTIONS\n",
      "evaluated from the joint distribution p(x)=p(xa,xb)simply by ﬁxing xbto the\n",
      "observed value and normalizing the resulting expression to obtain a valid probabilitydistribution over x\n",
      "a. Instead of performing this normalization explicitly, we can\n",
      "obtain the solution more efﬁciently by considering the quadratic form in the exponent\n",
      "of the Gaussian distribution given by (2.44) and then reinstating the normalizationcoefﬁcient at the end of the calculation. If we make use of the partitioning (2.65),\n",
      "(2.66), and (2.69), we obtain\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)=\n",
      "−1\n",
      "2(xa−µa)TΛaa(xa−µa)−1\n",
      "2(xa−µa)TΛab(xb−µb)\n",
      "−1\n",
      "2(xb−µb)TΛba(xa−µa)−1\n",
      "2(xb−µb)TΛbb(xb−µb).(2.70)\n",
      "We see that as a function of xa, this is again a quadratic form, and hence the cor-\n",
      "responding conditional distribution p(xa|xb)will be Gaussian. Because this distri-\n",
      "bution is completely characterized by its mean and its covariance, our goal will beto identify expressions for the mean and covariance of p(x\n",
      "a|xb)by inspection of\n",
      "(2.70).\n",
      "This is an example of a rather common operation associated with Gaussian\n",
      "distributions, sometimes called ‘completing the square’, in which we are given a\n",
      "quadratic form deﬁning the exponent terms in a Gaussian distribution, and we need\n",
      "to determine the corresponding mean and covariance. Such problems can be solvedstraightforwardly by noting that the exponent in a general Gaussian distribution\n",
      "N(x|µ,Σ)can be written\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)=−1\n",
      "2xTΣ−1x+xTΣ−1µ+c o n s t (2.71)\n",
      "where ‘const’ denotes terms which are independent of x, and we have made use of\n",
      "the symmetry of Σ. Thus if we take our general quadratic form and express it in\n",
      "the form given by the right-hand side of (2.71), then we can immediately equate the\n",
      "matrix of coefﬁcients entering the second order term in xto the inverse covariance\n",
      "matrixΣ−1and the coefﬁcient of the linear term in xtoΣ−1µ, from which we can\n",
      "obtain µ.\n",
      "Now let us apply this procedure to the conditional Gaussian distribution p(xa|xb)\n",
      "for which the quadratic form in the exponent is given by (2.70). We will denote themean and covariance of this distribution by µ\n",
      "a|bandΣa|b, respectively. Consider\n",
      "the functional dependence of (2.70) on xain which xbis regarded as a constant. If\n",
      "we pick out all terms that are second order in xa,w eh a v e\n",
      "−1\n",
      "2xT\n",
      "aΛaaxa (2.72)\n",
      "from which we can immediately conclude that the covariance (inverse precision) of\n",
      "p(xa|xb)is given by\n",
      "Σa|b=Λ−1\n",
      "aa. (2.73)2.3. The Gaussian Distribution 87\n",
      "Now consider all of the terms in (2.70) that are linear in xa\n",
      "xT\n",
      "a{Λaaµa−Λab(xb−µb)} (2.74)\n",
      "where we have used ΛT\n",
      "ba=Λab. From our discussion of the general form (2.71),\n",
      "the coefﬁcient of xain this expression must equal Σ−1\n",
      "a|bµa|band hence\n",
      "µa|b=Σa|b{Λaaµa−Λab(xb−µb)}\n",
      "=µa−Λ−1\n",
      "aaΛab(xb−µb) (2.75)\n",
      "where we have made use of (2.73).\n",
      "The results (2.73) and (2.75) are expressed in terms of the partitioned precision\n",
      "matrix of the original joint distribution p(xa,xb). We can also express these results\n",
      "in terms of the corresponding partitioned covariance matrix. To do this, we make use\n",
      "of the following identity for the inverse of a partitioned matrix Exercise 2.24\n",
      "(\n",
      "AB\n",
      "CD)−1\n",
      "=(\n",
      "M −MBD−1\n",
      "−D−1CM D−1+D−1CMBD−1)\n",
      "(2.76)\n",
      "where we have deﬁned\n",
      "M=(A−BD−1C)−1. (2.77)\n",
      "The quantity M−1is known as the Schur complement of the matrix on the left-hand\n",
      "side of (2.76) with respect to the submatrix D. Using the deﬁnition\n",
      "(\n",
      "ΣaaΣab\n",
      "ΣbaΣbb)−1\n",
      "=(\n",
      "ΛaaΛab\n",
      "ΛbaΛbb)\n",
      "(2.78)\n",
      "and making use of (2.76), we have\n",
      "Λaa=(Σaa−ΣabΣ−1\n",
      "bbΣba)−1(2.79)\n",
      "Λab=−(Σaa−ΣabΣ−1\n",
      "bbΣba)−1ΣabΣ−1\n",
      "bb. (2.80)\n",
      "From these we obtain the following expressions for the mean and covariance of the\n",
      "conditional distribution p(xa|xb)\n",
      "µa|b=µa+ΣabΣ−1\n",
      "bb(xb−µb) (2.81)\n",
      "Σa|b=Σaa−ΣabΣ−1\n",
      "bbΣba. (2.82)\n",
      "Comparing (2.73) and (2.82), we see that the conditional distribution p(xa|xb)takes\n",
      "a simpler form when expressed in terms of the partitioned precision matrix than\n",
      "when it is expressed in terms of the partitioned covariance matrix. Note that the\n",
      "mean of the conditional distribution p(xa|xb), given by (2.81), is a linear function of\n",
      "xband that the covariance, given by (2.82), is independent of xa. This represents an\n",
      "example of a linear-Gaussian model. Section 8.1.488 2. PROBABILITY DISTRIBUTIONS\n",
      "2.3.2 Marginal Gaussian distributions\n",
      "We have seen that if a joint distribution p(xa,xb)is Gaussian, then the condi-\n",
      "tional distribution p(xa|xb)will again be Gaussian. Now we turn to a discussion of\n",
      "the marginal distribution given by\n",
      "p(xa)=∫\n",
      "p(xa,xb)dxb (2.83)\n",
      "which, as we shall see, is also Gaussian. Once again, our strategy for evaluating this\n",
      "distribution efﬁciently will be to focus on the quadratic form in the exponent of the\n",
      "joint distribution and thereby to identify the mean and covariance of the marginal\n",
      "distribution p(xa).\n",
      "The quadratic form for the joint distribution can be expressed, using the par-\n",
      "titioned precision matrix, in the form (2.70). Because our goal is to integrate out\n",
      "xb, this is most easily achieved by ﬁrst considering the terms involving xband then\n",
      "completing the square in order to facilitate integration. Picking out just those terms\n",
      "that involve xb,w eh a v e\n",
      "−1\n",
      "2xT\n",
      "bΛbbxb+xT\n",
      "bm=−1\n",
      "2(xb−Λ−1\n",
      "bbm)TΛbb(xb−Λ−1\n",
      "bbm)+1\n",
      "2mTΛ−1\n",
      "bbm(2.84)\n",
      "where we have deﬁned\n",
      "m=Λbbµb−Λba(xa−µa). (2.85)\n",
      "We see that the dependence on xbhas been cast into the standard quadratic form of a\n",
      "Gaussian distribution corresponding to the ﬁrst term on the right-hand side of (2.84),\n",
      "plus a term that does not depend on xb(but that does depend on xa). Thus, when\n",
      "we take the exponential of this quadratic form, we see that the integration over xb\n",
      "required by (2.83) will take the form\n",
      "∫\n",
      "exp{\n",
      "−1\n",
      "2(xb−Λ−1\n",
      "bbm)TΛbb(xb−Λ−1\n",
      "bbm)}\n",
      "dxb. (2.86)\n",
      "This integration is easily performed by noting that it is the integral over an unnor-\n",
      "malized Gaussian, and so the result will be the reciprocal of the normalization co-efﬁcient. We know from the form of the normalized Gaussian given by (2.43), that\n",
      "this coefﬁcient is independent of the mean and depends only on the determinant of\n",
      "the covariance matrix. Thus, by completing the square with respect to x\n",
      "b, we can\n",
      "integrate out xband the only term remaining from the contributions on the left-hand\n",
      "side of (2.84) that depends on xais the last term on the right-hand side of (2.84) in\n",
      "whichmis given by (2.85). Combining this term with the remaining terms from2.3. The Gaussian Distribution 89\n",
      "(2.70) that depend on xa, we obtain\n",
      "1\n",
      "2[Λbbµb−Λba(xa−µa)]TΛ−1\n",
      "bb[Λbbµb−Λba(xa−µa)]\n",
      "−1\n",
      "2xT\n",
      "aΛaaxa+xT\n",
      "a(Λaaµa+Λabµb)+c o n s t\n",
      "=−1\n",
      "2xT\n",
      "a(Λaa−ΛabΛ−1\n",
      "bbΛba)xa\n",
      "+xT\n",
      "a(Λaa−ΛabΛ−1\n",
      "bbΛba)−1µa+c o n s t (2.87)\n",
      "where ‘const’ denotes quantities independent of xa. Again, by comparison with\n",
      "(2.71), we see that the covariance of the marginal distribution of p(xa)is given by\n",
      "Σa=(Λaa−ΛabΛ−1\n",
      "bbΛba)−1. (2.88)\n",
      "Similarly, the mean is given by\n",
      "Σa(Λaa−ΛabΛ−1\n",
      "bbΛba)µa=µa (2.89)\n",
      "where we have used (2.88). The covariance in (2.88) is expressed in terms of the\n",
      "partitioned precision matrix given by (2.69). We can rewrite this in terms of the\n",
      "corresponding partitioning of the covariance matrix given by (2.67), as we did forthe conditional distribution. These partitioned matrices are related by\n",
      "(\n",
      "ΛaaΛab\n",
      "ΛbaΛbb)−1\n",
      "=(\n",
      "ΣaaΣab\n",
      "ΣbaΣbb)\n",
      "(2.90)\n",
      "Making use of (2.76), we then have\n",
      "(\n",
      "Λaa−ΛabΛ−1\n",
      "bbΛba)−1=Σaa. (2.91)\n",
      "Thus we obtain the intuitively satisfying result that the marginal distribution p(xa)\n",
      "has mean and covariance given by\n",
      "E[xa]= µa (2.92)\n",
      "cov[xa]= Σaa. (2.93)\n",
      "We see that for a marginal distribution, the mean and covariance are most simply ex-\n",
      "pressed in terms of the partitioned covariance matrix, in contrast to the conditional\n",
      "distribution for which the partitioned precision matrix gives rise to simpler expres-\n",
      "sions.\n",
      "Our results for the marginal and conditional distributions of a partitioned Gaus-\n",
      "sian are summarized below.\n",
      "Partitioned Gaussians\n",
      "Given a joint Gaussian distribution N(x|µ,Σ)withΛ≡Σ−1and\n",
      "x=(\n",
      "xa\n",
      "xb)\n",
      ",µ=(\n",
      "µa\n",
      "µb)\n",
      "(2.94)90 2. PROBABILITY DISTRIBUTIONS\n",
      "xaxb=0.7xb\n",
      "p(xa,xb)\n",
      "0 0.5 100.51\n",
      "xap(xa)p(xa|xb=0.7)\n",
      "0 0.5 10510\n",
      "Figure 2.9 The plot on the left shows the contours of a Gaussian distribution p(xa,xb)over two variables, and\n",
      "the plot on the right shows the marginal distribution p(xa)(blue curve) and the conditional distribution p(xa|xb)\n",
      "forxb=0.7(red curve).\n",
      "Σ=(\n",
      "ΣaaΣab\n",
      "ΣbaΣbb)\n",
      ",Λ=(\n",
      "ΛaaΛab\n",
      "ΛbaΛbb)\n",
      ". (2.95)\n",
      "Conditional distribution:\n",
      "p(xa|xb)= N(x|µa|b,Λ−1\n",
      "aa) (2.96)\n",
      "µa|b=µa−Λ−1\n",
      "aaΛab(xb−µb). (2.97)\n",
      "Marginal distribution:\n",
      "p(xa)=N(xa|µa,Σaa). (2.98)\n",
      "We illustrate the idea of conditional and marginal distributions associated with\n",
      "a multivariate Gaussian using an example involving two variables in Figure 2.9.\n",
      "2.3.3 Bayes’ theorem for Gaussian variables\n",
      "In Sections 2.3.1 and 2.3.2, we considered a Gaussian p(x)in which we parti-\n",
      "tioned the vector xinto two subvectors x=(xa,xb)and then found expressions for\n",
      "the conditional distribution p(xa|xb)and the marginal distribution p(xa). We noted\n",
      "that the mean of the conditional distribution p(xa|xb)was a linear function of xb.\n",
      "Here we shall suppose that we are given a Gaussian marginal distribution p(x)and a\n",
      "Gaussian conditional distribution p(y|x)in which p(y|x)has a mean that is a linear\n",
      "function of x, and a covariance which is independent of x. This is an example of2.3. The Gaussian Distribution 91\n",
      "alinear Gaussian model (Roweis and Ghahramani, 1999), which we shall study in\n",
      "greater generality in Section 8.1.4. We wish to ﬁnd the marginal distribution p(y)\n",
      "and the conditional distribution p(x|y). This is a problem that will arise frequently\n",
      "in subsequent chapters, and it will prove convenient to derive the general results here.\n",
      "We shall take the marginal and conditional distributions to be\n",
      "p(x)= N(\n",
      "x|µ,Λ−1)\n",
      "(2.99)\n",
      "p(y|x)= N(\n",
      "y|Ax+b,L−1)\n",
      "(2.100)\n",
      "where µ,A, andbare parameters governing the means, and ΛandLare precision\n",
      "matrices. If xhas dimensionality Mandyhas dimensionality D, then the matrix A\n",
      "has size D×M.\n",
      "First we ﬁnd an expression for the joint distribution over xandy. To do this, we\n",
      "deﬁne\n",
      "z=(\n",
      "x\n",
      "y)\n",
      "(2.101)\n",
      "and then consider the log of the joint distribution\n",
      "lnp(z)=l n p(x)+l n p(y|x)\n",
      "=−1\n",
      "2(x−µ)TΛ(x−µ)\n",
      "−1\n",
      "2(y−Ax−b)TL(y−Ax−b) + const (2.102)\n",
      "where ‘const’ denotes terms independent of xandy. As before, we see that this is a\n",
      "quadratic function of the components of z, and hence p(z)is Gaussian distribution.\n",
      "To ﬁnd the precision of this Gaussian, we consider the second order terms in (2.102),which can be written as\n",
      "−1\n",
      "2xT(Λ+ATLA)x−1\n",
      "2yTLy+1\n",
      "2yTLAx+1\n",
      "2xTATLy\n",
      "=−1\n",
      "2(\n",
      "x\n",
      "y)T(\n",
      "Λ+ATLA−ATL\n",
      "−LA L)(\n",
      "x\n",
      "y)\n",
      "=−1\n",
      "2zTRz (2.103)\n",
      "and so the Gaussian distribution over zhas precision (inverse covariance) matrix\n",
      "given by\n",
      "R=(\n",
      "Λ+ATLA−ATL\n",
      "−LA L)\n",
      ". (2.104)\n",
      "The covariance matrix is found by taking the inverse of the precision, which can be\n",
      "done using the matrix inversion formula (2.76) to give Exercise 2.29\n",
      "cov[z]=R−1=(\n",
      "Λ−1Λ−1AT\n",
      "AΛ−1L−1+AΛ−1AT)\n",
      ". (2.105)92 2. PROBABILITY DISTRIBUTIONS\n",
      "Similarly, we can ﬁnd the mean of the Gaussian distribution over zby identify-\n",
      "ing the linear terms in (2.102), which are given by\n",
      "xTΛµ−xTATLb+yTLb=(\n",
      "x\n",
      "y)T(\n",
      "Λµ−ATLb\n",
      "Lb)\n",
      ". (2.106)\n",
      "Using our earlier result (2.71) obtained by completing the square over the quadratic\n",
      "form of a multivariate Gaussian, we ﬁnd that the mean of zis given by\n",
      "E[z]=R−1(\n",
      "Λµ−ATLb\n",
      "Lb)\n",
      ". (2.107)\n",
      "Making use of (2.105), we then obtain Exercise 2.30\n",
      "E[z]=(\n",
      "µ\n",
      "Aµ+b)\n",
      ". (2.108)\n",
      "Next we ﬁnd an expression for the marginal distribution p(y)in which we have\n",
      "marginalized over x. Recall that the marginal distribution over a subset of the com-\n",
      "ponents of a Gaussian random vector takes a particularly simple form when ex-pressed in terms of the partitioned covariance matrix. Speciﬁcally, its mean and Section 2.3\n",
      "covariance are given by (2.92) and (2.93), respectively. Making use of (2.105) and\n",
      "(2.108) we see that the mean and covariance of the marginal distribution p(y)are\n",
      "given by\n",
      "E[y]= Aµ+b (2.109)\n",
      "cov[y]= L\n",
      "−1+AΛ−1AT. (2.110)\n",
      "A special case of this result is when A=I, in which case it reduces to the convolu-\n",
      "tion of two Gaussians, for which we see that the mean of the convolution is the sum\n",
      "of the mean of the two Gaussians, and the covariance of the convolution is the sumof their covariances.\n",
      "Finally, we seek an expression for the conditional p(x|y). Recall that the results\n",
      "for the conditional distribution are most easily expressed in terms of the partitioned\n",
      "precision matrix, using (2.73) and (2.75). Applying these results to (2.105) and Section 2.3\n",
      "(2.108) we see that the conditional distribution p(x|y)has mean and covariance\n",
      "given by\n",
      "E[x|y]=( Λ+A\n",
      "TLA)−1{\n",
      "ATL(y−b)+Λµ}\n",
      "(2.111)\n",
      "cov[x|y]=( Λ+ATLA)−1. (2.112)\n",
      "The evaluation of this conditional can be seen as an example of Bayes’ theorem.\n",
      "We can interpret the distribution p(x)as a prior distribution over x. If the variable\n",
      "yis observed, then the conditional distribution p(x|y)represents the corresponding\n",
      "posterior distribution over x. Having found the marginal and conditional distribu-\n",
      "tions, we effectively expressed the joint distribution p(z)=p(x)p(y|x)in the form\n",
      "p(x|y)p(y). These results are summarized below.2.3. The Gaussian Distribution 93\n",
      "Marginal and Conditional Gaussians\n",
      "Given a marginal Gaussian distribution for xand a conditional Gaussian distri-\n",
      "bution for ygivenxin the form\n",
      "p(x)= N(x|µ,Λ−1) (2.113)\n",
      "p(y|x)= N(y|Ax+b,L−1) (2.114)\n",
      "the marginal distribution of yand the conditional distribution of xgivenyare\n",
      "given by\n",
      "p(y)= N(y|Aµ+b,L−1+AΛ−1AT) (2.115)\n",
      "p(x|y)= N(x|Σ{ATL(y−b)+Λµ},Σ) (2.116)\n",
      "where\n",
      "Σ=(Λ+ATLA)−1. (2.117)\n",
      "2.3.4 Maximum likelihood for the Gaussian\n",
      "Given a data set X=(x1,...,xN)Tin which the observations {xn}are as-\n",
      "sumed to be drawn independently from a multivariate Gaussian distribution, we canestimate the parameters of the distribution by maximum likelihood. The log likeli-\n",
      "hood function is given by\n",
      "lnp(X|µ,Σ)=−ND\n",
      "2ln(2π)−N\n",
      "2ln|Σ|−1\n",
      "2N∑\n",
      "n=1(xn−µ)TΣ−1(xn−µ).(2.118)\n",
      "By simple rearrangement, we see that the likelihood function depends on the data set\n",
      "only through the two quantities\n",
      "N∑\n",
      "n=1xn,N∑\n",
      "n=1xnxT\n",
      "n. (2.119)\n",
      "These are known as the sufﬁcient statistics for the Gaussian distribution. Using\n",
      "(C.19), the derivative of the log likelihood with respect to µis given by Appendix C\n",
      "∂\n",
      "∂µlnp(X|µ,Σ)=N∑\n",
      "n=1Σ−1(xn−µ) (2.120)\n",
      "and setting this derivative to zero, we obtain the solution for the maximum likelihood\n",
      "estimate of the mean given by\n",
      "µML=1\n",
      "NN∑\n",
      "n=1xn (2.121)94 2. PROBABILITY DISTRIBUTIONS\n",
      "which is the mean of the observed set of data points. The maximization of (2.118)\n",
      "with respect to Σis rather more involved. The simplest approach is to ignore the\n",
      "symmetry constraint and show that the resulting solution is symmetric as required. Exercise 2.34\n",
      "Alternative derivations of this result, which impose the symmetry and positive deﬁ-\n",
      "niteness constraints explicitly, can be found in Magnus and Neudecker (1999). Theresult is as expected and takes the form\n",
      "Σ\n",
      "ML=1\n",
      "NN∑\n",
      "n=1(xn−µML)(xn−µML)T(2.122)\n",
      "which involves µMLbecause this is the result of a joint maximization with respect\n",
      "toµandΣ. Note that the solution (2.121) for µMLdoes not depend on ΣML, and so\n",
      "we can ﬁrst evaluate µMLand then use this to evaluate ΣML.\n",
      "If we evaluate the expectations of the maximum likelihood solutions under the\n",
      "true distribution, we obtain the following results Exercise 2.35\n",
      "E[µML]= µ (2.123)\n",
      "E[ΣML]=N−1\n",
      "NΣ. (2.124)\n",
      "We see that the expectation of the maximum likelihood estimate for the mean is equal\n",
      "to the true mean. However, the maximum likelihood estimate for the covariance hasan expectation that is less than the true value, and hence it is biased. We can correct\n",
      "this bias by deﬁning a different estimator˜Σgiven by\n",
      "˜Σ=1\n",
      "N−1N∑\n",
      "n=1(xn−µML)(xn−µML)T. (2.125)\n",
      "Clearly from (2.122) and (2.124), the expectation of ˜Σis equal to Σ.\n",
      "2.3.5 Sequential estimation\n",
      "Our discussion of the maximum likelihood solution for the parameters of a Gaus-\n",
      "sian distribution provides a convenient opportunity to give a more general discussion\n",
      "of the topic of sequential estimation for maximum likelihood. Sequential methods\n",
      "allow data points to be processed one at a time and then discarded and are important\n",
      "for on-line applications, and also where large data sets are involved so that batch\n",
      "processing of all data points at once is infeasible.\n",
      "Consider the result (2.121) for the maximum likelihood estimator of the mean\n",
      "µML, which we will denote by µ(N)\n",
      "MLwhen it is based on Nobservations. If we2.3. The Gaussian Distribution 95\n",
      "Figure 2.10 A schematic illustration of two correlated ran-\n",
      "dom variables zandθ, together with the\n",
      "regression function f(θ)given by the con-\n",
      "ditional expectation E[z|θ]. The Robbins-\n",
      "Monro algorithm provides a general sequen-\n",
      "tial procedure for ﬁnding the root θ⋆of such\n",
      "functions. θz\n",
      "θ⋆f(θ)\n",
      "dissect out the contribution from the ﬁnal data point xN, we obtain\n",
      "µ(N)\n",
      "ML=1\n",
      "NN∑\n",
      "n=1xn\n",
      "=1\n",
      "NxN+1\n",
      "NN−1∑\n",
      "n=1xn\n",
      "=1\n",
      "NxN+N−1\n",
      "Nµ(N−1)\n",
      "ML\n",
      "=µ(N−1)\n",
      "ML +1\n",
      "N(xN−µ(N−1)\n",
      "ML). (2.126)\n",
      "This result has a nice interpretation, as follows. After observing N−1data points\n",
      "we have estimated µbyµ(N−1)\n",
      "ML . We now observe data point xN, and we obtain our\n",
      "revised estimate µ(N)\n",
      "MLby moving the old estimate a small amount, proportional to\n",
      "1/N, in the direction of the ‘error signal’ (xN−µ(N−1)\n",
      "ML). Note that, as Nincreases,\n",
      "so the contribution from successive data points gets smaller.\n",
      "The result (2.126) will clearly give the same answer as the batch result (2.121)\n",
      "because the two formulae are equivalent. However, we will not always be able to de-\n",
      "rive a sequential algorithm by this route, and so we seek a more general formulation\n",
      "of sequential learning, which leads us to the Robbins-Monro algorithm. Consider a\n",
      "pair of random variables θandzgoverned by a joint distribution p(z,θ). The con-\n",
      "ditional expectation of zgivenθdeﬁnes a deterministic function f(θ)that is given\n",
      "by\n",
      "f(θ)≡E[z|θ]=∫\n",
      "zp(z|θ)dz (2.127)\n",
      "and is illustrated schematically in Figure 2.10. Functions deﬁned in this way are\n",
      "called regression functions .\n",
      "Our goal is to ﬁnd the root θ⋆at which f(θ⋆)=0 . If we had a large data set\n",
      "of observations of zandθ, then we could model the regression function directly and\n",
      "then obtain an estimate of its root. Suppose, however, that we observe values of\n",
      "zone at a time and we wish to ﬁnd a corresponding sequential estimation scheme\n",
      "forθ⋆. The following general procedure for solving such problems was given by96 2. PROBABILITY DISTRIBUTIONS\n",
      "Robbins and Monro (1951). We shall assume that the conditional variance of zis\n",
      "ﬁnite so that\n",
      "E[\n",
      "(z−f)2|θ]\n",
      "<∞ (2.128)\n",
      "and we shall also, without loss of generality, consider the case where f(θ)>0for\n",
      "θ>θ⋆andf(θ)<0forθ<θ⋆, as is the case in Figure 2.10. The Robbins-Monro\n",
      "procedure then deﬁnes a sequence of successive estimates of the root θ⋆given by\n",
      "θ(N)=θ(N−1)+aN−1z(θ(N−1)) (2.129)\n",
      "where z(θ(N))is an observed value of zwhenθtakes the value θ(N). The coefﬁcients\n",
      "{aN}represent a sequence of positive numbers that satisfy the conditions\n",
      "lim\n",
      "N→∞aN=0 (2.130)\n",
      "∞∑\n",
      "N=1aN=∞ (2.131)\n",
      "∞∑\n",
      "N=1a2\n",
      "N<∞. (2.132)\n",
      "It can then be shown (Robbins and Monro, 1951; Fukunaga, 1990) that the sequence\n",
      "of estimates given by (2.129) does indeed converge to the root with probability one.\n",
      "Note that the ﬁrst condition (2.130) ensures that the successive corrections decrease\n",
      "in magnitude so that the process can converge to a limiting value. The second con-dition (2.131) is required to ensure that the algorithm does not converge short of the\n",
      "root, and the third condition (2.132) is needed to ensure that the accumulated noise\n",
      "has ﬁnite variance and hence does not spoil convergence.\n",
      "Now let us consider how a general maximum likelihood problem can be solved\n",
      "sequentially using the Robbins-Monro algorithm. By deﬁnition, the maximum like-lihood solution θ\n",
      "MLis a stationary point of the log likelihood function and hence\n",
      "satisﬁes\n",
      "∂\n",
      "∂θ{\n",
      "1\n",
      "NN∑\n",
      "n=1lnp(xn|θ)}⏐⏐⏐⏐⏐\n",
      "θML=0. (2.133)\n",
      "Exchanging the derivative and the summation, and taking the limit N→∞ we have\n",
      "lim\n",
      "N→∞1\n",
      "NN∑\n",
      "n=1∂\n",
      "∂θlnp(xn|θ)= Ex[∂\n",
      "∂θlnp(x|θ)]\n",
      "(2.134)\n",
      "and so we see that ﬁnding the maximum likelihood solution corresponds to ﬁnd-\n",
      "ing the root of a regression function. We can therefore apply the Robbins-Monroprocedure, which now takes the form\n",
      "θ\n",
      "(N)=θ(N−1)+aN−1∂\n",
      "∂θ(N−1)lnp(xN|θ(N−1)). (2.135)2.3. The Gaussian Distribution 97\n",
      "Figure 2.11 In the case of a Gaussian distribution, with θ\n",
      "corresponding to the mean µ, the regression\n",
      "function illustrated in Figure 2.10 takes the form\n",
      "of a straight line, as shown in red. In this\n",
      "case, the random variable zcorresponds to the\n",
      "derivative of the log likelihood function and is\n",
      "given by (x−µML)/σ2, and its expectation that\n",
      "deﬁnes the regression function is a straight line\n",
      "given by (µ−µML)/σ2. The root of the regres-\n",
      "sion function corresponds to the maximum like-\n",
      "lihood estimator µML.µz\n",
      "p(z|µ)\n",
      "µML\n",
      "As a speciﬁc example, we consider once again the sequential estimation of the\n",
      "mean of a Gaussian distribution, in which case the parameter θ(N)is the estimate\n",
      "µ(N)\n",
      "MLof the mean of the Gaussian, and the random variable zis given by\n",
      "z=∂\n",
      "∂µMLlnp(x|µML,σ2)=1\n",
      "σ2(x−µML). (2.136)\n",
      "Thus the distribution of zis Gaussian with mean µ−µML, as illustrated in Fig-\n",
      "ure 2.11. Substituting (2.136) into (2.135), we obtain the univariate form of (2.126),\n",
      "provided we choose the coefﬁcients aNto have the form aN=σ2/N. Note that\n",
      "although we have focussed on the case of a single variable, the same technique,\n",
      "together with the same restrictions (2.130)–(2.132) on the coefﬁcients aN, apply\n",
      "equally to the multivariate case (Blum, 1965).\n",
      "2.3.6 Bayesian inference for the Gaussian\n",
      "The maximum likelihood framework gave point estimates for the parameters µ\n",
      "andΣ. Now we develop a Bayesian treatment by introducing prior distributions\n",
      "over these parameters. Let us begin with a simple example in which we consider a\n",
      "single Gaussian random variable x. We shall suppose that the variance σ2is known,\n",
      "and we consider the task of inferring the mean µgiven a set of Nobservations\n",
      "X={x1,...,x N}. The likelihood function, that is the probability of the observed\n",
      "data given µ, viewed as a function of µ,i sg i v e nb y\n",
      "p(X|µ)=N∏\n",
      "n=1p(xn|µ)=1\n",
      "(2πσ2)N/2exp{\n",
      "−1\n",
      "2σ2N∑\n",
      "n=1(xn−µ)2}\n",
      ".(2.137)\n",
      "Again we emphasize that the likelihood function p(X|µ)is not a probability distri-\n",
      "bution over µand is not normalized.\n",
      "We see that the likelihood function takes the form of the exponential of a quad-\n",
      "ratic form in µ. Thus if we choose a prior p(µ)given by a Gaussian, it will be a98 2. PROBABILITY DISTRIBUTIONS\n",
      "conjugate distribution for this likelihood function because the corresponding poste-\n",
      "rior will be a product of two exponentials of quadratic functions of µand hence will\n",
      "also be Gaussian. We therefore take our prior distribution to be\n",
      "p(µ)=N(\n",
      "µ|µ0,σ2\n",
      "0)\n",
      "(2.138)\n",
      "and the posterior distribution is given by\n",
      "p(µ|X)∝p(X|µ)p(µ). (2.139)\n",
      "Simple manipulation involving completing the square in the exponent shows that the Exercise 2.38\n",
      "posterior distribution is given by\n",
      "p(µ|X)=N(\n",
      "µ|µN,σ2\n",
      "N)\n",
      "(2.140)\n",
      "where\n",
      "µN=σ2\n",
      "Nσ2\n",
      "0+σ2µ0+Nσ2\n",
      "0\n",
      "Nσ2\n",
      "0+σ2µML (2.141)\n",
      "1\n",
      "σ2\n",
      "N=1\n",
      "σ2\n",
      "0+N\n",
      "σ2(2.142)\n",
      "in which µMLis the maximum likelihood solution for µgiven by the sample mean\n",
      "µML=1\n",
      "NN∑\n",
      "n=1xn. (2.143)\n",
      "It is worth spending a moment studying the form of the posterior mean and\n",
      "variance. First of all, we note that the mean of the posterior distribution given by\n",
      "(2.141) is a compromise between the prior mean µ0and the maximum likelihood\n",
      "solution µML. If the number of observed data points N=0, then (2.141) reduces\n",
      "to the prior mean as expected. For N→∞ , the posterior mean is given by the\n",
      "maximum likelihood solution. Similarly, consider the result (2.142) for the variance\n",
      "of the posterior distribution. We see that this is most naturally expressed in termsof the inverse variance, which is called the precision. Furthermore, the precisions\n",
      "are additive, so that the precision of the posterior is given by the precision of the\n",
      "prior plus one contribution of the data precision from each of the observed datapoints. As we increase the number of observed data points, the precision steadily\n",
      "increases, corresponding to a posterior distribution with steadily decreasing variance.\n",
      "With no observed data points, we have the prior variance, whereas if the number of\n",
      "data points N→∞ , the variance σ\n",
      "2\n",
      "Ngoes to zero and the posterior distribution\n",
      "becomes inﬁnitely peaked around the maximum likelihood solution. We thereforesee that the maximum likelihood result of a point estimate for µgiven by (2.143) is\n",
      "recovered precisely from the Bayesian formalism in the limit of an inﬁnite number\n",
      "of observations. Note also that for ﬁnite N, if we take the limit σ\n",
      "2\n",
      "0→∞ in which the\n",
      "prior has inﬁnite variance then the posterior mean (2.141) reduces to the maximum\n",
      "likelihood result, while from (2.142) the posterior variance is given by σ2\n",
      "N=σ2/N.2.3. The Gaussian Distribution 99\n",
      "Figure 2.12 Illustration of Bayesian inference for\n",
      "the mean µof a Gaussian distri-\n",
      "bution, in which the variance is as-\n",
      "sumed to be known. The curves\n",
      "show the prior distribution over µ\n",
      "(the curve labelled N=0), which\n",
      "in this case is itself Gaussian, along\n",
      "with the posterior distribution given\n",
      "by (2.140) for increasing numbers N\n",
      "of data points. The data points are\n",
      "generated from a Gaussian of mean\n",
      "0.8and variance 0.1, and the prior is\n",
      "chosen to have mean 0. In both the\n",
      "prior and the likelihood function, the\n",
      "variance is set to the true value.N=0N=1N=2N=1 0\n",
      "−1 0 105\n",
      "We illustrate our analysis of Bayesian inference for the mean of a Gaussian\n",
      "distribution in Figure 2.12. The generalization of this result to the case of a D-\n",
      "dimensional Gaussian random variable xwith known covariance and unknown mean\n",
      "is straightforward. Exercise 2.40\n",
      "We have already seen how the maximum likelihood expression for the mean of\n",
      "a Gaussian can be re-cast as a sequential update formula in which the mean after Section 2.3.5\n",
      "observing Ndata points was expressed in terms of the mean after observing N−1\n",
      "data points together with the contribution from data point xN. In fact, the Bayesian\n",
      "paradigm leads very naturally to a sequential view of the inference problem. To see\n",
      "this in the context of the inference of the mean of a Gaussian, we write the posterior\n",
      "distribution with the contribution from the ﬁnal data point xNseparated out so that\n",
      "p(µ|D)∝[\n",
      "p(µ)N−1∏\n",
      "n=1p(xn|µ)]\n",
      "p(xN|µ). (2.144)\n",
      "The term in square brackets is (up to a normalization coefﬁcient) just the posterior\n",
      "distribution after observing N−1data points. We see that this can be viewed as\n",
      "a prior distribution, which is combined using Bayes’ theorem with the likelihood\n",
      "function associated with data point xNto arrive at the posterior distribution after\n",
      "observing Ndata points. This sequential view of Bayesian inference is very general\n",
      "and applies to any problem in which the observed data are assumed to be independent\n",
      "and identically distributed.\n",
      "So far, we have assumed that the variance of the Gaussian distribution over the\n",
      "data is known and our goal is to infer the mean. Now let us suppose that the mean\n",
      "is known and we wish to infer the variance. Again, our calculations will be greatly\n",
      "simpliﬁed if we choose a conjugate form for the prior distribution. It turns out to be\n",
      "most convenient to work with the precision λ≡1/σ2. The likelihood function for λ\n",
      "takes the form\n",
      "p(X|λ)=N∏\n",
      "n=1N(xn|µ, λ−1)∝λN/2exp{\n",
      "−λ\n",
      "2N∑\n",
      "n=1(xn−µ)2}\n",
      ". (2.145)100 2. PROBABILITY DISTRIBUTIONS\n",
      "λa=0.1\n",
      "b=0.1\n",
      "0 1 2012\n",
      "λa=1\n",
      "b=1\n",
      "0 1 2012\n",
      "λa=4\n",
      "b=6\n",
      "0 1 2012\n",
      "Figure 2.13 Plot of the gamma distribution Gam( λ|a, b)deﬁned by (2.146) for various values of the parameters\n",
      "aandb.\n",
      "The corresponding conjugate prior should therefore be proportional to the product\n",
      "of a power of λand the exponential of a linear function of λ. This corresponds to\n",
      "thegamma distribution which is deﬁned by\n",
      "Gam( λ|a, b)=1\n",
      "Γ(a)baλa−1exp(−bλ). (2.146)\n",
      "HereΓ(a)is the gamma function that is deﬁned by (1.141) and that ensures that\n",
      "(2.146) is correctly normalized. The gamma distribution has a ﬁnite integral if a>0, Exercise 2.41\n",
      "and the distribution itself is ﬁnite if a⩾1. It is plotted, for various values of aand\n",
      "b, in Figure 2.13. The mean and variance of the gamma distribution are given by Exercise 2.42\n",
      "E[λ]=a\n",
      "b(2.147)\n",
      "var[λ]=a\n",
      "b2. (2.148)\n",
      "Consider a prior distribution Gam( λ|a0,b0). If we multiply by the likelihood\n",
      "function (2.145), then we obtain a posterior distribution\n",
      "p(λ|X)∝λa0−1λN/2exp{\n",
      "−b0λ−λ\n",
      "2N∑\n",
      "n=1(xn−µ)2}\n",
      "(2.149)\n",
      "which we recognize as a gamma distribution of the form Gam( λ|aN,bN)where\n",
      "aN=a0+N\n",
      "2(2.150)\n",
      "bN=b0+1\n",
      "2N∑\n",
      "n=1(xn−µ)2=b0+N\n",
      "2σ2\n",
      "ML (2.151)\n",
      "where σ2\n",
      "MLis the maximum likelihood estimator of the variance. Note that in (2.149)\n",
      "there is no need to keep track of the normalization constants in the prior and the\n",
      "likelihood function because, if required, the correct coefﬁcient can be found at the\n",
      "end using the normalized form (2.146) for the gamma distribution.2.3. The Gaussian Distribution 101\n",
      "From (2.150), we see that the effect of observing Ndata points is to increase\n",
      "the value of the coefﬁcient abyN/2. Thus we can interpret the parameter a0in\n",
      "the prior in terms of 2a0‘effective’ prior observations. Similarly, from (2.151) we\n",
      "see that the Ndata points contribute Nσ2\n",
      "ML/2to the parameter b, where σ2\n",
      "MLis\n",
      "the variance, and so we can interpret the parameter b0in the prior as arising from\n",
      "the2a0‘effective’ prior observations having variance 2b0/(2a0)=b0/a0. Recall\n",
      "that we made an analogous interpretation for the Dirichlet prior. These distributions Section 2.2\n",
      "are examples of the exponential family, and we shall see that the interpretation ofa conjugate prior in terms of effective ﬁctitious data points is a general one for the\n",
      "exponential family of distributions.\n",
      "Instead of working with the precision, we can consider the variance itself. The\n",
      "conjugate prior in this case is called the inverse gamma distribution, although we\n",
      "shall not discuss this further because we will ﬁnd it more convenient to work withthe precision.\n",
      "Now suppose that both the mean and the precision are unknown. To ﬁnd a\n",
      "conjugate prior, we consider the dependence of the likelihood function on µandλ\n",
      "p(X|µ, λ)=\n",
      "N∏\n",
      "n=1(λ\n",
      "2π)1/2\n",
      "exp{\n",
      "−λ\n",
      "2(xn−µ)2}\n",
      "∝[\n",
      "λ1/2exp(\n",
      "−λµ2\n",
      "2)]N\n",
      "exp{\n",
      "λµN∑\n",
      "n=1xn−λ\n",
      "2N∑\n",
      "n=1x2\n",
      "n}\n",
      ".(2.152)\n",
      "We now wish to identify a prior distribution p(µ, λ)that has the same functional\n",
      "dependence on µandλas the likelihood function and that should therefore take the\n",
      "form\n",
      "p(µ, λ)∝[\n",
      "λ1/2exp(\n",
      "−λµ2\n",
      "2)]β\n",
      "exp{cλµ−dλ}\n",
      "=e x p{\n",
      "−βλ\n",
      "2(µ−c/β)2}\n",
      "λβ/2exp{\n",
      "−(\n",
      "d−c2\n",
      "2β)\n",
      "λ}\n",
      "(2.153)\n",
      "where c,d, andβare constants. Since we can always write p(µ, λ)=p(µ|λ)p(λ),\n",
      "we can ﬁnd p(µ|λ)andp(λ)by inspection. In particular, we see that p(µ|λ)is a\n",
      "Gaussian whose precision is a linear function of λand that p(λ)is a gamma distri-\n",
      "bution, so that the normalized prior takes the form\n",
      "p(µ, λ)=N(µ|µ0,(βλ)−1)Gam( λ|a, b) (2.154)\n",
      "where we have deﬁned new constants given by µ0=c/β,a=1 + β/2,b=\n",
      "d−c2/2β. The distribution (2.154) is called the normal-gamma orGaussian-gamma\n",
      "distribution and is plotted in Figure 2.14. Note that this is not simply the product\n",
      "of an independent Gaussian prior over µand a gamma prior over λ, because the\n",
      "precision of µis a linear function of λ. Even if we chose a prior in which µandλ\n",
      "were independent, the posterior distribution would exhibit a coupling between the\n",
      "precision of µand the value of λ.102 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.14 Contour plot of the normal-gamma\n",
      "distribution (2.154) for parameter\n",
      "values µ0=0,β=2,a=5and\n",
      "b=6.\n",
      "µλ\n",
      "−2 0 2012\n",
      "In the case of the multivariate Gaussian distribution N(\n",
      "x|µ,Λ−1)\n",
      "for aD-\n",
      "dimensional variable x, the conjugate prior distribution for the mean µ, assuming\n",
      "the precision is known, is again a Gaussian. For known mean and unknown precision\n",
      "matrixΛ, the conjugate prior is the Wishart distribution given by Exercise 2.45\n",
      "W(Λ|W,ν)=B|Λ|(ν−D−1)/2exp(\n",
      "−1\n",
      "2Tr(W−1Λ))\n",
      "(2.155)\n",
      "where νis called the number of degrees of freedom of the distribution, Wis aD×D\n",
      "scale matrix, and Tr (·)denotes the trace. The normalization constant Bis given by\n",
      "B(W,ν)=|W|−ν/2(\n",
      "2νD/2πD(D−1)/4D∏\n",
      "i=1Γ(ν+1−i\n",
      "2))−1\n",
      ". (2.156)\n",
      "Again, it is also possible to deﬁne a conjugate prior over the covariance matrix itself,\n",
      "rather than over the precision matrix, which leads to the inverse Wishart distribu-\n",
      "tion, although we shall not discuss this further. If both the mean and the precision\n",
      "are unknown, then, following a similar line of reasoning to the univariate case, the\n",
      "conjugate prior is given by\n",
      "p(µ,Λ|µ0,β,W,ν)=N(µ|µ0,(βΛ)−1)W(Λ|W,ν) (2.157)\n",
      "which is known as the normal-Wishart orGaussian-Wishart distribution.\n",
      "2.3.7 Student’s t-distribution\n",
      "We have seen that the conjugate prior for the precision of a Gaussian is given\n",
      "by a gamma distribution. If we have a univariate Gaussian N(x|µ, τ−1)together Section 2.3.6\n",
      "with a Gamma prior Gam( τ|a, b)and we integrate out the precision, we obtain the\n",
      "marginal distribution of xin the form Exercise 2.462.3. The Gaussian Distribution 103\n",
      "Figure 2.15 Plot of Student’s t-distribution (2.159)\n",
      "forµ=0andλ=1for various values\n",
      "ofν. The limit ν→∞ corresponds\n",
      "to a Gaussian distribution with mean\n",
      "µand precision λ.ν→∞\n",
      "ν=1.0\n",
      "ν=0.1\n",
      "−5 0 500.10.20.30.40.5\n",
      "p(x|µ, a, b)=∫∞\n",
      "0N(x|µ, τ−1)Gam( τ|a, b)dτ (2.158)\n",
      "=∫∞\n",
      "0bae(−bτ)τa−1\n",
      "Γ(a)(τ\n",
      "2π)1/2\n",
      "exp{\n",
      "−τ\n",
      "2(x−µ)2}\n",
      "dτ\n",
      "=ba\n",
      "Γ(a)(1\n",
      "2π)1/2[\n",
      "b+(x−µ)2\n",
      "2]−a−1/2\n",
      "Γ(a+1/2)\n",
      "where we have made the change of variable z=τ[b+(x−µ)2/2]. By convention\n",
      "we deﬁne new parameters given by ν=2aandλ=a/b, in terms of which the\n",
      "distribution p(x|µ, a, b)takes the form\n",
      "St(x|µ, λ, ν )=Γ(ν/2+1/2)\n",
      "Γ(ν/2)(λ\n",
      "πν)1/2[\n",
      "1+λ(x−µ)2\n",
      "ν]−ν/2−1/2\n",
      "(2.159)\n",
      "which is known as Student’s t-distribution . The parameter λis sometimes called the\n",
      "precision of the t-distribution, even though it is not in general equal to the inverse\n",
      "of the variance. The parameter νis called the degrees of freedom , and its effect is\n",
      "illustrated in Figure 2.15. For the particular case of ν=1, the t-distribution reduces\n",
      "to the Cauchy distribution, while in the limit ν→∞ the t-distribution St(x|µ, λ, ν )\n",
      "becomes a Gaussian N(x|µ, λ−1)with mean µand precision λ. Exercise 2.47\n",
      "From (2.158), we see that Student’s t-distribution is obtained by adding up an\n",
      "inﬁnite number of Gaussian distributions having the same mean but different preci-\n",
      "sions. This can be interpreted as an inﬁnite mixture of Gaussians (Gaussian mixtures\n",
      "will be discussed in detail in Section 2.3.9. The result is a distribution that in gen-\n",
      "eral has longer ‘tails’ than a Gaussian, as was seen in Figure 2.15. This gives the t-\n",
      "distribution an important property called robustness , which means that it is much less\n",
      "sensitive than the Gaussian to the presence of a few data points which are outliers .\n",
      "The robustness of the t-distribution is illustrated in Figure 2.16, which compares the\n",
      "maximum likelihood solutions for a Gaussian and a t-distribution. Note that the max-\n",
      "imum likelihood solution for the t-distribution can be found using the expectation-\n",
      "maximization (EM) algorithm. Here we see that the effect of a small number of Exercise 12.24104 2. PROBABILITY DISTRIBUTIONS\n",
      "(a)−5 0 5 1000.10.20.30.40.5\n",
      "(b)−5 0 5 1000.10.20.30.40.5\n",
      "Figure 2.16 Illustration of the robustness of Student’s t-distribution compared to a Gaussian. (a) Histogram\n",
      "distribution of 30 data points drawn from a Gaussian distribution, together with the maximum likelihood ﬁt ob-\n",
      "tained from a t-distribution (red curve) and a Gaussian (green curve, largely hidden by the red curve). Because\n",
      "the t-distribution contains the Gaussian as a special case it gives almost the same solution as the Gaussian.\n",
      "(b) The same data set but with three additional outlying data points showing how the Gaussian (green curve) is\n",
      "strongly distorted by the outliers, whereas the t-distribution (red curve) is relatively unaffected.\n",
      "outliers is much less signiﬁcant for the t-distribution than for the Gaussian. Outliers\n",
      "can arise in practical applications either because the process that generates the data\n",
      "corresponds to a distribution having a heavy tail or simply through mislabelled data.\n",
      "Robustness is also an important property for regression problems. Unsurprisingly,\n",
      "the least squares approach to regression does not exhibit robustness, because it cor-\n",
      "responds to maximum likelihood under a (conditional) Gaussian distribution. By\n",
      "basing a regression model on a heavy-tailed distribution such as a t-distribution, we\n",
      "obtain a more robust model.\n",
      "If we go back to (2.158) and substitute the alternative parameters ν=2a,λ=\n",
      "a/b, andη=τb/a , we see that the t-distribution can be written in the form\n",
      "St(x|µ, λ, ν )=∫∞\n",
      "0N(\n",
      "x|µ,(ηλ)−1)\n",
      "Gam( η|ν/2,ν/2) dη. (2.160)\n",
      "We can then generalize this to a multivariate Gaussian N(x|µ,Λ)to obtain the cor-\n",
      "responding multivariate Student’s t-distribution in the form\n",
      "St(x|µ,Λ,ν)=∫∞\n",
      "0N(x|µ,(ηΛ)−1)Gam( η|ν/2,ν/2) dη. (2.161)\n",
      "Using the same technique as for the univariate case, we can evaluate this integral to\n",
      "give Exercise 2.482.3. The Gaussian Distribution 105\n",
      "St(x|µ,Λ,ν)=Γ(D/2+ν/2)\n",
      "Γ(ν/2)|Λ|1/2\n",
      "(πν)D/2[\n",
      "1+∆2\n",
      "ν]−D/2−ν/2\n",
      "(2.162)\n",
      "where Dis the dimensionality of x, and∆2is the squared Mahalanobis distance\n",
      "deﬁned by\n",
      "∆2=(x−µ)TΛ(x−µ). (2.163)\n",
      "This is the multivariate form of Student’s t-distribution and satisﬁes the following\n",
      "properties Exercise 2.49\n",
      "E[x]= µ, ifν>1 (2.164)\n",
      "cov[x]=ν\n",
      "(ν−2)Λ−1, ifν>2 (2.165)\n",
      "mode[x]= µ (2.166)\n",
      "with corresponding results for the univariate case.\n",
      "2.3.8 Periodic variables\n",
      "Although Gaussian distributions are of great practical signiﬁcance, both in their\n",
      "own right and as building blocks for more complex probabilistic models, there are\n",
      "situations in which they are inappropriate as density models for continuous vari-\n",
      "ables. One important case, which arises in practical applications, is that of periodicvariables.\n",
      "An example of a periodic variable would be the wind direction at a particular\n",
      "geographical location. We might, for instance, measure values of wind direction on anumber of days and wish to summarize this using a parametric distribution. Another\n",
      "example is calendar time, where we may be interested in modelling quantities that\n",
      "are believed to be periodic over 24 hours or over an annual cycle. Such quantities\n",
      "can conveniently be represented using an angular (polar) coordinate 0⩽θ<2π.\n",
      "We might be tempted to treat periodic variables by choosing some direction\n",
      "as the origin and then applying a conventional distribution such as the Gaussian.\n",
      "Such an approach, however, would give results that were strongly dependent on the\n",
      "arbitrary choice of origin. Suppose, for instance, that we have two observations atθ\n",
      "1=1◦andθ2= 359◦, and we model them using a standard univariate Gaussian\n",
      "distribution. If we choose the origin at 0◦, then the sample mean of this data set\n",
      "will be 180◦with standard deviation 179◦, whereas if we choose the origin at 180◦,\n",
      "then the mean will be 0◦and the standard deviation will be 1◦. We clearly need to\n",
      "develop a special approach for the treatment of periodic variables.\n",
      "Let us consider the problem of evaluating the mean of a set of observations\n",
      "D={θ1,...,θ N}of a periodic variable. From now on, we shall assume that θis\n",
      "measured in radians. We have already seen that the simple average (θ1+···+θN)/N\n",
      "will be strongly coordinate dependent. To ﬁnd an invariant measure of the mean, wenote that the observations can be viewed as points on the unit circle and can therefore\n",
      "be described instead by two-dimensional unit vectors x\n",
      "1,...,xNwhere ∥xn∥=1\n",
      "forn=1,...,N , as illustrated in Figure 2.17. We can average the vectors {xn}106 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.17 Illustration of the representation of val-\n",
      "uesθnof a periodic variable as two-\n",
      "dimensional vectors xnliving on the unit\n",
      "circle. Also shown is the average xof\n",
      "those vectors.\n",
      "x1x2\n",
      "x1x2x3x4\n",
      "¯x\n",
      "¯r\n",
      "¯θ\n",
      "instead to give\n",
      "x=1\n",
      "NN∑\n",
      "n=1xn (2.167)\n",
      "and then ﬁnd the corresponding angle θof this average. Clearly, this deﬁnition will\n",
      "ensure that the location of the mean is independent of the origin of the angular coor-\n",
      "dinate. Note that xwill typically lie inside the unit circle. The Cartesian coordinates\n",
      "of the observations are given by xn=( c o s θn,sinθn), and we can write the Carte-\n",
      "sian coordinates of the sample mean in the form x=(rcosθ,rsinθ). Substituting\n",
      "into (2.167) and equating the x1andx2components then gives\n",
      "rcosθ=1\n",
      "NN∑\n",
      "n=1cosθn, rsinθ=1\n",
      "NN∑\n",
      "n=1sinθn. (2.168)\n",
      "Taking the ratio, and using the identity tanθ=s i n θ/cosθ, we can solve for θto\n",
      "give\n",
      "θ= tan−1{∑\n",
      "nsinθn∑\n",
      "ncosθn}\n",
      ". (2.169)\n",
      "Shortly, we shall see how this result arises naturally as the maximum likelihood\n",
      "estimator for an appropriately deﬁned distribution over a periodic variable.\n",
      "We now consider a periodic generalization of the Gaussian called the von Mises\n",
      "distribution. Here we shall limit our attention to univariate distributions, although\n",
      "periodic distributions can also be found over hyperspheres of arbitrary dimension.\n",
      "For an extensive discussion of periodic distributions, see Mardia and Jupp (2000).\n",
      "By convention, we will consider distributions p(θ)that have period 2π.A n y\n",
      "probability density p(θ)deﬁned over θmust not only be nonnegative and integrate2.3. The Gaussian Distribution 107\n",
      "Figure 2.18 The von Mises distribution can be derived by considering\n",
      "a two-dimensional Gaussian of the form (2.173), whose\n",
      "density contours are shown in blue and conditioning on\n",
      "the unit circle shown in red.\n",
      "x1x2\n",
      "p(x)\n",
      "r=1\n",
      "to one, but it must also be periodic. Thus p(θ)must satisfy the three conditions\n",
      "p(θ)⩾0 (2.170)∫2π\n",
      "0p(θ)dθ=1 (2.171)\n",
      "p(θ+2π)= p(θ). (2.172)\n",
      "From (2.172), it follows that p(θ+M2π)=p(θ)for any integer M.\n",
      "We can easily obtain a Gaussian-like distribution that satisﬁes these three prop-\n",
      "erties as follows. Consider a Gaussian distribution over two variables x=(x1,x2)\n",
      "having mean µ=(µ1,µ2)and a covariance matrix Σ=σ2IwhereIis the2×2\n",
      "identity matrix, so that\n",
      "p(x1,x2)=1\n",
      "2πσ2exp{\n",
      "−(x1−µ1)2+(x2−µ2)2\n",
      "2σ2}\n",
      ". (2.173)\n",
      "The contours of constant p(x)are circles, as illustrated in Figure 2.18. Now suppose\n",
      "we consider the value of this distribution along a circle of ﬁxed radius. Then by con-\n",
      "struction this distribution will be periodic, although it will not be normalized. We can\n",
      "determine the form of this distribution by transforming from Cartesian coordinates\n",
      "(x1,x2)to polar coordinates (r, θ)so that\n",
      "x1=rcosθ, x 2=rsinθ. (2.174)\n",
      "We also map the mean µinto polar coordinates by writing\n",
      "µ1=r0cosθ0,µ 2=r0sinθ0. (2.175)\n",
      "Next we substitute these transformations into the two-dimensional Gaussian distribu-\n",
      "tion (2.173), and then condition on the unit circle r=1, noting that we are interested\n",
      "only in the dependence on θ. Focussing on the exponent in the Gaussian distribution\n",
      "we have\n",
      "−1\n",
      "2σ2{\n",
      "(rcosθ−r0cosθ0)2+(rsinθ−r0sinθ0)2}\n",
      "=−1\n",
      "2σ2{\n",
      "1+r2\n",
      "0−2r0cosθcosθ0−2r0sinθsinθ0}\n",
      "=r0\n",
      "σ2cos(θ−θ0)+c o n s t (2.176)108 2. PROBABILITY DISTRIBUTIONS\n",
      "m=5 ,θ0=π/4\n",
      "m=1 ,θ0=3π/4\n",
      "2π0π/4\n",
      "3π/4\n",
      "m=5 ,θ0=π/4\n",
      "m=1 ,θ0=3π/4\n",
      "Figure 2.19 The von Mises distribution plotted for two different parameter values, shown as a Cartesian plot\n",
      "on the left and as the corresponding polar plot on the right.\n",
      "where ‘const’ denotes terms independent of θ, and we have made use of the following\n",
      "trigonometrical identities Exercise 2.51\n",
      "cos2A+s i n2A=1 (2.177)\n",
      "cosAcosB+s i nAsinB=c o s ( A−B). (2.178)\n",
      "If we now deﬁne m=r0/σ2, we obtain our ﬁnal expression for the distribution of\n",
      "p(θ)along the unit circle r=1in the form\n",
      "p(θ|θ0,m)=1\n",
      "2πI0(m)exp{mcos(θ−θ0)} (2.179)\n",
      "which is called the von Mises distribution, or the circular normal . Here the param-\n",
      "eterθ0corresponds to the mean of the distribution, while m, which is known as\n",
      "theconcentration parameter, is analogous to the inverse variance (precision) for the\n",
      "Gaussian. The normalization coefﬁcient in (2.179) is expressed in terms of I0(m),\n",
      "which is the zeroth-order Bessel function of the ﬁrst kind (Abramowitz and Stegun,\n",
      "1965) and is deﬁned by\n",
      "I0(m)=1\n",
      "2π∫2π\n",
      "0exp{mcosθ}dθ. (2.180)\n",
      "For large m, the distribution becomes approximately Gaussian. The von Mises dis- Exercise 2.52\n",
      "tribution is plotted in Figure 2.19, and the function I0(m)is plotted in Figure 2.20.\n",
      "Now consider the maximum likelihood estimators for the parameters θ0andm\n",
      "for the von Mises distribution. The log likelihood function is given by\n",
      "lnp(D|θ0,m)=−Nln(2π)−NlnI0(m)+mN∑\n",
      "n=1cos(θn−θ0). (2.181)2.3. The Gaussian Distribution 109\n",
      "I0(m)\n",
      "m0 5 100100020003000\n",
      "A(m)\n",
      "m0 5 1000.51\n",
      "Figure 2.20 Plot of the Bessel function I0(m)deﬁned by (2.180), together with the function A(m)deﬁned by\n",
      "(2.186).\n",
      "Setting the derivative with respect to θ0equal to zero gives\n",
      "N∑\n",
      "n=1sin(θn−θ0)=0. (2.182)\n",
      "To solve for θ0, we make use of the trigonometric identity\n",
      "sin(A−B)=c o s BsinA−cosAsinB (2.183)\n",
      "from which we obtain Exercise 2.53\n",
      "θML\n",
      "0= tan−1{∑\n",
      "nsinθn∑\n",
      "ncosθn}\n",
      "(2.184)\n",
      "which we recognize as the result (2.169) obtained earlier for the mean of the obser-\n",
      "vations viewed in a two-dimensional Cartesian space.\n",
      "Similarly, maximizing (2.181) with respect to m, and making use of I′\n",
      "0(m)=\n",
      "I1(m)(Abramowitz and Stegun, 1965), we have\n",
      "A(m)=1\n",
      "NN∑\n",
      "n=1cos(θn−θML\n",
      "0) (2.185)\n",
      "where we have substituted for the maximum likelihood solution for θML\n",
      "0(recalling\n",
      "that we are performing a joint optimization over θandm), and we have deﬁned\n",
      "A(m)=I1(m)\n",
      "I0(m). (2.186)\n",
      "The function A(m)is plotted in Figure 2.20. Making use of the trigonometric iden-\n",
      "tity (2.178), we can write (2.185) in the form\n",
      "A(mML)=(\n",
      "1\n",
      "NN∑\n",
      "n=1cosθn)\n",
      "cosθML\n",
      "0−(\n",
      "1\n",
      "NN∑\n",
      "n=1sinθn)\n",
      "sinθML\n",
      "0. (2.187)110 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.21 Plots of the ‘old faith-\n",
      "ful’ data in which the blue curves\n",
      "show contours of constant proba-\n",
      "bility density. On the left is a\n",
      "single Gaussian distribution which\n",
      "has been ﬁtted to the data us-\n",
      "ing maximum likelihood. Note that\n",
      "this distribution fails to capture the\n",
      "two clumps in the data and indeed\n",
      "places much of its probability mass\n",
      "in the central region between the\n",
      "clumps where the data are relatively\n",
      "sparse. On the right the distribution\n",
      "is given by a linear combination of\n",
      "two Gaussians which has been ﬁtted\n",
      "to the data by maximum likelihood\n",
      "using techniques discussed Chap-\n",
      "ter 9, and which gives a better rep-\n",
      "resentation of the data.1 2 3 4 5 6406080100\n",
      "1 2 3 4 5 6406080100\n",
      "The right-hand side of (2.187) is easily evaluated, and the function A(m)can be\n",
      "inverted numerically.\n",
      "For completeness, we mention brieﬂy some alternative techniques for the con-\n",
      "struction of periodic distributions. The simplest approach is to use a histogram of\n",
      "observations in which the angular coordinate is divided into ﬁxed bins. This has the\n",
      "virtue of simplicity and ﬂexibility but also suffers from signiﬁcant limitations, as we\n",
      "shall see when we discuss histogram methods in more detail in Section 2.5. Another\n",
      "approach starts, like the von Mises distribution, from a Gaussian distribution over a\n",
      "Euclidean space but now marginalizes onto the unit circle rather than conditioning\n",
      "(Mardia and Jupp, 2000). However, this leads to more complex forms of distribution\n",
      "and will not be discussed further. Finally, any valid distribution over the real axis\n",
      "(such as a Gaussian) can be turned into a periodic distribution by mapping succes-\n",
      "sive intervals of width 2πonto the periodic variable (0,2π), which corresponds to\n",
      "‘wrapping’ the real axis around unit circle. Again, the resulting distribution is more\n",
      "complex to handle than the von Mises distribution.\n",
      "One limitation of the von Mises distribution is that it is unimodal. By forming\n",
      "mixtures of von Mises distributions, we obtain a ﬂexible framework for modelling\n",
      "periodic variables that can handle multimodality. For an example of a machine learn-\n",
      "ing application that makes use of von Mises distributions, see Lawrence et al. (2002),\n",
      "and for extensions to modelling conditional densities for regression problems, see\n",
      "Bishop and Nabney (1996).\n",
      "2.3.9 Mixtures of Gaussians\n",
      "While the Gaussian distribution has some important analytical properties, it suf-\n",
      "fers from signiﬁcant limitations when it comes to modelling real data sets. Consider\n",
      "the example shown in Figure 2.21. This is known as the ‘Old Faithful’ data set,\n",
      "and comprises 272measurements of the eruption of the Old Faithful geyser at Yel-\n",
      "lowstone National Park in the USA. Each measurement comprises the duration of Appendix A2.3. The Gaussian Distribution 111\n",
      "Figure 2.22 Example of a Gaussian mixture distribution\n",
      "in one dimension showing three Gaussians\n",
      "(each scaled by a coefﬁcient) in blue and\n",
      "their sum in red.\n",
      "xp(x)\n",
      "the eruption in minutes (horizontal axis) and the time in minutes to the next erup-\n",
      "tion (vertical axis). We see that the data set forms two dominant clumps, and that\n",
      "a simple Gaussian distribution is unable to capture this structure, whereas a linear\n",
      "superposition of two Gaussians gives a better characterization of the data set.\n",
      "Such superpositions, formed by taking linear combinations of more basic dis-\n",
      "tributions such as Gaussians, can be formulated as probabilistic models known as\n",
      "mixture distributions (McLachlan and Basford, 1988; McLachlan and Peel, 2000).\n",
      "In Figure 2.22 we see that a linear combination of Gaussians can give rise to very\n",
      "complex densities. By using a sufﬁcient number of Gaussians, and by adjusting their\n",
      "means and covariances as well as the coefﬁcients in the linear combination, almost\n",
      "any continuous density can be approximated to arbitrary accuracy.\n",
      "We therefore consider a superposition of KGaussian densities of the form\n",
      "p(x)=K∑\n",
      "k=1πkN(x|µk,Σk) (2.188)\n",
      "which is called a mixture of Gaussians . Each Gaussian density N(x|µk,Σk)is\n",
      "called a component of the mixture and has its own mean µkand covariance Σk.\n",
      "Contour and surface plots for a Gaussian mixture having 3 components are shown in\n",
      "Figure 2.23.\n",
      "In this section we shall consider Gaussian components to illustrate the frame-\n",
      "work of mixture models. More generally, mixture models can comprise linear com-\n",
      "binations of other distributions. For instance, in Section 9.3.3 we shall consider\n",
      "mixtures of Bernoulli distributions as an example of a mixture model for discrete\n",
      "variables. Section 9.3.3\n",
      "The parameters πkin (2.188) are called mixing coefﬁcients . If we integrate both\n",
      "sides of (2.188) with respect to x, and note that both p(x)and the individual Gaussian\n",
      "components are normalized, we obtain\n",
      "K∑\n",
      "k=1πk=1. (2.189)\n",
      "Also, the requirement that p(x)⩾0, together with N(x|µk,Σk)⩾0, implies\n",
      "πk⩾0for all k. Combining this with the condition (2.189) we obtain\n",
      "0⩽πk⩽1. (2.190)112 2. PROBABILITY DISTRIBUTIONS\n",
      "0.50.30.2(a)\n",
      "0 0.5 100.51(b)\n",
      "0 0.5 100.51\n",
      "Figure 2.23 Illustration of a mixture of 3 Gaussians in a two-dimensional space. (a) Contours of constant\n",
      "density for each of the mixture components, in which the 3 components are denoted red, blue and green, and\n",
      "the values of the mixing coefﬁcients are shown below each component. (b) Contours of the marginal probability\n",
      "density p(x)of the mixture distribution. (c) A surface plot of the distribution p(x).\n",
      "We therefore see that the mixing coefﬁcients satisfy the requirements to be probabil-\n",
      "ities.\n",
      "From the sum and product rules, the marginal density is given by\n",
      "p(x)=K∑\n",
      "k=1p(k)p(x|k) (2.191)\n",
      "which is equivalent to (2.188) in which we can view πk=p(k)as the prior prob-\n",
      "ability of picking the kthcomponent, and the density N(x|µk,Σk)=p(x|k)as\n",
      "the probability of xconditioned on k. As we shall see in later chapters, an impor-\n",
      "tant role is played by the posterior probabilities p(k|x), which are also known as\n",
      "responsibilities . From Bayes’ theorem these are given by\n",
      "γk(x)≡p(k|x)\n",
      "=p(k)p(x|k)∑\n",
      "lp(l)p(x|l)\n",
      "=πkN(x|µk,Σk)∑\n",
      "lπlN(x|µl,Σl). (2.192)\n",
      "We shall discuss the probabilistic interpretation of the mixture distribution in greater\n",
      "detail in Chapter 9.\n",
      "The form of the Gaussian mixture distribution is governed by the parameters π,\n",
      "µandΣ, where we have used the notation π≡{π1,...,π K},µ≡{µ1,...,µK}\n",
      "andΣ≡{Σ1,...ΣK}. One way to set the values of these parameters is to use\n",
      "maximum likelihood. From (2.188) the log of the likelihood function is given by\n",
      "lnp(X|π,µ,Σ)=N∑\n",
      "n=1ln{K∑\n",
      "k=1πkN(xn|µk,Σk)}\n",
      "(2.193)2.4. The Exponential Family 113\n",
      "whereX={x1,...,xN}. We immediately see that the situation is now much\n",
      "more complex than with a single Gaussian, due to the presence of the summationoverkinside the logarithm. As a result, the maximum likelihood solution for the\n",
      "parameters no longer has a closed-form analytical solution. One approach to maxi-\n",
      "mizing the likelihood function is to use iterative numerical optimization techniques(Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008). Alterna-\n",
      "tively we can employ a powerful framework called expectation maximization , which\n",
      "will be discussed at length in Chapter 9.\n",
      "2.4. The Exponential Family\n",
      "The probability distributions that we have studied so far in this chapter (with theexception of the Gaussian mixture) are speciﬁc examples of a broad class of distri-\n",
      "butions called the exponential family (Duda and Hart, 1973; Bernardo and Smith,\n",
      "1994). Members of the exponential family have many important properties in com-mon, and it is illuminating to discuss these properties in some generality.\n",
      "The exponential family of distributions over x, given parameters η, is deﬁned to\n",
      "be the set of distributions of the form\n",
      "p(x|η)=h(x)g(η)e x p\n",
      "{\n",
      "ηTu(x)}\n",
      "(2.194)\n",
      "wherexmay be scalar or vector, and may be discrete or continuous. Here ηare\n",
      "called the natural parameters of the distribution, and u(x)is some function of x.\n",
      "The function g(η)can be interpreted as the coefﬁcient that ensures that the distribu-\n",
      "tion is normalized and therefore satisﬁes\n",
      "g(η)∫\n",
      "h(x)e x p{\n",
      "ηTu(x)}\n",
      "dx=1 (2.195)\n",
      "where the integration is replaced by summation if xis a discrete variable.\n",
      "We begin by taking some examples of the distributions introduced earlier in\n",
      "the chapter and showing that they are indeed members of the exponential family.\n",
      "Consider ﬁrst the Bernoulli distribution\n",
      "p(x|µ)=B e r n ( x|µ)=µx(1−µ)1−x. (2.196)\n",
      "Expressing the right-hand side as the exponential of the logarithm, we have\n",
      "p(x|µ) = exp {xlnµ+( 1−x)l n ( 1−µ)}\n",
      "=( 1 −µ)e x p{\n",
      "ln(µ\n",
      "1−µ)\n",
      "x}\n",
      ". (2.197)\n",
      "Comparison with (2.194) allows us to identify\n",
      "η=l n(µ\n",
      "1−µ)\n",
      "(2.198)114 2. PROBABILITY DISTRIBUTIONS\n",
      "which we can solve for µto give µ=σ(η), where\n",
      "σ(η)=1\n",
      "1+e x p ( −η)(2.199)\n",
      "is called the logistic sigmoid function. Thus we can write the Bernoulli distribution\n",
      "using the standard representation (2.194) in the form\n",
      "p(x|η)=σ(−η) exp(ηx) (2.200)\n",
      "where we have used 1−σ(η)=σ(−η), which is easily proved from (2.199). Com-\n",
      "parison with (2.194) shows that\n",
      "u(x)= x (2.201)\n",
      "h(x)=1 (2.202)\n",
      "g(η)= σ(−η). (2.203)\n",
      "Next consider the multinomial distribution that, for a single observation x, takes\n",
      "the form\n",
      "p(x|µ)=M∏\n",
      "k=1µxk\n",
      "k=e x p{M∑\n",
      "k=1xklnµk}\n",
      "(2.204)\n",
      "wherex=(x1,...,x N)T. Again, we can write this in the standard representation\n",
      "(2.194) so that\n",
      "p(x|η)=e x p ( ηTx) (2.205)\n",
      "where ηk=l nµk, and we have deﬁned η=(η1,...,η M)T. Again, comparing with\n",
      "(2.194) we have\n",
      "u(x)= x (2.206)\n",
      "h(x)=1 (2.207)\n",
      "g(η)=1 . (2.208)\n",
      "Note that the parameters ηkare not independent because the parameters µkare sub-\n",
      "ject to the constraint\n",
      "M∑\n",
      "k=1µk=1 (2.209)\n",
      "so that, given any M−1of the parameters µk, the value of the remaining parameter\n",
      "is ﬁxed. In some circumstances, it will be convenient to remove this constraint by\n",
      "expressing the distribution in terms of only M−1parameters. This can be achieved\n",
      "by using the relationship (2.209) to eliminate µMby expressing it in terms of the\n",
      "remaining {µk}where k=1,...,M −1, thereby leaving M−1parameters. Note\n",
      "that these remaining parameters are still subject to the constraints\n",
      "0⩽µk⩽1,M−1∑\n",
      "k=1µk⩽1. (2.210)2.4. The Exponential Family 115\n",
      "Making use of the constraint (2.209), the multinomial distribution in this representa-\n",
      "tion then becomes\n",
      "exp{M∑\n",
      "k=1xklnµk}\n",
      "=e x p{M−1∑\n",
      "k=1xklnµk+(\n",
      "1−M−1∑\n",
      "k=1xk)\n",
      "ln(\n",
      "1−M−1∑\n",
      "k=1µk)}\n",
      "=e x p{M−1∑\n",
      "k=1xkln(\n",
      "µk\n",
      "1−∑M−1\n",
      "j=1µj)\n",
      "+l n(\n",
      "1−M−1∑\n",
      "k=1µk)}\n",
      ".(2.211)\n",
      "We now identify\n",
      "ln(\n",
      "µk\n",
      "1−∑\n",
      "jµj)\n",
      "=ηk (2.212)\n",
      "which we can solve for µkby ﬁrst summing both sides over kand then rearranging\n",
      "and back-substituting to give\n",
      "µk=exp(ηk)\n",
      "1+∑\n",
      "jexp(ηj). (2.213)\n",
      "This is called the softmax function, or the normalized exponential . In this represen-\n",
      "tation, the multinomial distribution therefore takes the form\n",
      "p(x|η)=(\n",
      "1+M−1∑\n",
      "k=1exp(ηk))−1\n",
      "exp(ηTx). (2.214)\n",
      "This is the standard form of the exponential family, with parameter vector η=\n",
      "(η1,...,η M−1)Tin which\n",
      "u(x)= x (2.215)\n",
      "h(x)=1 (2.216)\n",
      "g(η)=(\n",
      "1+M−1∑\n",
      "k=1exp(ηk))−1\n",
      ". (2.217)\n",
      "Finally, let us consider the Gaussian distribution. For the univariate Gaussian,\n",
      "we have\n",
      "p(x|µ, σ2)=1\n",
      "(2πσ2)1/2exp{\n",
      "−1\n",
      "2σ2(x−µ)2}\n",
      "(2.218)\n",
      "=1\n",
      "(2πσ2)1/2exp{\n",
      "−1\n",
      "2σ2x2+µ\n",
      "σ2x−1\n",
      "2σ2µ2}\n",
      "(2.219)116 2. PROBABILITY DISTRIBUTIONS\n",
      "which, after some simple rearrangement, can be cast in the standard exponential\n",
      "family form (2.194) with Exercise 2.57\n",
      "η=(\n",
      "µ/σ2\n",
      "−1/2σ2)\n",
      "(2.220)\n",
      "u(x)=(\n",
      "x\n",
      "x2)\n",
      "(2.221)\n",
      "h(x)=( 2 π)−1/2(2.222)\n",
      "g(η)=( −2η2)1/2exp(η2\n",
      "1\n",
      "4η2)\n",
      ". (2.223)\n",
      "2.4.1 Maximum likelihood and sufﬁcient statistics\n",
      "Let us now consider the problem of estimating the parameter vector ηin the gen-\n",
      "eral exponential family distribution (2.194) using the technique of maximum likeli-\n",
      "hood. Taking the gradient of both sides of (2.195) with respect to η,w eh a v e\n",
      "∇g(η)∫\n",
      "h(x)e x p{\n",
      "ηTu(x)}\n",
      "dx\n",
      "+g(η)∫\n",
      "h(x)e x p{\n",
      "ηTu(x)}\n",
      "u(x)dx=0. (2.224)\n",
      "Rearranging, and making use again of (2.195) then gives\n",
      "−1\n",
      "g(η)∇g(η)=g(η)∫\n",
      "h(x)e x p{\n",
      "ηTu(x)}\n",
      "u(x)dx=E[u(x)] (2.225)\n",
      "where we have used (2.194). We therefore obtain the result\n",
      "−∇lng(η)= E[u(x)]. (2.226)\n",
      "Note that the covariance of u(x)can be expressed in terms of the second derivatives\n",
      "ofg(η), and similarly for higher order moments. Thus, provided we can normalize a Exercise 2.58\n",
      "distribution from the exponential family, we can always ﬁnd its moments by simple\n",
      "differentiation.\n",
      "Now consider a set of independent identically distributed data denoted by X=\n",
      "{x1,...,xn}, for which the likelihood function is given by\n",
      "p(X|η)=(N∏\n",
      "n=1h(xn))\n",
      "g(η)Nexp{\n",
      "ηTN∑\n",
      "n=1u(xn)}\n",
      ". (2.227)\n",
      "Setting the gradient of lnp(X|η)with respect to ηto zero, we get the following\n",
      "condition to be satisﬁed by the maximum likelihood estimator ηML\n",
      "−∇lng(ηML)=1\n",
      "NN∑\n",
      "n=1u(xn) (2.228)2.4. The Exponential Family 117\n",
      "which can in principle be solved to obtain ηML. We see that the solution for the\n",
      "maximum likelihood estimator depends on the data only through∑\n",
      "nu(xn), which\n",
      "is therefore called the sufﬁcient statistic of the distribution (2.194). We do not need\n",
      "to store the entire data set itself but only the value of the sufﬁcient statistic. For\n",
      "the Bernoulli distribution, for example, the function u(x)is given just by xand\n",
      "so we need only keep the sum of the data points {xn}, whereas for the Gaussian\n",
      "u(x)=(x, x2)T, and so we should keep both the sum of {xn}and the sum of {x2\n",
      "n}.\n",
      "If we consider the limit N→∞ , then the right-hand side of (2.228) becomes\n",
      "E[u(x)], and so by comparing with (2.226) we see that in this limit ηMLwill equal\n",
      "the true value η.\n",
      "In fact, this sufﬁciency property holds also for Bayesian inference, although\n",
      "we shall defer discussion of this until Chapter 8 when we have equipped ourselves\n",
      "with the tools of graphical models and can thereby gain a deeper insight into theseimportant concepts.\n",
      "2.4.2 Conjugate priors\n",
      "We have already encountered the concept of a conjugate prior several times, for\n",
      "example in the context of the Bernoulli distribution (for which the conjugate prior\n",
      "is the beta distribution) or the Gaussian (where the conjugate prior for the mean is\n",
      "a Gaussian, and the conjugate prior for the precision is the Wishart distribution). Ingeneral, for a given probability distribution p(x|η), we can seek a prior p(η)that is\n",
      "conjugate to the likelihood function, so that the posterior distribution has the same\n",
      "functional form as the prior. For any member of the exponential family (2.194), there\n",
      "exists a conjugate prior that can be written in the form\n",
      "p(η|χ,ν)=f(χ,ν)g(η)\n",
      "νexp{\n",
      "νηTχ}\n",
      "(2.229)\n",
      "where f(χ,ν)is a normalization coefﬁcient, and g(η)is the same function as ap-\n",
      "pears in (2.194). To see that this is indeed conjugate, let us multiply the prior (2.229)\n",
      "by the likelihood function (2.227) to obtain the posterior distribution, up to a nor-\n",
      "malization coefﬁcient, in the form\n",
      "p(η|X,χ,ν)∝g(η)ν+Nexp{\n",
      "ηT(N∑\n",
      "n=1u(xn)+νχ)}\n",
      ". (2.230)\n",
      "This again takes the same functional form as the prior (2.229), conﬁrming conjugacy.\n",
      "Furthermore, we see that the parameter νcan be interpreted as a effective number of\n",
      "pseudo-observations in the prior, each of which has a value for the sufﬁcient statistic\n",
      "u(x)given by χ.\n",
      "2.4.3 Noninformative priors\n",
      "In some applications of probabilistic inference, we may have prior knowledge\n",
      "that can be conveniently expressed through the prior distribution. For example, if\n",
      "the prior assigns zero probability to some value of variable, then the posterior dis-tribution will necessarily also assign zero probability to that value, irrespective of118 2. PROBABILITY DISTRIBUTIONS\n",
      "any subsequent observations of data. In many cases, however, we may have little\n",
      "idea of what form the distribution should take. We may then seek a form of priordistribution, called a noninformative prior , which is intended to have as little inﬂu-\n",
      "ence on the posterior distribution as possible (Jeffries, 1946; Box and Tao, 1973;\n",
      "Bernardo and Smith, 1994). This is sometimes referred to as ‘letting the data speakfor themselves’.\n",
      "If we have a distribution p(x|λ)governed by a parameter λ, we might be tempted\n",
      "to propose a prior distribution p(λ) = const as a suitable prior. If λis a discrete\n",
      "variable with Kstates, this simply amounts to setting the prior probability of each\n",
      "state to 1/K. In the case of continuous parameters, however, there are two potential\n",
      "difﬁculties with this approach. The ﬁrst is that, if the domain of λis unbounded,\n",
      "this prior distribution cannot be correctly normalized because the integral over λ\n",
      "diverges. Such priors are called improper . In practice, improper priors can often\n",
      "be used provided the corresponding posterior distribution is proper , i.e., that it can\n",
      "be correctly normalized. For instance, if we put a uniform prior distribution over\n",
      "the mean of a Gaussian, then the posterior distribution for the mean, once we haveobserved at least one data point, will be proper.\n",
      "A second difﬁculty arises from the transformation behaviour of a probability\n",
      "density under a nonlinear change of variables, given by (1.27). If a function h(λ)\n",
      "is constant, and we change variables to λ=η\n",
      "2, thenˆh(η)=h(η2)will also be\n",
      "constant. However, if we choose the density pλ(λ)to be constant, then the density\n",
      "ofηwill be given, from (1.27), by\n",
      "pη(η)=pλ(λ)⏐⏐⏐⏐dλ\n",
      "dη⏐⏐⏐⏐=pλ(η2)2η∝η (2.231)\n",
      "and so the density over ηwill not be constant. This issue does not arise when we use\n",
      "maximum likelihood, because the likelihood function p(x|λ)is a simple function of\n",
      "λand so we are free to use any convenient parameterization. If, however, we are to\n",
      "choose a prior distribution that is constant, we must take care to use an appropriate\n",
      "representation for the parameters.\n",
      "Here we consider two simple examples of noninformative priors (Berger, 1985).\n",
      "First of all, if a density takes the form\n",
      "p(x|µ)=f(x−µ) (2.232)\n",
      "then the parameter µis known as a location parameter . This family of densities\n",
      "exhibits translation invariance because if we shift xby a constant to give ˆx=x+c,\n",
      "then\n",
      "p(ˆx|ˆµ)=f(ˆx−ˆµ) (2.233)\n",
      "where we have deﬁned ˆµ=µ+c. Thus the density takes the same form in the\n",
      "new variable as in the original one, and so the density is independent of the choiceof origin. We would like to choose a prior distribution that reﬂects this translation\n",
      "invariance property, and so we choose a prior that assigns equal probability mass to2.4. The Exponential Family 119\n",
      "an interval A⩽µ⩽Bas to the shifted interval A−c⩽µ⩽B−c. This implies\n",
      "∫B\n",
      "Ap(µ)dµ=∫B−c\n",
      "A−cp(µ)dµ=∫B\n",
      "Ap(µ−c)dµ (2.234)\n",
      "and because this must hold for all choices of AandB,w eh a v e\n",
      "p(µ−c)=p(µ) (2.235)\n",
      "which implies that p(µ)is constant. An example of a location parameter would be\n",
      "the mean µof a Gaussian distribution. As we have seen, the conjugate prior distri-\n",
      "bution for µin this case is a Gaussian p(µ|µ0,σ2\n",
      "0)=N(µ|µ0,σ2\n",
      "0), and we obtain a\n",
      "noninformative prior by taking the limit σ2\n",
      "0→∞ . Indeed, from (2.141) and (2.142)\n",
      "we see that this gives a posterior distribution over µin which the contributions from\n",
      "the prior vanish.\n",
      "As a second example, consider a density of the form\n",
      "p(x|σ)=1\n",
      "σf(x\n",
      "σ)\n",
      "(2.236)\n",
      "where σ>0. Note that this will be a normalized density provided f(x)is correctly\n",
      "normalized. The parameter σis known as a scale parameter , and the density exhibits Exercise 2.59\n",
      "scale invariance because if we scale xby a constant to give ˆx=cx, then\n",
      "p(ˆx|ˆσ)=1\n",
      "ˆσf(ˆx\n",
      "ˆσ)\n",
      "(2.237)\n",
      "where we have deﬁned ˆσ=cσ. This transformation corresponds to a change of\n",
      "scale, for example from meters to kilometers if xis a length, and we would like\n",
      "to choose a prior distribution that reﬂects this scale invariance. If we consider aninterval A⩽σ⩽B, and a scaled interval A/c⩽σ⩽B/c, then the prior should\n",
      "assign equal probability mass to these two intervals. Thus we have\n",
      "∫B\n",
      "Ap(σ)dσ=∫B/c\n",
      "A/cp(σ)dσ=∫B\n",
      "Ap(1\n",
      "cσ)1\n",
      "cdσ (2.238)\n",
      "and because this must hold for choices of AandB,w eh a v e\n",
      "p(σ)=p(1\n",
      "cσ)1\n",
      "c(2.239)\n",
      "and hence p(σ)∝1/σ. Note that again this is an improper prior because the integral\n",
      "of the distribution over 0⩽σ⩽∞is divergent. It is sometimes also convenient\n",
      "to think of the prior distribution for a scale parameter in terms of the density of the\n",
      "log of the parameter. Using the transformation rule (1.27) for densities we see thatp(lnσ) = const . Thus, for this prior there is the same probability mass in the range\n",
      "1⩽σ⩽10as in the range 10⩽σ⩽100and in 100⩽σ⩽1000 .120 2. PROBABILITY DISTRIBUTIONS\n",
      "An example of a scale parameter would be the standard deviation σof a Gaussian\n",
      "distribution, after we have taken account of the location parameter µ, because\n",
      "N(x|µ, σ2)∝σ−1exp{\n",
      "−(˜x/σ)2}\n",
      "(2.240)\n",
      "where˜x=x−µ. As discussed earlier, it is often more convenient to work in terms\n",
      "of the precision λ=1/σ2rather than σitself. Using the transformation rule for\n",
      "densities, we see that a distribution p(σ)∝1/σcorresponds to a distribution over λ\n",
      "of the form p(λ)∝1/λ. We have seen that the conjugate prior for λwas the gamma\n",
      "distribution Gam( λ|a0,b0)given by (2.146). The noninformative prior is obtained Section 2.3\n",
      "as the special case a0=b0=0. Again, if we examine the results (2.150) and (2.151)\n",
      "for the posterior distribution of λ, we see that for a0=b0=0, the posterior depends\n",
      "only on terms arising from the data and not from the prior.\n",
      "2.5. Nonparametric Methods\n",
      "Throughout this chapter, we have focussed on the use of probability distributionshaving speciﬁc functional forms governed by a small number of parameters whosevalues are to be determined from a data set. This is called the parametric approach\n",
      "to density modelling. An important limitation of this approach is that the chosen\n",
      "density might be a poor model of the distribution that generates the data, which canresult in poor predictive performance. For instance, if the process that generates the\n",
      "data is multimodal, then this aspect of the distribution can never be captured by a\n",
      "Gaussian, which is necessarily unimodal.\n",
      "In this ﬁnal section, we consider some nonparametric approaches to density es-\n",
      "timation that make few assumptions about the form of the distribution. Here we shall\n",
      "focus mainly on simple frequentist methods. The reader should be aware, however,that nonparametric Bayesian methods are attracting increasing interest (Walker et al. ,\n",
      "1999; Neal, 2000; M ¨uller and Quintana, 2004; Teh et al. , 2006).\n",
      "Let us start with a discussion of histogram methods for density estimation, which\n",
      "we have already encountered in the context of marginal and conditional distributions\n",
      "in Figure 1.11 and in the context of the central limit theorem in Figure 2.6. Here weexplore the properties of histogram density models in more detail, focussing on the\n",
      "case of a single continuous variable x. Standard histograms simply partition xinto\n",
      "distinct bins of width ∆\n",
      "iand then count the number niof observations of xfalling\n",
      "in bin i. In order to turn this count into a normalized probability density, we simply\n",
      "divide by the total number Nof observations and by the width ∆iof the bins to\n",
      "obtain probability values for each bin given by\n",
      "pi=ni\n",
      "N∆i(2.241)\n",
      "for which it is easily seen that∫\n",
      "p(x)dx=1. This gives a model for the density\n",
      "p(x)that is constant over the width of each bin, and often the bins are chosen to have\n",
      "the same width ∆i=∆ .2.5. Nonparametric Methods 121\n",
      "Figure 2.24 An illustration of the histogram approach\n",
      "to density estimation, in which a data set\n",
      "of50data points is generated from the\n",
      "distribution shown by the green curve.\n",
      "Histogram density estimates, based on\n",
      "(2.241), with a common bin width ∆are\n",
      "shown for various values of ∆.∆=0 .04\n",
      "0 0.5 105\n",
      "∆=0 .08\n",
      "0 0.5 105\n",
      "∆=0 .25\n",
      "0 0.5 105\n",
      "In Figure 2.24, we show an example of histogram density estimation. Here\n",
      "the data is drawn from the distribution, corresponding to the green curve, which is\n",
      "formed from a mixture of two Gaussians. Also shown are three examples of his-\n",
      "togram density estimates corresponding to three different choices for the bin width\n",
      "∆. We see that when ∆is very small (top ﬁgure), the resulting density model is very\n",
      "spiky, with a lot of structure that is not present in the underlying distribution that\n",
      "generated the data set. Conversely, if ∆is too large (bottom ﬁgure) then the result is\n",
      "a model that is too smooth and that consequently fails to capture the bimodal prop-\n",
      "erty of the green curve. The best results are obtained for some intermediate value\n",
      "of∆(middle ﬁgure). In principle, a histogram density model is also dependent on\n",
      "the choice of edge location for the bins, though this is typically much less signiﬁcant\n",
      "than the value of ∆.\n",
      "Note that the histogram method has the property (unlike the methods to be dis-\n",
      "cussed shortly) that, once the histogram has been computed, the data set itself can\n",
      "be discarded, which can be advantageous if the data set is large. Also, the histogram\n",
      "approach is easily applied if the data points are arriving sequentially.\n",
      "In practice, the histogram technique can be useful for obtaining a quick visual-\n",
      "ization of data in one or two dimensions but is unsuited to most density estimation\n",
      "applications. One obvious problem is that the estimated density has discontinuities\n",
      "that are due to the bin edges rather than any property of the underlying distribution\n",
      "that generated the data. Another major limitation of the histogram approach is its\n",
      "scaling with dimensionality. If we divide each variable in a D-dimensional space\n",
      "intoMbins, then the total number of bins will be MD. This exponential scaling\n",
      "withDis an example of the curse of dimensionality. In a space of high dimensional- Section 1.4\n",
      "ity, the quantity of data needed to provide meaningful estimates of local probability\n",
      "density would be prohibitive.\n",
      "The histogram approach to density estimation does, however, teach us two im-\n",
      "portant lessons. First, to estimate the probability density at a particular location,\n",
      "we should consider the data points that lie within some local neighbourhood of that\n",
      "point. Note that the concept of locality requires that we assume some form of dis-\n",
      "tance measure, and here we have been assuming Euclidean distance. For histograms,122 2. PROBABILITY DISTRIBUTIONS\n",
      "this neighbourhood property was deﬁned by the bins, and there is a natural ‘smooth-\n",
      "ing’ parameter describing the spatial extent of the local region, in this case the binwidth. Second, the value of the smoothing parameter should be neither too large nor\n",
      "too small in order to obtain good results. This is reminiscent of the choice of model\n",
      "complexity in polynomial curve ﬁtting discussed in Chapter 1 where the degree M\n",
      "of the polynomial, or alternatively the value αof the regularization parameter, was\n",
      "optimal for some intermediate value, neither too large nor too small. Armed with\n",
      "these insights, we turn now to a discussion of two widely used nonparametric tech-niques for density estimation, kernel estimators and nearest neighbours, which have\n",
      "better scaling with dimensionality than the simple histogram model.\n",
      "2.5.1 Kernel density estimators\n",
      "Let us suppose that observations are being drawn from some unknown probabil-\n",
      "ity density p(x)in some D-dimensional space, which we shall take to be Euclidean,\n",
      "and we wish to estimate the value of p(x). From our earlier discussion of locality,\n",
      "let us consider some small region Rcontaining x. The probability mass associated\n",
      "with this region is given by\n",
      "P=∫\n",
      "Rp(x)dx. (2.242)\n",
      "Now suppose that we have collected a data set comprising Nobservations drawn\n",
      "fromp(x). Because each data point has a probability Pof falling within R, the total\n",
      "number Kof points that lie inside Rwill be distributed according to the binomial\n",
      "distribution Section 2.1\n",
      "Bin(K|N,P)=N!\n",
      "K!(N−K)!PK(1−P)1−K. (2.243)\n",
      "Using (2.11), we see that the mean fraction of points falling inside the region is\n",
      "E[K/N]=P, and similarly using (2.12) we see that the variance around this mean\n",
      "isvar[K/N]=P(1−P)/N. For large N, this distribution will be sharply peaked\n",
      "around the mean and so\n",
      "K≃NP. (2.244)\n",
      "If, however, we also assume that the region Ris sufﬁciently small that the probability\n",
      "density p(x)is roughly constant over the region, then we have\n",
      "P≃p(x)V (2.245)\n",
      "where Vis the volume of R. Combining (2.244) and (2.245), we obtain our density\n",
      "estimate in the form\n",
      "p(x)=K\n",
      "NV. (2.246)\n",
      "Note that the validity of (2.246) depends on two contradictory assumptions, namely\n",
      "that the region Rbe sufﬁciently small that the density is approximately constant over\n",
      "the region and yet sufﬁciently large (in relation to the value of that density) that the\n",
      "number Kof points falling inside the region is sufﬁcient for the binomial distribution\n",
      "to be sharply peaked.2.5. Nonparametric Methods 123\n",
      "We can exploit the result (2.246) in two different ways. Either we can ﬁx Kand\n",
      "determine the value of Vfrom the data, which gives rise to the K-nearest-neighbour\n",
      "technique discussed shortly, or we can ﬁx Vand determine Kfrom the data, giv-\n",
      "ing rise to the kernel approach. It can be shown that both the K-nearest-neighbour\n",
      "density estimator and the kernel density estimator converge to the true probabilitydensity in the limit N→∞ provided Vshrinks suitably with N, andKgrows with\n",
      "N(Duda and Hart, 1973).\n",
      "We begin by discussing the kernel method in detail, and to start with we take\n",
      "the region Rto be a small hypercube centred on the point xat which we wish to\n",
      "determine the probability density. In order to count the number Kof points falling\n",
      "within this region, it is convenient to deﬁne the following function\n",
      "k(u)={\n",
      "1,|ui|⩽1/2,i=1,...,D ,\n",
      "0,otherwise(2.247)\n",
      "which represents a unit cube centred on the origin. The function k(u)is an example\n",
      "of a kernel function , and in this context is also called a Parzen window . From (2.247),\n",
      "the quantity k((x−xn)/h)will be one if the data point xnlies inside a cube of side\n",
      "hcentred on x, and zero otherwise. The total number of data points lying inside this\n",
      "cube will therefore be\n",
      "K=N∑\n",
      "n=1k(x−xn\n",
      "h)\n",
      ". (2.248)\n",
      "Substituting this expression into (2.246) then gives the following result for the esti-\n",
      "mated density at x\n",
      "p(x)=1\n",
      "NN∑\n",
      "n=11\n",
      "hDk(x−xn\n",
      "h)\n",
      "(2.249)\n",
      "where we have used V=hDfor the volume of a hypercube of side hinDdi-\n",
      "mensions. Using the symmetry of the function k(u), we can now re-interpret this\n",
      "equation, not as a single cube centred on xbut as the sum over Ncubes centred on\n",
      "theNdata points xn.\n",
      "As it stands, the kernel density estimator (2.249) will suffer from one of the same\n",
      "problems that the histogram method suffered from, namely the presence of artiﬁcialdiscontinuities, in this case at the boundaries of the cubes. We can obtain a smoother\n",
      "density model if we choose a smoother kernel function, and a common choice is the\n",
      "Gaussian, which gives rise to the following kernel density model\n",
      "p(x)=1\n",
      "NN∑\n",
      "n=11\n",
      "(2πh2)1/2exp{\n",
      "−∥x−xn∥2\n",
      "2h2}\n",
      "(2.250)\n",
      "where hrepresents the standard deviation of the Gaussian components. Thus our\n",
      "density model is obtained by placing a Gaussian over each data point and then addingup the contributions over the whole data set, and then dividing by Nso that the den-\n",
      "sity is correctly normalized. In Figure 2.25, we apply the model (2.250) to the data124 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.25 Illustration of the kernel density model\n",
      "(2.250) applied to the same data set used\n",
      "to demonstrate the histogram approach in\n",
      "Figure 2.24. We see that hacts as a\n",
      "smoothing parameter and that if it is set\n",
      "too small (top panel), the result is a very\n",
      "noisy density model, whereas if it is set\n",
      "too large (bottom panel), then the bimodal\n",
      "nature of the underlying distribution from\n",
      "which the data is generated (shown by the\n",
      "green curve) is washed out. The best den-\n",
      "sity model is obtained for some intermedi-\n",
      "ate value of h(middle panel).h=0.005\n",
      "0 0.5 105\n",
      "h=0.07\n",
      "0 0.5 105\n",
      "h=0.2\n",
      "0 0.5 105\n",
      "set used earlier to demonstrate the histogram technique. We see that, as expected,\n",
      "the parameter hplays the role of a smoothing parameter, and there is a trade-off\n",
      "between sensitivity to noise at small hand over-smoothing at large h. Again, the\n",
      "optimization of his a problem in model complexity, analogous to the choice of bin\n",
      "width in histogram density estimation, or the degree of the polynomial used in curve\n",
      "ﬁtting.\n",
      "We can choose any other kernel function k(u)in (2.249) subject to the condi-\n",
      "tions\n",
      "k(u)⩾0, (2.251)∫\n",
      "k(u)du=1 (2.252)\n",
      "which ensure that the resulting probability distribution is nonnegative everywhere\n",
      "and integrates to one. The class of density model given by (2.249) is called a kernel\n",
      "density estimator, or Parzen estimator. It has a great merit that there is no compu-\n",
      "tation involved in the ‘training’ phase because this simply requires storage of the\n",
      "training set. However, this is also one of its great weaknesses because the computa-\n",
      "tional cost of evaluating the density grows linearly with the size of the data set.\n",
      "2.5.2 Nearest-neighbour methods\n",
      "One of the difﬁculties with the kernel approach to density estimation is that the\n",
      "parameter hgoverning the kernel width is ﬁxed for all kernels. In regions of high\n",
      "data density, a large value of hmay lead to over-smoothing and a washing out of\n",
      "structure that might otherwise be extracted from the data. However, reducing hmay\n",
      "lead to noisy estimates elsewhere in data space where the density is smaller. Thus\n",
      "the optimal choice for hmay be dependent on location within the data space. This\n",
      "issue is addressed by nearest-neighbour methods for density estimation.\n",
      "We therefore return to our general result (2.246) for local density estimation,\n",
      "and instead of ﬁxing Vand determining the value of Kfrom the data, we consider\n",
      "a ﬁxed value of Kand use the data to ﬁnd an appropriate value for V. To do this,\n",
      "we consider a small sphere centred on the point xat which we wish to estimate the2.5. Nonparametric Methods 125\n",
      "Figure 2.26 Illustration of K-nearest-neighbour den-\n",
      "sity estimation using the same data set\n",
      "as in Figures 2.25 and 2.24. We see\n",
      "that the parameter Kgoverns the degree\n",
      "of smoothing, so that a small value of\n",
      "Kleads to a very noisy density model\n",
      "(top panel), whereas a large value (bot-\n",
      "tom panel) smoothes out the bimodal na-\n",
      "ture of the true distribution (shown by the\n",
      "green curve) from which the data set was\n",
      "generated.K=1\n",
      "0 0.5 105\n",
      "K=5\n",
      "0 0.5 105\n",
      "K=3 0\n",
      "0 0.5 105\n",
      "density p(x), and we allow the radius of the sphere to grow until it contains precisely\n",
      "Kdata points. The estimate of the density p(x)is then given by (2.246) with Vset to\n",
      "the volume of the resulting sphere. This technique is known as Knearest neighbours\n",
      "and is illustrated in Figure 2.26, for various choices of the parameter K, using the\n",
      "same data set as used in Figure 2.24 and Figure 2.25. We see that the value of K\n",
      "now governs the degree of smoothing and that again there is an optimum choice for\n",
      "Kthat is neither too large nor too small. Note that the model produced by Knearest\n",
      "neighbours is not a true density model because the integral over all space diverges. Exercise 2.61\n",
      "We close this chapter by showing how the K-nearest-neighbour technique for\n",
      "density estimation can be extended to the problem of classiﬁcation. To do this, we\n",
      "apply the K-nearest-neighbour density estimation technique to each class separately\n",
      "and then make use of Bayes’ theorem. Let us suppose that we have a data set com-\n",
      "prising Nkpoints in class CkwithNpoints in total, so that∑\n",
      "kNk=N.I f w e\n",
      "wish to classify a new point x, we draw a sphere centred on xcontaining precisely\n",
      "Kpoints irrespective of their class. Suppose this sphere has volume Vand contains\n",
      "Kkpoints from class Ck. Then (2.246) provides an estimate of the density associated\n",
      "with each class\n",
      "p(x|Ck)=Kk\n",
      "NkV. (2.253)\n",
      "Similarly, the unconditional density is given by\n",
      "p(x)=K\n",
      "NV(2.254)\n",
      "while the class priors are given by\n",
      "p(Ck)=Nk\n",
      "N. (2.255)\n",
      "We can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain\n",
      "the posterior probability of class membership\n",
      "p(Ck|x)=p(x|Ck)p(Ck)\n",
      "p(x)=Kk\n",
      "K. (2.256)126 2. PROBABILITY DISTRIBUTIONS\n",
      "Figure 2.27 (a) In the K-nearest-\n",
      "neighbour classiﬁer, a new point,\n",
      "shown by the black diamond, is clas-\n",
      "siﬁed according to the majority class\n",
      "membership of the Kclosest train-\n",
      "ing data points, in this case K=\n",
      "3. (b) In the nearest-neighbour\n",
      "(K=1) approach to classiﬁcation,\n",
      "the resulting decision boundary is\n",
      "composed of hyperplanes that form\n",
      "perpendicular bisectors of pairs of\n",
      "points from different classes.\n",
      "x1x2\n",
      "(a)x1x2\n",
      "(b)\n",
      "If we wish to minimize the probability of misclassiﬁcation, this is done by assigning\n",
      "the test point xto the class having the largest posterior probability, corresponding to\n",
      "the largest value of Kk/K. Thus to classify a new point, we identify the Knearest\n",
      "points from the training data set and then assign the new point to the class having the\n",
      "largest number of representatives amongst this set. Ties can be broken at random.\n",
      "The particular case of K=1 is called the nearest-neighbour rule, because a test\n",
      "point is simply assigned to the same class as the nearest point from the training set.\n",
      "These concepts are illustrated in Figure 2.27.\n",
      "In Figure 2.28, we show the results of applying the K-nearest-neighbour algo-\n",
      "rithm to the oil ﬂow data, introduced in Chapter 1, for various values of K.A s\n",
      "expected, we see that Kcontrols the degree of smoothing, so that small Kproduces\n",
      "many small regions of each class, whereas large Kleads to fewer larger regions.\n",
      "x6x7\n",
      "K=1\n",
      "0 1 2012\n",
      "x6x7K=3\n",
      "0 1 2012\n",
      "x6x7K=31\n",
      "0 1 2012\n",
      "Figure 2.28 Plot of 200 data points from the oil data set showing values of x6plotted against x7, where the\n",
      "red, green, and blue points correspond to the ‘laminar’, ‘annular’, and ‘homogeneous’ classes, respectively. Also\n",
      "shown are the classiﬁcations of the input space given by the K-nearest-neighbour algorithm for various values\n",
      "ofK.Exercises 127\n",
      "An interesting property of the nearest-neighbour ( K=1) classiﬁer is that, in the\n",
      "limitN→∞ , the error rate is never more than twice the minimum achievable error\n",
      "rate of an optimal classiﬁer, i.e., one that uses the true class distributions (Cover and\n",
      "Hart, 1967) .\n",
      "As discussed so far, both the K-nearest-neighbour method, and the kernel den-\n",
      "sity estimator, require the entire training data set to be stored, leading to expensive\n",
      "computation if the data set is large. This effect can be offset, at the expense of some\n",
      "additional one-off computation, by constructing tree-based search structures to allow(approximate) near neighbours to be found efﬁciently without doing an exhaustive\n",
      "search of the data set. Nevertheless, these nonparametric methods are still severely\n",
      "limited. On the other hand, we have seen that simple parametric models are very\n",
      "restricted in terms of the forms of distribution that they can represent. We therefore\n",
      "need to ﬁnd density models that are very ﬂexible and yet for which the complexityof the models can be controlled independently of the size of the training set, and we\n",
      "shall see in subsequent chapters how to achieve this.\n",
      "Exercises\n",
      "2.1 (⋆)www Verify that the Bernoulli distribution (2.2) satisﬁes the following prop-\n",
      "erties\n",
      "1∑\n",
      "x=0p(x|µ)=1 (2.257)\n",
      "E[x]= µ (2.258)\n",
      "var[x]= µ(1−µ). (2.259)\n",
      "Show that the entropy H[x]of a Bernoulli distributed random binary variable xis\n",
      "given by\n",
      "H[x]=−µlnµ−(1−µ)l n ( 1−µ). (2.260)\n",
      "2.2 (⋆⋆)The form of the Bernoulli distribution given by (2.2) is not symmetric be-\n",
      "tween the two values of x. In some situations, it will be more convenient to use an\n",
      "equivalent formulation for which x∈{ −1,1}, in which case the distribution can be\n",
      "written\n",
      "p(x|µ)=(1−µ\n",
      "2)(1−x)/2(1+µ\n",
      "2)(1+x)/2\n",
      "(2.261)\n",
      "where µ∈[−1,1]. Show that the distribution (2.261) is normalized, and evaluate its\n",
      "mean, variance, and entropy.\n",
      "2.3 (⋆⋆)www In this exercise, we prove that the binomial distribution (2.9) is nor-\n",
      "malized. First use the deﬁnition (2.10) of the number of combinations of midentical\n",
      "objects chosen from a total of Nto show that\n",
      "(N\n",
      "m)\n",
      "+(N\n",
      "m−1)\n",
      "=(N+1\n",
      "m)\n",
      ". (2.262)128 2. PROBABILITY DISTRIBUTIONS\n",
      "Use this result to prove by induction the following result\n",
      "(1 +x)N=N∑\n",
      "m=0(N\n",
      "m)\n",
      "xm(2.263)\n",
      "which is known as the binomial theorem , and which is valid for all real values of x.\n",
      "Finally, show that the binomial distribution is normalized, so that\n",
      "N∑\n",
      "m=0(N\n",
      "m)\n",
      "µm(1−µ)N−m=1 (2.264)\n",
      "which can be done by ﬁrst pulling out a factor (1−µ)Nout of the summation and\n",
      "then making use of the binomial theorem.\n",
      "2.4 (⋆⋆)Show that the mean of the binomial distribution is given by (2.11). To do this,\n",
      "differentiate both sides of the normalization condition (2.264) with respect to µand\n",
      "then rearrange to obtain an expression for the mean of n. Similarly, by differentiating\n",
      "(2.264) twice with respect to µand making use of the result (2.11) for the mean of\n",
      "the binomial distribution prove the result (2.12) for the variance of the binomial.\n",
      "2.5 (⋆⋆)www In this exercise, we prove that the beta distribution, given by (2.13), is\n",
      "correctly normalized, so that (2.14) holds. This is equivalent to showing that\n",
      "∫1\n",
      "0µa−1(1−µ)b−1dµ=Γ(a)Γ(b)\n",
      "Γ(a+b). (2.265)\n",
      "From the deﬁnition (1.141) of the gamma function, we have\n",
      "Γ(a)Γ(b)=∫∞\n",
      "0exp(−x)xa−1dx∫∞\n",
      "0exp(−y)yb−1dy. (2.266)\n",
      "Use this expression to prove (2.265) as follows. First bring the integral over yinside\n",
      "the integrand of the integral over x, next make the change of variable t=y+x\n",
      "where xis ﬁxed, then interchange the order of the xandtintegrations, and ﬁnally\n",
      "make the change of variable x=tµwhere tis ﬁxed.\n",
      "2.6 (⋆)Make use of the result (2.265) to show that the mean, variance, and mode of the\n",
      "beta distribution (2.13) are given respectively by\n",
      "E[µ]=a\n",
      "a+b(2.267)\n",
      "var[µ]=ab\n",
      "(a+b)2(a+b+1 )(2.268)\n",
      "mode[ µ]=a−1\n",
      "a+b−2. (2.269)Exercises 129\n",
      "2.7 (⋆⋆)Consider a binomial random variable xgiven by (2.9), with prior distribution\n",
      "forµgiven by the beta distribution (2.13), and suppose we have observed moccur-\n",
      "rences of x=1andloccurrences of x=0. Show that the posterior mean value of x\n",
      "lies between the prior mean and the maximum likelihood estimate for µ. To do this,\n",
      "show that the posterior mean can be written as λtimes the prior mean plus (1−λ)\n",
      "times the maximum likelihood estimate, where 0⩽λ⩽1. This illustrates the con-\n",
      "cept of the posterior distribution being a compromise between the prior distribution\n",
      "and the maximum likelihood solution.\n",
      "2.8 (⋆)Consider two variables xandywith joint distribution p(x, y). Prove the follow-\n",
      "ing two results\n",
      "E[x]= Ey[Ex[x|y]] (2.270)\n",
      "var[x]= Ey[varx[x|y]] + var y[Ex[x|y]]. (2.271)\n",
      "Here Ex[x|y]denotes the expectation of xunder the conditional distribution p(x|y),\n",
      "with a similar notation for the conditional variance.\n",
      "2.9 (⋆⋆⋆ )www . In this exercise, we prove the normalization of the Dirichlet dis-\n",
      "tribution (2.38) using induction. We have already shown in Exercise 2.5 that thebeta distribution, which is a special case of the Dirichlet for M=2, is normalized.\n",
      "We now assume that the Dirichlet distribution is normalized for M−1variables\n",
      "and prove that it is normalized for Mvariables. To do this, consider the Dirichlet\n",
      "distribution over Mvariables, and take account of the constraint ∑M\n",
      "k=1µk=1 by\n",
      "eliminating µM, so that the Dirichlet is written\n",
      "pM(µ1,...,µ M−1)=CMM−1∏\n",
      "k=1µαk−1\n",
      "k(\n",
      "1−M−1∑\n",
      "j=1µj)αM−1\n",
      "(2.272)\n",
      "and our goal is to ﬁnd an expression for CM. To do this, integrate over µM−1, taking\n",
      "care over the limits of integration, and then make a change of variable so that this\n",
      "integral has limits 0and1. By assuming the correct result for CM−1and making use\n",
      "of (2.265), derive the expression for CM.\n",
      "2.10 (⋆⋆)Using the property Γ(x+1 ) = xΓ(x)of the gamma function, derive the\n",
      "following results for the mean, variance, and covariance of the Dirichlet distribution\n",
      "given by (2.38)\n",
      "E[µj]=αj\n",
      "α0(2.273)\n",
      "var[µj]=αj(α0−αj)\n",
      "α2\n",
      "0(α0+1 )(2.274)\n",
      "cov[µjµl]= −αjαl\n",
      "α2\n",
      "0(α0+1 ),j ̸=l (2.275)\n",
      "where α0is deﬁned by (2.39).130 2. PROBABILITY DISTRIBUTIONS\n",
      "2.11 (⋆)www By expressing the expectation of lnµjunder the Dirichlet distribution\n",
      "(2.38) as a derivative with respect to αj, show that\n",
      "E[lnµj]=ψ(αj)−ψ(α0) (2.276)\n",
      "where α0is given by (2.39) and\n",
      "ψ(a)≡d\n",
      "dalnΓ(a) (2.277)\n",
      "is the digamma function.\n",
      "2.12 (⋆)The uniform distribution for a continuous variable xis deﬁned by\n",
      "U(x|a, b)=1\n",
      "b−a,a ⩽x⩽b. (2.278)\n",
      "Verify that this distribution is normalized, and ﬁnd expressions for its mean and\n",
      "variance.\n",
      "2.13 (⋆⋆)Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians\n",
      "p(x)=N(x|µ,Σ)andq(x)=N(x|m,L).\n",
      "2.14 (⋆⋆)www This exercise demonstrates that the multivariate distribution with max-\n",
      "imum entropy, for a given covariance, is a Gaussian. The entropy of a distributionp(x)is given by\n",
      "H[x]=−∫\n",
      "p(x)l np(x)dx. (2.279)\n",
      "We wish to maximize H[x]over all distributions p(x)subject to the constraints that\n",
      "p(x)be normalized and that it have a speciﬁc mean and covariance, so that\n",
      "∫\n",
      "p(x)dx=1 (2.280)\n",
      "∫\n",
      "p(x)xdx=µ (2.281)\n",
      "∫\n",
      "p(x)(x−µ)(x−µ)Tdx=Σ. (2.282)\n",
      "By performing a variational maximization of (2.279) and using Lagrange multipliers\n",
      "to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum\n",
      "likelihood distribution is given by the Gaussian (2.43).\n",
      "2.15 (⋆⋆)Show that the entropy of the multivariate Gaussian N(x|µ,Σ)is given by\n",
      "H[x]=1\n",
      "2ln|Σ|+D\n",
      "2(1 + ln(2 π)) (2.283)\n",
      "where Dis the dimensionality of x.Exercises 131\n",
      "2.16 (⋆⋆⋆ )www Consider two random variables x1andx2having Gaussian distri-\n",
      "butions with means µ1,µ2and precisions τ1,τ2respectively. Derive an expression\n",
      "for the differential entropy of the variable x=x1+x2. To do this, ﬁrst ﬁnd the\n",
      "distribution of xby using the relation\n",
      "p(x)=∫∞\n",
      "−∞p(x|x2)p(x2)dx2 (2.284)\n",
      "and completing the square in the exponent. Then observe that this represents the\n",
      "convolution of two Gaussian distributions, which itself will be Gaussian, and ﬁnally\n",
      "make use of the result (1.110) for the entropy of the univariate Gaussian.\n",
      "2.17 (⋆)www Consider the multivariate Gaussian distribution given by (2.43). By\n",
      "writing the precision matrix (inverse covariance matrix) Σ−1as the sum of a sym-\n",
      "metric and an anti-symmetric matrix, show that the anti-symmetric term does not\n",
      "appear in the exponent of the Gaussian, and hence that the precision matrix may be\n",
      "taken to be symmetric without loss of generality. Because the inverse of a symmetric\n",
      "matrix is also symmetric (see Exercise 2.22), it follows that the covariance matrix\n",
      "may also be chosen to be symmetric without loss of generality.\n",
      "2.18 (⋆⋆⋆ )Consider a real, symmetric matrix Σwhose eigenvalue equation is given\n",
      "by (2.45). By taking the complex conjugate of this equation and subtracting theoriginal equation, and then forming the inner product with eigenvector u\n",
      "i, show that\n",
      "the eigenvalues λiare real. Similarly, use the symmetry property of Σto show that\n",
      "two eigenvectors uiandujwill be orthogonal provided λj̸=λi. Finally, show that\n",
      "without loss of generality, the set of eigenvectors can be chosen to be orthonormal,\n",
      "so that they satisfy (2.46), even if some of the eigenvalues are zero.\n",
      "2.19 (⋆⋆)Show that a real, symmetric matrix Σhaving the eigenvector equation (2.45)\n",
      "can be expressed as an expansion in the eigenvectors, with coefﬁcients given by the\n",
      "eigenvalues, of the form (2.48). Similarly, show that the inverse matrix Σ−1has a\n",
      "representation of the form (2.49).\n",
      "2.20 (⋆⋆)www A positive deﬁnite matrix Σcan be deﬁned as one for which the\n",
      "quadratic form\n",
      "aTΣa (2.285)\n",
      "is positive for any real value of the vector a. Show that a necessary and sufﬁcient\n",
      "condition for Σto be positive deﬁnite is that all of the eigenvalues λiofΣ, deﬁned\n",
      "by (2.45), are positive.\n",
      "2.21 (⋆)Show that a real, symmetric matrix of size D×DhasD(D+1)/2independent\n",
      "parameters.\n",
      "2.22 (⋆)www Show that the inverse of a symmetric matrix is itself symmetric.\n",
      "2.23 (⋆⋆)By diagonalizing the coordinate system using the eigenvector expansion (2.45),\n",
      "show that the volume contained within the hyperellipsoid corresponding to a constant132 2. PROBABILITY DISTRIBUTIONS\n",
      "Mahalanobis distance ∆is given by\n",
      "VD|Σ|1/2∆D(2.286)\n",
      "where VDis the volume of the unit sphere in Ddimensions, and the Mahalanobis\n",
      "distance is deﬁned by (2.44).\n",
      "2.24 (⋆⋆)www Prove the identity (2.76) by multiplying both sides by the matrix\n",
      "(\n",
      "AB\n",
      "CD)\n",
      "(2.287)\n",
      "and making use of the deﬁnition (2.77).\n",
      "2.25 (⋆⋆)In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distri-\n",
      "butions for a multivariate Gaussian. More generally, we can consider a partitioningof the components of xinto three groups x\n",
      "a,xb, andxc, with a corresponding par-\n",
      "titioning of the mean vector µand of the covariance matrix Σin the form\n",
      "µ=(µa\n",
      "µb\n",
      "µc)\n",
      ", Σ=(ΣaaΣabΣac\n",
      "ΣbaΣbbΣbc\n",
      "ΣcaΣcbΣcc)\n",
      ". (2.288)\n",
      "By making use of the results of Section 2.3, ﬁnd an expression for the conditional\n",
      "distribution p(xa|xb)in which xchas been marginalized out.\n",
      "2.26 (⋆⋆)A very useful result from linear algebra is the Woodbury matrix inversion\n",
      "formula given by\n",
      "(A+BCD )−1=A−1−A−1B(C−1+DA−1B)−1DA−1. (2.289)\n",
      "By multiplying both sides by (A+BCD )prove the correctness of this result.\n",
      "2.27 (⋆)Letxandzbe two independent random vectors, so that p(x,z)=p(x)p(z).\n",
      "Show that the mean of their sum y=x+zis given by the sum of the means of each\n",
      "of the variable separately. Similarly, show that the covariance matrix of yis given by\n",
      "the sum of the covariance matrices of xandz. Conﬁrm that this result agrees with\n",
      "that of Exercise 1.10.\n",
      "2.28 (⋆⋆⋆ )www Consider a joint distribution over the variable\n",
      "z=(\n",
      "x\n",
      "y)\n",
      "(2.290)\n",
      "whose mean and covariance are given by (2.108) and (2.105) respectively. By mak-\n",
      "ing use of the results (2.92) and (2.93) show that the marginal distribution p(x)is\n",
      "given (2.99). Similarly, by making use of the results (2.81) and (2.82) show that theconditional distribution p(y|x)is given by (2.100).Exercises 133\n",
      "2.29 (⋆⋆)Using the partitioned matrix inversion formula (2.76), show that the inverse of\n",
      "the precision matrix (2.104) is given by the covariance matrix (2.105).\n",
      "2.30 (⋆)By starting from (2.107) and making use of the result (2.105), verify the result\n",
      "(2.108).\n",
      "2.31 (⋆⋆)Consider two multidimensional random vectors xandzhaving Gaussian\n",
      "distributions p(x)=N(x|µx,Σx)andp(z)=N(z|µz,Σz)respectively, together\n",
      "with their sum y=x+z. Use the results (2.109) and (2.110) to ﬁnd an expression for\n",
      "the marginal distribution p(y)by considering the linear-Gaussian model comprising\n",
      "the product of the marginal distribution p(x)and the conditional distribution p(y|x).\n",
      "2.32 (⋆⋆⋆ )www This exercise and the next provide practice at manipulating the\n",
      "quadratic forms that arise in linear-Gaussian models, as well as giving an indepen-dent check of results derived in the main text. Consider a joint distribution p(x,y)\n",
      "deﬁned by the marginal and conditional distributions given by (2.99) and (2.100).\n",
      "By examining the quadratic form in the exponent of the joint distribution, and usingthe technique of ‘completing the square’ discussed in Section 2.3, ﬁnd expressions\n",
      "for the mean and covariance of the marginal distribution p(y)in which the variable\n",
      "xhas been integrated out. To do this, make use of the Woodbury matrix inversion\n",
      "formula (2.289). Verify that these results agree with (2.109) and (2.110) obtained\n",
      "using the results of Chapter 2.\n",
      "2.33 (⋆⋆⋆ )Consider the same joint distribution as in Exercise 2.32, but now use the\n",
      "technique of completing the square to ﬁnd expressions for the mean and covarianceof the conditional distribution p(x|y). Again, verify that these agree with the corre-\n",
      "sponding expressions (2.111) and (2.112).\n",
      "2.34 (⋆⋆)\n",
      "www To ﬁnd the maximum likelihood solution for the covariance matrix\n",
      "of a multivariate Gaussian, we need to maximize the log likelihood function (2.118)with respect to Σ, noting that the covariance matrix must be symmetric and positive\n",
      "deﬁnite. Here we proceed by ignoring these constraints and doing a straightforward\n",
      "maximization. Using the results (C.21), (C.26), and (C.28) from Appendix C, showthat the covariance matrix Σthat maximizes the log likelihood function (2.118) is\n",
      "given by the sample covariance (2.122). We note that the ﬁnal result is necessarily\n",
      "symmetric and positive deﬁnite (provided the sample covariance is nonsingular).\n",
      "2.35 (⋆⋆)Use the result (2.59) to prove (2.62). Now, using the results (2.59), and (2.62),\n",
      "show that\n",
      "E[x\n",
      "nxm]=µµT+InmΣ (2.291)\n",
      "wherexndenotes a data point sampled from a Gaussian distribution with mean µ\n",
      "and covariance Σ, andInmdenotes the (n, m)element of the identity matrix. Hence\n",
      "prove the result (2.124).\n",
      "2.36 (⋆⋆)www Using an analogous procedure to that used to obtain (2.126), derive\n",
      "an expression for the sequential estimation of the variance of a univariate Gaussian134 2. PROBABILITY DISTRIBUTIONS\n",
      "distribution, by starting with the maximum likelihood expression\n",
      "σ2\n",
      "ML=1\n",
      "NN∑\n",
      "n=1(xn−µ)2. (2.292)\n",
      "Verify that substituting the expression for a Gaussian distribution into the Robbins-\n",
      "Monro sequential estimation formula (2.135) gives a result of the same form, andhence obtain an expression for the corresponding coefﬁcients a\n",
      "N.\n",
      "2.37 (⋆⋆)Using an analogous procedure to that used to obtain (2.126), derive an ex-\n",
      "pression for the sequential estimation of the covariance of a multivariate Gaussiandistribution, by starting with the maximum likelihood expression (2.122). Verify that\n",
      "substituting the expression for a Gaussian distribution into the Robbins-Monro se-\n",
      "quential estimation formula (2.135) gives a result of the same form, and hence obtainan expression for the corresponding coefﬁcients a\n",
      "N.\n",
      "2.38 (⋆)Use the technique of completing the square for the quadratic form in the expo-\n",
      "nent to derive the results (2.141) and (2.142).\n",
      "2.39 (⋆⋆)Starting from the results (2.141) and (2.142) for the posterior distribution\n",
      "of the mean of a Gaussian random variable, dissect out the contributions from the\n",
      "ﬁrstN−1data points and hence obtain expressions for the sequential update of\n",
      "µNandσ2\n",
      "N. Now derive the same results starting from the posterior distribution\n",
      "p(µ|x1,...,x N−1)=N(µ|µN−1,σ2\n",
      "N−1)and multiplying by the likelihood func-\n",
      "tionp(xN|µ)=N(xN|µ, σ2)and then completing the square and normalizing to\n",
      "obtain the posterior distribution after Nobservations.\n",
      "2.40 (⋆⋆)www Consider a D-dimensional Gaussian random variable xwith distribu-\n",
      "tionN(x|µ,Σ)in which the covariance Σis known and for which we wish to infer\n",
      "the mean µfrom a set of observations X={x1,...,xN}. Given a prior distribution\n",
      "p(µ)=N(µ|µ0,Σ0), ﬁnd the corresponding posterior distribution p(µ|X).\n",
      "2.41 (⋆)Use the deﬁnition of the gamma function (1.141) to show that the gamma dis-\n",
      "tribution (2.146) is normalized.\n",
      "2.42 (⋆⋆)Evaluate the mean, variance, and mode of the gamma distribution (2.146).\n",
      "2.43 (⋆)The following distribution\n",
      "p(x|σ2,q)=q\n",
      "2(2σ2)1/qΓ(1/q)exp(\n",
      "−|x|q\n",
      "2σ2)\n",
      "(2.293)\n",
      "is a generalization of the univariate Gaussian distribution. Show that this distribution\n",
      "is normalized so that ∫∞\n",
      "−∞p(x|σ2,q)dx=1 (2.294)\n",
      "and that it reduces to the Gaussian when q=2. Consider a regression model in\n",
      "which the target variable is given by t=y(x,w)+ϵandϵis a random noiseExercises 135\n",
      "variable drawn from the distribution (2.293). Show that the log likelihood function\n",
      "overwandσ2, for an observed data set of input vectors X={x1,...,xN}and\n",
      "corresponding target variables t=(t1,...,t N)T,i sg i v e nb y\n",
      "lnp(t|X,w,σ2)=−1\n",
      "2σ2N∑\n",
      "n=1|y(xn,w)−tn|q−N\n",
      "qln(2σ2)+c o n s t (2.295)\n",
      "where ‘ const ’ denotes terms independent of both wandσ2. Note that, as a function\n",
      "ofw, this is the Lqerror function considered in Section 1.5.5.\n",
      "2.44 (⋆⋆)Consider a univariate Gaussian distribution N(x|µ, τ−1)having conjugate\n",
      "Gaussian-gamma prior given by (2.154), and a data set x={x1,...,x N}of i.i.d.\n",
      "observations. Show that the posterior distribution is also a Gaussian-gamma distri-\n",
      "bution of the same functional form as the prior, and write down expressions for the\n",
      "parameters of this posterior distribution.\n",
      "2.45 (⋆)Verify that the Wishart distribution deﬁned by (2.155) is indeed a conjugate\n",
      "prior for the precision matrix of a multivariate Gaussian.\n",
      "2.46 (⋆)www Verify that evaluating the integral in (2.158) leads to the result (2.159).\n",
      "2.47 (⋆)www Show that in the limit ν→∞ , the t-distribution (2.159) becomes a\n",
      "Gaussian. Hint: ignore the normalization coefﬁcient, and simply look at the depen-\n",
      "dence on x.\n",
      "2.48 (⋆)By following analogous steps to those used to derive the univariate Student’s\n",
      "t-distribution (2.159), verify the result (2.162) for the multivariate form of the Stu-\n",
      "dent’s t-distribution, by marginalizing over the variable ηin (2.161). Using the\n",
      "deﬁnition (2.161), show by exchanging integration variables that the multivariate\n",
      "t-distribution is correctly normalized.\n",
      "2.49 (⋆⋆)By using the deﬁnition (2.161) of the multivariate Student’s t-distribution as a\n",
      "convolution of a Gaussian with a gamma distribution, verify the properties (2.164),\n",
      "(2.165), and (2.166) for the multivariate t-distribution deﬁned by (2.162).\n",
      "2.50 (⋆)Show that in the limit ν→∞ , the multivariate Student’s t-distribution (2.162)\n",
      "reduces to a Gaussian with mean µand precision Λ.\n",
      "2.51 (⋆)www The various trigonometric identities used in the discussion of periodic\n",
      "variables in this chapter can be proven easily from the relation\n",
      "exp(iA)=c o s A+isinA (2.296)\n",
      "in which iis the square root of minus one. By considering the identity\n",
      "exp(iA) exp(−iA)=1 (2.297)\n",
      "prove the result (2.177). Similarly, using the identity\n",
      "cos(A−B)=ℜexp{i(A−B)} (2.298)136 2. PROBABILITY DISTRIBUTIONS\n",
      "where ℜdenotes the real part, prove (2.178). Finally, by using sin(A−B)=\n",
      "ℑexp{i(A−B)}, where ℑdenotes the imaginary part, prove the result (2.183).\n",
      "2.52 (⋆⋆)For large m, the von Mises distribution (2.179) becomes sharply peaked\n",
      "around the mode θ0. By deﬁning ξ=m1/2(θ−θ0)and making the Taylor ex-\n",
      "pansion of the cosine function given by\n",
      "cosα=1−α2\n",
      "2+O(α4) (2.299)\n",
      "show that as m→∞ , the von Mises distribution tends to a Gaussian.\n",
      "2.53 (⋆)Using the trigonometric identity (2.183), show that solution of (2.182) for θ0is\n",
      "given by (2.184).\n",
      "2.54 (⋆)By computing ﬁrst and second derivatives of the von Mises distribution (2.179),\n",
      "and using I0(m)>0form>0, show that the maximum of the distribution occurs\n",
      "whenθ=θ0and that the minimum occurs when θ=θ0+π(mod2 π).\n",
      "2.55 (⋆)By making use of the result (2.168), together with (2.184) and the trigonometric\n",
      "identity (2.178), show that the maximum likelihood solution mMLfor the concentra-\n",
      "tion of the von Mises distribution satisﬁes A(mML)=rwhere ris the radius of the\n",
      "mean of the observations viewed as unit vectors in the two-dimensional Euclidean\n",
      "plane, as illustrated in Figure 2.17.\n",
      "2.56 (⋆⋆)www Express the beta distribution (2.13), the gamma distribution (2.146),\n",
      "and the von Mises distribution (2.179) as members of the exponential family (2.194)and thereby identify their natural parameters.\n",
      "2.57 (⋆)Verify that the multivariate Gaussian distribution can be cast in exponential\n",
      "family form (2.194) and derive expressions for η,u(x),h(x)andg(η)analogous to\n",
      "(2.220)–(2.223).\n",
      "2.58 (⋆)The result (2.226) showed that the negative gradient of lng(η)for the exponen-\n",
      "tial family is given by the expectation of u(x). By taking the second derivatives of\n",
      "(2.195), show that\n",
      "−∇∇ lng(η)= E[u(x)u(x)\n",
      "T]−E[u(x)]E[u(x)T]=c o v [ u(x)]. (2.300)\n",
      "2.59 (⋆)By changing variables using y=x/σ, show that the density (2.236) will be\n",
      "correctly normalized, provided f(x)is correctly normalized.\n",
      "2.60 (⋆⋆)www Consider a histogram-like density model in which the space xis di-\n",
      "vided into ﬁxed regions for which the density p(x)takes the constant value hiover\n",
      "theithregion, and that the volume of region iis denoted ∆i. Suppose we have a set\n",
      "ofNobservations of xsuch that niof these observations fall in region i. Using a\n",
      "Lagrange multiplier to enforce the normalization constraint on the density, derive an\n",
      "expression for the maximum likelihood estimator for the {hi}.\n",
      "2.61 (⋆)Show that the K-nearest-neighbour density model deﬁnes an improper distribu-\n",
      "tion whose integral over all space is divergent.3\n",
      "Linear\n",
      "Models for\n",
      "Regression\n",
      "The focus so far in this book has been on unsupervised learning, including topics\n",
      "such as density estimation and data clustering. We turn now to a discussion of super-vised learning, starting with regression. The goal of regression is to predict the value\n",
      "of one or more continuous target variables tgiven the value of a D-dimensional vec-\n",
      "torxofinput variables. We have already encountered an example of a regression\n",
      "problem when we considered polynomial curve ﬁtting in Chapter 1. The polynomial\n",
      "is a speciﬁc example of a broad class of functions called linear regression models,\n",
      "which share the property of being linear functions of the adjustable parameters, andwhich will form the focus of this chapter. The simplest form of linear regression\n",
      "models are also linear functions of the input variables. However, we can obtain a\n",
      "much more useful class of functions by taking linear combinations of a ﬁxed set ofnonlinear functions of the input variables, known as basis functions . Such models\n",
      "are linear functions of the parameters, which gives them simple analytical properties,\n",
      "and yet can be nonlinear with respect to the input variables.\n",
      "137138 3. LINEAR MODELS FOR REGRESSION\n",
      "Given a training data set comprising Nobservations {xn}, where n=1,...,N ,\n",
      "together with corresponding target values {tn}, the goal is to predict the value of t\n",
      "for a new value of x. In the simplest approach, this can be done by directly con-\n",
      "structing an appropriate function y(x)whose values for new inputs xconstitute the\n",
      "predictions for the corresponding values of t. More generally, from a probabilistic\n",
      "perspective, we aim to model the predictive distribution p(t|x)because this expresses\n",
      "our uncertainty about the value of tfor each value of x. From this conditional dis-\n",
      "tribution we can make predictions of t, for any new value of x, in such a way as to\n",
      "minimize the expected value of a suitably chosen loss function. As discussed in Sec-\n",
      "tion 1.5.5, a common choice of loss function for real-valued variables is the squared\n",
      "loss, for which the optimal solution is given by the conditional expectation of t.\n",
      "Although linear models have signiﬁcant limitations as practical techniques for\n",
      "pattern recognition, particularly for problems involving input spaces of high dimen-sionality, they have nice analytical properties and form the foundation for more so-\n",
      "phisticated models to be discussed in later chapters.\n",
      "3.1. Linear Basis Function Models\n",
      "The simplest linear model for regression is one that involves a linear combination ofthe input variables\n",
      "y(x,w)=w\n",
      "0+w1x1+...+wDxD (3.1)\n",
      "wherex=(x1,...,x D)T. This is often simply known as linear regression . The key\n",
      "property of this model is that it is a linear function of the parameters w0,...,w D.I ti s\n",
      "also, however, a linear function of the input variables xi, and this imposes signiﬁcant\n",
      "limitations on the model. We therefore extend the class of models by considering\n",
      "linear combinations of ﬁxed nonlinear functions of the input variables, of the form\n",
      "y(x,w)=w0+M−1∑\n",
      "j=1wjφj(x) (3.2)\n",
      "where φj(x)are known as basis functions . By denoting the maximum value of the\n",
      "indexjbyM−1, the total number of parameters in this model will be M.\n",
      "The parameter w0allows for any ﬁxed offset in the data and is sometimes called\n",
      "abias parameter (not to be confused with ‘bias’ in a statistical sense). It is often\n",
      "convenient to deﬁne an additional dummy ‘basis function’ φ0(x)=1 so that\n",
      "y(x,w)=M−1∑\n",
      "j=0wjφj(x)=wTφ(x) (3.3)\n",
      "wherew=(w0,...,w M−1)Tandφ=(φ0,...,φ M−1)T. In many practical ap-\n",
      "plications of pattern recognition, we will apply some form of ﬁxed pre-processing,3.1. Linear Basis Function Models 139\n",
      "or feature extraction, to the original data variables. If the original variables com-\n",
      "prise the vector x, then the features can be expressed in terms of the basis functions\n",
      "{φj(x)}.\n",
      "By using nonlinear basis functions, we allow the function y(x,w)to be a non-\n",
      "linear function of the input vector x. Functions of the form (3.2) are called linear\n",
      "models, however, because this function is linear in w. It is this linearity in the pa-\n",
      "rameters that will greatly simplify the analysis of this class of models. However, it\n",
      "also leads to some signiﬁcant limitations, as we discuss in Section 3.6.\n",
      "The example of polynomial regression considered in Chapter 1 is a particular\n",
      "example of this model in which there is a single input variable x, and the basis func-\n",
      "tions take the form of powers of xso that φj(x)=xj. One limitation of polynomial\n",
      "basis functions is that they are global functions of the input variable, so that changes\n",
      "in one region of input space affect all other regions. This can be resolved by dividingthe input space up into regions and ﬁt a different polynomial in each region, leading\n",
      "tospline functions (Hastie et al. , 2001).\n",
      "There are many other possible choices for the basis functions, for example\n",
      "φ\n",
      "j(x)=e x p{\n",
      "−(x−µj)2\n",
      "2s2}\n",
      "(3.4)\n",
      "where the µjgovern the locations of the basis functions in input space, and the pa-\n",
      "rameter sgoverns their spatial scale. These are usually referred to as ‘Gaussian’\n",
      "basis functions, although it should be noted that they are not required to have a prob-abilistic interpretation, and in particular the normalization coefﬁcient is unimportant\n",
      "because these basis functions will be multiplied by adaptive parameters w\n",
      "j.\n",
      "Another possibility is the sigmoidal basis function of the form\n",
      "φj(x)=σ(x−µj\n",
      "s)\n",
      "(3.5)\n",
      "where σ(a)is the logistic sigmoid function deﬁned by\n",
      "σ(a)=1\n",
      "1+e x p ( −a). (3.6)\n",
      "Equivalently, we can use the ‘ tanh ’ function because this is related to the logistic\n",
      "sigmoid by tanh(a)=2σ(a)−1, and so a general linear combination of logistic\n",
      "sigmoid functions is equivalent to a general linear combination of ‘ tanh ’ functions.\n",
      "These various choices of basis function are illustrated in Figure 3.1.\n",
      "Yet another possible choice of basis function is the Fourier basis, which leads to\n",
      "an expansion in sinusoidal functions. Each basis function represents a speciﬁc fre-\n",
      "quency and has inﬁnite spatial extent. By contrast, basis functions that are localizedto ﬁnite regions of input space necessarily comprise a spectrum of different spatial\n",
      "frequencies. In many signal processing applications, it is of interest to consider ba-\n",
      "sis functions that are localized in both space and frequency, leading to a class offunctions known as wavelets . These are also deﬁned to be mutually orthogonal, to\n",
      "simplify their application. Wavelets are most applicable when the input values live140 3. LINEAR MODELS FOR REGRESSION\n",
      "−1 0 1−1−0.500.51\n",
      "−1 0 100.250.50.751\n",
      "−1 0 100.250.50.751\n",
      "Figure 3.1 Examples of basis functions, showing polynomials on the left, Gaussians of the form (3.4) in the\n",
      "centre, and sigmoidal of the form (3.5) on the right.\n",
      "on a regular lattice, such as the successive time points in a temporal sequence, or the\n",
      "pixels in an image. Useful texts on wavelets include Ogden (1997), Mallat (1999),\n",
      "and Vidakovic (1999).\n",
      "Most of the discussion in this chapter, however, is independent of the particular\n",
      "choice of basis function set, and so for most of our discussion we shall not specify\n",
      "the particular form of the basis functions, except for the purposes of numerical il-\n",
      "lustration. Indeed, much of our discussion will be equally applicable to the situation\n",
      "in which the vector φ(x)of basis functions is simply the identity φ(x)=x. Fur-\n",
      "thermore, in order to keep the notation simple, we shall focus on the case of a single\n",
      "target variable t. However, in Section 3.1.5, we consider brieﬂy the modiﬁcations\n",
      "needed to deal with multiple target variables.\n",
      "3.1.1 Maximum likelihood and least squares\n",
      "In Chapter 1, we ﬁtted polynomial functions to data sets by minimizing a sum-\n",
      "of-squares error function. We also showed that this error function could be motivated\n",
      "as the maximum likelihood solution under an assumed Gaussian noise model. Let\n",
      "us return to this discussion and consider the least squares approach, and its relation\n",
      "to maximum likelihood, in more detail.\n",
      "As before, we assume that the target variable tis given by a deterministic func-\n",
      "tiony(x,w)with additive Gaussian noise so that\n",
      "t=y(x,w)+ϵ (3.7)\n",
      "where ϵis a zero mean Gaussian random variable with precision (inverse variance)\n",
      "β. Thus we can write\n",
      "p(t|x,w,β)=N(t|y(x,w),β−1). (3.8)\n",
      "Recall that, if we assume a squared loss function, then the optimal prediction, for a\n",
      "new value of x, will be given by the conditional mean of the target variable. In the Section 1.5.5\n",
      "case of a Gaussian conditional distribution of the form (3.8), the conditional mean3.1. Linear Basis Function Models 141\n",
      "will be simply\n",
      "E[t|x]=∫\n",
      "tp(t|x)dt=y(x,w). (3.9)\n",
      "Note that the Gaussian noise assumption implies that the conditional distribution of\n",
      "tgivenxis unimodal, which may be inappropriate for some applications. An ex-\n",
      "tension to mixtures of conditional Gaussian distributions, which permit multimodalconditional distributions, will be discussed in Section 14.5.1.\n",
      "Now consider a data set of inputs X={x\n",
      "1,...,xN}with corresponding target\n",
      "values t1,...,t N. We group the target variables {tn}into a column vector that we\n",
      "denote by twhere the typeface is chosen to distinguish it from a single observation\n",
      "of a multivariate target, which would be denoted t. Making the assumption that\n",
      "these data points are drawn independently from the distribution (3.8), we obtain thefollowing expression for the likelihood function, which is a function of the adjustable\n",
      "parameters wandβ, in the form\n",
      "p(t|X,w,β)=N∏\n",
      "n=1N(tn|wTφ(xn),β−1) (3.10)\n",
      "where we have used (3.3). Note that in supervised learning problems such as regres-\n",
      "sion (and classiﬁcation), we are not seeking to model the distribution of the inputvariables. Thus xwill always appear in the set of conditioning variables, and so\n",
      "from now on we will drop the explicit xfrom expressions such as p(t|x,w,β)in or-\n",
      "der to keep the notation uncluttered. Taking the logarithm of the likelihood function,and making use of the standard form (1.46) for the univariate Gaussian, we have\n",
      "lnp(t|w,β)=\n",
      "N∑\n",
      "n=1lnN(tn|wTφ(xn),β−1)\n",
      "=N\n",
      "2lnβ−N\n",
      "2ln(2π)−βED(w) (3.11)\n",
      "where the sum-of-squares error function is deﬁned by\n",
      "ED(w)=1\n",
      "2N∑\n",
      "n=1{tn−wTφ(xn)}2. (3.12)\n",
      "Having written down the likelihood function, we can use maximum likelihood to\n",
      "determine wandβ. Consider ﬁrst the maximization with respect to w. As observed\n",
      "already in Section 1.2.5, we see that maximization of the likelihood function under a\n",
      "conditional Gaussian noise distribution for a linear model is equivalent to minimizing\n",
      "a sum-of-squares error function given by ED(w). The gradient of the log likelihood\n",
      "function (3.11) takes the form\n",
      "∇lnp(t|w,β)=N∑\n",
      "n=1{\n",
      "tn−wTφ(xn)}\n",
      "φ(xn)T. (3.13)142 3. LINEAR MODELS FOR REGRESSION\n",
      "Setting this gradient to zero gives\n",
      "0=N∑\n",
      "n=1tnφ(xn)T−wT(N∑\n",
      "n=1φ(xn)φ(xn)T)\n",
      ". (3.14)\n",
      "Solving for wwe obtain\n",
      "wML=(\n",
      "ΦTΦ)−1ΦTt (3.15)\n",
      "which are known as the normal equations for the least squares problem. Here Φis an\n",
      "N×Mmatrix, called the design matrix , whose elements are given by Φnj=φj(xn),\n",
      "so that\n",
      "Φ=⎛\n",
      "⎜⎜⎝φ0(x1)φ1(x1)···φM−1(x1)\n",
      "φ0(x2)φ1(x2)···φM−1(x2)\n",
      "............\n",
      "φ0(xN)φ1(xN)···φM−1(xN)⎞\n",
      "⎟⎟⎠. (3.16)\n",
      "The quantity\n",
      "Φ†≡(\n",
      "ΦTΦ)−1ΦT(3.17)\n",
      "is known as the Moore-Penrose pseudo-inverse of the matrix Φ(Rao and Mitra,\n",
      "1971; Golub and Van Loan, 1996). It can be regarded as a generalization of the\n",
      "notion of matrix inverse to nonsquare matrices. Indeed, if Φis square and invertible,\n",
      "then using the property (AB)−1=B−1A−1we see that Φ†≡Φ−1.\n",
      "At this point, we can gain some insight into the role of the bias parameter w0.I f\n",
      "we make the bias parameter explicit, then the error function (3.12) becomes\n",
      "ED(w)=1\n",
      "2N∑\n",
      "n=1{tn−w0−M−1∑\n",
      "j=1wjφj(xn)}2. (3.18)\n",
      "Setting the derivative with respect to w0equal to zero, and solving for w0, we obtain\n",
      "w0=t−M−1∑\n",
      "j=1wjφj (3.19)\n",
      "where we have deﬁned\n",
      "t=1\n",
      "NN∑\n",
      "n=1tn, φj=1\n",
      "NN∑\n",
      "n=1φj(xn). (3.20)\n",
      "Thus the bias w0compensates for the difference between the averages (over the\n",
      "training set) of the target values and the weighted sum of the averages of the basis\n",
      "function values.\n",
      "We can also maximize the log likelihood function (3.11) with respect to the noise\n",
      "precision parameter β, giving\n",
      "1\n",
      "βML=1\n",
      "NN∑\n",
      "n=1{tn−wT\n",
      "MLφ(xn)}2(3.21)3.1. Linear Basis Function Models 143\n",
      "Figure 3.2 Geometrical interpretation of the least-squares\n",
      "solution, in an N-dimensional space whose axes\n",
      "are the values of t1,...,t N. The least-squares\n",
      "regression function is obtained by ﬁnding the or-\n",
      "thogonal projection of the data vector tonto the\n",
      "subspace spanned by the basis functions φj(x)\n",
      "in which each basis function is viewed as a vec-\n",
      "torϕjof length Nwith elements φj(xn).S\n",
      "t\n",
      "y ϕ1ϕ2\n",
      "and so we see that the inverse of the noise precision is given by the residual variance\n",
      "of the target values around the regression function.\n",
      "3.1.2 Geometry of least squares\n",
      "At this point, it is instructive to consider the geometrical interpretation of the\n",
      "least-squares solution. To do this we consider an N-dimensional space whose axes\n",
      "are given by the tn, so that t=(t1,...,t N)Tis a vector in this space. Each basis\n",
      "function φj(xn), evaluated at the Ndata points, can also be represented as a vector in\n",
      "the same space, denoted by ϕj, as illustrated in Figure 3.2. Note that ϕjcorresponds\n",
      "to the jthcolumn of Φ, whereas φ(xn)corresponds to the nthrow of Φ. If the\n",
      "number Mof basis functions is smaller than the number Nof data points, then the\n",
      "Mvectors φj(xn)will span a linear subspace Sof dimensionality M. We deﬁne\n",
      "yto be an N-dimensional vector whose nthelement is given by y(xn,w), where\n",
      "n=1,...,N . Because yis an arbitrary linear combination of the vectors ϕj, it can\n",
      "live anywhere in the M-dimensional subspace. The sum-of-squares error (3.12) is\n",
      "then equal (up to a factor of 1/2) to the squared Euclidean distance between yand\n",
      "t. Thus the least-squares solution for wcorresponds to that choice of ythat lies in\n",
      "subspace Sand that is closest to t. Intuitively, from Figure 3.2, we anticipate that\n",
      "this solution corresponds to the orthogonal projection of tonto the subspace S. This\n",
      "is indeed the case, as can easily be veriﬁed by noting that the solution for yis given\n",
      "byΦwML, and then conﬁrming that this takes the form of an orthogonal projection. Exercise 3.2\n",
      "In practice, a direct solution of the normal equations can lead to numerical difﬁ-\n",
      "culties when ΦTΦis close to singular. In particular, when two or more of the basis\n",
      "vectors ϕjare co-linear, or nearly so, the resulting parameter values can have large\n",
      "magnitudes. Such near degeneracies will not be uncommon when dealing with real\n",
      "data sets. The resulting numerical difﬁculties can be addressed using the technique\n",
      "ofsingular value decomposition ,o r SVD (Press et al. , 1992; Bishop and Nabney,\n",
      "2008). Note that the addition of a regularization term ensures that the matrix is non-\n",
      "singular, even in the presence of degeneracies.\n",
      "3.1.3 Sequential learning\n",
      "Batch techniques, such as the maximum likelihood solution (3.15), which in-\n",
      "volve processing the entire training set in one go, can be computationally costly for\n",
      "large data sets. As we have discussed in Chapter 1, if the data set is sufﬁciently large,\n",
      "it may be worthwhile to use sequential algorithms, also known as on-line algorithms,144 3. LINEAR MODELS FOR REGRESSION\n",
      "in which the data points are considered one at a time, and the model parameters up-\n",
      "dated after each such presentation. Sequential learning is also appropriate for real-time applications in which the data observations are arriving in a continuous stream,\n",
      "and predictions must be made before all of the data points are seen.\n",
      "We can obtain a sequential learning algorithm by applying the technique of\n",
      "stochastic gradient descent , also known as sequential gradient descent , as follows. If\n",
      "the error function comprises a sum over data points E=∑\n",
      "nEn, then after presen-\n",
      "tation of pattern n, the stochastic gradient descent algorithm updates the parameter\n",
      "vectorwusing\n",
      "w(τ+1)=w(τ)−η∇En (3.22)\n",
      "where τdenotes the iteration number, and ηis a learning rate parameter. We shall\n",
      "discuss the choice of value for ηshortly. The value of wis initialized to some starting\n",
      "vectorw(0). For the case of the sum-of-squares error function (3.12), this gives\n",
      "w(τ+1)=w(τ)+η(tn−w(τ)Tφn)φn (3.23)\n",
      "where φn=φ(xn). This is known as least-mean-squares or the LMS algorithm .\n",
      "The value of ηneeds to be chosen with care to ensure that the algorithm converges\n",
      "(Bishop and Nabney, 2008).\n",
      "3.1.4 Regularized least squares\n",
      "In Section 1.1, we introduced the idea of adding a regularization term to an\n",
      "error function in order to control over-ﬁtting, so that the total error function to be\n",
      "minimized takes the form\n",
      "ED(w)+λEW(w) (3.24)\n",
      "where λis the regularization coefﬁcient that controls the relative importance of the\n",
      "data-dependent error ED(w)and the regularization term EW(w). One of the sim-\n",
      "plest forms of regularizer is given by the sum-of-squares of the weight vector ele-\n",
      "ments\n",
      "EW(w)=1\n",
      "2wTw. (3.25)\n",
      "If we also consider the sum-of-squares error function given by\n",
      "E(w)=1\n",
      "2N∑\n",
      "n=1{tn−wTφ(xn)}2(3.26)\n",
      "then the total error function becomes\n",
      "1\n",
      "2N∑\n",
      "n=1{tn−wTφ(xn)}2+λ\n",
      "2wTw. (3.27)\n",
      "This particular choice of regularizer is known in the machine learning literature as\n",
      "weight decay because in sequential learning algorithms, it encourages weight values\n",
      "to decay towards zero, unless supported by the data. In statistics, it provides an ex-\n",
      "ample of a parameter shrinkage method because it shrinks parameter values towards3.1. Linear Basis Function Models 145\n",
      "q=0.5 q=1 q=2 q=4\n",
      "Figure 3.3 Contours of the regularization term in (3.29) for various values of the parameter q.\n",
      "zero. It has the advantage that the error function remains a quadratic function of\n",
      "w, and so its exact minimizer can be found in closed form. Speciﬁcally, setting the\n",
      "gradient of (3.27) with respect to wto zero, and solving for was before, we obtain\n",
      "w=(\n",
      "λI+ΦTΦ)−1ΦTt. (3.28)\n",
      "This represents a simple extension of the least-squares solution (3.15).\n",
      "A more general regularizer is sometimes used, for which the regularized error\n",
      "takes the form\n",
      "1\n",
      "2N∑\n",
      "n=1{tn−wTφ(xn)}2+λ\n",
      "2M∑\n",
      "j=1|wj|q(3.29)\n",
      "where q=2corresponds to the quadratic regularizer (3.27). Figure 3.3 shows con-\n",
      "tours of the regularization function for different values of q.\n",
      "The case of q=1 is know as the lasso in the statistics literature (Tibshirani,\n",
      "1996). It has the property that if λis sufﬁciently large, some of the coefﬁcients\n",
      "wjare driven to zero, leading to a sparse model in which the corresponding basis\n",
      "functions play no role. To see this, we ﬁrst note that minimizing (3.29) is equivalent\n",
      "to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint Exercise 3.5\n",
      "M∑\n",
      "j=1|wj|q⩽η (3.30)\n",
      "for an appropriate value of the parameter η, where the two approaches can be related\n",
      "using Lagrange multipliers. The origin of the sparsity can be seen from Figure 3.4, Appendix E\n",
      "which shows that the minimum of the error function, subject to the constraint (3.30).\n",
      "Asλis increased, so an increasing number of parameters are driven to zero.\n",
      "Regularization allows complex models to be trained on data sets of limited size\n",
      "without severe over-ﬁtting, essentially by limiting the effective model complexity.\n",
      "However, the problem of determining the optimal model complexity is then shifted\n",
      "from one of ﬁnding the appropriate number of basis functions to one of determining\n",
      "a suitable value of the regularization coefﬁcient λ. We shall return to the issue of\n",
      "model complexity later in this chapter.146 3. LINEAR MODELS FOR REGRESSION\n",
      "Figure 3.4 Plot of the contours\n",
      "of the unregularized error function\n",
      "(blue) along with the constraint re-\n",
      "gion (3.30) for the quadratic regular-\n",
      "izerq=2on the left and the lasso\n",
      "regularizer q=1 on the right, in\n",
      "which the optimum value for the pa-\n",
      "rameter vector wis denoted by w⋆.\n",
      "The lasso gives a sparse solution in\n",
      "which w⋆\n",
      "1=0.\n",
      "w1w2\n",
      "w⋆\n",
      "w1w2\n",
      "w⋆\n",
      "For the remainder of this chapter we shall focus on the quadratic regularizer\n",
      "(3.27) both for its practical importance and its analytical tractability.\n",
      "3.1.5 Multiple outputs\n",
      "So far, we have considered the case of a single target variable t. In some applica-\n",
      "tions, we may wish to predict K>1target variables, which we denote collectively\n",
      "by the target vector t. This could be done by introducing a different set of basis func-\n",
      "tions for each component of t, leading to multiple, independent regression problems.\n",
      "However, a more interesting, and more common, approach is to use the same set of\n",
      "basis functions to model all of the components of the target vector so that\n",
      "y(x,w)=WTφ(x) (3.31)\n",
      "whereyis aK-dimensional column vector, Wis anM×Kmatrix of parameters,\n",
      "andφ(x)is anM-dimensional column vector with elements φj(x), with φ0(x)=1\n",
      "as before. Suppose we take the conditional distribution of the target vector to be an\n",
      "isotropic Gaussian of the form\n",
      "p(t|x,W,β)=N(t|WTφ(x),β−1I). (3.32)\n",
      "If we have a set of observations t1,...,tN, we can combine these into a matrix T\n",
      "of size N×Ksuch that the nthrow is given by tT\n",
      "n. Similarly, we can combine the\n",
      "input vectors x1,...,xNinto a matrix X. The log likelihood function is then given\n",
      "by\n",
      "lnp(T|X,W,β)=N∑\n",
      "n=1lnN(tn|WTφ(xn),β−1I)\n",
      "=NK\n",
      "2ln(β\n",
      "2π)\n",
      "−β\n",
      "2N∑\n",
      "n=1tn−WTφ(xn)2.(3.33)3.2. The Bias-Variance Decomposition 147\n",
      "As before, we can maximize this function with respect to W, giving\n",
      "WML=(\n",
      "ΦTΦ)−1ΦTT. (3.34)\n",
      "If we examine this result for each target variable tk,w eh a v e\n",
      "wk=(\n",
      "ΦTΦ)−1ΦTtk=Φ†tk (3.35)\n",
      "where tkis anN-dimensional column vector with components tnkforn=1,...N .\n",
      "Thus the solution to the regression problem decouples between the different target\n",
      "variables, and we need only compute a single pseudo-inverse matrix Φ†, which is\n",
      "shared by all of the vectors wk.\n",
      "The extension to general Gaussian noise distributions having arbitrary covari-\n",
      "ance matrices is straightforward. Again, this leads to a decoupling into Kinde- Exercise 3.6\n",
      "pendent regression problems. This result is unsurprising because the parameters W\n",
      "deﬁne only the mean of the Gaussian noise distribution, and we know from Sec-\n",
      "tion 2.3.4 that the maximum likelihood solution for the mean of a multivariate Gaus-\n",
      "sian is independent of the covariance. From now on, we shall therefore consider asingle target variable tfor simplicity.\n",
      "3.2. The Bias-Variance Decomposition\n",
      "So far in our discussion of linear models for regression, we have assumed that the\n",
      "form and number of basis functions are both ﬁxed. As we have seen in Chapter 1,\n",
      "the use of maximum likelihood, or equivalently least squares, can lead to severe\n",
      "over-ﬁtting if complex models are trained using data sets of limited size. However,limiting the number of basis functions in order to avoid over-ﬁtting has the side\n",
      "effect of limiting the ﬂexibility of the model to capture interesting and important\n",
      "trends in the data. Although the introduction of regularization terms can controlover-ﬁtting for models with many parameters, this raises the question of how to\n",
      "determine a suitable value for the regularization coefﬁcient λ. Seeking the solution\n",
      "that minimizes the regularized error function with respect to both the weight vectorwand the regularization coefﬁcient λis clearly not the right approach since this\n",
      "leads to the unregularized solution with λ=0.\n",
      "As we have seen in earlier chapters, the phenomenon of over-ﬁtting is really an\n",
      "unfortunate property of maximum likelihood and does not arise when we marginalize\n",
      "over parameters in a Bayesian setting. In this chapter, we shall consider the Bayesian\n",
      "view of model complexity in some depth. Before doing so, however, it is instructive\n",
      "to consider a frequentist viewpoint of the model complexity issue, known as the bias-\n",
      "variance trade-off. Although we shall introduce this concept in the context of linear\n",
      "basis function models, where it is easy to illustrate the ideas using simple examples,\n",
      "the discussion has more general applicability.\n",
      "In Section 1.5.5, when we discussed decision theory for regression problems,\n",
      "we considered various loss functions each of which leads to a corresponding optimal\n",
      "prediction once we are given the conditional distribution p(t|x). A popular choice is148 3. LINEAR MODELS FOR REGRESSION\n",
      "the squared loss function, for which the optimal prediction is given by the conditional\n",
      "expectation, which we denote by h(x)and which is given by\n",
      "h(x)= E[t|x]=∫\n",
      "tp(t|x)dt. (3.36)\n",
      "At this point, it is worth distinguishing between the squared loss function arising\n",
      "from decision theory and the sum-of-squares error function that arose in the maxi-\n",
      "mum likelihood estimation of model parameters. We might use more sophisticatedtechniques than least squares, for example regularization or a fully Bayesian ap-\n",
      "proach, to determine the conditional distribution p(t|x). These can all be combined\n",
      "with the squared loss function for the purpose of making predictions.\n",
      "We showed in Section 1.5.5 that the expected squared loss can be written in the\n",
      "form\n",
      "E[L]=∫\n",
      "{y(x)−h(x)}2p(x)dx+∫\n",
      "{h(x)−t}2p(x,t)dxdt. (3.37)\n",
      "Recall that the second term, which is independent of y(x), arises from the intrinsic\n",
      "noise on the data and represents the minimum achievable value of the expected loss.The ﬁrst term depends on our choice for the function y(x), and we will seek a so-\n",
      "lution for y(x)which makes this term a minimum. Because it is nonnegative, the\n",
      "smallest that we can hope to make this term is zero. If we had an unlimited supply ofdata (and unlimited computational resources), we could in principle ﬁnd the regres-\n",
      "sion function h(x)to any desired degree of accuracy, and this would represent the\n",
      "optimal choice for y(x). However, in practice we have a data set Dcontaining only\n",
      "a ﬁnite number Nof data points, and consequently we do not know the regression\n",
      "function h(x)exactly.\n",
      "If we model the h(x)using a parametric function y(x,w)governed by a pa-\n",
      "rameter vector w, then from a Bayesian perspective the uncertainty in our model is\n",
      "expressed through a posterior distribution over w. A frequentist treatment, however,\n",
      "involves making a point estimate of wbased on the data set D, and tries instead\n",
      "to interpret the uncertainty of this estimate through the following thought experi-\n",
      "ment. Suppose we had a large number of data sets each of size Nand each drawn\n",
      "independently from the distribution p(t,x). For any given data set D, we can run\n",
      "our learning algorithm and obtain a prediction function y(x;D). Different data sets\n",
      "from the ensemble will give different functions and consequently different values ofthe squared loss. The performance of a particular learning algorithm is then assessed\n",
      "by taking the average over this ensemble of data sets.\n",
      "Consider the integrand of the ﬁrst term in (3.37), which for a particular data set\n",
      "Dtakes the form\n",
      "{y(x;D)−h(x)}2. (3.38)\n",
      "Because this quantity will be dependent on the particular data set D, we take its aver-\n",
      "age over the ensemble of data sets. If we add and subtract the quantity ED[y(x;D)]3.2. The Bias-Variance Decomposition 149\n",
      "inside the braces, and then expand, we obtain\n",
      "{y(x;D)−ED[y(x;D)] + ED[y(x;D)]−h(x)}2\n",
      "={y(x;D)−ED[y(x;D)]}2+{ED[y(x;D)]−h(x)}2\n",
      "+2{y(x;D)−ED[y(x;D)]}{ED[y(x;D)]−h(x)}. (3.39)\n",
      "We now take the expectation of this expression with respect to Dand note that the\n",
      "ﬁnal term will vanish, giving\n",
      "ED[\n",
      "{y(x;D)−h(x)}2]\n",
      "={ED[y(x;D)]−h(x)}2\n",
      " \n",
      "(bias)2+ED[\n",
      "{y(x;D)−ED[y(x;D)]}2]\n",
      "  \n",
      "variance.(3.40)\n",
      "We see that the expected squared difference between y(x;D)and the regression\n",
      "function h(x)can be expressed as the sum of two terms. The ﬁrst term, called the\n",
      "squared bias, represents the extent to which the average prediction over all data sets\n",
      "differs from the desired regression function. The second term, called the variance ,\n",
      "measures the extent to which the solutions for individual data sets vary around their\n",
      "average, and hence this measures the extent to which the function y(x;D)is sensitive\n",
      "to the particular choice of data set. We shall provide some intuition to support these\n",
      "deﬁnitions shortly when we consider a simple example.\n",
      "So far, we have considered a single input value x. If we substitute this expansion\n",
      "back into (3.37), we obtain the following decomposition of the expected squared loss\n",
      "expected loss =(bias)2+variance +noise (3.41)\n",
      "where\n",
      "(bias)2=∫\n",
      "{ED[y(x;D)]−h(x)}2p(x)dx (3.42)\n",
      "variance =∫\n",
      "ED[\n",
      "{y(x;D)−ED[y(x;D)]}2]\n",
      "p(x)dx (3.43)\n",
      "noise =∫\n",
      "{h(x)−t}2p(x,t)dxdt (3.44)\n",
      "and the bias and variance terms now refer to integrated quantities.\n",
      "Our goal is to minimize the expected loss, which we have decomposed into the\n",
      "sum of a (squared) bias, a variance, and a constant noise term. As we shall see, there\n",
      "is a trade-off between bias and variance, with very ﬂexible models having low bias\n",
      "and high variance, and relatively rigid models having high bias and low variance.The model with the optimal predictive capability is the one that leads to the best\n",
      "balance between bias and variance. This is illustrated by considering the sinusoidal\n",
      "data set from Chapter 1. Here we generate 100 data sets, each containing N=2 5 Appendix A\n",
      "data points, independently from the sinusoidal curve h(x)=s i n ( 2 πx). The data\n",
      "sets are indexed by l=1,...,L , where L= 100 , and for each data set D\n",
      "(l)we150 3. LINEAR MODELS FOR REGRESSION\n",
      "xtlnλ=2.6\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "xtlnλ=−0.31\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "xtlnλ=−2.4\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "Figure 3.5 Illustration of the dependence of bias and variance on model complexity, governed by a regulariza-\n",
      "tion parameter λ, using the sinusoidal data set from Chapter 1. There are L= 100 data sets, each having N=2 5\n",
      "data points, and there are 24Gaussian basis functions in the model so that the total number of parameters is\n",
      "M=2 5 including the bias parameter. The left column shows the result of ﬁtting the model to the data sets for\n",
      "various values of lnλ(for clarity, only 20 of the 100 ﬁts are shown). The right column shows the corresponding\n",
      "average of the 100 ﬁts (red) along with the sinusoidal function from which the data sets were generated (green).3.2. The Bias-Variance Decomposition 151\n",
      "Figure 3.6 Plot of squared bias and variance,\n",
      "together with their sum, correspond-\n",
      "ing to the results shown in Fig-\n",
      "ure 3.5. Also shown is the average\n",
      "test set error for a test data set size\n",
      "of 1000 points. The minimum value\n",
      "of(bias)2+ variance occurs around\n",
      "lnλ=−0.31, which is close to the\n",
      "value that gives the minimum error\n",
      "on the test data.\n",
      "lnλ−3 −2 −1 0 1 200.030.060.090.120.15\n",
      "(bias)2\n",
      "variance\n",
      "(bias)2 + variance\n",
      "test error\n",
      "ﬁt a model with 24Gaussian basis functions by minimizing the regularized error\n",
      "function (3.27) to give a prediction function y(l)(x)as shown in Figure 3.5. The\n",
      "top row corresponds to a large value of the regularization coefﬁcient λthat gives low\n",
      "variance (because the red curves in the left plot look similar) but high bias (because\n",
      "the two curves in the right plot are very different). Conversely on the bottom row, for\n",
      "which λis small, there is large variance (shown by the high variability between the\n",
      "red curves in the left plot) but low bias (shown by the good ﬁt between the average\n",
      "model ﬁt and the original sinusoidal function). Note that the result of averaging many\n",
      "solutions for the complex model with M=2 5 is a very good ﬁt to the regression\n",
      "function, which suggests that averaging may be a beneﬁcial procedure. Indeed, a\n",
      "weighted averaging of multiple solutions lies at the heart of a Bayesian approach,\n",
      "although the averaging is with respect to the posterior distribution of parameters, not\n",
      "with respect to multiple data sets.\n",
      "We can also examine the bias-variance trade-off quantitatively for this example.\n",
      "The average prediction is estimated from\n",
      "y(x)=1\n",
      "LL∑\n",
      "l=1y(l)(x) (3.45)\n",
      "and the integrated squared bias and integrated variance are then given by\n",
      "(bias)2=1\n",
      "NN∑\n",
      "n=1{y(xn)−h(xn)}2(3.46)\n",
      "variance =1\n",
      "NN∑\n",
      "n=11\n",
      "LL∑\n",
      "l=1{\n",
      "y(l)(xn)−y(xn)}2(3.47)\n",
      "where the integral over xweighted by the distribution p(x)is approximated by a\n",
      "ﬁnite sum over data points drawn from that distribution. These quantities, along\n",
      "with their sum, are plotted as a function of lnλin Figure 3.6. We see that small\n",
      "values of λallow the model to become ﬁnely tuned to the noise on each individual152 3. LINEAR MODELS FOR REGRESSION\n",
      "data set leading to large variance. Conversely, a large value of λpulls the weight\n",
      "parameters towards zero leading to large bias.\n",
      "Although the bias-variance decomposition may provide some interesting in-\n",
      "sights into the model complexity issue from a frequentist perspective, it is of lim-\n",
      "ited practical value, because the bias-variance decomposition is based on averageswith respect to ensembles of data sets, whereas in practice we have only the single\n",
      "observed data set. If we had a large number of independent training sets of a given\n",
      "size, we would be better off combining them into a single large training set, whichof course would reduce the level of over-ﬁtting for a given model complexity.\n",
      "Given these limitations, we turn in the next section to a Bayesian treatment of\n",
      "linear basis function models, which not only provides powerful insights into the\n",
      "issues of over-ﬁtting but which also leads to practical techniques for addressing the\n",
      "question model complexity.\n",
      "3.3. Bayesian Linear Regression\n",
      "In our discussion of maximum likelihood for setting the parameters of a linear re-\n",
      "gression model, we have seen that the effective model complexity, governed by the\n",
      "number of basis functions, needs to be controlled according to the size of the data\n",
      "set. Adding a regularization term to the log likelihood function means the effective\n",
      "model complexity can then be controlled by the value of the regularization coefﬁ-cient, although the choice of the number and form of the basis functions is of course\n",
      "still important in determining the overall behaviour of the model.\n",
      "This leaves the issue of deciding the appropriate model complexity for the par-\n",
      "ticular problem, which cannot be decided simply by maximizing the likelihood func-\n",
      "tion, because this always leads to excessively complex models and over-ﬁtting. In-\n",
      "dependent hold-out data can be used to determine model complexity, as discussedin Section 1.3, but this can be both computationally expensive and wasteful of valu-\n",
      "able data. We therefore turn to a Bayesian treatment of linear regression, which will\n",
      "avoid the over-ﬁtting problem of maximum likelihood, and which will also lead toautomatic methods of determining model complexity using the training data alone.\n",
      "Again, for simplicity we will focus on the case of a single target variable t. Ex-\n",
      "tension to multiple target variables is straightforward and follows the discussion ofSection 3.1.5.\n",
      "3.3.1 Parameter distribution\n",
      "We begin our discussion of the Bayesian treatment of linear regression by in-\n",
      "troducing a prior probability distribution over the model parameters w. For the mo-\n",
      "ment, we shall treat the noise precision parameter βas a known constant. First note\n",
      "that the likelihood function p(t|w)deﬁned by (3.10) is the exponential of a quadratic\n",
      "function of w. The corresponding conjugate prior is therefore given by a Gaussian\n",
      "distribution of the form\n",
      "p(w)=N(w|m0,S0) (3.48)\n",
      "having mean m0and covariance S0.3.3. Bayesian Linear Regression 153\n",
      "Next we compute the posterior distribution, which is proportional to the product\n",
      "of the likelihood function and the prior. Due to the choice of a conjugate Gaus-sian prior distribution, the posterior will also be Gaussian. We can evaluate this\n",
      "distribution by the usual procedure of completing the square in the exponential, and\n",
      "then ﬁnding the normalization coefﬁcient using the standard result for a normalizedGaussian. However, we have already done the necessary work in deriving the gen- Exercise 3.7\n",
      "eral result (2.116), which allows us to write down the posterior distribution directly\n",
      "in the form\n",
      "p(w|t)=N(w|m\n",
      "N,SN) (3.49)\n",
      "where\n",
      "mN=SN(\n",
      "S−1\n",
      "0m0+βΦTt)\n",
      "(3.50)\n",
      "S−1\n",
      "N=S−1\n",
      "0+βΦTΦ. (3.51)\n",
      "Note that because the posterior distribution is Gaussian, its mode coincides with its\n",
      "mean. Thus the maximum posterior weight vector is simply given by wMAP=mN.\n",
      "If we consider an inﬁnitely broad prior S0=α−1Iwithα→0, the mean mN\n",
      "of the posterior distribution reduces to the maximum likelihood value wMLgiven\n",
      "by (3.15). Similarly, if N=0, then the posterior distribution reverts to the prior.\n",
      "Furthermore, if data points arrive sequentially, then the posterior distribution at any\n",
      "stage acts as the prior distribution for the subsequent data point, such that the newposterior distribution is again given by (3.49). Exercise 3.8\n",
      "For the remainder of this chapter, we shall consider a particular form of Gaus-\n",
      "sian prior in order to simplify the treatment. Speciﬁcally, we consider a zero-meanisotropic Gaussian governed by a single precision parameter αso that\n",
      "p(w|α)=N(w|0,α\n",
      "−1I) (3.52)\n",
      "and the corresponding posterior distribution over wis then given by (3.49) with\n",
      "mN=βSNΦTt (3.53)\n",
      "S−1\n",
      "N=αI+βΦTΦ. (3.54)\n",
      "The log of the posterior distribution is given by the sum of the log likelihood and\n",
      "the log of the prior and, as a function of w, takes the form\n",
      "lnp(w|t)=−β\n",
      "2N∑\n",
      "n=1{tn−wTφ(xn)}2−α\n",
      "2wTw+c o n s t . (3.55)\n",
      "Maximization of this posterior distribution with respect to wis therefore equiva-\n",
      "lent to the minimization of the sum-of-squares error function with the addition of a\n",
      "quadratic regularization term, corresponding to (3.27) with λ=α/β.\n",
      "We can illustrate Bayesian learning in a linear basis function model, as well as\n",
      "the sequential update of a posterior distribution, using a simple example involving\n",
      "straight-line ﬁtting. Consider a single input variable x, a single target variable tand154 3. LINEAR MODELS FOR REGRESSION\n",
      "a linear model of the form y(x,w)=w0+w1x. Because this has just two adap-\n",
      "tive parameters, we can plot the prior and posterior distributions directly in parameterspace. We generate synthetic data from the function f(x,a)=a\n",
      "0+a1xwith param-\n",
      "eter values a0=−0.3anda1=0.5by ﬁrst choosing values of xnfrom the uniform\n",
      "distribution U(x|−1,1), then evaluating f(xn,a), and ﬁnally adding Gaussian noise\n",
      "with standard deviation of 0.2to obtain the target values tn. Our goal is to recover\n",
      "the values of a0anda1from such data, and we will explore the dependence on the\n",
      "size of the data set. We assume here that the noise variance is known and hence weset the precision parameter to its true value β=( 1/0.2)\n",
      "2=2 5 . Similarly, we ﬁx\n",
      "the parameter αto2.0. We shall shortly discuss strategies for determining αand\n",
      "βfrom the training data. Figure 3.7 shows the results of Bayesian learning in this\n",
      "model as the size of the data set is increased and demonstrates the sequential nature\n",
      "of Bayesian learning in which the current posterior distribution forms the prior whena new data point is observed. It is worth taking time to study this ﬁgure in detail as\n",
      "it illustrates several important aspects of Bayesian inference. The ﬁrst row of this\n",
      "ﬁgure corresponds to the situation before any data points are observed and shows aplot of the prior distribution in wspace together with six samples of the function\n",
      "y(x,w)in which the values of ware drawn from the prior. In the second row, we\n",
      "see the situation after observing a single data point. The location (x, t)of the data\n",
      "point is shown by a blue circle in the right-hand column. In the left-hand column is a\n",
      "plot of the likelihood function p(t|x,w)for this data point as a function of w. Note\n",
      "that the likelihood function provides a soft constraint that the line must pass close tothe data point, where close is determined by the noise precision β. For comparison,\n",
      "the true parameter values a\n",
      "0=−0.3anda1=0.5used to generate the data set\n",
      "are shown by a white cross in the plots in the left column of Figure 3.7. When wemultiply this likelihood function by the prior from the top row, and normalize, we\n",
      "obtain the posterior distribution shown in the middle plot on the second row. Sam-\n",
      "ples of the regression function y(x,w)obtained by drawing samples of wfrom this\n",
      "posterior distribution are shown in the right-hand plot. Note that these sample lines\n",
      "all pass close to the data point. The third row of this ﬁgure shows the effect of ob-serving a second data point, again shown by a blue circle in the plot in the right-hand\n",
      "column. The corresponding likelihood function for this second data point alone is\n",
      "shown in the left plot. When we multiply this likelihood function by the posteriordistribution from the second row, we obtain the posterior distribution shown in the\n",
      "middle plot of the third row. Note that this is exactly the same posterior distribution\n",
      "as would be obtained by combining the original prior with the likelihood functionfor the two data points. This posterior has now been inﬂuenced by two data points,\n",
      "and because two points are sufﬁcient to deﬁne a line this already gives a relatively\n",
      "compact posterior distribution. Samples from this posterior distribution give rise tothe functions shown in red in the third column, and we see that these functions pass\n",
      "close to both of the data points. The fourth row shows the effect of observing a total\n",
      "of20data points. The left-hand plot shows the likelihood function for the 20\n",
      "thdata\n",
      "point alone, and the middle plot shows the resulting posterior distribution that has\n",
      "now absorbed information from all 20observations. Note how the posterior is much\n",
      "sharper than in the third row. In the limit of an inﬁnite number of data points, the3.3. Bayesian Linear Regression 155\n",
      "Figure 3.7 Illustration of sequential Bayesian learning for a simple linear model of the form y(x,w)=\n",
      "w0+w1x. A detailed description of this ﬁgure is given in the text.156 3. LINEAR MODELS FOR REGRESSION\n",
      "posterior distribution would become a delta function centred on the true parameter\n",
      "values, shown by the white cross.\n",
      "Other forms of prior over the parameters can be considered. For instance, we\n",
      "can generalize the Gaussian prior to give\n",
      "p(w|α)=[q\n",
      "2(α\n",
      "2)1/q1\n",
      "Γ(1/q)]M\n",
      "exp(\n",
      "−α\n",
      "2M∑\n",
      "j=1|wj|q)\n",
      "(3.56)\n",
      "in which q=2corresponds to the Gaussian distribution, and only in this case is the\n",
      "prior conjugate to the likelihood function (3.10). Finding the maximum of the poste-\n",
      "rior distribution over wcorresponds to minimization of the regularized error function\n",
      "(3.29). In the case of the Gaussian prior, the mode of the posterior distribution was\n",
      "equal to the mean, although this will no longer hold if q̸=2.\n",
      "3.3.2 Predictive distribution\n",
      "In practice, we are not usually interested in the value of witself but rather in\n",
      "making predictions of tfor new values of x. This requires that we evaluate the\n",
      "predictive distribution deﬁned by\n",
      "p(t|t,α,β)=∫\n",
      "p(t|w,β)p(w|t,α,β)dw (3.57)\n",
      "in which tis the vector of target values from the training set, and we have omitted the\n",
      "corresponding input vectors from the right-hand side of the conditioning statements\n",
      "to simplify the notation. The conditional distribution p(t|x,w,β)of the target vari-\n",
      "able is given by (3.8), and the posterior weight distribution is given by (3.49). We\n",
      "see that (3.57) involves the convolution of two Gaussian distributions, and so making\n",
      "use of the result (2.115) from Section 8.1.4, we see that the predictive distributiontakes the form Exercise 3.10\n",
      "p(t|x,t,α,β)=N(t|m\n",
      "T\n",
      "Nφ(x),σ2\n",
      "N(x)) (3.58)\n",
      "where the variance σ2\n",
      "N(x)of the predictive distribution is given by\n",
      "σ2\n",
      "N(x)=1\n",
      "β+φ(x)TSNφ(x). (3.59)\n",
      "The ﬁrst term in (3.59) represents the noise on the data whereas the second term\n",
      "reﬂects the uncertainty associated with the parameters w. Because the noise process\n",
      "and the distribution of ware independent Gaussians, their variances are additive.\n",
      "Note that, as additional data points are observed, the posterior distribution becomesnarrower. As a consequence it can be shown (Qazaz et al. , 1997) that σ\n",
      "2\n",
      "N+1(x)⩽\n",
      "σ2\n",
      "N(x). In the limit N→∞ , the second term in (3.59) goes to zero, and the variance Exercise 3.11\n",
      "of the predictive distribution arises solely from the additive noise governed by theparameter β.\n",
      "As an illustration of the predictive distribution for Bayesian linear regression\n",
      "models, let us return to the synthetic sinusoidal data set of Section 1.1. In Figure 3.8,3.3. Bayesian Linear Regression 157\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "Figure 3.8 Examples of the predictive distribution (3.58) for a model consisting of 9Gaussian basis functions\n",
      "of the form (3.4) using the synthetic sinusoidal data set of Section 1.1. See the text for a detailed discussion.\n",
      "we ﬁt a model comprising a linear combination of Gaussian basis functions to data\n",
      "sets of various sizes and then look at the corresponding posterior distributions. Here\n",
      "the green curves correspond to the function sin(2πx)from which the data points\n",
      "were generated (with the addition of Gaussian noise). Data sets of size N=1,\n",
      "N=2,N=4, andN=2 5 are shown in the four plots by the blue circles. For\n",
      "each plot, the red curve shows the mean of the corresponding Gaussian predictive\n",
      "distribution, and the red shaded region spans one standard deviation either side of\n",
      "the mean. Note that the predictive uncertainty depends on xand is smallest in the\n",
      "neighbourhood of the data points. Also note that the level of uncertainty decreases\n",
      "as more data points are observed.\n",
      "The plots in Figure 3.8 only show the point-wise predictive variance as a func-\n",
      "tion of x. In order to gain insight into the covariance between the predictions at\n",
      "different values of x, we can draw samples from the posterior distribution over w,\n",
      "and then plot the corresponding functions y(x,w), as shown in Figure 3.9.158 3. LINEAR MODELS FOR REGRESSION\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "Figure 3.9 Plots of the function y(x,w)using samples from the posterior distributions over wcorresponding to\n",
      "the plots in Figure 3.8.\n",
      "If we used localized basis functions such as Gaussians, then in regions away\n",
      "from the basis function centres, the contribution from the second term in the predic-\n",
      "tive variance (3.59) will go to zero, leaving only the noise contribution β−1. Thus,\n",
      "the model becomes very conﬁdent in its predictions when extrapolating outside the\n",
      "region occupied by the basis functions, which is generally an undesirable behaviour.\n",
      "This problem can be avoided by adopting an alternative Bayesian approach to re-\n",
      "gression known as a Gaussian process. Section 6.4\n",
      "Note that, if both wandβare treated as unknown, then we can introduce a\n",
      "conjugate prior distribution p(w,β)that, from the discussion in Section 2.3.6, will\n",
      "be given by a Gaussian-gamma distribution (Denison et al. , 2002). In this case, the Exercise 3.12\n",
      "predictive distribution is a Student’s t-distribution. Exercise 3.133.3. Bayesian Linear Regression 159\n",
      "Figure 3.10 The equivalent ker-\n",
      "nelk(x, x′)for the Gaussian basis\n",
      "functions in Figure 3.1, shown as\n",
      "a plot of xversus x′, together with\n",
      "three slices through this matrix cor-\n",
      "responding to three different valuesofx. The data set used to generate\n",
      "this kernel comprised 200 values of\n",
      "xequally spaced over the interval\n",
      "(−1,1).\n",
      "3.3.3 Equivalent kernel\n",
      "The posterior mean solution (3.53) for the linear basis function model has an in-\n",
      "teresting interpretation that will set the stage for kernel methods, including Gaussian\n",
      "processes. If we substitute (3.53) into the expression (3.3), we see that the predictive Chapter 6\n",
      "mean can be written in the form\n",
      "y(x,mN)=mT\n",
      "Nφ(x)=βφ(x)TSNΦTt=N∑\n",
      "n=1βφ(x)TSNφ(xn)tn (3.60)\n",
      "whereSNis deﬁned by (3.51). Thus the mean of the predictive distribution at a point\n",
      "xis given by a linear combination of the training set target variables tn, so that we\n",
      "can write\n",
      "y(x,mN)=N∑\n",
      "n=1k(x,xn)tn (3.61)\n",
      "where the function\n",
      "k(x,x′)=βφ(x)TSNφ(x′) (3.62)\n",
      "is known as the smoother matrix or the equivalent kernel . Regression functions, such\n",
      "as this, which make predictions by taking linear combinations of the training set\n",
      "target values are known as linear smoothers . Note that the equivalent kernel depends\n",
      "on the input values xnfrom the data set because these appear in the deﬁnition of\n",
      "SN. The equivalent kernel is illustrated for the case of Gaussian basis functions in\n",
      "Figure 3.10 in which the kernel functions k(x, x′)have been plotted as a function of\n",
      "x′for three different values of x. We see that they are localized around x, and so the\n",
      "mean of the predictive distribution at x, given by y(x,mN), is obtained by forming\n",
      "a weighted combination of the target values in which data points close to xare given\n",
      "higher weight than points further removed from x. Intuitively, it seems reasonable\n",
      "that we should weight local evidence more strongly than distant evidence. Note thatthis localization property holds not only for the localized Gaussian basis functions\n",
      "but also for the nonlocal polynomial and sigmoidal basis functions, as illustrated in\n",
      "Figure 3.11.\n",
      "160 3. LINEAR MODELS FOR REGRESSION\n",
      "Figure 3.11 Examples of equiva-\n",
      "lent kernels k(x, x′)forx=0\n",
      "plotted as a function of x′, corre-\n",
      "sponding (left) to the polynomial ba-\n",
      "sis functions and (right) to the sig-\n",
      "moidal basis functions shown in Fig-\n",
      "ure 3.1. Note that these are local-\n",
      "ized functions of x′even though the\n",
      "corresponding basis functions are\n",
      "nonlocal. −1 0 100.020.04\n",
      "−1 0 100.020.04\n",
      "Further insight into the role of the equivalent kernel can be obtained by consid-\n",
      "ering the covariance between y(x)andy(x′), which is given by\n",
      "cov[y(x),y(x′) ]=c o v [ φ(x)Tw,wTφ(x′)]\n",
      "=φ(x)TSNφ(x′)=β−1k(x,x′) (3.63)\n",
      "where we have made use of (3.49) and (3.62). From the form of the equivalent\n",
      "kernel, we see that the predictive mean at nearby points will be highly correlated,\n",
      "whereas for more distant pairs of points the correlation will be smaller.\n",
      "The predictive distribution shown in Figure 3.8 allows us to visualize the point-\n",
      "wise uncertainty in the predictions, governed by (3.59). However, by drawing sam-\n",
      "ples from the posterior distribution over w, and plotting the corresponding model\n",
      "functions y(x,w)as in Figure 3.9, we are visualizing the joint uncertainty in the\n",
      "posterior distribution between the yvalues at two (or more) xvalues, as governed by\n",
      "the equivalent kernel.\n",
      "The formulation of linear regression in terms of a kernel function suggests an\n",
      "alternative approach to regression as follows. Instead of introducing a set of basis\n",
      "functions, which implicitly determines an equivalent kernel, we can instead deﬁne\n",
      "a localized kernel directly and use this to make predictions for new input vectors x,\n",
      "given the observed training set. This leads to a practical framework for regression\n",
      "(and classiﬁcation) called Gaussian processes , which will be discussed in detail in\n",
      "Section 6.4.\n",
      "We have seen that the effective kernel deﬁnes the weights by which the training\n",
      "set target values are combined in order to make a prediction at a new value of x, and\n",
      "it can be shown that these weights sum to one, in other words\n",
      "N∑\n",
      "n=1k(x,xn)=1 (3.64)\n",
      "for all values of x. This intuitively pleasing result can easily be proven informally Exercise 3.14\n",
      "by noting that the summation is equivalent to considering the predictive mean ˆy(x)\n",
      "for a set of target data in which tn=1 for all n. Provided the basis functions are\n",
      "linearly independent, that there are more data points than basis functions, and that\n",
      "one of the basis functions is constant (corresponding to the bias parameter), then it is\n",
      "clear that we can ﬁt the training data exactly and hence that the predictive mean will3.4. Bayesian Model Comparison 161\n",
      "be simplyˆy(x)=1 , from which we obtain (3.64). Note that the kernel function can\n",
      "be negative as well as positive, so although it satisﬁes a summation constraint, thecorresponding predictions are not necessarily convex combinations of the training\n",
      "set target variables.\n",
      "Finally, we note that the equivalent kernel (3.62) satisﬁes an important property\n",
      "shared by kernel functions in general, namely that it can be expressed in the form an Chapter 6\n",
      "inner product with respect to a vector ψ(x)of nonlinear functions, so that\n",
      "k(x,z)=ψ(x)\n",
      "Tψ(z) (3.65)\n",
      "where ψ(x)=β1/2S1/2\n",
      "Nφ(x).\n",
      "3.4. Bayesian Model Comparison\n",
      "In Chapter 1, we highlighted the problem of over-ﬁtting as well as the use of cross-\n",
      "validation as a technique for setting the values of regularization parameters or for\n",
      "choosing between alternative models. Here we consider the problem of model se-lection from a Bayesian perspective. In this section, our discussion will be very\n",
      "general, and then in Section 3.5 we shall see how these ideas can be applied to the\n",
      "determination of regularization parameters in linear regression.\n",
      "As we shall see, the over-ﬁtting associated with maximum likelihood can be\n",
      "avoided by marginalizing (summing or integrating) over the model parameters in-\n",
      "stead of making point estimates of their values. Models can then be compared di-rectly on the training data, without the need for a validation set. This allows all\n",
      "available data to be used for training and avoids the multiple training runs for each\n",
      "model associated with cross-validation. It also allows multiple complexity parame-ters to be determined simultaneously as part of the training process. For example,\n",
      "in Chapter 7 we shall introduce the relevance vector machine , which is a Bayesian\n",
      "model having one complexity parameter for every training data point.\n",
      "The Bayesian view of model comparison simply involves the use of probabilities\n",
      "to represent uncertainty in the choice of model, along with a consistent applicationof the sum and product rules of probability. Suppose we wish to compare a set of L\n",
      "models {M\n",
      "i}where i=1,...,L . Here a model refers to a probability distribution\n",
      "over the observed data D. In the case of the polynomial curve-ﬁtting problem, the\n",
      "distribution is deﬁned over the set of target values t, while the set of input values X\n",
      "is assumed to be known. Other types of model deﬁne a joint distributions over X\n",
      "andt. We shall suppose that the data is generated from one of these models but we Section 1.5.4\n",
      "are uncertain which one. Our uncertainty is expressed through a prior probability\n",
      "distribution p(Mi). Given a training set D, we then wish to evaluate the posterior\n",
      "distribution\n",
      "p(Mi|D)∝p(Mi)p(D|M i). (3.66)\n",
      "The prior allows us to express a preference for different models. Let us simply\n",
      "assume that all models are given equal prior probability. The interesting term is\n",
      "themodel evidence p(D|M i)which expresses the preference shown by the data for162 3. LINEAR MODELS FOR REGRESSION\n",
      "different models, and we shall examine this term in more detail shortly. The model\n",
      "evidence is sometimes also called the marginal likelihood because it can be viewed\n",
      "as a likelihood function over the space of models, in which the parameters have been\n",
      "marginalized out. The ratio of model evidences p(D|M i)/p(D|M j)for two models\n",
      "is known as a Bayes factor (Kass and Raftery, 1995).\n",
      "Once we know the posterior distribution over models, the predictive distribution\n",
      "is given, from the sum and product rules, by\n",
      "p(t|x,D)=L∑\n",
      "i=1p(t|x,Mi,D)p(Mi|D). (3.67)\n",
      "This is an example of a mixture distribution in which the overall predictive distribu-\n",
      "tion is obtained by averaging the predictive distributions p(t|x,Mi,D)of individual\n",
      "models, weighted by the posterior probabilities p(Mi|D)of those models. For in-\n",
      "stance, if we have two models that are a-posteriori equally likely and one predicts\n",
      "a narrow distribution around t=awhile the other predicts a narrow distribution\n",
      "around t=b, the overall predictive distribution will be a bimodal distribution with\n",
      "modes at t=aandt=b, not a single model at t=(a+b)/2.\n",
      "A simple approximation to model averaging is to use the single most probable\n",
      "model alone to make predictions. This is known as model selection .\n",
      "For a model governed by a set of parameters w, the model evidence is given,\n",
      "from the sum and product rules of probability, by\n",
      "p(D|M i)=∫\n",
      "p(D|w,Mi)p(w|Mi)dw. (3.68)\n",
      "From a sampling perspective, the marginal likelihood can be viewed as the proba- Chapter 11\n",
      "bility of generating the data set Dfrom a model whose parameters are sampled at\n",
      "random from the prior. It is also interesting to note that the evidence is precisely the\n",
      "normalizing term that appears in the denominator in Bayes’ theorem when evaluatingthe posterior distribution over parameters because\n",
      "p(w|D,M\n",
      "i)=p(D|w,Mi)p(w|Mi)\n",
      "p(D|M i). (3.69)\n",
      "We can obtain some insight into the model evidence by making a simple approx-\n",
      "imation to the integral over parameters. Consider ﬁrst the case of a model having a\n",
      "single parameter w. The posterior distribution over parameters is proportional to\n",
      "p(D|w)p(w), where we omit the dependence on the model Mito keep the notation\n",
      "uncluttered. If we assume that the posterior distribution is sharply peaked around the\n",
      "most probable value wMAP, with width ∆wposterior , then we can approximate the in-\n",
      "tegral by the value of the integrand at its maximum times the width of the peak. If we\n",
      "further assume that the prior is ﬂat with width ∆wprior so that p(w)=1/∆wprior,\n",
      "then we have\n",
      "p(D)=∫\n",
      "p(D|w)p(w)dw≃p(D|wMAP)∆wposterior\n",
      "∆wprior(3.70)3.4. Bayesian Model Comparison 163\n",
      "Figure 3.12 We can obtain a rough approximation to\n",
      "the model evidence if we assume that\n",
      "the posterior distribution over parame-\n",
      "ters is sharply peaked around its mode\n",
      "wMAP.∆wposterior\n",
      "∆wpriorwMAP w\n",
      "and so taking logs we obtain\n",
      "lnp(D)≃lnp(D|wMAP)+l n(∆wposterior\n",
      "∆wprior)\n",
      ". (3.71)\n",
      "This approximation is illustrated in Figure 3.12. The ﬁrst term represents the ﬁt to\n",
      "the data given by the most probable parameter values, and for a ﬂat prior this would\n",
      "correspond to the log likelihood. The second term penalizes the model according to\n",
      "its complexity. Because ∆wposterior <∆wprior this term is negative, and it increases\n",
      "in magnitude as the ratio ∆wposterior /∆wprior gets smaller. Thus, if parameters are\n",
      "ﬁnely tuned to the data in the posterior distribution, then the penalty term is large.\n",
      "For a model having a set of Mparameters, we can make a similar approximation\n",
      "for each parameter in turn. Assuming that all parameters have the same ratio of\n",
      "∆wposterior /∆wprior, we obtain\n",
      "lnp(D)≃lnp(D|wMAP)+Mln(∆wposterior\n",
      "∆wprior)\n",
      ". (3.72)\n",
      "Thus, in this very simple approximation, the size of the complexity penalty increases\n",
      "linearly with the number Mof adaptive parameters in the model. As we increase\n",
      "the complexity of the model, the ﬁrst term will typically decrease, because a more\n",
      "complex model is better able to ﬁt the data, whereas the second term will increase\n",
      "due to the dependence on M. The optimal model complexity, as determined by\n",
      "the maximum evidence, will be given by a trade-off between these two competing\n",
      "terms. We shall later develop a more reﬁned version of this approximation, based on\n",
      "a Gaussian approximation to the posterior distribution. Section 4.4.1\n",
      "We can gain further insight into Bayesian model comparison and understand\n",
      "how the marginal likelihood can favour models of intermediate complexity by con-\n",
      "sidering Figure 3.13. Here the horizontal axis is a one-dimensional representation\n",
      "of the space of possible data sets, so that each point on this axis corresponds to a\n",
      "speciﬁc data set. We now consider three models M1,M2andM3of successively\n",
      "increasing complexity. Imagine running these models generatively to produce exam-\n",
      "ple data sets, and then looking at the distribution of data sets that result. Any given164 3. LINEAR MODELS FOR REGRESSION\n",
      "Figure 3.13 Schematic illustration of the\n",
      "distribution of data sets for\n",
      "three models of different com-\n",
      "plexity, in which M1is the\n",
      "simplest and M3is the most\n",
      "complex. Note that the dis-\n",
      "tributions are normalized. In\n",
      "this example, for the partic-\n",
      "ular observed data set D0,\n",
      "the model M2with intermedi-\n",
      "ate complexity has the largest\n",
      "evidence.p(D)\n",
      "DD0M1\n",
      "M2\n",
      "M3\n",
      "model can generate a variety of different data sets since the parameters are governed\n",
      "by a prior probability distribution, and for any choice of the parameters there may\n",
      "be random noise on the target variables. To generate a particular data set from a spe-\n",
      "ciﬁc model, we ﬁrst choose the values of the parameters from their prior distribution\n",
      "p(w), and then for these parameter values we sample the data from p(D|w). A sim-\n",
      "ple model (for example, based on a ﬁrst order polynomial) has little variability and\n",
      "so will generate data sets that are fairly similar to each other. Its distribution p(D)\n",
      "is therefore conﬁned to a relatively small region of the horizontal axis. By contrast,\n",
      "a complex model (such as a ninth order polynomial) can generate a great variety of\n",
      "different data sets, and so its distribution p(D)is spread over a large region of the\n",
      "space of data sets. Because the distributions p(D|M i)are normalized, we see that\n",
      "the particular data set D0can have the highest value of the evidence for the model\n",
      "of intermediate complexity. Essentially, the simpler model cannot ﬁt the data well,\n",
      "whereas the more complex model spreads its predictive probability over too broad a\n",
      "range of data sets and so assigns relatively small probability to any one of them.\n",
      "Implicit in the Bayesian model comparison framework is the assumption that\n",
      "the true distribution from which the data are generated is contained within the set of\n",
      "models under consideration. Provided this is so, we can show that Bayesian model\n",
      "comparison will on average favour the correct model. To see this, consider two\n",
      "models M1andM2in which the truth corresponds to M1. For a given ﬁnite data\n",
      "set, it is possible for the Bayes factor to be larger for the incorrect model. However, if\n",
      "we average the Bayes factor over the distribution of data sets, we obtain the expected\n",
      "Bayes factor in the form\n",
      "∫\n",
      "p(D|M 1)l np(D|M 1)\n",
      "p(D|M 2)dD (3.73)\n",
      "where the average has been taken with respect to the true distribution of the data.\n",
      "This quantity is an example of the Kullback-Leibler divergence and satisﬁes the prop- Section 1.6.1\n",
      "erty of always being positive unless the two distributions are equal in which case it\n",
      "is zero. Thus on average the Bayes factor will always favour the correct model.\n",
      "We have seen that the Bayesian framework avoids the problem of over-ﬁtting\n",
      "and allows models to be compared on the basis of the training data alone. However,3.5. The Evidence Approximation 165\n",
      "a Bayesian approach, like any approach to pattern recognition, needs to make as-\n",
      "sumptions about the form of the model, and if these are invalid then the results canbe misleading. In particular, we see from Figure 3.12 that the model evidence can\n",
      "be sensitive to many aspects of the prior, such as the behaviour in the tails. Indeed,\n",
      "the evidence is not deﬁned if the prior is improper, as can be seen by noting thatan improper prior has an arbitrary scaling factor (in other words, the normalization\n",
      "coefﬁcient is not deﬁned because the distribution cannot be normalized). If we con-\n",
      "sider a proper prior and then take a suitable limit in order to obtain an improper prior(for example, a Gaussian prior in which we take the limit of inﬁnite variance) then\n",
      "the evidence will go to zero, as can be seen from (3.70) and Figure 3.12. It may,\n",
      "however, be possible to consider the evidence ratio between two models ﬁrst and\n",
      "then take a limit to obtain a meaningful answer.\n",
      "In a practical application, therefore, it will be wise to keep aside an independent\n",
      "test set of data on which to evaluate the overall performance of the ﬁnal system.\n",
      "3.5. The Evidence Approximation\n",
      "In a fully Bayesian treatment of the linear basis function model, we would intro-duce prior distributions over the hyperparameters αandβand make predictions by\n",
      "marginalizing with respect to these hyperparameters as well as with respect to the\n",
      "parameters w. However, although we can integrate analytically over either wor\n",
      "over the hyperparameters, the complete marginalization over all of these variablesis analytically intractable. Here we discuss an approximation in which we set the\n",
      "hyperparameters to speciﬁc values determined by maximizing the marginal likeli-\n",
      "hood function obtained by ﬁrst integrating over the parameters w. This framework\n",
      "is known in the statistics literature as empirical Bayes (Bernardo and Smith, 1994;\n",
      "Gelman et al. , 2004), or type 2 maximum likelihood (Berger, 1985), or generalized\n",
      "maximum likelihood (Wahba, 1975), and in the machine learning literature is also\n",
      "called the evidence approximation (Gull, 1989; MacKay, 1992a).\n",
      "If we introduce hyperpriors over αandβ, the predictive distribution is obtained\n",
      "by marginalizing over w,αandβso that\n",
      "p(t|t)=∫∫∫\n",
      "p(t|w,β)p(w|t,α,β)p(α,β|t)dwdαdβ (3.74)\n",
      "where p(t|w,β)is given by (3.8) and p(w|t,α,β)is given by (3.49) with mNand\n",
      "SNdeﬁned by (3.53) and (3.54) respectively. Here we have omitted the dependence\n",
      "on the input variable xto keep the notation uncluttered. If the posterior distribution\n",
      "p(α,β|t)is sharply peaked around values ˆαandˆβ, then the predictive distribution is\n",
      "obtained simply by marginalizing over win which αandβare ﬁxed to the values ˆα\n",
      "andˆβ, so that\n",
      "p(t|t)≃p(t|t,ˆα,ˆβ)=∫\n",
      "p(t|w,ˆβ)p(w|t,ˆα,ˆβ)dw. (3.75)166 3. LINEAR MODELS FOR REGRESSION\n",
      "From Bayes’ theorem, the posterior distribution for αandβis given by\n",
      "p(α,β|t)∝p(t|α,β)p(α,β). (3.76)\n",
      "If the prior is relatively ﬂat, then in the evidence framework the values of ˆαand\n",
      "ˆβare obtained by maximizing the marginal likelihood function p(t|α,β). We shall\n",
      "proceed by evaluating the marginal likelihood for the linear basis function model and\n",
      "then ﬁnding its maxima. This will allow us to determine values for these hyperpa-rameters from the training data alone, without recourse to cross-validation. Recall\n",
      "that the ratio α/β is analogous to a regularization parameter.\n",
      "As an aside it is worth noting that, if we deﬁne conjugate (Gamma) prior distri-\n",
      "butions over αandβ, then the marginalization over these hyperparameters in (3.74)\n",
      "can be performed analytically to give a Student’s t-distribution over w(see Sec-\n",
      "tion 2.3.7). Although the resulting integral over wis no longer analytically tractable,\n",
      "it might be thought that approximating this integral, for example using the Laplace\n",
      "approximation discussed (Section 4.4) which is based on a local Gaussian approxi-\n",
      "mation centred on the mode of the posterior distribution, might provide a practical\n",
      "alternative to the evidence framework (Buntine and Weigend, 1991). However, the\n",
      "integrand as a function of wtypically has a strongly skewed mode so that the Laplace\n",
      "approximation fails to capture the bulk of the probability mass, leading to poorer re-\n",
      "sults than those obtained by maximizing the evidence (MacKay, 1999).\n",
      "Returning to the evidence framework, we note that there are two approaches that\n",
      "we can take to the maximization of the log evidence. We can evaluate the evidence\n",
      "function analytically and then set its derivative equal to zero to obtain re-estimation\n",
      "equations for αandβ, which we shall do in Section 3.5.2. Alternatively we use a\n",
      "technique called the expectation maximization (EM) algorithm, which will be dis-\n",
      "cussed in Section 9.3.4 where we shall also show that these two approaches converge\n",
      "to the same solution.\n",
      "3.5.1 Evaluation of the evidence function\n",
      "The marginal likelihood function p(t|α,β)is obtained by integrating over the\n",
      "weight parameters w, so that\n",
      "p(t|α,β)=∫\n",
      "p(t|w,β)p(w|α)dw. (3.77)\n",
      "One way to evaluate this integral is to make use once again of the result (2.115)\n",
      "for the conditional distribution in a linear-Gaussian model. Here we shall evaluate Exercise 3.16\n",
      "the integral instead by completing the square in the exponent and making use of the\n",
      "standard form for the normalization coefﬁcient of a Gaussian.\n",
      "From (3.11), (3.12), and (3.52), we can write the evidence function in the form Exercise 3.17\n",
      "p(t|α,β)=(β\n",
      "2π)N/2(α\n",
      "2π)M/2∫\n",
      "exp{−E(w)}dw (3.78)3.5. The Evidence Approximation 167\n",
      "where Mis the dimensionality of w, and we have deﬁned\n",
      "E(w)= βED(w)+αEW(w)\n",
      "=β\n",
      "2∥t−Φw∥2+α\n",
      "2wTw. (3.79)\n",
      "We recognize (3.79) as being equal, up to a constant of proportionality, to the reg-\n",
      "ularized sum-of-squares error function (3.27). We now complete the square over w Exercise 3.18\n",
      "giving\n",
      "E(w)=E(mN)+1\n",
      "2(w−mN)TA(w−mN) (3.80)\n",
      "where we have introduced\n",
      "A=αI+βΦTΦ (3.81)\n",
      "together with\n",
      "E(mN)=β\n",
      "2∥t−Φm N∥2+α\n",
      "2mT\n",
      "NmN. (3.82)\n",
      "Note that Acorresponds to the matrix of second derivatives of the error function\n",
      "A=∇∇E(w) (3.83)\n",
      "and is known as the Hessian matrix . Here we have also deﬁned mNgiven by\n",
      "mN=βA−1ΦTt. (3.84)\n",
      "Using (3.54), we see that A=S−1\n",
      "N, and hence (3.84) is equivalent to the previous\n",
      "deﬁnition (3.53), and therefore represents the mean of the posterior distribution.\n",
      "The integral over wcan now be evaluated simply by appealing to the standard\n",
      "result for the normalization coefﬁcient of a multivariate Gaussian, giving Exercise 3.19\n",
      "∫\n",
      "exp{−E(w)}dw\n",
      "=e x p {−E(mN)}∫\n",
      "exp{\n",
      "−1\n",
      "2(w−mN)TA(w−mN)}\n",
      "dw\n",
      "=e x p {−E(mN)}(2π)M/2|A|−1/2. (3.85)\n",
      "Using (3.78) we can then write the log of the marginal likelihood in the form\n",
      "lnp(t|α,β)=M\n",
      "2lnα+N\n",
      "2lnβ−E(mN)−1\n",
      "2ln|A|−N\n",
      "2ln(2π) (3.86)\n",
      "which is the required expression for the evidence function.\n",
      "Returning to the polynomial regression problem, we can plot the model evidence\n",
      "against the order of the polynomial, as shown in Figure 3.14. Here we have assumed\n",
      "a prior of the form (1.65) with the parameter αﬁxed at α=5×10−3. The form\n",
      "of this plot is very instructive. Referring back to Figure 1.4, we see that the M=0\n",
      "polynomial has very poor ﬁt to the data and consequently gives a relatively low value168 3. LINEAR MODELS FOR REGRESSION\n",
      "Figure 3.14 Plot of the model evidence versus\n",
      "the order M, for the polynomial re-\n",
      "gression model, showing that the\n",
      "evidence favours the model with\n",
      "M=3.\n",
      "M0 2 4 6 8−26−24−22−20−18\n",
      "for the evidence. Going to the M=1polynomial greatly improves the data ﬁt, and\n",
      "hence the evidence is signiﬁcantly higher. However, in going to M=2, the data\n",
      "ﬁt is improved only very marginally, due to the fact that the underlying sinusoidal\n",
      "function from which the data is generated is an odd function and so has no even terms\n",
      "in a polynomial expansion. Indeed, Figure 1.5 shows that the residual data error is\n",
      "reduced only slightly in going from M=1 toM=2. Because this richer model\n",
      "suffers a greater complexity penalty, the evidence actually falls in going from M=1\n",
      "toM=2. When we go to M=3 we obtain a signiﬁcant further improvement in\n",
      "data ﬁt, as seen in Figure 1.4, and so the evidence is increased again, giving the\n",
      "highest overall evidence for any of the polynomials. Further increases in the value\n",
      "ofMproduce only small improvements in the ﬁt to the data but suffer increasing\n",
      "complexity penalty, leading overall to a decrease in the evidence values. Looking\n",
      "again at Figure 1.5, we see that the generalization error is roughly constant between\n",
      "M=3 andM=8, and it would be difﬁcult to choose between these models on\n",
      "the basis of this plot alone. The evidence values, however, show a clear preference\n",
      "forM=3, since this is the simplest model which gives a good explanation for the\n",
      "observed data.\n",
      "3.5.2 Maximizing the evidence function\n",
      "Let us ﬁrst consider the maximization of p(t|α,β)with respect to α. This can\n",
      "be done by ﬁrst deﬁning the following eigenvector equation\n",
      "(\n",
      "βΦTΦ)\n",
      "ui=λiui. (3.87)\n",
      "From (3.81), it then follows that Ahas eigenvalues α+λi. Now consider the deriva-\n",
      "tive of the term involving ln|A|in (3.86) with respect to α.W eh a v e\n",
      "d\n",
      "dαln|A|=d\n",
      "dαln∏\n",
      "i(λi+α)=d\n",
      "dα∑\n",
      "iln(λi+α)=∑\n",
      "i1\n",
      "λi+α.(3.88)\n",
      "Thus the stationary points of (3.86) with respect to αsatisfy\n",
      "0=M\n",
      "2α−1\n",
      "2mT\n",
      "NmN−1\n",
      "2∑\n",
      "i1\n",
      "λi+α. (3.89)3.5. The Evidence Approximation 169\n",
      "Multiplying through by 2αand rearranging, we obtain\n",
      "αmT\n",
      "NmN=M−α∑\n",
      "i1\n",
      "λi+α=γ. (3.90)\n",
      "Since there are Mterms in the sum over i, the quantity γcan be written\n",
      "γ=∑\n",
      "iλi\n",
      "α+λi. (3.91)\n",
      "The interpretation of the quantity γwill be discussed shortly. From (3.90) we see\n",
      "that the value of αthat maximizes the marginal likelihood satisﬁes Exercise 3.20\n",
      "α=γ\n",
      "mT\n",
      "NmN. (3.92)\n",
      "Note that this is an implicit solution for αnot only because γdepends on α, but also\n",
      "because the mode mNof the posterior distribution itself depends on the choice of\n",
      "α. We therefore adopt an iterative procedure in which we make an initial choice for\n",
      "αand use this to ﬁnd mN, which is given by (3.53), and also to evaluate γ, which\n",
      "is given by (3.91). These values are then used to re-estimate αusing (3.92), and the\n",
      "process repeated until convergence. Note that because the matrix ΦTΦis ﬁxed, we\n",
      "can compute its eigenvalues once at the start and then simply multiply these by βto\n",
      "obtain the λi.\n",
      "It should be emphasized that the value of αhas been determined purely by look-\n",
      "ing at the training data. In contrast to maximum likelihood methods, no independent\n",
      "data set is required in order to optimize the model complexity.\n",
      "We can similarly maximize the log marginal likelihood (3.86) with respect to β.\n",
      "To do this, we note that the eigenvalues λideﬁned by (3.87) are proportional to β,\n",
      "and hence dλi/dβ=λi/βgiving\n",
      "d\n",
      "dβln|A|=d\n",
      "dβ∑\n",
      "iln(λi+α)=1\n",
      "β∑\n",
      "iλi\n",
      "λi+α=γ\n",
      "β. (3.93)\n",
      "The stationary point of the marginal likelihood therefore satisﬁes\n",
      "0=N\n",
      "2β−1\n",
      "2N∑\n",
      "n=1{\n",
      "tn−mT\n",
      "Nφ(xn)}2−γ\n",
      "2β(3.94)\n",
      "and rearranging we obtain Exercise 3.22\n",
      "1\n",
      "β=1\n",
      "N−γN∑\n",
      "n=1{\n",
      "tn−mT\n",
      "Nφ(xn)}2. (3.95)\n",
      "Again, this is an implicit solution for βand can be solved by choosing an initial\n",
      "value for βand then using this to calculate mNandγand then re-estimate βusing\n",
      "(3.95), repeating until convergence. If both αandβare to be determined from the\n",
      "data, then their values can be re-estimated together after each update of γ.170 3. LINEAR MODELS FOR REGRESSION\n",
      "Figure 3.15 Contours of the likelihood function (red)\n",
      "and the prior (green) in which the axes in parameter\n",
      "space have been rotated to align with the eigenvectors\n",
      "uiof the Hessian. For α=0, the mode of the poste-\n",
      "rior is given by the maximum likelihood solution wML,\n",
      "whereas for nonzero αthe mode is at wMAP=mN.I n\n",
      "the direction w1the eigenvalue λ1, deﬁned by (3.87), is\n",
      "small compared with αand so the quantity λ1/(λ1+α)\n",
      "is close to zero, and the corresponding MAP value of\n",
      "w1is also close to zero. By contrast, in the direction w2\n",
      "the eigenvalue λ2is large compared with αand so the\n",
      "quantity λ2/(λ2+α)is close to unity, and the MAP value\n",
      "ofw2is close to its maximum likelihood value.u1u2\n",
      "w1w2\n",
      "wMAPwML\n",
      "3.5.3 Effective number of parameters\n",
      "The result (3.92) has an elegant interpretation (MacKay, 1992a), which provides\n",
      "insight into the Bayesian solution for α. To see this, consider the contours of the like-\n",
      "lihood function and the prior as illustrated in Figure 3.15. Here we have implicitly\n",
      "transformed to a rotated set of axes in parameter space aligned with the eigenvec-\n",
      "torsuideﬁned in (3.87). Contours of the likelihood function are then axis-aligned\n",
      "ellipses. The eigenvalues λimeasure the curvature of the likelihood function, and\n",
      "so in Figure 3.15 the eigenvalue λ1is small compared with λ2(because a smaller\n",
      "curvature corresponds to a greater elongation of the contours of the likelihood func-\n",
      "tion). Because βΦTΦis a positive deﬁnite matrix, it will have positive eigenvalues,\n",
      "and so the ratio λi/(λi+α)will lie between 0 and 1. Consequently, the quantity γ\n",
      "deﬁned by (3.91) will lie in the range 0⩽γ⩽M. For directions in which λi≫α,\n",
      "the corresponding parameter wiwill be close to its maximum likelihood value, and\n",
      "the ratio λi/(λi+α)will be close to 1. Such parameters are called well determined\n",
      "because their values are tightly constrained by the data. Conversely, for directions\n",
      "in which λi≪α, the corresponding parameters wiwill be close to zero, as will the\n",
      "ratiosλi/(λi+α). These are directions in which the likelihood function is relatively\n",
      "insensitive to the parameter value and so the parameter has been set to a small value\n",
      "by the prior. The quantity γdeﬁned by (3.91) therefore measures the effective total\n",
      "number of well determined parameters.\n",
      "We can obtain some insight into the result (3.95) for re-estimating βby com-\n",
      "paring it with the corresponding maximum likelihood result given by (3.21). Both\n",
      "of these formulae express the variance (the inverse precision) as an average of the\n",
      "squared differences between the targets and the model predictions. However, they\n",
      "differ in that the number of data points Nin the denominator of the maximum like-\n",
      "lihood result is replaced by N−γin the Bayesian result. We recall from (1.56) that\n",
      "the maximum likelihood estimate of the variance for a Gaussian distribution over a3.5. The Evidence Approximation 171\n",
      "single variable xis given by\n",
      "σ2\n",
      "ML=1\n",
      "NN∑\n",
      "n=1(xn−µML)2(3.96)\n",
      "and that this estimate is biased because the maximum likelihood solution µMLfor\n",
      "the mean has ﬁtted some of the noise on the data. In effect, this has used up onedegree of freedom in the model. The corresponding unbiased estimate is given by\n",
      "(1.59) and takes the form\n",
      "σ\n",
      "2\n",
      "MAP=1\n",
      "N−1N∑\n",
      "n=1(xn−µML)2. (3.97)\n",
      "We shall see in Section 10.1.3 that this result can be obtained from a Bayesian treat-\n",
      "ment in which we marginalize over the unknown mean. The factor of N−1in the\n",
      "denominator of the Bayesian result takes account of the fact that one degree of free-dom has been used in ﬁtting the mean and removes the bias of maximum likelihood.\n",
      "Now consider the corresponding results for the linear regression model. The mean\n",
      "of the target distribution is now given by the function w\n",
      "Tφ(x), which contains M\n",
      "parameters. However, not all of these parameters are tuned to the data. The effective\n",
      "number of parameters that are determined by the data is γ, with the remaining M−γ\n",
      "parameters set to small values by the prior. This is reﬂected in the Bayesian resultfor the variance that has a factor N−γin the denominator, thereby correcting for\n",
      "the bias of the maximum likelihood result.\n",
      "We can illustrate the evidence framework for setting hyperparameters using the\n",
      "sinusoidal synthetic data set from Section 1.1, together with the Gaussian basis func-\n",
      "tion model comprising 9basis functions, so that the total number of parameters in\n",
      "the model is given by M=1 0 including the bias. Here, for simplicity of illustra-\n",
      "tion, we have set βto its true value of 11.1and then used the evidence framework to\n",
      "determine α, as shown in Figure 3.16.\n",
      "We can also see how the parameter αcontrols the magnitude of the parameters\n",
      "{w\n",
      "i}, by plotting the individual parameters versus the effective number γof param-\n",
      "eters, as shown in Figure 3.17.\n",
      "If we consider the limit N≫Min which the number of data points is large in\n",
      "relation to the number of parameters, then from (3.87) all of the parameters will be\n",
      "well determined by the data because ΦTΦinvolves an implicit sum over data points,\n",
      "and so the eigenvalues λiincrease with the size of the data set. In this case, γ=M,\n",
      "and the re-estimation equations for αandβbecome\n",
      "α=M\n",
      "2EW(mN)(3.98)\n",
      "β=N\n",
      "2ED(mN)(3.99)\n",
      "where EWandEDare deﬁned by (3.25) and (3.26), respectively. These results\n",
      "can be used as an easy-to-compute approximation to the full evidence re-estimation172 3. LINEAR MODELS FOR REGRESSION\n",
      "lnα−5 0 5\n",
      "lnα−5 0 5\n",
      "Figure 3.16 The left plot shows γ(red curve) and 2αEW(mN)(blue curve) versus lnαfor the sinusoidal\n",
      "synthetic data set. It is the intersection of these two curves that deﬁnes the optimum value for αgiven by the\n",
      "evidence procedure. The right plot shows the corresponding graph of log evidence lnp(t|α, β)versus lnα(red\n",
      "curve) showing that the peak coincides with the crossing point of the curves in the left plot. Also shown is the\n",
      "test set error (blue curve) showing that the evidence maximum occurs close to the point of best generalization.\n",
      "formulae, because they do not require evaluation of the eigenvalue spectrum of the\n",
      "Hessian.\n",
      "Figure 3.17 Plot of the 10parameters wi\n",
      "from the Gaussian basis function\n",
      "model versus the effective num-\n",
      "ber of parameters γ, in which the\n",
      "hyperparameter αis varied in the\n",
      "range 0⩽α⩽∞causing γto\n",
      "vary in the range 0⩽γ⩽M.\n",
      "9713625480\n",
      "γwi\n",
      "0 2 4 6 8 10−2−1012\n",
      "3.6. Limitations of Fixed Basis Functions\n",
      "Throughout this chapter, we have focussed on models comprising a linear combina-\n",
      "tion of ﬁxed, nonlinear basis functions. We have seen that the assumption of linearity\n",
      "in the parameters led to a range of useful properties including closed-form solutions\n",
      "to the least-squares problem, as well as a tractable Bayesian treatment. Furthermore,\n",
      "for a suitable choice of basis functions, we can model arbitrary nonlinearities in theExercises 173\n",
      "mapping from input variables to targets. In the next chapter, we shall study an anal-\n",
      "ogous class of models for classiﬁcation.\n",
      "It might appear, therefore, that such linear models constitute a general purpose\n",
      "framework for solving problems in pattern recognition. Unfortunately, there are\n",
      "some signiﬁcant shortcomings with linear models, which will cause us to turn inlater chapters to more complex models such as support vector machines and neural\n",
      "networks.\n",
      "The difﬁculty stems from the assumption that the basis functions φ\n",
      "j(x)are ﬁxed\n",
      "before the training data set is observed and is a manifestation of the curse of dimen-\n",
      "sionality discussed in Section 1.4. As a consequence, the number of basis functions\n",
      "needs to grow rapidly, often exponentially, with the dimensionality Dof the input\n",
      "space.\n",
      "Fortunately, there are two properties of real data sets that we can exploit to help\n",
      "alleviate this problem. First of all, the data vectors {xn}typically lie close to a non-\n",
      "linear manifold whose intrinsic dimensionality is smaller than that of the input space\n",
      "as a result of strong correlations between the input variables. We will see an exampleof this when we consider images of handwritten digits in Chapter 12. If we are using\n",
      "localized basis functions, we can arrange that they are scattered in input space only\n",
      "in regions containing data. This approach is used in radial basis function networksand also in support vector and relevance vector machines. Neural network models,\n",
      "which use adaptive basis functions having sigmoidal nonlinearities, can adapt the\n",
      "parameters so that the regions of input space over which the basis functions varycorresponds to the data manifold. The second property is that target variables may\n",
      "have signiﬁcant dependence on only a small number of possible directions within the\n",
      "data manifold. Neural networks can exploit this property by choosing the directionsin input space to which the basis functions respond.\n",
      "Exercises\n",
      "3.1 (⋆)www Show that the ‘ tanh ’ function and the logistic sigmoid function (3.6)\n",
      "are related by\n",
      "tanh(a)=2σ(2a)−1. (3.100)\n",
      "Hence show that a general linear combination of logistic sigmoid functions of the\n",
      "form\n",
      "y(x,w)=w0+M∑\n",
      "j=1wjσ(x−µj\n",
      "s)\n",
      "(3.101)\n",
      "is equivalent to a linear combination of ‘ tanh ’ functions of the form\n",
      "y(x,u)=u0+M∑\n",
      "j=1ujtanh(x−µj\n",
      "s)\n",
      "(3.102)\n",
      "and ﬁnd expressions to relate the new parameters {u1,...,u M}to the original pa-\n",
      "rameters {w1,...,w M}.174 3. LINEAR MODELS FOR REGRESSION\n",
      "3.2 (⋆⋆)Show that the matrix\n",
      "Φ(ΦTΦ)−1ΦT(3.103)\n",
      "takes any vector vand projects it onto the space spanned by the columns of Φ. Use\n",
      "this result to show that the least-squares solution (3.15) corresponds to an orthogonalprojection of the vector tonto the manifold Sas shown in Figure 3.2.\n",
      "3.3 (⋆)Consider a data set in which each data point t\n",
      "nis associated with a weighting\n",
      "factor rn>0, so that the sum-of-squares error function becomes\n",
      "ED(w)=1\n",
      "2N∑\n",
      "n=1rn{\n",
      "tn−wTφ(xn)}2. (3.104)\n",
      "Find an expression for the solution w⋆that minimizes this error function. Give two\n",
      "alternative interpretations of the weighted sum-of-squares error function in terms of\n",
      "(i) data dependent noise variance and (ii) replicated data points.\n",
      "3.4 (⋆)www Consider a linear model of the form\n",
      "y(x,w)=w0+D∑\n",
      "i=1wixi (3.105)\n",
      "together with a sum-of-squares error function of the form\n",
      "ED(w)=1\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2. (3.106)\n",
      "Now suppose that Gaussian noise ϵiwith zero mean and variance σ2is added in-\n",
      "dependently to each of the input variables xi. By making use of E[ϵi]=0 and\n",
      "E[ϵiϵj]=δijσ2, show that minimizing EDaveraged over the noise distribution is\n",
      "equivalent to minimizing the sum-of-squares error for noise-free input variables with\n",
      "the addition of a weight-decay regularization term, in which the bias parameter w0\n",
      "is omitted from the regularizer.\n",
      "3.5 (⋆)www Using the technique of Lagrange multipliers, discussed in Appendix E,\n",
      "show that minimization of the regularized error function (3.29) is equivalent to mini-\n",
      "mizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).\n",
      "Discuss the relationship between the parameters ηandλ.\n",
      "3.6 (⋆)www Consider a linear basis function regression model for a multivariate\n",
      "target variable thaving a Gaussian distribution of the form\n",
      "p(t|W,Σ)=N(t|y(x,W),Σ) (3.107)\n",
      "where\n",
      "y(x,W)=WTφ(x) (3.108)Exercises 175\n",
      "together with a training data set comprising input basis vectors φ(xn)and corre-\n",
      "sponding target vectors tn, with n=1,...,N . Show that the maximum likelihood\n",
      "solution WMLfor the parameter matrix Whas the property that each column is\n",
      "given by an expression of the form (3.15), which was the solution for an isotropic\n",
      "noise distribution. Note that this is independent of the covariance matrix Σ. Show\n",
      "that the maximum likelihood solution for Σis given by\n",
      "Σ=1\n",
      "NN∑\n",
      "n=1(\n",
      "tn−WT\n",
      "MLφ(xn))(\n",
      "tn−WT\n",
      "MLφ(xn))T. (3.109)\n",
      "3.7 (⋆)By using the technique of completing the square, verify the result (3.49) for the\n",
      "posterior distribution of the parameters win the linear basis function model in which\n",
      "mNandSNare deﬁned by (3.50) and (3.51) respectively.\n",
      "3.8 (⋆⋆)www Consider the linear basis function model in Section 3.1, and suppose\n",
      "that we have already observed Ndata points, so that the posterior distribution over\n",
      "wis given by (3.49). This posterior can be regarded as the prior for the next obser-\n",
      "vation. By considering an additional data point (xN+1,tN+1), and by completing\n",
      "the square in the exponential, show that the resulting posterior distribution is againgiven by (3.49) but with S\n",
      "Nreplaced by SN+1andmNreplaced by mN+1.\n",
      "3.9 (⋆⋆)Repeat the previous exercise but instead of completing the square by hand,\n",
      "make use of the general result for linear-Gaussian models given by (2.116).\n",
      "3.10 (⋆⋆)www By making use of the result (2.115) to evaluate the integral in (3.57),\n",
      "verify that the predictive distribution for the Bayesian linear regression model is\n",
      "given by (3.58) in which the input-dependent variance is given by (3.59).\n",
      "3.11 (⋆⋆)We have seen that, as the size of a data set increases, the uncertainty associated\n",
      "with the posterior distribution over model parameters decreases. Make use of the\n",
      "matrix identity (Appendix C)\n",
      "(\n",
      "M+vvT)−1=M−1−(M−1v)(\n",
      "vTM−1)\n",
      "1+vTM−1v(3.110)\n",
      "to show that the uncertainty σ2\n",
      "N(x)associated with the linear regression function\n",
      "given by (3.59) satisﬁes\n",
      "σ2\n",
      "N+1(x)⩽σ2\n",
      "N(x). (3.111)\n",
      "3.12 (⋆⋆)We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution\n",
      "with unknown mean and unknown precision (inverse variance) is a normal-gamma\n",
      "distribution. This property also holds for the case of the conditional Gaussian dis-\n",
      "tribution p(t|x,w,β)of the linear regression model. If we consider the likelihood\n",
      "function (3.10), then the conjugate prior for wandβis given by\n",
      "p(w,β)=N(w|m0,β−1S0)Gam( β|a0,b0). (3.112)176 3. LINEAR MODELS FOR REGRESSION\n",
      "Show that the corresponding posterior distribution takes the same functional form,\n",
      "so that\n",
      "p(w,β|t)=N(w|mN,β−1SN)Gam( β|aN,bN) (3.113)\n",
      "and ﬁnd expressions for the posterior parameters mN,SN,aN, andbN.\n",
      "3.13 (⋆⋆)Show that the predictive distribution p(t|x,t)for the model discussed in Ex-\n",
      "ercise 3.12 is given by a Student’s t-distribution of the form\n",
      "p(t|x,t)=S t ( t|µ, λ, ν ) (3.114)\n",
      "and obtain expressions for µ,λandν.\n",
      "3.14 (⋆⋆)In this exercise, we explore in more detail the properties of the equivalent\n",
      "kernel deﬁned by (3.62), where SNis deﬁned by (3.54). Suppose that the basis\n",
      "functions φj(x)are linearly independent and that the number Nof data points is\n",
      "greater than the number Mof basis functions. Furthermore, let one of the basis\n",
      "functions be constant, say φ0(x)=1 . By taking suitable linear combinations of\n",
      "these basis functions, we can construct a new basis set ψj(x)spanning the same\n",
      "space but that are orthonormal, so that\n",
      "N∑\n",
      "n=1ψj(xn)ψk(xn)=Ijk (3.115)\n",
      "where Ijkis deﬁned to be 1ifj=kand0otherwise, and we take ψ0(x)=1 . Show\n",
      "that for α=0, the equivalent kernel can be written as k(x,x′)=ψ(x)Tψ(x′)\n",
      "where ψ=(ψ1,...,ψ M)T. Use this result to show that the kernel satisﬁes the\n",
      "summation constraint\n",
      "N∑\n",
      "n=1k(x,xn)=1. (3.116)\n",
      "3.15 (⋆)www Consider a linear basis function model for regression in which the pa-\n",
      "rameters αandβare set using the evidence framework. Show that the function\n",
      "E(mN)deﬁned by (3.82) satisﬁes the relation 2E(mN)=N.\n",
      "3.16 (⋆⋆)Derive the result (3.86) for the log evidence function p(t|α,β)of the linear\n",
      "regression model by making use of (2.115) to evaluate the integral (3.77) directly.\n",
      "3.17 (⋆)Show that the evidence function for the Bayesian linear regression model can\n",
      "be written in the form (3.78) in which E(w)is deﬁned by (3.79).\n",
      "3.18 (⋆⋆)www By completing the square over w, show that the error function (3.79)\n",
      "in Bayesian linear regression can be written in the form (3.80).\n",
      "3.19 (⋆⋆)Show that the integration over win the Bayesian linear regression model gives\n",
      "the result (3.85). Hence show that the log marginal likelihood is given by (3.86).Exercises 177\n",
      "3.20 (⋆⋆)www Starting from (3.86) verify all of the steps needed to show that maxi-\n",
      "mization of the log marginal likelihood function (3.86) with respect to αleads to the\n",
      "re-estimation equation (3.92).\n",
      "3.21 (⋆⋆)An alternative way to derive the result (3.92) for the optimal value of αin the\n",
      "evidence framework is to make use of the identity\n",
      "d\n",
      "dαln|A|=Tr(\n",
      "A−1d\n",
      "dαA)\n",
      ". (3.117)\n",
      "Prove this identity by considering the eigenvalue expansion of a real, symmetric\n",
      "matrix A, and making use of the standard results for the determinant and trace of\n",
      "Aexpressed in terms of its eigenvalues (Appendix C). Then make use of (3.117) to\n",
      "derive (3.92) starting from (3.86).\n",
      "3.22 (⋆⋆)Starting from (3.86) verify all of the steps needed to show that maximiza-\n",
      "tion of the log marginal likelihood function (3.86) with respect to βleads to the\n",
      "re-estimation equation (3.95).\n",
      "3.23 (⋆⋆)www Show that the marginal probability of the data, in other words the\n",
      "model evidence, for the model described in Exercise 3.12 is given by\n",
      "p(t)=1\n",
      "(2π)N/2ba0\n",
      "0\n",
      "baN\n",
      "NΓ(aN)\n",
      "Γ(a0)|SN|1/2\n",
      "|S0|1/2(3.118)\n",
      "by ﬁrst marginalizing with respect to wand then with respect to β.\n",
      "3.24 (⋆⋆)Repeat the previous exercise but now use Bayes’ theorem in the form\n",
      "p(t)=p(t|w,β)p(w,β)\n",
      "p(w,β|t)(3.119)\n",
      "and then substitute for the prior and posterior distributions and the likelihood func-\n",
      "tion in order to derive the result (3.118).4\n",
      "Linear\n",
      "Models for\n",
      "Classiﬁcation\n",
      "In the previous chapter, we explored a class of regression models having particularly\n",
      "simple analytical and computational properties. We now discuss an analogous classof models for solving classiﬁcation problems. The goal in classiﬁcation is to take an\n",
      "input vector xand to assign it to one of Kdiscrete classes C\n",
      "kwhere k=1,...,K .\n",
      "In the most common scenario, the classes are taken to be disjoint, so that each input isassigned to one and only one class. The input space is thereby divided into decision\n",
      "regions whose boundaries are called decision boundaries ordecision surfaces .I n\n",
      "this chapter, we consider linear models for classiﬁcation, by which we mean that thedecision surfaces are linear functions of the input vector xand hence are deﬁned\n",
      "by(D−1)-dimensional hyperplanes within the D-dimensional input space. Data\n",
      "sets whose classes can be separated exactly by linear decision surfaces are said to belinearly separable .\n",
      "For regression problems, the target variable twas simply the vector of real num-\n",
      "bers whose values we wish to predict. In the case of classiﬁcation, there are various\n",
      "179180 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "ways of using target values to represent class labels. For probabilistic models, the\n",
      "most convenient, in the case of two-class problems, is the binary representation inwhich there is a single target variable t∈{0,1}such that t=1represents class C\n",
      "1\n",
      "andt=0represents class C2. We can interpret the value of tas the probability that\n",
      "the class is C1, with the values of probability taking only the extreme values of 0 and\n",
      "1. For K>2classes, it is convenient to use a 1-of- Kcoding scheme in which tis\n",
      "a vector of length Ksuch that if the class is Cj, then all elements tkoftare zero\n",
      "except element tj, which takes the value 1. For instance, if we have K=5classes,\n",
      "then a pattern from class 2would be given the target vector\n",
      "t=( 0,1,0,0,0)T. (4.1)\n",
      "Again, we can interpret the value of tkas the probability that the class is Ck.F o r\n",
      "nonprobabilistic models, alternative choices of target variable representation will\n",
      "sometimes prove convenient.\n",
      "In Chapter 1, we identiﬁed three distinct approaches to the classiﬁcation prob-\n",
      "lem. The simplest involves constructing a discriminant function that directly assigns\n",
      "each vector xto a speciﬁc class. A more powerful approach, however, models the\n",
      "conditional probability distribution p(Ck|x)in an inference stage, and then subse-\n",
      "quently uses this distribution to make optimal decisions. By separating inference\n",
      "and decision, we gain numerous beneﬁts, as discussed in Section 1.5.4. There are\n",
      "two different approaches to determining the conditional probabilities p(Ck|x). One\n",
      "technique is to model them directly, for example by representing them as parametric\n",
      "models and then optimizing the parameters using a training set. Alternatively, we\n",
      "can adopt a generative approach in which we model the class-conditional densities\n",
      "given by p(x|Ck), together with the prior probabilities p(Ck)for the classes, and then\n",
      "we compute the required posterior probabilities using Bayes’ theorem\n",
      "p(Ck|x)=p(x|Ck)p(Ck)\n",
      "p(x). (4.2)\n",
      "We shall discuss examples of all three approaches in this chapter.\n",
      "In the linear regression models considered in Chapter 3, the model prediction\n",
      "y(x,w)was given by a linear function of the parameters w. In the simplest case,\n",
      "the model is also linear in the input variables and therefore takes the form y(x)=\n",
      "wTx+w0, so that yis a real number. For classiﬁcation problems, however, we wish\n",
      "to predict discrete class labels, or more generally posterior probabilities that lie inthe range (0,1). To achieve this, we consider a generalization of this model in which\n",
      "we transform the linear function of wusing a nonlinear function f(·)so that\n",
      "y(x)=f(\n",
      "wTx+w0)\n",
      ". (4.3)\n",
      "In the machine learning literature f(·)is known as an activation function , whereas\n",
      "its inverse is called a link function in the statistics literature. The decision surfaces\n",
      "correspond to y(x) = constant , so that wTx+w0= constant and hence the deci-\n",
      "sion surfaces are linear functions of x, even if the function f(·)is nonlinear. For this\n",
      "reason, the class of models described by (4.3) are called generalized linear models4.1. Discriminant Functions 181\n",
      "(McCullagh and Nelder, 1989). Note, however, that in contrast to the models used\n",
      "for regression, they are no longer linear in the parameters due to the presence of thenonlinear function f(·). This will lead to more complex analytical and computa-\n",
      "tional properties than for linear regression models. Nevertheless, these models are\n",
      "still relatively simple compared to the more general nonlinear models that will bestudied in subsequent chapters.\n",
      "The algorithms discussed in this chapter will be equally applicable if we ﬁrst\n",
      "make a ﬁxed nonlinear transformation of the input variables using a vector of basisfunctions φ(x)as we did for regression models in Chapter 3. We begin by consider-\n",
      "ing classiﬁcation directly in the original input space x, while in Section 4.3 we shall\n",
      "ﬁnd it convenient to switch to a notation involving basis functions for consistency\n",
      "with later chapters.\n",
      "4.1. Discriminant Functions\n",
      "A discriminant is a function that takes an input vector xand assigns it to one of K\n",
      "classes, denoted Ck. In this chapter, we shall restrict attention to linear discriminants ,\n",
      "namely those for which the decision surfaces are hyperplanes. To simplify the dis-\n",
      "cussion, we consider ﬁrst the case of two classes and then investigate the extension\n",
      "toK>2classes.\n",
      "4.1.1 Two classes\n",
      "The simplest representation of a linear discriminant function is obtained by tak-\n",
      "ing a linear function of the input vector so that\n",
      "y(x)=wTx+w0 (4.4)\n",
      "wherewis called a weight vector , andw0is a bias (not to be confused with bias in\n",
      "the statistical sense). The negative of the bias is sometimes called a threshold .A n\n",
      "input vector xis assigned to class C1ify(x)⩾0and to class C2otherwise. The cor-\n",
      "responding decision boundary is therefore deﬁned by the relation y(x)=0 , which\n",
      "corresponds to a (D−1)-dimensional hyperplane within the D-dimensional input\n",
      "space. Consider two points xAandxBboth of which lie on the decision surface.\n",
      "Because y(xA)=y(xB)=0 ,w eh a v e wT(xA−xB)=0 and hence the vector wis\n",
      "orthogonal to every vector lying within the decision surface, and so wdetermines the\n",
      "orientation of the decision surface. Similarly, if xis a point on the decision surface,\n",
      "theny(x)=0 , and so the normal distance from the origin to the decision surface is\n",
      "given by\n",
      "wTx\n",
      "∥w∥=−w0\n",
      "∥w∥. (4.5)\n",
      "We therefore see that the bias parameter w0determines the location of the decision\n",
      "surface. These properties are illustrated for the case of D=2in Figure 4.1.\n",
      "Furthermore, we note that the value of y(x)gives a signed measure of the per-\n",
      "pendicular distance rof the point xfrom the decision surface. To see this, consider182 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "Figure 4.1 Illustration of the geometry of a\n",
      "linear discriminant function in two dimensions.\n",
      "The decision surface, shown in red, is perpen-\n",
      "dicular to w, and its displacement from the\n",
      "origin is controlled by the bias parameter w0.\n",
      "Also, the signed orthogonal distance of a gen-\n",
      "eral point xfrom the decision surface is given\n",
      "byy(x)/∥w∥.x2\n",
      "x1wx\n",
      "y(x)\n",
      "∥w∥\n",
      "x⊥\n",
      "−w0\n",
      "∥w∥y=0\n",
      "y<0y>0\n",
      "R2R1\n",
      "an arbitrary point xand let x⊥be its orthogonal projection onto the decision surface,\n",
      "so that\n",
      "x=x⊥+rw\n",
      "∥w∥. (4.6)\n",
      "Multiplying both sides of this result by wTand adding w0, and making use of y(x)=\n",
      "wTx+w0andy(x⊥)=wTx⊥+w0=0,w eh a v e\n",
      "r=y(x)\n",
      "∥w∥. (4.7)\n",
      "This result is illustrated in Figure 4.1.\n",
      "As with the linear regression models in Chapter 3, it is sometimes convenient\n",
      "to use a more compact notation in which we introduce an additional dummy ‘input’\n",
      "valuex0=1and then deﬁne ˜w=(w0,w)and˜x=(x0,x)so that\n",
      "y(x)=˜wT˜x. (4.8)\n",
      "In this case, the decision surfaces are D-dimensional hyperplanes passing through\n",
      "the origin of the D+1-dimensional expanded input space.\n",
      "4.1.2 Multiple classes\n",
      "Now consider the extension of linear discriminants to K>2classes. We might\n",
      "be tempted be to build a K-class discriminant by combining a number of two-class\n",
      "discriminant functions. However, this leads to some serious difﬁculties (Duda and\n",
      "Hart, 1973) as we now show.\n",
      "Consider the use of K−1classiﬁers each of which solves a two-class problem of\n",
      "separating points in a particular class Ckfrom points not in that class. This is known\n",
      "as a one-versus-the-rest classiﬁer. The left-hand example in Figure 4.2 shows an4.1. Discriminant Functions 183\n",
      "R1\n",
      "R2\n",
      "R3?\n",
      "C1\n",
      "notC1C2\n",
      "notC2R1\n",
      "R2R3\n",
      "?C1\n",
      "C2C1C3\n",
      "C2C3\n",
      "Figure 4.2 Attempting to construct a Kclass discriminant from a set of two class discriminants leads to am-\n",
      "biguous regions, shown in green. On the left is an example involving the use of two discriminants designed to\n",
      "distinguish points in class Ckfrom points not in class Ck. On the right is an example involving three discriminant\n",
      "functions each of which is used to separate a pair of classes CkandCj.\n",
      "example involving three classes where this approach leads to regions of input space\n",
      "that are ambiguously classiﬁed.\n",
      "An alternative is to introduce K(K−1)/2binary discriminant functions, one\n",
      "for every possible pair of classes. This is known as a one-versus-one classiﬁer. Each\n",
      "point is then classiﬁed according to a majority vote amongst the discriminant func-\n",
      "tions. However, this too runs into the problem of ambiguous regions, as illustrated\n",
      "in the right-hand diagram of Figure 4.2.\n",
      "We can avoid these difﬁculties by considering a single K-class discriminant\n",
      "comprising Klinear functions of the form\n",
      "yk(x)=wT\n",
      "kx+wk0 (4.9)\n",
      "and then assigning a point xto class Ckifyk(x)>yj(x)for all j̸=k. The decision\n",
      "boundary between class Ckand class Cjis therefore given by yk(x)=yj(x)and\n",
      "hence corresponds to a (D−1)-dimensional hyperplane deﬁned by\n",
      "(wk−wj)Tx+(wk0−wj0)=0. (4.10)\n",
      "This has the same form as the decision boundary for the two-class case discussed in\n",
      "Section 4.1.1, and so analogous geometrical properties apply.\n",
      "The decision regions of such a discriminant are always singly connected and\n",
      "convex. To see this, consider two points xAandxBboth of which lie inside decision\n",
      "region Rk, as illustrated in Figure 4.3. Any point ˆxthat lies on the line connecting\n",
      "xAandxBcan be expressed in the form\n",
      "ˆx=λxA+( 1−λ)xB (4.11)184 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "Figure 4.3 Illustration of the decision regions for a mul-\n",
      "ticlass linear discriminant, with the decision\n",
      "boundaries shown in red. If two points xA\n",
      "andxBboth lie inside the same decision re-\n",
      "gionRk, then any point bxthat lies on the line\n",
      "connecting these two points must also lie in\n",
      "Rk, and hence the decision region must be\n",
      "singly connected and convex.RiRj\n",
      "Rk\n",
      "xAxB\n",
      "ˆx\n",
      "where 0⩽λ⩽1. From the linearity of the discriminant functions, it follows that\n",
      "yk(ˆx)=λyk(xA)+( 1 −λ)yk(xB). (4.12)\n",
      "Because both xAandxBlie inside Rk, it follows that yk(xA)>y j(xA), and\n",
      "yk(xB)>y j(xB), for all j̸=k, and hence yk(ˆx)>y j(ˆx), and soˆxalso lies\n",
      "insideRk. Thus Rkis singly connected and convex.\n",
      "Note that for two classes, we can either employ the formalism discussed here,\n",
      "based on two discriminant functions y1(x)andy2(x), or else use the simpler but\n",
      "equivalent formulation described in Section 4.1.1 based on a single discriminant\n",
      "function y(x).\n",
      "We now explore three approaches to learning the parameters of linear discrimi-\n",
      "nant functions, based on least squares, Fisher’s linear discriminant, and the percep-\n",
      "tron algorithm.\n",
      "4.1.3 Least squares for classiﬁcation\n",
      "In Chapter 3, we considered models that were linear functions of the parame-\n",
      "ters, and we saw that the minimization of a sum-of-squares error function led to a\n",
      "simple closed-form solution for the parameter values. It is therefore tempting to see\n",
      "if we can apply the same formalism to classiﬁcation problems. Consider a general\n",
      "classiﬁcation problem with Kclasses, with a 1-of- Kbinary coding scheme for the\n",
      "target vector t. One justiﬁcation for using least squares in such a context is that it\n",
      "approximates the conditional expectation E[t|x]of the target values given the input\n",
      "vector. For the binary coding scheme, this conditional expectation is given by the\n",
      "vector of posterior class probabilities. Unfortunately, however, these probabilities\n",
      "are typically approximated rather poorly, indeed the approximations can have values\n",
      "outside the range (0,1), due to the limited ﬂexibility of a linear model as we shall\n",
      "see shortly.\n",
      "Each class Ckis described by its own linear model so that\n",
      "yk(x)=wT\n",
      "kx+wk0 (4.13)\n",
      "where k=1,...,K . We can conveniently group these together using vector nota-\n",
      "tion so that\n",
      "y(x)=˜WT˜x (4.14)4.1. Discriminant Functions 185\n",
      "where˜Wis a matrix whose kthcolumn comprises the D+1-dimensional vector\n",
      "˜wk=(wk0,wT\n",
      "k)Tand˜xis the corresponding augmented input vector (1,xT)Twith\n",
      "a dummy input x0=1. This representation was discussed in detail in Section 3.1. A\n",
      "new input xis then assigned to the class for which the output yk=˜wT\n",
      "k˜xis largest.\n",
      "We now determine the parameter matrix ˜Wby minimizing a sum-of-squares\n",
      "error function, as we did for regression in Chapter 3. Consider a training data set\n",
      "{xn,tn}where n=1,...,N , and deﬁne a matrix Twhose nthrow is the vector tT\n",
      "n,\n",
      "together with a matrix ˜Xwhose nthrow is˜xT\n",
      "n. The sum-of-squares error function\n",
      "can then be written as\n",
      "ED(˜W)=1\n",
      "2Tr{\n",
      "(˜X˜W−T)T(˜X˜W−T)}\n",
      ". (4.15)\n",
      "Setting the derivative with respect to ˜Wto zero, and rearranging, we then obtain the\n",
      "solution for ˜Win the form\n",
      "˜W=(˜XT˜X)−1˜XTT=˜X†T (4.16)\n",
      "where˜X†is the pseudo-inverse of the matrix ˜X, as discussed in Section 3.1.1. We\n",
      "then obtain the discriminant function in the form\n",
      "y(x)=˜WT˜x=TT(\n",
      "˜X†)T\n",
      "˜x. (4.17)\n",
      "An interesting property of least-squares solutions with multiple target variables\n",
      "is that if every target vector in the training set satisﬁes some linear constraint\n",
      "aTtn+b=0 (4.18)\n",
      "for some constants aandb, then the model prediction for any value of xwill satisfy\n",
      "the same constraint so that Exercise 4.2\n",
      "aTy(x)+b=0. (4.19)\n",
      "Thus if we use a 1-of- Kcoding scheme for Kclasses, then the predictions made\n",
      "by the model will have the property that the elements of y(x)will sum to 1 for any\n",
      "value of x. However, this summation constraint alone is not sufﬁcient to allow the\n",
      "model outputs to be interpreted as probabilities because they are not constrained to\n",
      "lie within the interval (0,1).\n",
      "The least-squares approach gives an exact closed-form solution for the discrimi-\n",
      "nant function parameters. However, even as a discriminant function (where we use it\n",
      "to make decisions directly and dispense with any probabilistic interpretation) it suf-\n",
      "fers from some severe problems. We have already seen that least-squares solutions Section 2.3.7\n",
      "lack robustness to outliers, and this applies equally to the classiﬁcation application,as illustrated in Figure 4.4. Here we see that the additional data points in the right-\n",
      "hand ﬁgure produce a signiﬁcant change in the location of the decision boundary,\n",
      "even though these point would be correctly classiﬁed by the original decision bound-ary in the left-hand ﬁgure. The sum-of-squares error function penalizes predictions\n",
      "that are ‘too correct’ in that they lie a long way on the correct side of the decision186 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "−4 −2 0 2 4 6 8−8−6−4−2024\n",
      "−4 −2 0 2 4 6 8−8−6−4−2024\n",
      "Figure 4.4 The left plot shows data from two classes, denoted by red crosses and blue circles, together with\n",
      "the decision boundary found by least squares (magenta curve) and also by the logistic regression model (green\n",
      "curve), which is discussed later in Section 4.3.2. The right-hand plot shows the corresponding results obtained\n",
      "when extra data points are added at the bottom left of the diagram, showing that least squares is highly sensitive\n",
      "to outliers, unlike logistic regression.\n",
      "boundary. In Section 7.1.2, we shall consider several alternative error functions for\n",
      "classiﬁcation and we shall see that they do not suffer from this difﬁculty.\n",
      "However, problems with least squares can be more severe than simply lack of\n",
      "robustness, as illustrated in Figure 4.5. This shows a synthetic data set drawn from\n",
      "three classes in a two-dimensional input space (x1,x2), having the property that lin-\n",
      "ear decision boundaries can give excellent separation between the classes. Indeed,\n",
      "the technique of logistic regression, described later in this chapter, gives a satisfac-\n",
      "tory solution as seen in the right-hand plot. However, the least-squares solution gives\n",
      "poor results, with only a small region of the input space assigned to the green class.\n",
      "The failure of least squares should not surprise us when we recall that it cor-\n",
      "responds to maximum likelihood under the assumption of a Gaussian conditional\n",
      "distribution, whereas binary target vectors clearly have a distribution that is far from\n",
      "Gaussian. By adopting more appropriate probabilistic models, we shall obtain clas-\n",
      "siﬁcation techniques with much better properties than least squares. For the moment,\n",
      "however, we continue to explore alternative nonprobabilistic methods for setting the\n",
      "parameters in the linear classiﬁcation models.\n",
      "4.1.4 Fisher’s linear discriminant\n",
      "One way to view a linear classiﬁcation model is in terms of dimensionality\n",
      "reduction. Consider ﬁrst the case of two classes, and suppose we take the D-4.1. Discriminant Functions 187\n",
      "−6 −4 −2 0 2 4 6−6−4−20246\n",
      "−6 −4 −2 0 2 4 6−6−4−20246\n",
      "Figure 4.5 Example of a synthetic data set comprising three classes, with training data points denoted in red\n",
      "(×), green (+), and blue (◦). Lines denote the decision boundaries, and the background colours denote the\n",
      "respective classes of the decision regions. On the left is the result of using a least-squares discriminant. We see\n",
      "that the region of input space assigned to the green class is too small and so most of the points from this class\n",
      "are misclassiﬁed. On the right is the result of using logistic regressions as described in Section 4.3.2 showing\n",
      "correct classiﬁcation of the training data.\n",
      "dimensional input vector xand project it down to one dimension using\n",
      "y=wTx. (4.20)\n",
      "If we place a threshold on yand classify y⩾−w0as class C1, and otherwise class\n",
      "C2, then we obtain our standard linear classiﬁer discussed in the previous section.\n",
      "In general, the projection onto one dimension leads to a considerable loss of infor-\n",
      "mation, and classes that are well separated in the original D-dimensional space may\n",
      "become strongly overlapping in one dimension. However, by adjusting the com-\n",
      "ponents of the weight vector w, we can select a projection that maximizes the class\n",
      "separation. To begin with, consider a two-class problem in which there are N1points\n",
      "of class C1andN2points of class C2, so that the mean vectors of the two classes are\n",
      "given by\n",
      "m1=1\n",
      "N1∑\n",
      "n∈C 1xn, m2=1\n",
      "N2∑\n",
      "n∈C 2xn. (4.21)\n",
      "The simplest measure of the separation of the classes, when projected onto w,i st h e\n",
      "separation of the projected class means. This suggests that we might choose wso as\n",
      "to maximize\n",
      "m2−m1=wT(m2−m1) (4.22)\n",
      "where\n",
      "mk=wTmk (4.23)188 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "−2 2 6−2024\n",
      "−2 2 6−2024\n",
      "Figure 4.6 The left plot shows samples from two classes (depicted in red and blue) along with the histograms\n",
      "resulting from projection onto the line joining the class means. Note that there is considerable class overlap in\n",
      "the projected space. The right plot shows the corresponding projection based on the Fisher linear discriminant,\n",
      "showing the greatly improved class separation.\n",
      "is the mean of the projected data from class Ck. However, this expression can be\n",
      "made arbitrarily large simply by increasing the magnitude of w. To solve this\n",
      "problem, we could constrain wto have unit length, so that∑\n",
      "iw2\n",
      "i=1. Using\n",
      "a Lagrange multiplier to perform the constrained maximization, we then ﬁnd that Appendix E\n",
      "w∝(m2−m1). There is still a problem with this approach, however, as illustrated Exercise 4.4\n",
      "in Figure 4.6. This shows two classes that are well separated in the original two-\n",
      "dimensional space (x1,x2)but that have considerable overlap when projected onto\n",
      "the line joining their means. This difﬁculty arises from the strongly nondiagonal\n",
      "covariances of the class distributions. The idea proposed by Fisher is to maximize\n",
      "a function that will give a large separation between the projected class means while\n",
      "also giving a small variance within each class, thereby minimizing the class overlap.\n",
      "The projection formula (4.20) transforms the set of labelled data points in x\n",
      "into a labelled set in the one-dimensional space y. The within-class variance of the\n",
      "transformed data from class Ckis therefore given by\n",
      "s2\n",
      "k=∑\n",
      "n∈Ck(yn−mk)2(4.24)\n",
      "where yn=wTxn. We can deﬁne the total within-class variance for the whole\n",
      "data set to be simply s2\n",
      "1+s2\n",
      "2. The Fisher criterion is deﬁned to be the ratio of the\n",
      "between-class variance to the within-class variance and is given by\n",
      "J(w)=(m2−m1)2\n",
      "s2\n",
      "1+s2\n",
      "2. (4.25)\n",
      "We can make the dependence on wexplicit by using (4.20), (4.23), and (4.24) to\n",
      "rewrite the Fisher criterion in the form Exercise 4.54.1. Discriminant Functions 189\n",
      "J(w)=wTSBw\n",
      "wTSWw(4.26)\n",
      "whereSBis the between-class covariance matrix and is given by\n",
      "SB=(m2−m1)(m2−m1)T(4.27)\n",
      "andSWis the total within-class covariance matrix, given by\n",
      "SW=∑\n",
      "n∈C 1(xn−m1)(xn−m1)T+∑\n",
      "n∈C 2(xn−m2)(xn−m2)T. (4.28)\n",
      "Differentiating (4.26) with respect to w, we ﬁnd that J(w)is maximized when\n",
      "(wTSBw)SWw=(wTSWw)SBw. (4.29)\n",
      "From (4.27), we see that SBwis always in the direction of (m2−m1). Furthermore,\n",
      "we do not care about the magnitude of w, only its direction, and so we can drop the\n",
      "scalar factors (wTSBw)and(wTSWw). Multiplying both sides of (4.29) by S−1\n",
      "W\n",
      "we then obtain\n",
      "w∝S−1\n",
      "W(m2−m1). (4.30)\n",
      "Note that if the within-class covariance is isotropic, so that SWis proportional to the\n",
      "unit matrix, we ﬁnd that wis proportional to the difference of the class means, as\n",
      "discussed above.\n",
      "The result (4.30) is known as Fisher’s linear discriminant , although strictly it\n",
      "is not a discriminant but rather a speciﬁc choice of direction for projection of the\n",
      "data down to one dimension. However, the projected data can subsequently be used\n",
      "to construct a discriminant, by choosing a threshold y0so that we classify a new\n",
      "point as belonging to C1ify(x)⩾y0and classify it as belonging to C2otherwise.\n",
      "For example, we can model the class-conditional densities p(y|Ck)using Gaussian\n",
      "distributions and then use the techniques of Section 1.2.4 to ﬁnd the parametersof the Gaussian distributions by maximum likelihood. Having found Gaussian ap-\n",
      "proximations to the projected classes, the formalism of Section 1.5.1 then gives an\n",
      "expression for the optimal threshold. Some justiﬁcation for the Gaussian assumptioncomes from the central limit theorem by noting that y=w\n",
      "Txis the sum of a set of\n",
      "random variables.\n",
      "4.1.5 Relation to least squares\n",
      "The least-squares approach to the determination of a linear discriminant was\n",
      "based on the goal of making the model predictions as close as possible to a set of\n",
      "target values. By contrast, the Fisher criterion was derived by requiring maximum\n",
      "class separation in the output space. It is interesting to see the relationship between\n",
      "these two approaches. In particular, we shall show that, for the two-class problem,the Fisher criterion can be obtained as a special case of least squares.\n",
      "So far we have considered 1-of- Kcoding for the target values. If, however, we\n",
      "adopt a slightly different target coding scheme, then the least-squares solution for190 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "the weights becomes equivalent to the Fisher solution (Duda and Hart, 1973). In\n",
      "particular, we shall take the targets for class C1to beN/N 1, where N1is the number\n",
      "of patterns in class C1, andNis the total number of patterns. This target value\n",
      "approximates the reciprocal of the prior probability for class C1. For class C2,w e\n",
      "shall take the targets to be −N/N 2, where N2is the number of patterns in class C2.\n",
      "The sum-of-squares error function can be written\n",
      "E=1\n",
      "2N∑\n",
      "n=1(\n",
      "wTxn+w0−tn)2. (4.31)\n",
      "Setting the derivatives of Ewith respect to w0andwto zero, we obtain respectively\n",
      "N∑\n",
      "n=1(\n",
      "wTxn+w0−tn)\n",
      "=0 (4.32)\n",
      "N∑\n",
      "n=1(\n",
      "wTxn+w0−tn)\n",
      "xn=0. (4.33)\n",
      "From (4.32), and making use of our choice of target coding scheme for the tn,w e\n",
      "obtain an expression for the bias in the form\n",
      "w0=−wTm (4.34)\n",
      "where we have used\n",
      "N∑\n",
      "n=1tn=N1N\n",
      "N1−N2N\n",
      "N2=0 (4.35)\n",
      "and where mis the mean of the total data set and is given by\n",
      "m=1\n",
      "NN∑\n",
      "n=1xn=1\n",
      "N(N1m1+N2m2). (4.36)\n",
      "After some straightforward algebra, and again making use of the choice of tn, the\n",
      "second equation (4.33) becomes Exercise 4.6\n",
      "(\n",
      "SW+N1N2\n",
      "NSB)\n",
      "w=N(m1−m2) (4.37)\n",
      "whereSWis deﬁned by (4.28), SBis deﬁned by (4.27), and we have substituted for\n",
      "the bias using (4.34). Using (4.27), we note that SBwis always in the direction of\n",
      "(m2−m1). Thus we can write\n",
      "w∝S−1\n",
      "W(m2−m1) (4.38)\n",
      "where we have ignored irrelevant scale factors. Thus the weight vector coincides\n",
      "with that found from the Fisher criterion. In addition, we have also found an expres-sion for the bias value w\n",
      "0given by (4.34). This tells us that a new vector xshould be\n",
      "classiﬁed as belonging to class C1ify(x)=wT(x−m)>0and class C2otherwise.4.1. Discriminant Functions 191\n",
      "4.1.6 Fisher’s discriminant for multiple classes\n",
      "We now consider the generalization of the Fisher discriminant to K>2classes,\n",
      "and we shall assume that the dimensionality Dof the input space is greater than the\n",
      "number Kof classes. Next, we introduce D′>1linear ‘features’ yk=wT\n",
      "kx, where\n",
      "k=1,...,D′. These feature values can conveniently be grouped together to form\n",
      "a vector y. Similarly, the weight vectors {wk}can be considered to be the columns\n",
      "of a matrix W, so that\n",
      "y=WTx. (4.39)\n",
      "Note that again we are not including any bias parameters in the deﬁnition of y. The\n",
      "generalization of the within-class covariance matrix to the case of Kclasses follows\n",
      "from (4.28) to give\n",
      "SW=K∑\n",
      "k=1Sk (4.40)\n",
      "where\n",
      "Sk=∑\n",
      "n∈Ck(xn−mk)(xn−mk)T(4.41)\n",
      "mk=1\n",
      "Nk∑\n",
      "n∈Ckxn (4.42)\n",
      "andNkis the number of patterns in class Ck. In order to ﬁnd a generalization of the\n",
      "between-class covariance matrix, we follow Duda and Hart (1973) and consider ﬁrst\n",
      "the total covariance matrix\n",
      "ST=N∑\n",
      "n=1(xn−m)(xn−m)T(4.43)\n",
      "wheremis the mean of the total data set\n",
      "m=1\n",
      "NN∑\n",
      "n=1xn=1\n",
      "NK∑\n",
      "k=1Nkmk (4.44)\n",
      "andN=∑\n",
      "kNkis the total number of data points. The total covariance matrix can\n",
      "be decomposed into the sum of the within-class covariance matrix, given by (4.40)\n",
      "and (4.41), plus an additional matrix SB, which we identify as a measure of the\n",
      "between-class covariance\n",
      "ST=SW+SB (4.45)\n",
      "where\n",
      "SB=K∑\n",
      "k=1Nk(mk−m)(mk−m)T. (4.46)192 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "These covariance matrices have been deﬁned in the original x-space. We can now\n",
      "deﬁne similar matrices in the projected D′-dimensional y-space\n",
      "sW=K∑\n",
      "k=1∑\n",
      "n∈Ck(yn−µk)(yn−µk)T(4.47)\n",
      "and\n",
      "sB=K∑\n",
      "k=1Nk(µk−µ)(µk−µ)T(4.48)\n",
      "where\n",
      "µk=1\n",
      "Nk∑\n",
      "n∈Ckyn, µ=1\n",
      "NK∑\n",
      "k=1Nkµk. (4.49)\n",
      "Again we wish to construct a scalar that is large when the between-class covariance\n",
      "is large and when the within-class covariance is small. There are now many possible\n",
      "choices of criterion (Fukunaga, 1990). One example is given by\n",
      "J(W)=Tr{\n",
      "s−1\n",
      "WsB}\n",
      ". (4.50)\n",
      "This criterion can then be rewritten as an explicit function of the projection matrix\n",
      "Win the form\n",
      "J(w)=Tr{\n",
      "(WSWWT)−1(WSBWT)}\n",
      ". (4.51)\n",
      "Maximization of such criteria is straightforward, though somewhat involved, and is\n",
      "discussed at length in Fukunaga (1990). The weight values are determined by thoseeigenvectors of S\n",
      "−1\n",
      "WSBthat correspond to the D′largest eigenvalues.\n",
      "There is one important result that is common to all such criteria, which is worth\n",
      "emphasizing. We ﬁrst note from (4.46) that SBis composed of the sum of Kma-\n",
      "trices, each of which is an outer product of two vectors and therefore of rank 1. In\n",
      "addition, only (K−1)of these matrices are independent as a result of the constraint\n",
      "(4.44). Thus, SBhas rank at most equal to (K−1)and so there are at most (K−1)\n",
      "nonzero eigenvalues. This shows that the projection onto the (K−1)-dimensional\n",
      "subspace spanned by the eigenvectors of SBdoes not alter the value of J(w), and\n",
      "so we are therefore unable to ﬁnd more than (K−1)linear ‘features’ by this means\n",
      "(Fukunaga, 1990).\n",
      "4.1.7 The perceptron algorithm\n",
      "Another example of a linear discriminant model is the perceptron of Rosenblatt\n",
      "(1962), which occupies an important place in the history of pattern recognition al-\n",
      "gorithms. It corresponds to a two-class model in which the input vector xis ﬁrst\n",
      "transformed using a ﬁxed nonlinear transformation to give a feature vector φ(x),\n",
      "and this is then used to construct a generalized linear model of the form\n",
      "y(x)=f(\n",
      "wTφ(x))\n",
      "(4.52)4.1. Discriminant Functions 193\n",
      "where the nonlinear activation function f(·)is given by a step function of the form\n",
      "f(a)={\n",
      "+1,a⩾0\n",
      "−1,a < 0.(4.53)\n",
      "The vector φ(x)will typically include a bias component φ0(x)=1 . In earlier\n",
      "discussions of two-class classiﬁcation problems, we have focussed on a target coding\n",
      "scheme in which t∈{0,1}, which is appropriate in the context of probabilistic\n",
      "models. For the perceptron, however, it is more convenient to use target values\n",
      "t=+ 1 for class C1andt=−1for class C2, which matches the choice of activation\n",
      "function.\n",
      "The algorithm used to determine the parameters wof the perceptron can most\n",
      "easily be motivated by error function minimization. A natural choice of error func-\n",
      "tion would be the total number of misclassiﬁed patterns. However, this does not leadto a simple learning algorithm because the error is a piecewise constant function\n",
      "ofw, with discontinuities wherever a change in wcauses the decision boundary to\n",
      "move across one of the data points. Methods based on changing wusing the gradi-\n",
      "ent of the error function cannot then be applied, because the gradient is zero almost\n",
      "everywhere.\n",
      "We therefore consider an alternative error function known as the perceptron cri-\n",
      "terion . To derive this, we note that we are seeking a weight vector wsuch that\n",
      "patterns x\n",
      "nin class C1will have wTφ(xn)>0, whereas patterns xnin class C2\n",
      "havewTφ(xn)<0. Using the t∈{ −1,+1}target coding scheme it follows that\n",
      "we would like all patterns to satisfy wTφ(xn)tn>0. The perceptron criterion\n",
      "associates zero error with any pattern that is correctly classiﬁed, whereas for a mis-\n",
      "classiﬁed pattern xnit tries to minimize the quantity −wTφ(xn)tn. The perceptron\n",
      "criterion is therefore given by\n",
      "EP(w)=−∑\n",
      "n∈MwTφntn (4.54)\n",
      "Frank Rosenblatt\n",
      "1928–1969\n",
      "Rosenblatt’s perceptron played an\n",
      "important role in the history of ma-chine learning. Initially, Rosenblattsimulated the perceptron on an IBM704 computer at Cornell in 1957,but by the early 1960s he had built\n",
      "special-purpose hardware that provided a direct, par-allel implementation of perceptron learning. Many ofhis ideas were encapsulated in “Principles of Neuro-dynamics: Perceptrons and the Theory of Brain Mech-anisms” published in 1962. Rosenblatt’s work wascriticized by Marvin Minksy, whose objections werepublished in the book “Perceptrons”, co-authored withSeymour Papert. This book was widely misinter-\n",
      "preted at the time as showing that neural networkswere fatally ﬂawed and could only learn solutions forlinearly separable problems. In fact, it only provedsuch limitations in the case of single-layer networkssuch as the perceptron and merely conjectured (in-correctly) that they applied to more general networkmodels. Unfortunately, however, this book contributedto the substantial decline in research funding for neu-ral computing, a situation that was not reversed un-til the mid-1980s. Today, there are many hundreds,if not thousands, of applications of neural networksin widespread use, with examples in areas such ashandwriting recognition and information retrieval be-ing used routinely by millions of people.194 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "where Mdenotes the set of all misclassiﬁed patterns. The contribution to the error\n",
      "associated with a particular misclassiﬁed pattern is a linear function of win regions\n",
      "ofwspace where the pattern is misclassiﬁed and zero in regions where it is correctly\n",
      "classiﬁed. The total error function is therefore piecewise linear.\n",
      "We now apply the stochastic gradient descent algorithm to this error function. Section 3.1.3\n",
      "The change in the weight vector wis then given by\n",
      "w(τ+1)=w(τ)−η∇EP(w)=w(τ)+ηφntn (4.55)\n",
      "where ηis the learning rate parameter and τis an integer that indexes the steps of\n",
      "the algorithm. Because the perceptron function y(x,w)is unchanged if we multiply\n",
      "wby a constant, we can set the learning rate parameter ηequal to 1 without of\n",
      "generality. Note that, as the weight vector evolves during training, the set of patternsthat are misclassiﬁed will change.\n",
      "The perceptron learning algorithm has a simple interpretation, as follows. We\n",
      "cycle through the training patterns in turn, and for each pattern x\n",
      "nwe evaluate the\n",
      "perceptron function (4.52). If the pattern is correctly classiﬁed, then the weight\n",
      "vector remains unchanged, whereas if it is incorrectly classiﬁed, then for class C1\n",
      "we add the vector φ(xn)onto the current estimate of weight vector wwhile for\n",
      "classC2we subtract the vector φ(xn)fromw. The perceptron learning algorithm is\n",
      "illustrated in Figure 4.7.\n",
      "If we consider the effect of a single update in the perceptron learning algorithm,\n",
      "we see that the contribution to the error from a misclassiﬁed pattern will be reduced\n",
      "because from (4.55) we have\n",
      "−w(τ+1)Tφntn=−w(τ)Tφntn−(φntn)Tφntn<−w(τ)Tφntn (4.56)\n",
      "where we have set η=1, and made use of ∥φntn∥2>0. Of course, this does\n",
      "not imply that the contribution to the error function from the other misclassiﬁedpatterns will have been reduced. Furthermore, the change in weight vector may have\n",
      "caused some previously correctly classiﬁed patterns to become misclassiﬁed. Thus\n",
      "the perceptron learning rule is not guaranteed to reduce the total error function ateach stage.\n",
      "However, the perceptron convergence theorem states that if there exists an ex-\n",
      "act solution (in other words, if the training data set is linearly separable), then theperceptron learning algorithm is guaranteed to ﬁnd an exact solution in a ﬁnite num-\n",
      "ber of steps. Proofs of this theorem can be found for example in Rosenblatt (1962),\n",
      "Block (1962), Nilsson (1965), Minsky and Papert (1969), Hertz et al. (1991), and\n",
      "Bishop (1995a). Note, however, that the number of steps required to achieve con-\n",
      "vergence could still be substantial, and in practice, until convergence is achieved,\n",
      "we will not be able to distinguish between a nonseparable problem and one that is\n",
      "simply slow to converge.\n",
      "Even when the data set is linearly separable, there may be many solutions, and\n",
      "which one is found will depend on the initialization of the parameters and on the or-\n",
      "der of presentation of the data points. Furthermore, for data sets that are not linearly\n",
      "separable, the perceptron learning algorithm will never converge.4.1. Discriminant Functions 195\n",
      "−1 −0.5 0 0.5 1−1−0.500.51\n",
      "−1 −0.5 0 0.5 1−1−0.500.51\n",
      "−1 −0.5 0 0.5 1−1−0.500.51\n",
      "−1 −0.5 0 0.5 1−1−0.500.51\n",
      "Figure 4.7 Illustration of the convergence of the perceptron learning algorithm, showing data points from two\n",
      "classes (red and blue) in a two-dimensional feature space (φ1,φ2). The top left plot shows the initial parameter\n",
      "vector wshown as a black arrow together with the corresponding decision boundary (black line), in which the\n",
      "arrow points towards the decision region which classiﬁed as belonging to the red class. The data point circled\n",
      "in green is misclassiﬁed and so its feature vector is added to the current weight vector, giving the new decision\n",
      "boundary shown in the top right plot. The bottom left plot shows the next misclassiﬁed point to be considered,\n",
      "indicated by the green circle, and its feature vector is again added to the weight vector giving the decision\n",
      "boundary shown in the bottom right plot for which all data points are correctly classiﬁed.196 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs\n",
      "were obtained using a simple camera system in which an input scene, in this case a printed character, wasilluminated by powerful lights, and an image focussed onto a 20×20array of cadmium sulphide photocells,\n",
      "giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph,which allowed different conﬁgurations of input features to be tried. Often these were wired up at random todemonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a moderndigital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight wasimplemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor therebyallowing the value of the weight to be adjusted automatically by the learning algorithm.\n",
      "Aside from difﬁculties with the learning algorithm, the perceptron does not pro-\n",
      "vide probabilistic outputs, nor does it generalize readily to K>2classes. The most\n",
      "important limitation, however, arises from the fact that (in common with all of themodels discussed in this chapter and the previous one) it is based on linear com-\n",
      "binations of ﬁxed basis functions. More detailed discussions of the limitations of\n",
      "perceptrons can be found in Minsky and Papert (1969) and Bishop (1995a).\n",
      "Analogue hardware implementations of the perceptron were built by Rosenblatt,\n",
      "based on motor-driven variable resistors to implement the adaptive parameters w\n",
      "j.\n",
      "These are illustrated in Figure 4.8. The inputs were obtained from a simple camera\n",
      "system based on an array of photo-sensors, while the basis functions φcould be\n",
      "chosen in a variety of ways, for example based on simple ﬁxed functions of randomlychosen subsets of pixels from the input image. Typical applications involved learning\n",
      "to discriminate simple shapes or characters.\n",
      "At the same time that the perceptron was being developed, a closely related\n",
      "system called the adaline , which is short for ‘adaptive linear element’, was being\n",
      "explored by Widrow and co-workers. The functional form of the model was the same\n",
      "as for the perceptron, but a different approach to training was adopted (Widrow andHoff, 1960; Widrow and Lehr, 1990).\n",
      "4.2. Probabilistic Generative Models\n",
      "We turn next to a probabilistic view of classiﬁcation and show how models withlinear decision boundaries arise from simple assumptions about the distribution of\n",
      "the data. In Section 1.5.4, we discussed the distinction between the discriminative\n",
      "and the generative approaches to classiﬁcation. Here we shall adopt a generative4.2. Probabilistic Generative Models 197\n",
      "Figure 4.9 Plot of the logistic sigmoid function\n",
      "σ(a)deﬁned by (4.59), shown in\n",
      "red, together with the scaled pro-\n",
      "bit function Φ(λa),f o rλ2=π/8,\n",
      "shown in dashed blue, where Φ(a)\n",
      "is deﬁned by (4.114). The scal-\n",
      "ing factor π/8is chosen so that the\n",
      "derivatives of the two curves are\n",
      "equal for a=0.\n",
      "−5 0 500.51\n",
      "approach in which we model the class-conditional densities p(x|Ck), as well as the\n",
      "class priors p(Ck), and then use these to compute posterior probabilities p(Ck|x)\n",
      "through Bayes’ theorem.\n",
      "Consider ﬁrst of all the case of two classes. The posterior probability for class\n",
      "C1can be written as\n",
      "p(C1|x)=p(x|C1)p(C1)\n",
      "p(x|C1)p(C1)+p(x|C2)p(C2)\n",
      "=1\n",
      "1+e x p ( −a)=σ(a) (4.57)\n",
      "where we have deﬁned\n",
      "a=l np(x|C1)p(C1)\n",
      "p(x|C2)p(C2)(4.58)\n",
      "andσ(a)is the logistic sigmoid function deﬁned by\n",
      "σ(a)=1\n",
      "1+e x p ( −a)(4.59)\n",
      "which is plotted in Figure 4.9. The term ‘sigmoid’ means S-shaped. This type of\n",
      "function is sometimes also called a ‘squashing function’ because it maps the whole\n",
      "real axis into a ﬁnite interval. The logistic sigmoid has been encountered already\n",
      "in earlier chapters and plays an important role in many classiﬁcation algorithms. It\n",
      "satisﬁes the following symmetry property\n",
      "σ(−a)=1−σ(a) (4.60)\n",
      "as is easily veriﬁed. The inverse of the logistic sigmoid is given by\n",
      "a=l n(σ\n",
      "1−σ)\n",
      "(4.61)\n",
      "and is known as the logit function. It represents the log of the ratio of probabilities\n",
      "ln[p(C1|x)/p(C2|x)]for the two classes, also known as the log odds .198 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "Note that in (4.57) we have simply rewritten the posterior probabilities in an\n",
      "equivalent form, and so the appearance of the logistic sigmoid may seem rather vac-uous. However, it will have signiﬁcance provided a(x)takes a simple functional\n",
      "form. We shall shortly consider situations in which a(x)is a linear function of x,i n\n",
      "which case the posterior probability is governed by a generalized linear model.\n",
      "For the case of K>2classes, we have\n",
      "p(C\n",
      "k|x)=p(x|Ck)p(Ck)∑\n",
      "jp(x|Cj)p(Cj)\n",
      "=exp(ak)∑\n",
      "jexp(aj)(4.62)\n",
      "which is known as the normalized exponential and can be regarded as a multiclass\n",
      "generalization of the logistic sigmoid. Here the quantities akare deﬁned by\n",
      "ak=l np(x|Ck)p(Ck). (4.63)\n",
      "The normalized exponential is also known as the softmax function , as it represents\n",
      "a smoothed version of the ‘max’ function because, if ak≫ajfor all j̸=k, then\n",
      "p(Ck|x)≃1, andp(Cj|x)≃0.\n",
      "We now investigate the consequences of choosing speciﬁc forms for the class-\n",
      "conditional densities, looking ﬁrst at continuous input variables xand then dis-\n",
      "cussing brieﬂy the case of discrete inputs.\n",
      "4.2.1 Continuous inputs\n",
      "Let us assume that the class-conditional densities are Gaussian and then explore\n",
      "the resulting form for the posterior probabilities. To start with, we shall assume that\n",
      "all classes share the same covariance matrix. Thus the density for class Ckis given\n",
      "by\n",
      "p(x|Ck)=1\n",
      "(2π)D/21\n",
      "|Σ|1/2exp{\n",
      "−1\n",
      "2(x−µk)TΣ−1(x−µk)}\n",
      ". (4.64)\n",
      "Consider ﬁrst the case of two classes. From (4.57) and (4.58), we have\n",
      "p(C1|x)=σ(wTx+w0) (4.65)\n",
      "where we have deﬁned\n",
      "w=Σ−1(µ1−µ2) (4.66)\n",
      "w0=−1\n",
      "2µT\n",
      "1Σ−1µ1+1\n",
      "2µT\n",
      "2Σ−1µ2+l np(C1)\n",
      "p(C2). (4.67)\n",
      "We see that the quadratic terms in xfrom the exponents of the Gaussian densities\n",
      "have cancelled (due to the assumption of common covariance matrices) leading toa linear function of xin the argument of the logistic sigmoid. This result is illus-\n",
      "trated for the case of a two-dimensional input space xin Figure 4.10. The resulting4.2. Probabilistic Generative Models 199\n",
      "Figure 4.10 The left-hand plot shows the class-conditional densities for two classes, denoted red and blue.\n",
      "On the right is the corresponding posterior probability p(C1|x), which is given by a logistic sigmoid of a linear\n",
      "function of x. The surface in the right-hand plot is coloured using a proportion of red ink given by p(C1|x)and a\n",
      "proportion of blue ink given by p(C2|x)=1−p(C1|x).\n",
      "decision boundaries correspond to surfaces along which the posterior probabilities\n",
      "p(Ck|x)are constant and so will be given by linear functions of x, and therefore\n",
      "the decision boundaries are linear in input space. The prior probabilities p(Ck)enter\n",
      "only through the bias parameter w0so that changes in the priors have the effect of\n",
      "making parallel shifts of the decision boundary and more generally of the parallel\n",
      "contours of constant posterior probability.\n",
      "For the general case of Kclasses we have, from (4.62) and (4.63),\n",
      "ak(x)=wT\n",
      "kx+wk0 (4.68)\n",
      "where we have deﬁned\n",
      "wk=Σ−1µk (4.69)\n",
      "wk0=−1\n",
      "2µT\n",
      "kΣ−1µk+l np(Ck). (4.70)\n",
      "We see that the ak(x)are again linear functions of xas a consequence of the cancel-\n",
      "lation of the quadratic terms due to the shared covariances. The resulting decisionboundaries, corresponding to the minimum misclassiﬁcation rate, will occur when\n",
      "two of the posterior probabilities (the two largest) are equal, and so will be deﬁnedby linear functions of x, and so again we have a generalized linear model.\n",
      "If we relax the assumption of a shared covariance matrix and allow each class-\n",
      "conditional density p(x|C\n",
      "k)to have its own covariance matrix Σk, then the earlier\n",
      "cancellations will no longer occur, and we will obtain quadratic functions of x,g i v -\n",
      "ing rise to a quadratic discriminant . The linear and quadratic decision boundaries\n",
      "are illustrated in Figure 4.11.200 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "−2 −1 0 1 2−2.5−2−1.5−1−0.500.511.522.5\n",
      "Figure 4.11 The left-hand plot shows the class-conditional densities for three classes each having a Gaussian\n",
      "distribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix.\n",
      "The right-hand plot shows the corresponding posterior probabilities, in which the RGB colour vector represents\n",
      "the posterior probabilities for the respective three classes. The decision boundaries are also shown. Notice that\n",
      "the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas\n",
      "those between the other pairs of classes are quadratic.\n",
      "4.2.2 Maximum likelihood solution\n",
      "Once we have speciﬁed a parametric functional form for the class-conditional\n",
      "densities p(x|Ck), we can then determine the values of the parameters, together with\n",
      "the prior class probabilities p(Ck), using maximum likelihood. This requires a data\n",
      "set comprising observations of xalong with their corresponding class labels.\n",
      "Consider ﬁrst the case of two classes, each having a Gaussian class-conditional\n",
      "density with a shared covariance matrix, and suppose we have a data set {xn,tn}\n",
      "where n=1,...,N . Here tn=1denotes class C1andtn=0denotes class C2.W e\n",
      "denote the prior class probability p(C1)=π, so that p(C2)=1−π. For a data point\n",
      "xnfrom class C1,w eh a v e tn=1and hence\n",
      "p(xn,C1)=p(C1)p(xn|C1)=πN(xn|µ1,Σ).\n",
      "Similarly for class C2,w eh a v e tn=0and hence\n",
      "p(xn,C2)=p(C2)p(xn|C2)=( 1 −π)N(xn|µ2,Σ).\n",
      "Thus the likelihood function is given by\n",
      "p(t|π,µ1,µ2,Σ)=N∏\n",
      "n=1[πN(xn|µ1,Σ)]tn[(1−π)N(xn|µ2,Σ)]1−tn(4.71)\n",
      "where t=(t1,...,t N)T. As usual, it is convenient to maximize the log of the\n",
      "likelihood function. Consider ﬁrst the maximization with respect to π. The terms in4.2. Probabilistic Generative Models 201\n",
      "the log likelihood function that depend on πare\n",
      "N∑\n",
      "n=1{tnlnπ+( 1−tn)l n ( 1−π)}. (4.72)\n",
      "Setting the derivative with respect to πequal to zero and rearranging, we obtain\n",
      "π=1\n",
      "NN∑\n",
      "n=1tn=N1\n",
      "N=N1\n",
      "N1+N2(4.73)\n",
      "where N1denotes the total number of data points in class C1, andN2denotes the total\n",
      "number of data points in class C2. Thus the maximum likelihood estimate for πis\n",
      "simply the fraction of points in class C1as expected. This result is easily generalized\n",
      "to the multiclass case where again the maximum likelihood estimate of the prior\n",
      "probability associated with class Ckis given by the fraction of the training set points\n",
      "assigned to that class. Exercise 4.9\n",
      "Now consider the maximization with respect to µ1. Again we can pick out of\n",
      "the log likelihood function those terms that depend on µ1giving\n",
      "N∑\n",
      "n=1tnlnN(xn|µ1,Σ)=−1\n",
      "2N∑\n",
      "n=1tn(xn−µ1)TΣ−1(xn−µ1)+c o n s t .(4.74)\n",
      "Setting the derivative with respect to µ1to zero and rearranging, we obtain\n",
      "µ1=1\n",
      "N1N∑\n",
      "n=1tnxn (4.75)\n",
      "which is simply the mean of all the input vectors xnassigned to class C1.B y a\n",
      "similar argument, the corresponding result for µ2is given by\n",
      "µ2=1\n",
      "N2N∑\n",
      "n=1(1−tn)xn (4.76)\n",
      "which again is the mean of all the input vectors xnassigned to class C2.\n",
      "Finally, consider the maximum likelihood solution for the shared covariance\n",
      "matrixΣ. Picking out the terms in the log likelihood function that depend on Σ,w e\n",
      "have\n",
      "−1\n",
      "2N∑\n",
      "n=1tnln|Σ|−1\n",
      "2N∑\n",
      "n=1tn(xn−µ1)TΣ−1(xn−µ1)\n",
      "−1\n",
      "2N∑\n",
      "n=1(1−tn)l n|Σ|−1\n",
      "2N∑\n",
      "n=1(1−tn)(xn−µ2)TΣ−1(xn−µ2)\n",
      "=−N\n",
      "2ln|Σ|−N\n",
      "2Tr{\n",
      "Σ−1S}\n",
      "(4.77)202 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "where we have deﬁned\n",
      "S=N1\n",
      "NS1+N2\n",
      "NS2 (4.78)\n",
      "S1=1\n",
      "N1∑\n",
      "n∈C 1(xn−µ1)(xn−µ1)T(4.79)\n",
      "S2=1\n",
      "N2∑\n",
      "n∈C 2(xn−µ2)(xn−µ2)T. (4.80)\n",
      "Using the standard result for the maximum likelihood solution for a Gaussian distri-\n",
      "bution, we see that Σ=S, which represents a weighted average of the covariance\n",
      "matrices associated with each of the two classes separately.\n",
      "This result is easily extended to the Kclass problem to obtain the corresponding\n",
      "maximum likelihood solutions for the parameters in which each class-conditional\n",
      "density is Gaussian with a shared covariance matrix. Note that the approach of ﬁtting Exercise 4.10\n",
      "Gaussian distributions to the classes is not robust to outliers, because the maximum\n",
      "likelihood estimation of a Gaussian is not robust. Section 2.3.7\n",
      "4.2.3 Discrete features\n",
      "Let us now consider the case of discrete feature values xi. For simplicity, we\n",
      "begin by looking at binary feature values xi∈{0,1}and discuss the extension to\n",
      "more general discrete features shortly. If there are Dinputs, then a general distribu-\n",
      "tion would correspond to a table of 2Dnumbers for each class, containing 2D−1\n",
      "independent variables (due to the summation constraint). Because this grows expo-\n",
      "nentially with the number of features, we might seek a more restricted representa-tion. Here we will make the naive Bayes assumption in which the feature values are Section 8.2.2\n",
      "treated as independent, conditioned on the class C\n",
      "k. Thus we have class-conditional\n",
      "distributions of the form\n",
      "p(x|Ck)=D∏\n",
      "i=1µxi\n",
      "ki(1−µki)1−xi(4.81)\n",
      "which contain Dindependent parameters for each class. Substituting into (4.63) then\n",
      "gives\n",
      "ak(x)=D∑\n",
      "i=1{xilnµki+( 1−xi)l n ( 1−µki)}+l np(Ck) (4.82)\n",
      "which again are linear functions of the input values xi. For the case of K=2classes,\n",
      "we can alternatively consider the logistic sigmoid formulation given by (4.57). Anal-\n",
      "ogous results are obtained for discrete variables each of which can take M> 2\n",
      "states. Exercise 4.11\n",
      "4.2.4 Exponential family\n",
      "As we have seen, for both Gaussian distributed and discrete inputs, the posterior\n",
      "class probabilities are given by generalized linear models with logistic sigmoid ( K=4.3. Probabilistic Discriminative Models 203\n",
      "2classes) or softmax ( K⩾2classes) activation functions. These are particular cases\n",
      "of a more general result obtained by assuming that the class-conditional densitiesp(x|C\n",
      "k)are members of the exponential family of distributions.\n",
      "Using the form (2.194) for members of the exponential family, we see that the\n",
      "distribution of xcan be written in the form\n",
      "p(x|λk)=h(x)g(λk)e x p{\n",
      "λT\n",
      "ku(x)}\n",
      ". (4.83)\n",
      "We now restrict attention to the subclass of such distributions for which u(x)=x.\n",
      "Then we make use of (2.236) to introduce a scaling parameter s, so that we obtain\n",
      "the restricted set of exponential family class-conditional densities of the form\n",
      "p(x|λk,s)=1\n",
      "sh(1\n",
      "sx)\n",
      "g(λk)e x p{1\n",
      "sλT\n",
      "kx}\n",
      ". (4.84)\n",
      "Note that we are allowing each class to have its own parameter vector λkbut we are\n",
      "assuming that the classes share the same scale parameter s.\n",
      "For the two-class problem, we substitute this expression for the class-conditional\n",
      "densities into (4.58) and we see that the posterior class probability is again given bya logistic sigmoid acting on a linear function a(x)which is given by\n",
      "a(x)=(λ\n",
      "1−λ2)Tx+l ng(λ1)−lng(λ2)+l n p(C1)−lnp(C2). (4.85)\n",
      "Similarly, for the K-class problem, we substitute the class-conditional density ex-\n",
      "pression into (4.63) to give\n",
      "ak(x)=λT\n",
      "kx+l ng(λk)+l n p(Ck) (4.86)\n",
      "and so again is a linear function of x.\n",
      "4.3. Probabilistic Discriminative Models\n",
      "For the two-class classiﬁcation problem, we have seen that the posterior probability\n",
      "of class C1can be written as a logistic sigmoid acting on a linear function of x, for a\n",
      "wide choice of class-conditional distributions p(x|Ck). Similarly, for the multiclass\n",
      "case, the posterior probability of class Ckis given by a softmax transformation of a\n",
      "linear function of x. For speciﬁc choices of the class-conditional densities p(x|Ck),\n",
      "we have used maximum likelihood to determine the parameters of the densities as\n",
      "well as the class priors p(Ck)and then used Bayes’ theorem to ﬁnd the posterior class\n",
      "probabilities.\n",
      "However, an alternative approach is to use the functional form of the generalized\n",
      "linear model explicitly and to determine its parameters directly by using maximum\n",
      "likelihood. We shall see that there is an efﬁcient algorithm ﬁnding such solutions\n",
      "known as iterative reweighted least squares ,o r IRLS .\n",
      "The indirect approach to ﬁnding the parameters of a generalized linear model,\n",
      "by ﬁtting class-conditional densities and class priors separately and then applying204 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "x1x2\n",
      "−1 0 1−101\n",
      "φ1φ2\n",
      "0 0.5 100.51\n",
      "Figure 4.12 Illustration of the role of nonlinear basis functions in linear classiﬁcation models. The left plot\n",
      "shows the original input space (x1,x2)together with data points from two classes labelled red and blue. Two\n",
      "‘Gaussian’ basis functions φ1(x)andφ2(x)are deﬁned in this space with centres shown by the green crosses\n",
      "and with contours shown by the green circles. The right-hand plot shows the corresponding feature space\n",
      "(φ1,φ2)together with the linear decision boundary obtained given by a logistic regression model of the form\n",
      "discussed in Section 4.3.2. This corresponds to a nonlinear decision boundary in the original input space,\n",
      "shown by the black curve in the left-hand plot.\n",
      "Bayes’ theorem, represents an example of generative modelling, because we could\n",
      "take such a model and generate synthetic data by drawing values of xfrom the\n",
      "marginal distribution p(x). In the direct approach, we are maximizing a likelihood\n",
      "function deﬁned through the conditional distribution p(Ck|x), which represents a\n",
      "form of discriminative training. One advantage of the discriminative approach is\n",
      "that there will typically be fewer adaptive parameters to be determined, as we shall\n",
      "see shortly. It may also lead to improved predictive performance, particularly when\n",
      "the class-conditional density assumptions give a poor approximation to the true dis-\n",
      "tributions.\n",
      "4.3.1 Fixed basis functions\n",
      "So far in this chapter, we have considered classiﬁcation models that work di-\n",
      "rectly with the original input vector x. However, all of the algorithms are equally\n",
      "applicable if we ﬁrst make a ﬁxed nonlinear transformation of the inputs using a\n",
      "vector of basis functions φ(x). The resulting decision boundaries will be linear in\n",
      "the feature space φ, and these correspond to nonlinear decision boundaries in the\n",
      "original xspace, as illustrated in Figure 4.12. Classes that are linearly separable\n",
      "in the feature space φ(x)need not be linearly separable in the original observation\n",
      "spacex. Note that as in our discussion of linear models for regression, one of the4.3. Probabilistic Discriminative Models 205\n",
      "basis functions is typically set to a constant, say φ0(x)=1 , so that the correspond-\n",
      "ing parameter w0plays the role of a bias. For the remainder of this chapter, we shall\n",
      "include a ﬁxed basis function transformation φ(x), as this will highlight some useful\n",
      "similarities to the regression models discussed in Chapter 3.\n",
      "For many problems of practical interest, there is signiﬁcant overlap between\n",
      "the class-conditional densities p(x|Ck). This corresponds to posterior probabilities\n",
      "p(Ck|x), which, for at least some values of x, are not 0 or 1. In such cases, the opti-\n",
      "mal solution is obtained by modelling the posterior probabilities accurately and thenapplying standard decision theory, as discussed in Chapter 1. Note that nonlinear\n",
      "transformations φ(x)cannot remove such class overlap. Indeed, they can increase\n",
      "the level of overlap, or create overlap where none existed in the original observation\n",
      "space. However, suitable choices of nonlinearity can make the process of modelling\n",
      "the posterior probabilities easier.\n",
      "Such ﬁxed basis function models have important limitations, and these will be Section 3.6\n",
      "resolved in later chapters by allowing the basis functions themselves to adapt to the\n",
      "data. Notwithstanding these limitations, models with ﬁxed nonlinear basis functionsplay an important role in applications, and a discussion of such models will intro-\n",
      "duce many of the key concepts needed for an understanding of their more complex\n",
      "counterparts.\n",
      "4.3.2 Logistic regression\n",
      "We begin our treatment of generalized linear models by considering the problem\n",
      "of two-class classiﬁcation. In our discussion of generative approaches in Section 4.2,\n",
      "we saw that under rather general assumptions, the posterior probability of class C1\n",
      "can be written as a logistic sigmoid acting on a linear function of the feature vector\n",
      "φso that\n",
      "p(C1|φ)=y(φ)=σ(\n",
      "wTφ)\n",
      "(4.87)\n",
      "withp(C2|φ)=1−p(C1|φ). Here σ(·)is the logistic sigmoid function deﬁned by\n",
      "(4.59). In the terminology of statistics, this model is known as logistic regression ,\n",
      "although it should be emphasized that this is a model for classiﬁcation rather than\n",
      "regression.\n",
      "For an M-dimensional feature space φ, this model has Madjustable parameters.\n",
      "By contrast, if we had ﬁtted Gaussian class conditional densities using maximumlikelihood, we would have used 2Mparameters for the means and M(M+1 )/2\n",
      "parameters for the (shared) covariance matrix. Together with the class prior p(C\n",
      "1),\n",
      "this gives a total of M(M+5)/2+1 parameters, which grows quadratically with M,\n",
      "in contrast to the linear dependence on Mof the number of parameters in logistic\n",
      "regression. For large values of M, there is a clear advantage in working with the\n",
      "logistic regression model directly.\n",
      "We now use maximum likelihood to determine the parameters of the logistic\n",
      "regression model. To do this, we shall make use of the derivative of the logistic sig-\n",
      "moid function, which can conveniently be expressed in terms of the sigmoid functionitself Exercise 4.12\n",
      "dσ\n",
      "da=σ(1−σ). (4.88)206 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "For a data set {φn,tn}, where tn∈{0,1}andφn=φ(xn), with n=\n",
      "1,...,N , the likelihood function can be written\n",
      "p(t|w)=N∏\n",
      "n=1ytn\n",
      "n{1−yn}1−tn(4.89)\n",
      "where t=(t1,...,t N)Tandyn=p(C1|φn). As usual, we can deﬁne an error\n",
      "function by taking the negative logarithm of the likelihood, which gives the cross-\n",
      "entropy error function in the form\n",
      "E(w)=−lnp(t|w)=−N∑\n",
      "n=1{tnlnyn+( 1−tn)l n ( 1−yn)} (4.90)\n",
      "where yn=σ(an)andan=wTφn. Taking the gradient of the error function with\n",
      "respect to w, we obtain Exercise 4.13\n",
      "∇E(w)=N∑\n",
      "n=1(yn−tn)φn (4.91)\n",
      "where we have made use of (4.88). We see that the factor involving the derivative\n",
      "of the logistic sigmoid has cancelled, leading to a simpliﬁed form for the gradient\n",
      "of the log likelihood. In particular, the contribution to the gradient from data point\n",
      "nis given by the ‘error’ yn−tnbetween the target value and the prediction of the\n",
      "model, times the basis function vector φn. Furthermore, comparison with (3.13)\n",
      "shows that this takes precisely the same form as the gradient of the sum-of-squares\n",
      "error function for the linear regression model. Section 3.1.1\n",
      "If desired, we could make use of the result (4.91) to give a sequential algorithm\n",
      "in which patterns are presented one at a time, in which each of the weight vectors is\n",
      "updated using (3.22) in which ∇Enis thenthterm in (4.91).\n",
      "It is worth noting that maximum likelihood can exhibit severe over-ﬁtting for\n",
      "data sets that are linearly separable. This arises because the maximum likelihood so-\n",
      "lution occurs when the hyperplane corresponding to σ=0.5, equivalent to wTφ=\n",
      "0, separates the two classes and the magnitude of wgoes to inﬁnity. In this case, the\n",
      "logistic sigmoid function becomes inﬁnitely steep in feature space, corresponding to\n",
      "a Heaviside step function, so that every training point from each class kis assigned\n",
      "a posterior probability p(Ck|x)=1 . Furthermore, there is typically a continuum Exercise 4.14\n",
      "of such solutions because any separating hyperplane will give rise to the same pos-\n",
      "terior probabilities at the training data points, as will be seen later in Figure 10.13.\n",
      "Maximum likelihood provides no way to favour one such solution over another, and\n",
      "which solution is found in practice will depend on the choice of optimization algo-rithm and on the parameter initialization. Note that the problem will arise even if\n",
      "the number of data points is large compared with the number of parameters in the\n",
      "model, so long as the training data set is linearly separable. The singularity can beavoided by inclusion of a prior and ﬁnding a MAP solution for w, or equivalently by\n",
      "adding a regularization term to the error function.4.3. Probabilistic Discriminative Models 207\n",
      "4.3.3 Iterative reweighted least squares\n",
      "In the case of the linear regression models discussed in Chapter 3, the maxi-\n",
      "mum likelihood solution, on the assumption of a Gaussian noise model, leads to a\n",
      "closed-form solution. This was a consequence of the quadratic dependence of the\n",
      "log likelihood function on the parameter vector w. For logistic regression, there\n",
      "is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid\n",
      "function. However, the departure from a quadratic form is not substantial. To be\n",
      "precise, the error function is concave, as we shall see shortly, and hence has a unique\n",
      "minimum. Furthermore, the error function can be minimized by an efﬁcient iterative\n",
      "technique based on the Newton-Raphson iterative optimization scheme, which uses a\n",
      "local quadratic approximation to the log likelihood function. The Newton-Raphson\n",
      "update, for minimizing a function E(w), takes the form (Fletcher, 1987; Bishop and\n",
      "Nabney, 2008)\n",
      "w(new)=w(old)−H−1∇E(w). (4.92)\n",
      "whereHis the Hessian matrix whose elements comprise the second derivatives of\n",
      "E(w)with respect to the components of w.\n",
      "Let us ﬁrst of all apply the Newton-Raphson method to the linear regression\n",
      "model (3.3) with the sum-of-squares error function (3.12). The gradient and Hessian\n",
      "of this error function are given by\n",
      "∇E(w)=N∑\n",
      "n=1(wTφn−tn)φn=ΦTΦw−ΦTt (4.93)\n",
      "H=∇∇E(w)=N∑\n",
      "n=1φnφT\n",
      "n=ΦTΦ (4.94)\n",
      "whereΦis theN×Mdesign matrix, whose nthrow is given by φT\n",
      "n. The Newton- Section 3.1.1\n",
      "Raphson update then takes the form\n",
      "w(new)=w(old)−(ΦTΦ)−1{\n",
      "ΦTΦw(old)−ΦTt}\n",
      "=(ΦTΦ)−1ΦTt (4.95)\n",
      "which we recognize as the standard least-squares solution. Note that the error func-\n",
      "tion in this case is quadratic and hence the Newton-Raphson formula gives the exact\n",
      "solution in one step.\n",
      "Now let us apply the Newton-Raphson update to the cross-entropy error function\n",
      "(4.90) for the logistic regression model. From (4.91) we see that the gradient and\n",
      "Hessian of this error function are given by\n",
      "∇E(w)=N∑\n",
      "n=1(yn−tn)φn=ΦT(y−t) (4.96)\n",
      "H=∇∇E(w)=N∑\n",
      "n=1yn(1−yn)φnφT\n",
      "n=ΦTRΦ (4.97)208 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "where we have made use of (4.88). Also, we have introduced the N×Ndiagonal\n",
      "matrixRwith elements\n",
      "Rnn=yn(1−yn). (4.98)\n",
      "We see that the Hessian is no longer constant but depends on wthrough the weight-\n",
      "ing matrix R, corresponding to the fact that the error function is no longer quadratic.\n",
      "Using the property 0<yn<1, which follows from the form of the logistic sigmoid\n",
      "function, we see that uTHu>0for an arbitrary vector u, and so the Hessian matrix\n",
      "His positive deﬁnite. It follows that the error function is a concave function of w\n",
      "and hence has a unique minimum. Exercise 4.15\n",
      "The Newton-Raphson update formula for the logistic regression model then be-\n",
      "comes\n",
      "w(new)=w(old)−(ΦTRΦ)−1ΦT(y−t)\n",
      "=(ΦTRΦ)−1{\n",
      "ΦTRΦw(old)−ΦT(y−t)}\n",
      "=(ΦTRΦ)−1ΦTRz (4.99)\n",
      "where zis anN-dimensional vector with elements\n",
      "z=Φw(old)−R−1(y−t). (4.100)\n",
      "We see that the update formula (4.99) takes the form of a set of normal equations for a\n",
      "weighted least-squares problem. Because the weighing matrix Ris not constant but\n",
      "depends on the parameter vector w, we must apply the normal equations iteratively,\n",
      "each time using the new weight vector wto compute a revised weighing matrix\n",
      "R. For this reason, the algorithm is known as iterative reweighted least squares ,o r\n",
      "IRLS (Rubin, 1983). As in the weighted least-squares problem, the elements of the\n",
      "diagonal weighting matrix Rcan be interpreted as variances because the mean and\n",
      "variance of tin the logistic regression model are given by\n",
      "E[t]= σ(x)=y (4.101)\n",
      "var[t]= E[t2]−E[t]2=σ(x)−σ(x)2=y(1−y) (4.102)\n",
      "where we have used the property t2=tfort∈{0,1}. In fact, we can interpret IRLS\n",
      "as the solution to a linearized problem in the space of the variable a=wTφ. The\n",
      "quantity zn, which corresponds to the nthelement of z, can then be given a simple\n",
      "interpretation as an effective target value in this space obtained by making a local\n",
      "linear approximation to the logistic sigmoid function around the current operatingpointw\n",
      "(old)\n",
      "an(w)≃an(w(old))+dan\n",
      "dyn⏐⏐⏐⏐\n",
      "w(old)(tn−yn)\n",
      "=φT\n",
      "nw(old)−(yn−tn)\n",
      "yn(1−yn)=zn. (4.103)4.3. Probabilistic Discriminative Models 209\n",
      "4.3.4 Multiclass logistic regression\n",
      "In our discussion of generative models for multiclass classiﬁcation, we have Section 4.2\n",
      "seen that for a large class of distributions, the posterior probabilities are given by a\n",
      "softmax transformation of linear functions of the feature variables, so that\n",
      "p(Ck|φ)=yk(φ)=exp(ak)∑\n",
      "jexp(aj)(4.104)\n",
      "where the ‘activations’ akare given by\n",
      "ak=wT\n",
      "kφ. (4.105)\n",
      "There we used maximum likelihood to determine separately the class-conditional\n",
      "densities and the class priors and then found the corresponding posterior probabilities\n",
      "using Bayes’ theorem, thereby implicitly determining the parameters {wk}. Here we\n",
      "consider the use of maximum likelihood to determine the parameters {wk}of this\n",
      "model directly. To do this, we will require the derivatives of ykwith respect to all of\n",
      "the activations aj. These are given by Exercise 4.17\n",
      "∂yk\n",
      "∂aj=yk(Ikj−yj) (4.106)\n",
      "where Ikjare the elements of the identity matrix.\n",
      "Next we write down the likelihood function. This is most easily done using\n",
      "the 1-of- Kcoding scheme in which the target vector tnfor a feature vector φn\n",
      "belonging to class Ckis a binary vector with all elements zero except for element k,\n",
      "which equals one. The likelihood function is then given by\n",
      "p(T|w1,...,wK)=N∏\n",
      "n=1K∏\n",
      "k=1p(Ck|φn)tnk=N∏\n",
      "n=1K∏\n",
      "k=1ytnk\n",
      "nk(4.107)\n",
      "where ynk=yk(φn), andTis anN×Kmatrix of target variables with elements\n",
      "tnk. Taking the negative logarithm then gives\n",
      "E(w1,...,wK)=−lnp(T|w1,...,wK)=−N∑\n",
      "n=1K∑\n",
      "k=1tnklnynk (4.108)\n",
      "which is known as the cross-entropy error function for the multiclass classiﬁcation\n",
      "problem.\n",
      "We now take the gradient of the error function with respect to one of the param-\n",
      "eter vectors wj. Making use of the result (4.106) for the derivatives of the softmax\n",
      "function, we obtain Exercise 4.18\n",
      "∇wjE(w1,...,wK)=N∑\n",
      "n=1(ynj−tnj)φn (4.109)210 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "where we have made use of∑\n",
      "ktnk=1. Once again, we see the same form arising\n",
      "for the gradient as was found for the sum-of-squares error function with the linearmodel and the cross-entropy error for the logistic regression model, namely the prod-\n",
      "uct of the error (y\n",
      "nj−tnj)times the basis function φn. Again, we could use this\n",
      "to formulate a sequential algorithm in which patterns are presented one at a time, inwhich each of the weight vectors is updated using (3.22).\n",
      "We have seen that the derivative of the log likelihood function for a linear regres-\n",
      "sion model with respect to the parameter vector wfor a data point ntook the form\n",
      "of the ‘error’ y\n",
      "n−tntimes the feature vector φn. Similarly, for the combination\n",
      "of logistic sigmoid activation function and cross-entropy error function (4.90), and\n",
      "for the softmax activation function with the multiclass cross-entropy error function\n",
      "(4.108), we again obtain this same simple form. This is an example of a more general\n",
      "result, as we shall see in Section 4.3.6.\n",
      "To ﬁnd a batch algorithm, we again appeal to the Newton-Raphson update to\n",
      "obtain the corresponding IRLS algorithm for the multiclass problem. This requires\n",
      "evaluation of the Hessian matrix that comprises blocks of size M×Min which\n",
      "blockj,kis given by\n",
      "∇wk∇wjE(w1,...,wK)=−N∑\n",
      "n=1ynk(Ikj−ynj)φnφT\n",
      "n. (4.110)\n",
      "As with the two-class problem, the Hessian matrix for the multiclass logistic regres-\n",
      "sion model is positive deﬁnite and so the error function again has a unique minimum. Exercise 4.20\n",
      "Practical details of IRLS for the multiclass case can be found in Bishop and Nabney(2008).\n",
      "4.3.5 Probit regression\n",
      "We have seen that, for a broad range of class-conditional distributions, described\n",
      "by the exponential family, the resulting posterior class probabilities are given by alogistic (or softmax) transformation acting on a linear function of the feature vari-\n",
      "ables. However, not all choices of class-conditional density give rise to such a simple\n",
      "form for the posterior probabilities (for instance, if the class-conditional densities aremodelled using Gaussian mixtures). This suggests that it might be worth exploring\n",
      "other types of discriminative probabilistic model. For the purposes of this chapter,\n",
      "however, we shall return to the two-class case, and again remain within the frame-work of generalized linear models so that\n",
      "p(t=1|a)=f(a) (4.111)\n",
      "where a=w\n",
      "Tφ, andf(·)is the activation function.\n",
      "One way to motivate an alternative choice for the link function is to consider a\n",
      "noisy threshold model, as follows. For each input φn, we evaluate an=wTφnand\n",
      "then we set the target value according to\n",
      "{tn=1 ifan⩾θ\n",
      "tn=0 otherwise .(4.112)4.3. Probabilistic Discriminative Models 211\n",
      "Figure 4.13 Schematic example of a probability density p(θ)\n",
      "shown by the blue curve, given in this example by a mixture\n",
      "of two Gaussians, along with its cumulative distribution function\n",
      "f(a), shown by the red curve. Note that the value of the blue\n",
      "curve at any point, such as that indicated by the vertical green\n",
      "line, corresponds to the slope of the red curve at the same point.\n",
      "Conversely, the value of the red curve at this point corresponds\n",
      "to the area under the blue curve indicated by the shaded green\n",
      "region. In the stochastic threshold model, the class label takes\n",
      "the value t=1if the value of a=wTφexceeds a threshold, oth-\n",
      "erwise it takes the value t=0. This is equivalent to an activation\n",
      "function given by the cumulative distribution function f(a).\n",
      "0 1 2 3 400.20.40.60.81\n",
      "If the value of θis drawn from a probability density p(θ), then the corresponding\n",
      "activation function will be given by the cumulative distribution function\n",
      "f(a)=∫a\n",
      "−∞p(θ)dθ (4.113)\n",
      "as illustrated in Figure 4.13.\n",
      "As a speciﬁc example, suppose that the density p(θ)is given by a zero mean,\n",
      "unit variance Gaussian. The corresponding cumulative distribution function is given\n",
      "by\n",
      "Φ(a)=∫a\n",
      "−∞N(θ|0,1) dθ (4.114)\n",
      "which is known as the probit function. It has a sigmoidal shape and is compared\n",
      "with the logistic sigmoid function in Figure 4.9. Note that the use of a more gen-\n",
      "eral Gaussian distribution does not change the model because this is equivalent to\n",
      "a re-scaling of the linear coefﬁcients w. Many numerical packages provide for the\n",
      "evaluation of a closely related function deﬁned by\n",
      "erf(a)=2√π∫a\n",
      "0exp(−θ2/2) dθ (4.115)\n",
      "and known as the erf function orerror function (not to be confused with the error\n",
      "function of a machine learning model). It is related to the probit function by Exercise 4.21\n",
      "Φ(a)=1\n",
      "2{\n",
      "1+1√\n",
      "2erf(a)}\n",
      ". (4.116)\n",
      "The generalized linear model based on a probit activation function is known as probit\n",
      "regression .\n",
      "We can determine the parameters of this model using maximum likelihood, by a\n",
      "straightforward extension of the ideas discussed earlier. In practice, the results found\n",
      "using probit regression tend to be similar to those of logistic regression. We shall,212 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "however, ﬁnd another use for the probit model when we discuss Bayesian treatments\n",
      "of logistic regression in Section 4.5.\n",
      "One issue that can occur in practical applications is that of outliers , which can\n",
      "arise for instance through errors in measuring the input vector xor through misla-\n",
      "belling of the target value t. Because such points can lie a long way to the wrong side\n",
      "of the ideal decision boundary, they can seriously distort the classiﬁer. Note that the\n",
      "logistic and probit regression models behave differently in this respect because the\n",
      "tails of the logistic sigmoid decay asymptotically like exp(−x)forx→∞ , whereas\n",
      "for the probit activation function they decay like exp(−x2), and so the probit model\n",
      "can be signiﬁcantly more sensitive to outliers.\n",
      "However, both the logistic and the probit models assume the data is correctly\n",
      "labelled. The effect of mislabelling is easily incorporated into a probabilistic model\n",
      "by introducing a probability ϵthat the target value thas been ﬂipped to the wrong\n",
      "value (Opper and Winther, 2000a), leading to a target value distribution for data point\n",
      "xof the form\n",
      "p(t|x)=( 1 −ϵ)σ(x)+ϵ(1−σ(x))\n",
      "=ϵ+( 1−2ϵ)σ(x) (4.117)\n",
      "where σ(x)is the activation function with input vector x. Here ϵmay be set in\n",
      "advance, or it may be treated as a hyperparameter whose value is inferred from the\n",
      "data.\n",
      "4.3.6 Canonical link functions\n",
      "For the linear regression model with a Gaussian noise distribution, the error\n",
      "function, corresponding to the negative log likelihood, is given by (3.12). If we takethe derivative with respect to the parameter vector wof the contribution to the error\n",
      "function from a data point n, this takes the form of the ‘error’ y\n",
      "n−tntimes the\n",
      "feature vector φn, where yn=wTφn. Similarly, for the combination of the logistic\n",
      "sigmoid activation function and the cross-entropy error function (4.90), and for the\n",
      "softmax activation function with the multiclass cross-entropy error function (4.108),\n",
      "we again obtain this same simple form. We now show that this is a general resultof assuming a conditional distribution for the target variable from the exponential\n",
      "family, along with a corresponding choice for the activation function known as the\n",
      "canonical link function .\n",
      "We again make use of the restricted form (4.84) of exponential family distribu-\n",
      "tions. Note that here we are applying the assumption of exponential family distribu-tion to the target variable t, in contrast to Section 4.2.4 where we applied it to the\n",
      "input vector x. We therefore consider conditional distributions of the target variable\n",
      "of the form\n",
      "p(t|η,s)=1\n",
      "sh(t\n",
      "s)\n",
      "g(η)e x p{ηt\n",
      "s}\n",
      ". (4.118)\n",
      "Using the same line of argument as led to the derivation of the result (2.226), we see\n",
      "that the conditional mean of t, which we denote by y, is given by\n",
      "y≡E[t|η]=−sd\n",
      "dηlng(η). (4.119)4.4. The Laplace Approximation 213\n",
      "Thusyandηmust related, and we denote this relation through η=ψ(y).\n",
      "Following Nelder and Wedderburn (1972), we deﬁne a generalized linear model\n",
      "to be one for which yis a nonlinear function of a linear combination of the input (or\n",
      "feature) variables so that\n",
      "y=f(wTφ) (4.120)\n",
      "where f(·)is known as the activation function in the machine learning literature, and\n",
      "f−1(·)is known as the link function in statistics.\n",
      "Now consider the log likelihood function for this model, which, as a function of\n",
      "η,i sg i v e nb y\n",
      "lnp(t|η,s)=N∑\n",
      "n=1lnp(tn|η,s)=N∑\n",
      "n=1{\n",
      "lng(ηn)+ηntn\n",
      "s}\n",
      "+c o n s t (4.121)\n",
      "where we are assuming that all observations share a common scale parameter (which\n",
      "corresponds to the noise variance for a Gaussian distribution for instance) and so s\n",
      "is independent of n. The derivative of the log likelihood with respect to the model\n",
      "parameters wis then given by\n",
      "∇wlnp(t|η,s)=N∑\n",
      "n=1{d\n",
      "dηnlng(ηn)+tn\n",
      "s}dηn\n",
      "dyndyn\n",
      "dan∇an\n",
      "=N∑\n",
      "n=11\n",
      "s{tn−yn}ψ′(yn)f′(an)φn (4.122)\n",
      "where an=wTφn, and we have used yn=f(an)together with the result (4.119)\n",
      "for E[t|η]. We now see that there is a considerable simpliﬁcation if we choose a\n",
      "particular form for the link function f−1(y)given by\n",
      "f−1(y)=ψ(y) (4.123)\n",
      "which gives f(ψ(y)) =yand hence f′(ψ)ψ′(y)=1 . Also, because a=f−1(y),\n",
      "we have a=ψand hence f′(a)ψ′(y)=1 . In this case, the gradient of the error\n",
      "function reduces to\n",
      "∇lnE(w)=1\n",
      "sN∑\n",
      "n=1{yn−tn}φn. (4.124)\n",
      "For the Gaussian s=β−1, whereas for the logistic model s=1.\n",
      "4.4. The Laplace Approximation\n",
      "In Section 4.5 we shall discuss the Bayesian treatment of logistic regression. As\n",
      "we shall see, this is more complex than the Bayesian treatment of linear regression\n",
      "models, discussed in Sections 3.3 and 3.5. In particular, we cannot integrate exactly214 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "over the parameter vector wsince the posterior distribution is no longer Gaussian.\n",
      "It is therefore necessary to introduce some form of approximation. Later in thebook we shall consider a range of techniques based on analytical approximations Chapter 10\n",
      "and numerical sampling. Chapter 11\n",
      "Here we introduce a simple, but widely used, framework called the Laplace ap-\n",
      "proximation, that aims to ﬁnd a Gaussian approximation to a probability density\n",
      "deﬁned over a set of continuous variables. Consider ﬁrst the case of a single contin-\n",
      "uous variable z, and suppose the distribution p(z)is deﬁned by\n",
      "p(z)=1\n",
      "Zf(z) (4.125)\n",
      "where Z=∫\n",
      "f(z)dzis the normalization coefﬁcient. We shall suppose that the\n",
      "value of Zis unknown. In the Laplace method the goal is to ﬁnd a Gaussian approx-\n",
      "imation q(z)which is centred on a mode of the distribution p(z). The ﬁrst step is to\n",
      "ﬁnd a mode of p(z), in other words a point z0such that p′(z0)=0 , or equivalently\n",
      "df(z)\n",
      "dz⏐⏐⏐⏐\n",
      "z=z0=0. (4.126)\n",
      "A Gaussian distribution has the property that its logarithm is a quadratic function\n",
      "of the variables. We therefore consider a Taylor expansion of lnf(z)centred on the\n",
      "modez0so that\n",
      "lnf(z)≃lnf(z0)−1\n",
      "2A(z−z0)2(4.127)\n",
      "where\n",
      "A=−d2\n",
      "dz2lnf(z)⏐⏐⏐⏐\n",
      "z=z0. (4.128)\n",
      "Note that the ﬁrst-order term in the Taylor expansion does not appear since z0is a\n",
      "local maximum of the distribution. Taking the exponential we obtain\n",
      "f(z)≃f(z0)e x p{\n",
      "−A\n",
      "2(z−z0)2}\n",
      ". (4.129)\n",
      "We can then obtain a normalized distribution q(z)by making use of the standard\n",
      "result for the normalization of a Gaussian, so that\n",
      "q(z)=(A\n",
      "2π)1/2\n",
      "exp{\n",
      "−A\n",
      "2(z−z0)2}\n",
      ". (4.130)\n",
      "The Laplace approximation is illustrated in Figure 4.14. Note that the Gaussian\n",
      "approximation will only be well deﬁned if its precision A>0, in other words the\n",
      "stationary point z0must be a local maximum, so that the second derivative of f(z)\n",
      "at the point z0is negative.4.4. The Laplace Approximation 215\n",
      "−2 −1 0 1 2 3 400.20.40.60.8\n",
      "−2 −1 0 1 2 3 4010203040\n",
      "Figure 4.14 Illustration of the Laplace approximation applied to the distribution p(z)∝exp(−z2/2)σ(20z+4 )\n",
      "where σ(z)is the logistic sigmoid function deﬁned by σ(z)=( 1+ e−z)−1. The left plot shows the normalized\n",
      "distribution p(z)in yellow, together with the Laplace approximation centred on the mode z0ofp(z)in red. The\n",
      "right plot shows the negative logarithms of the corresponding curves.\n",
      "We can extend the Laplace method to approximate a distribution p(z)=f(z)/Z\n",
      "deﬁned over an M-dimensional space z. At a stationary point z0the gradient ∇f(z)\n",
      "will vanish. Expanding around this stationary point we have\n",
      "lnf(z)≃lnf(z0)−1\n",
      "2(z−z0)TA(z−z0) (4.131)\n",
      "where the M×MHessian matrix Ais deﬁned by\n",
      "A=−∇ ∇ lnf(z)|z=z0(4.132)\n",
      "and∇is the gradient operator. Taking the exponential of both sides we obtain\n",
      "f(z)≃f(z0)e x p{\n",
      "−1\n",
      "2(z−z0)TA(z−z0)}\n",
      ". (4.133)\n",
      "The distribution q(z)is proportional to f(z)and the appropriate normalization coef-\n",
      "ﬁcient can be found by inspection, using the standard result (2.43) for a normalized\n",
      "multivariate Gaussian, giving\n",
      "q(z)=|A|1/2\n",
      "(2π)M/2exp{\n",
      "−1\n",
      "2(z−z0)TA(z−z0)}\n",
      "=N(z|z0,A−1) (4.134)\n",
      "where |A|denotes the determinant of A. This Gaussian distribution will be well\n",
      "deﬁned provided its precision matrix, given by A, is positive deﬁnite, which implies\n",
      "that the stationary point z0must be a local maximum, not a minimum or a saddle\n",
      "point.\n",
      "In order to apply the Laplace approximation we ﬁrst need to ﬁnd the mode z0,\n",
      "and then evaluate the Hessian matrix at that mode. In practice a mode will typi-\n",
      "cally be found by running some form of numerical optimization algorithm (Bishop216 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "and Nabney, 2008). Many of the distributions encountered in practice will be mul-\n",
      "timodal and so there will be different Laplace approximations according to which\n",
      "mode is being considered. Note that the normalization constant Zof the true distri-\n",
      "bution does not need to be known in order to apply the Laplace method. As a result\n",
      "of the central limit theorem, the posterior distribution for a model is expected to\n",
      "become increasingly better approximated by a Gaussian as the number of observed\n",
      "data points is increased, and so we would expect the Laplace approximation to be\n",
      "most useful in situations where the number of data points is relatively large.\n",
      "One major weakness of the Laplace approximation is that, since it is based on a\n",
      "Gaussian distribution, it is only directly applicable to real variables. In other cases\n",
      "it may be possible to apply the Laplace approximation to a transformation of the\n",
      "variable. For instance if 0⩽τ<∞then we can consider a Laplace approximation\n",
      "oflnτ. The most serious limitation of the Laplace framework, however, is that\n",
      "it is based purely on the aspects of the true distribution at a speciﬁc value of the\n",
      "variable, and so can fail to capture important global properties. In Chapter 10 we\n",
      "shall consider alternative approaches which adopt a more global perspective.\n",
      "4.4.1 Model comparison and BIC\n",
      "As well as approximating the distribution p(z)we can also obtain an approxi-\n",
      "mation to the normalization constant Z. Using the approximation (4.133) we have\n",
      "Z=∫\n",
      "f(z)dz\n",
      "≃f(z0)∫\n",
      "exp{\n",
      "−1\n",
      "2(z−z0)TA(z−z0)}\n",
      "dz\n",
      "=f(z0)(2π)M/2\n",
      "|A|1/2(4.135)\n",
      "where we have noted that the integrand is Gaussian and made use of the standard\n",
      "result (2.43) for a normalized Gaussian distribution. We can use the result (4.135) to\n",
      "obtain an approximation to the model evidence which, as discussed in Section 3.4,\n",
      "plays a central role in Bayesian model comparison.\n",
      "Consider a data set Dand a set of models {Mi}having parameters {θi}.F o r\n",
      "each model we deﬁne a likelihood function p(D|θi,Mi). If we introduce a prior\n",
      "p(θi|Mi)over the parameters, then we are interested in computing the model evi-\n",
      "dence p(D|M i)for the various models. From now on we omit the conditioning on\n",
      "Mito keep the notation uncluttered. From Bayes’ theorem the model evidence is\n",
      "given by\n",
      "p(D)=∫\n",
      "p(D|θ)p(θ)dθ. (4.136)\n",
      "Identifying f(θ)=p(D|θ)p(θ)andZ=p(D), and applying the result (4.135), we\n",
      "obtain Exercise 4.22\n",
      "lnp(D)≃lnp(D|θMAP)+lnp(θMAP)+M\n",
      "2ln(2π)−1\n",
      "2ln|A|\n",
      "  \n",
      "Occam factor(4.137)4.5. Bayesian Logistic Regression 217\n",
      "where θMAP is the value of θat the mode of the posterior distribution, and Ais the\n",
      "Hessian matrix of second derivatives of the negative log posterior\n",
      "A=−∇∇ lnp(D|θMAP)p(θMAP)=−∇∇ lnp(θMAP|D). (4.138)\n",
      "The ﬁrst term on the right hand side of (4.137) represents the log likelihood evalu-\n",
      "ated using the optimized parameters, while the remaining three terms comprise the‘Occam factor’ which penalizes model complexity.\n",
      "If we assume that the Gaussian prior distribution over parameters is broad, and\n",
      "that the Hessian has full rank, then we can approximate (4.137) very roughly using Exercise 4.23\n",
      "lnp(D)≃lnp(D|θ\n",
      "MAP)−1\n",
      "2MlnN (4.139)\n",
      "where Nis the number of data points, Mis the number of parameters in θand\n",
      "we have omitted additive constants. This is known as the Bayesian Information\n",
      "Criterion (BIC) or the Schwarz criterion (Schwarz, 1978). Note that, compared to\n",
      "AIC given by (1.73), this penalizes model complexity more heavily.\n",
      "Complexity measures such as AIC and BIC have the virtue of being easy to\n",
      "evaluate, but can also give misleading results. In particular, the assumption that the\n",
      "Hessian matrix has full rank is often not valid since many of the parameters are not‘well-determined’. We can use the result (4.137) to obtain a more accurate estimate Section 3.5.3\n",
      "of the model evidence starting from the Laplace approximation, as we illustrate in\n",
      "the context of neural networks in Section 5.7.\n",
      "4.5. Bayesian Logistic Regression\n",
      "We now turn to a Bayesian treatment of logistic regression. Exact Bayesian infer-ence for logistic regression is intractable. In particular, evaluation of the posterior\n",
      "distribution would require normalization of the product of a prior distribution and a\n",
      "likelihood function that itself comprises a product of logistic sigmoid functions, onefor every data point. Evaluation of the predictive distribution is similarly intractable.\n",
      "Here we consider the application of the Laplace approximation to the problem of\n",
      "Bayesian logistic regression (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b).\n",
      "4.5.1 Laplace approximation\n",
      "Recall from Section 4.4 that the Laplace approximation is obtained by ﬁnding\n",
      "the mode of the posterior distribution and then ﬁtting a Gaussian centred at that\n",
      "mode. This requires evaluation of the second derivatives of the log posterior, which\n",
      "is equivalent to ﬁnding the Hessian matrix.\n",
      "Because we seek a Gaussian representation for the posterior distribution, it is\n",
      "natural to begin with a Gaussian prior, which we write in the general form\n",
      "p(w)=N(w|m0,S0) (4.140)218 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "wherem0andS0are ﬁxed hyperparameters. The posterior distribution over wis\n",
      "given by\n",
      "p(w|t)∝p(w)p(t|w) (4.141)\n",
      "where t=(t1,...,t N)T. Taking the log of both sides, and substituting for the prior\n",
      "distribution using (4.140), and for the likelihood function using (4.89), we obtain\n",
      "lnp(w|t)= −1\n",
      "2(w−m0)TS−1\n",
      "0(w−m0)\n",
      "+N∑\n",
      "n=1{tnlnyn+( 1−tn)l n ( 1−yn)}+c o n s t (4.142)\n",
      "where yn=σ(wTφn). To obtain a Gaussian approximation to the posterior dis-\n",
      "tribution, we ﬁrst maximize the posterior distribution to give the MAP (maximumposterior) solution w\n",
      "MAP, which deﬁnes the mean of the Gaussian. The covariance\n",
      "is then given by the inverse of the matrix of second derivatives of the negative log\n",
      "likelihood, which takes the form\n",
      "SN=−∇∇ lnp(w|t)=S−1\n",
      "0+N∑\n",
      "n=1yn(1−yn)φnφT\n",
      "n. (4.143)\n",
      "The Gaussian approximation to the posterior distribution therefore takes the form\n",
      "q(w)=N(w|wMAP,SN). (4.144)\n",
      "Having obtained a Gaussian approximation to the posterior distribution, there\n",
      "remains the task of marginalizing with respect to this distribution in order to make\n",
      "predictions.\n",
      "4.5.2 Predictive distribution\n",
      "The predictive distribution for class C1, given a new feature vector φ(x),i s\n",
      "obtained by marginalizing with respect to the posterior distribution p(w|t), which is\n",
      "itself approximated by a Gaussian distribution q(w)so that\n",
      "p(C1|φ,t)=∫\n",
      "p(C1|φ,w)p(w|t)dw≃∫\n",
      "σ(wTφ)q(w)dw (4.145)\n",
      "with the corresponding probability for class C2given by p(C2|φ,t)=1−p(C1|φ,t).\n",
      "To evaluate the predictive distribution, we ﬁrst note that the function σ(wTφ)de-\n",
      "pends on wonly through its projection onto φ. Denoting a=wTφ,w eh a v e\n",
      "σ(wTφ)=∫\n",
      "δ(a−wTφ)σ(a)da (4.146)\n",
      "where δ(·)is the Dirac delta function. From this we obtain\n",
      "∫\n",
      "σ(wTφ)q(w)dw=∫\n",
      "σ(a)p(a)da (4.147)4.5. Bayesian Logistic Regression 219\n",
      "where\n",
      "p(a)=∫\n",
      "δ(a−wTφ)q(w)dw. (4.148)\n",
      "We can evaluate p(a)by noting that the delta function imposes a linear constraint\n",
      "onwand so forms a marginal distribution from the joint distribution q(w)by inte-\n",
      "grating out all directions orthogonal to φ. Because q(w)is Gaussian, we know from\n",
      "Section 2.3.2 that the marginal distribution will also be Gaussian. We can evaluatethe mean and covariance of this distribution by taking moments, and interchanging\n",
      "the order of integration over aandw, so that\n",
      "µ\n",
      "a=E[a]=∫\n",
      "p(a)ada=∫\n",
      "q(w)wTφdw=wT\n",
      "MAPφ (4.149)\n",
      "where we have used the result (4.144) for the variational posterior distribution q(w).\n",
      "Similarly\n",
      "σ2\n",
      "a=v a r [ a]=∫\n",
      "p(a){\n",
      "a2−E[a]2}\n",
      "da\n",
      "=∫\n",
      "q(w){\n",
      "(wTφ)2−(mT\n",
      "Nφ)2}\n",
      "dw=φTSNφ. (4.150)\n",
      "Note that the distribution of atakes the same form as the predictive distribution\n",
      "(3.58) for the linear regression model, with the noise variance set to zero. Thus our\n",
      "variational approximation to the predictive distribution becomes\n",
      "p(C1|t)=∫\n",
      "σ(a)p(a)da=∫\n",
      "σ(a)N(a|µa,σ2\n",
      "a)da. (4.151)\n",
      "This result can also be derived directly by making use of the results for the marginal\n",
      "of a Gaussian distribution given in Section 2.3.2. Exercise 4.24\n",
      "The integral over arepresents the convolution of a Gaussian with a logistic sig-\n",
      "moid, and cannot be evaluated analytically. We can, however, obtain a good approx-imation (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b; Barber and Bishop,\n",
      "1998a) by making use of the close similarity between the logistic sigmoid function\n",
      "σ(a)deﬁned by (4.59) and the probit function Φ(a)deﬁned by (4.114). In order to\n",
      "obtain the best approximation to the logistic function we need to re-scale the hori-\n",
      "zontal axis, so that we approximate σ(a)byΦ(λa). We can ﬁnd a suitable value of\n",
      "λby requiring that the two functions have the same slope at the origin, which gives\n",
      "λ\n",
      "2=π/8. The similarity of the logistic sigmoid and the probit function, for this Exercise 4.25\n",
      "choice of λ, is illustrated in Figure 4.9.\n",
      "The advantage of using a probit function is that its convolution with a Gaussian\n",
      "can be expressed analytically in terms of another probit function. Speciﬁcally we\n",
      "can show that Exercise 4.26\n",
      "∫\n",
      "Φ(λa)N(a|µ, σ2)da=Φ(µ\n",
      "(λ−2+σ2)1/2)\n",
      ". (4.152)220 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "We now apply the approximation σ(a)≃Φ(λa)to the probit functions appearing\n",
      "on both sides of this equation, leading to the following approximation for the convo-lution of a logistic sigmoid with a Gaussian\n",
      "∫\n",
      "σ(a)N(a|µ, σ2)da≃σ(\n",
      "κ(σ2)µ)\n",
      "(4.153)\n",
      "where we have deﬁned\n",
      "κ(σ2) = (1 + πσ2/8)−1/2. (4.154)\n",
      "Applying this result to (4.151) we obtain the approximate predictive distribution\n",
      "in the form\n",
      "p(C1|φ,t)=σ(\n",
      "κ(σ2\n",
      "a)µa)\n",
      "(4.155)\n",
      "where µaandσ2\n",
      "aare deﬁned by (4.149) and (4.150), respectively, and κ(σ2\n",
      "a)is de-\n",
      "ﬁned by (4.154).\n",
      "Note that the decision boundary corresponding to p(C1|φ,t)=0.5is given by\n",
      "µa=0, which is the same as the decision boundary obtained by using the MAP\n",
      "value for w. Thus if the decision criterion is based on minimizing misclassiﬁca-\n",
      "tion rate, with equal prior probabilities, then the marginalization over whas no ef-\n",
      "fect. However, for more complex decision criteria it will play an important role.Marginalization of the logistic sigmoid model under a Gaussian approximation to\n",
      "the posterior distribution will be illustrated in the context of variational inference in\n",
      "Figure 10.13.\n",
      "Exercises\n",
      "4.1 (⋆⋆)Given a set of data points {xn}, we can deﬁne the convex hull to be the set of\n",
      "all points xgiven by\n",
      "x=∑\n",
      "nαnxn (4.156)\n",
      "where αn⩾0and∑\n",
      "nαn=1. Consider a second set of points {yn}together with\n",
      "their corresponding convex hull. By deﬁnition, the two sets of points will be linearly\n",
      "separable if there exists a vector ˆwand a scalar w0such thatˆwTxn+w0>0for all\n",
      "xn, andˆwTyn+w0<0for allyn. Show that if their convex hulls intersect, the two\n",
      "sets of points cannot be linearly separable, and conversely that if they are linearly\n",
      "separable, their convex hulls do not intersect.\n",
      "4.2 (⋆⋆)www Consider the minimization of a sum-of-squares error function (4.15),\n",
      "and suppose that all of the target vectors in the training set satisfy a linear constraint\n",
      "aTtn+b=0 (4.157)\n",
      "wheretncorresponds to the nthrow of the matrix Tin (4.15). Show that as a\n",
      "consequence of this constraint, the elements of the model prediction y(x)given by\n",
      "the least-squares solution (4.17) also satisfy this constraint, so that\n",
      "aTy(x)+b=0. (4.158)Exercises 221\n",
      "To do so, assume that one of the basis functions φ0(x)=1 so that the corresponding\n",
      "parameter w0plays the role of a bias.\n",
      "4.3 (⋆⋆)Extend the result of Exercise 4.2 to show that if multiple linear constraints\n",
      "are satisﬁed simultaneously by the target vectors, then the same constraints will also\n",
      "be satisﬁed by the least-squares prediction of a linear model.\n",
      "4.4 (⋆)www Show that maximization of the class separation criterion given by (4.23)\n",
      "with respect to w, using a Lagrange multiplier to enforce the constraint wTw=1,\n",
      "leads to the result that w∝(m2−m1).\n",
      "4.5 (⋆)By making use of (4.20), (4.23), and (4.24), show that the Fisher criterion (4.25)\n",
      "can be written in the form (4.26).\n",
      "4.6 (⋆)Using the deﬁnitions of the between-class and within-class covariance matrices\n",
      "given by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the\n",
      "choice of target values described in Section 4.1.5, show that the expression (4.33)\n",
      "that minimizes the sum-of-squares error function can be written in the form (4.37).\n",
      "4.7 (⋆)www Show that the logistic sigmoid function (4.59) satisﬁes the property\n",
      "σ(−a)=1−σ(a)and that its inverse is given by σ−1(y)=l n {y/(1−y)}.\n",
      "4.8 (⋆)Using (4.57) and (4.58), derive the result (4.65) for the posterior class probability\n",
      "in the two-class generative model with Gaussian densities, and verify the results(4.66) and (4.67) for the parameters wandw\n",
      "0.\n",
      "4.9 (⋆)www Consider a generative classiﬁcation model for Kclasses deﬁned by\n",
      "prior class probabilities p(Ck)=πkand general class-conditional densities p(φ|Ck)\n",
      "where φis the input feature vector. Suppose we are given a training data set {φn,tn}\n",
      "where n=1,...,N , andtnis a binary target vector of length Kthat uses the 1-of-\n",
      "Kcoding scheme, so that it has components tnj=Ijkif pattern nis from class Ck.\n",
      "Assuming that the data points are drawn independently from this model, show thatthe maximum-likelihood solution for the prior probabilities is given by\n",
      "π\n",
      "k=Nk\n",
      "N(4.159)\n",
      "where Nkis the number of data points assigned to class Ck.\n",
      "4.10 (⋆⋆)Consider the classiﬁcation model of Exercise 4.9 and now suppose that the\n",
      "class-conditional densities are given by Gaussian distributions with a shared covari-\n",
      "ance matrix, so that\n",
      "p(φ|Ck)=N(φ|µk,Σ). (4.160)\n",
      "Show that the maximum likelihood solution for the mean of the Gaussian distribution\n",
      "for class Ckis given by\n",
      "µk=1\n",
      "NkN∑\n",
      "n=1tnkφn (4.161)222 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "which represents the mean of those feature vectors assigned to class Ck. Similarly,\n",
      "show that the maximum likelihood solution for the shared covariance matrix is givenby\n",
      "Σ=K∑\n",
      "k=1Nk\n",
      "NSk (4.162)\n",
      "where\n",
      "Sk=1\n",
      "NkN∑\n",
      "n=1tnk(φn−µk)(φn−µk)T. (4.163)\n",
      "ThusΣis given by a weighted average of the covariances of the data associated with\n",
      "each class, in which the weighting coefﬁcients are given by the prior probabilities ofthe classes.\n",
      "4.11 (⋆⋆)Consider a classiﬁcation problem with Kclasses for which the feature vector\n",
      "φhasMcomponents each of which can take Ldiscrete states. Let the values of the\n",
      "components be represented by a 1-of- Lbinary coding scheme. Further suppose that,\n",
      "conditioned on the class C\n",
      "k, theMcomponents of φare independent, so that the\n",
      "class-conditional density factorizes with respect to the feature vector components.Show that the quantities a\n",
      "kgiven by (4.63), which appear in the argument to the\n",
      "softmax function describing the posterior class probabilities, are linear functions of\n",
      "the components of φ. Note that this represents an example of the naive Bayes model\n",
      "which is discussed in Section 8.2.2.\n",
      "4.12 (⋆)www Verify the relation (4.88) for the derivative of the logistic sigmoid func-\n",
      "tion deﬁned by (4.59).\n",
      "4.13 (⋆)www By making use of the result (4.88) for the derivative of the logistic sig-\n",
      "moid, show that the derivative of the error function (4.90) for the logistic regressionmodel is given by (4.91).\n",
      "4.14 (⋆)Show that for a linearly separable data set, the maximum likelihood solution\n",
      "for the logistic regression model is obtained by ﬁnding a vector wwhose decision\n",
      "boundary w\n",
      "Tφ(x)=0 separates the classes and then taking the magnitude of wto\n",
      "inﬁnity.\n",
      "4.15 (⋆⋆)Show that the Hessian matrix Hfor the logistic regression model, given by\n",
      "(4.97), is positive deﬁnite. Here Ris a diagonal matrix with elements yn(1−yn),\n",
      "andynis the output of the logistic regression model for input vector xn. Hence show\n",
      "that the error function is a concave function of wand that it has a unique minimum.\n",
      "4.16 (⋆)Consider a binary classiﬁcation problem in which each observation xnis known\n",
      "to belong to one of two classes, corresponding to t=0andt=1, and suppose that\n",
      "the procedure for collecting training data is imperfect, so that training points are\n",
      "sometimes mislabelled. For every data point xn, instead of having a value tfor the\n",
      "class label, we have instead a value πnrepresenting the probability that tn=1.\n",
      "Given a probabilistic model p(t=1|φ), write down the log likelihood function\n",
      "appropriate to such a data set.Exercises 223\n",
      "4.17 (⋆)www Show that the derivatives of the softmax activation function (4.104),\n",
      "where the akare deﬁned by (4.105), are given by (4.106).\n",
      "4.18 (⋆)Using the result (4.91) for the derivatives of the softmax activation function,\n",
      "show that the gradients of the cross-entropy error (4.108) are given by (4.109).\n",
      "4.19 (⋆)www Write down expressions for the gradient of the log likelihood, as well\n",
      "as the corresponding Hessian matrix, for the probit regression model deﬁned in Sec-\n",
      "tion 4.3.5. These are the quantities that would be required to train such a model usingIRLS.\n",
      "4.20 (⋆⋆)Show that the Hessian matrix for the multiclass logistic regression problem,\n",
      "deﬁned by (4.110), is positive semideﬁnite. Note that the full Hessian matrix for\n",
      "this problem is of size MK×MK , where Mis the number of parameters and K\n",
      "is the number of classes. To prove the positive semideﬁnite property, consider the\n",
      "product u\n",
      "THuwhereuis an arbitrary vector of length MK , and then apply Jensen’s\n",
      "inequality.\n",
      "4.21 (⋆)Show that the probit function (4.114) and the erf function (4.115) are related by\n",
      "(4.116).\n",
      "4.22 (⋆)Using the result (4.135), derive the expression (4.137) for the log model evi-\n",
      "dence under the Laplace approximation.\n",
      "4.23 (⋆⋆)www In this exercise, we derive the BIC result (4.139) starting from the\n",
      "Laplace approximation to the model evidence given by (4.137). Show that if the\n",
      "prior over parameters is Gaussian of the form p(θ)=N(θ|m,V0), the log model\n",
      "evidence under the Laplace approximation takes the form\n",
      "lnp(D)≃lnp(D|θMAP)−1\n",
      "2(θMAP−m)TV−1\n",
      "0(θMAP−m)−1\n",
      "2ln|H|+c o n s t\n",
      "whereHis the matrix of second derivatives of the log likelihood lnp(D|θ)evaluated\n",
      "atθMAP. Now assume that the prior is broad so that V−1\n",
      "0is small and the second\n",
      "term on the right-hand side above can be neglected. Furthermore, consider the case\n",
      "of independent, identically distributed data so that His the sum of terms one for each\n",
      "data point. Show that the log model evidence can then be written approximately in\n",
      "the form of the BIC expression (4.139).\n",
      "4.24 (⋆⋆)Use the results from Section 2.3.2 to derive the result (4.151) for the marginal-\n",
      "ization of the logistic regression model with respect to a Gaussian posterior distribu-\n",
      "tion over the parameters w.\n",
      "4.25 (⋆⋆)Suppose we wish to approximate the logistic sigmoid σ(a)deﬁned by (4.59)\n",
      "by a scaled probit function Φ(λa), where Φ(a)is deﬁned by (4.114). Show that if\n",
      "λis chosen so that the derivatives of the two functions are equal at a=0, then\n",
      "λ2=π/8.224 4. LINEAR MODELS FOR CLASSIFICATION\n",
      "4.26 (⋆⋆)In this exercise, we prove the relation (4.152) for the convolution of a probit\n",
      "function with a Gaussian distribution. To do this, show that the derivative of the left-hand side with respect to µis equal to the derivative of the right-hand side, and then\n",
      "integrate both sides with respect to µand then show that the constant of integration\n",
      "vanishes. Note that before differentiating the left-hand side, it is convenient ﬁrstto introduce a change of variable given by a=µ+σzso that the integral over a\n",
      "is replaced by an integral over z. When we differentiate the left-hand side of the\n",
      "relation (4.152), we will then obtain a Gaussian integral over zthat can be evaluated\n",
      "analytically.5\n",
      "Neural\n",
      "Networks\n",
      "In Chapters 3 and 4 we considered models for regression and classiﬁcation that com-\n",
      "prised linear combinations of ﬁxed basis functions. We saw that such models haveuseful analytical and computational properties but that their practical applicability\n",
      "was limited by the curse of dimensionality. In order to apply such models to large-\n",
      "scale problems, it is necessary to adapt the basis functions to the data.\n",
      "Support vector machines (SVMs), discussed in Chapter 7, address this by ﬁrst\n",
      "deﬁning basis functions that are centred on the training data points and then selecting\n",
      "a subset of these during training. One advantage of SVMs is that, although thetraining involves nonlinear optimization, the objective function is convex, and so the\n",
      "solution of the optimization problem is relatively straightforward. The number of\n",
      "basis functions in the resulting models is generally much smaller than the number oftraining points, although it is often still relatively large and typically increases with\n",
      "the size of the training set. The relevance vector machine, discussed in Section 7.2,\n",
      "also chooses a subset from a ﬁxed set of basis functions and typically results in much\n",
      "225226 5. NEURAL NETWORKS\n",
      "sparser models. Unlike the SVM it also produces probabilistic outputs, although this\n",
      "is at the expense of a nonconvex optimization during training.\n",
      "An alternative approach is to ﬁx the number of basis functions in advance but\n",
      "allow them to be adaptive, in other words to use parametric forms for the basis func-\n",
      "tions in which the parameter values are adapted during training. The most successfulmodel of this type in the context of pattern recognition is the feed-forward neural\n",
      "network, also known as the multilayer perceptron , discussed in this chapter. In fact,\n",
      "‘multilayer perceptron’ is really a misnomer, because the model comprises multi-ple layers of logistic regression models (with continuous nonlinearities) rather than\n",
      "multiple perceptrons (with discontinuous nonlinearities). For many applications, the\n",
      "resulting model can be signiﬁcantly more compact, and hence faster to evaluate, than\n",
      "a support vector machine having the same generalization performance. The price to\n",
      "be paid for this compactness, as with the relevance vector machine, is that the like-lihood function, which forms the basis for network training, is no longer a convex\n",
      "function of the model parameters. In practice, however, it is often worth investing\n",
      "substantial computational resources during the training phase in order to obtain acompact model that is fast at processing new data.\n",
      "The term ‘neural network’ has its origins in attempts to ﬁnd mathematical rep-\n",
      "resentations of information processing in biological systems (McCulloch and Pitts,1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al. , 1986). Indeed,\n",
      "it has been used very broadly to cover a wide range of different models, many of\n",
      "which have been the subject of exaggerated claims regarding their biological plau-sibility. From the perspective of practical applications of pattern recognition, how-\n",
      "ever, biological realism would impose entirely unnecessary constraints. Our focus in\n",
      "this chapter is therefore on neural networks as efﬁcient models for statistical patternrecognition. In particular, we shall restrict our attention to the speciﬁc class of neu-\n",
      "ral networks that have proven to be of greatest practical value, namely the multilayer\n",
      "perceptron.\n",
      "We begin by considering the functional form of the network model, including\n",
      "the speciﬁc parameterization of the basis functions, and we then discuss the prob-lem of determining the network parameters within a maximum likelihood frame-\n",
      "work, which involves the solution of a nonlinear optimization problem. This requires\n",
      "the evaluation of derivatives of the log likelihood function with respect to the net-work parameters, and we shall see how these can be obtained efﬁciently using the\n",
      "technique of error backpropagation . We shall also show how the backpropagation\n",
      "framework can be extended to allow other derivatives to be evaluated, such as theJacobian and Hessian matrices. Next we discuss various approaches to regulariza-\n",
      "tion of neural network training and the relationships between them. We also consider\n",
      "some extensions to the neural network model, and in particular we describe a gen-eral framework for modelling conditional probability distributions known as mixture\n",
      "density networks . Finally, we discuss the use of Bayesian treatments of neural net-\n",
      "works. Additional background on neural network models can be found in Bishop(1995a).5.1. Feed-forward Network Functions 227\n",
      "5.1. Feed-forward Network Functions\n",
      "The linear models for regression and classiﬁcation discussed in Chapters 3 and 4, re-\n",
      "spectively, are based on linear combinations of ﬁxed nonlinear basis functions φj(x)\n",
      "and take the form\n",
      "y(x,w)=f(M∑\n",
      "j=1wjφj(x))\n",
      "(5.1)\n",
      "where f(·)is a nonlinear activation function in the case of classiﬁcation and is the\n",
      "identity in the case of regression. Our goal is to extend this model by making the\n",
      "basis functions φj(x)depend on parameters and then to allow these parameters to\n",
      "be adjusted, along with the coefﬁcients {wj}, during training. There are, of course,\n",
      "many ways to construct parametric nonlinear basis functions. Neural networks use\n",
      "basis functions that follow the same form as (5.1), so that each basis function is itselfa nonlinear function of a linear combination of the inputs, where the coefﬁcients in\n",
      "the linear combination are adaptive parameters.\n",
      "This leads to the basic neural network model, which can be described a series\n",
      "of functional transformations. First we construct Mlinear combinations of the input\n",
      "variables x\n",
      "1,...,x Din the form\n",
      "aj=D∑\n",
      "i=1w(1)\n",
      "jixi+w(1)\n",
      "j0 (5.2)\n",
      "where j=1,...,M , and the superscript (1)indicates that the corresponding param-\n",
      "eters are in the ﬁrst ‘layer’ of the network. We shall refer to the parameters w(1)\n",
      "jias\n",
      "weights and the parameters w(1)\n",
      "j0asbiases , following the nomenclature of Chapter 3.\n",
      "The quantities ajare known as activations . Each of them is then transformed using\n",
      "a differentiable, nonlinear activation function h(·)to give\n",
      "zj=h(aj). (5.3)\n",
      "These quantities correspond to the outputs of the basis functions in (5.1) that, in the\n",
      "context of neural networks, are called hidden units . The nonlinear functions h(·)are\n",
      "generally chosen to be sigmoidal functions such as the logistic sigmoid or the ‘ tanh ’\n",
      "function. Following (5.1), these values are again linearly combined to give output Exercise 5.1\n",
      "unit activations\n",
      "ak=M∑\n",
      "j=1w(2)\n",
      "kjzj+w(2)\n",
      "k0(5.4)\n",
      "where k=1,...,K , andKis the total number of outputs. This transformation cor-\n",
      "responds to the second layer of the network, and again the w(2)\n",
      "k0are bias parameters.\n",
      "Finally, the output unit activations are transformed using an appropriate activationfunction to give a set of network outputs y\n",
      "k. The choice of activation function is\n",
      "determined by the nature of the data and the assumed distribution of target variables228 5. NEURAL NETWORKS\n",
      "Figure 5.1 Network diagram for the two-\n",
      "layer neural network corre-\n",
      "sponding to (5.7). The input,\n",
      "hidden, and output variables\n",
      "are represented by nodes, and\n",
      "the weight parameters are rep-\n",
      "resented by links between the\n",
      "nodes, in which the bias pa-\n",
      "rameters are denoted by links\n",
      "coming from additional input\n",
      "and hidden variables x0and\n",
      "z0. Arrows denote the direc-\n",
      "tion of information ﬂow through\n",
      "the network during forward\n",
      "propagation.x0x1xD\n",
      "z0z1zM\n",
      "y1yKw(1)\n",
      "MDw(2)\n",
      "KM\n",
      "w(2)\n",
      "10hidden units\n",
      "inputs outputs\n",
      "and follows the same considerations as for linear models discussed in Chapters 3 and\n",
      "4. Thus for standard regression problems, the activation function is the identity so\n",
      "thatyk=ak. Similarly, for multiple binary classiﬁcation problems, each output unit\n",
      "activation is transformed using a logistic sigmoid function so that\n",
      "yk=σ(ak) (5.5)\n",
      "where\n",
      "σ(a)=1\n",
      "1+e x p ( −a). (5.6)\n",
      "Finally, for multiclass problems, a softmax activation function of the form (4.62)\n",
      "is used. The choice of output unit activation function is discussed in detail in Sec-\n",
      "tion 5.2.\n",
      "We can combine these various stages to give the overall network function that,\n",
      "for sigmoidal output unit activation functions, takes the form\n",
      "yk(x,w)=σ(M∑\n",
      "j=1w(2)\n",
      "kjh(D∑\n",
      "i=1w(1)\n",
      "jixi+w(1)\n",
      "j0)\n",
      "+w(2)\n",
      "k0)\n",
      "(5.7)\n",
      "where the set of all weight and bias parameters have been grouped together into a\n",
      "vectorw. Thus the neural network model is simply a nonlinear function from a set\n",
      "of input variables {xi}to a set of output variables {yk}controlled by a vector wof\n",
      "adjustable parameters.\n",
      "This function can be represented in the form of a network diagram as shown\n",
      "in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward\n",
      "propagation of information through the network. It should be emphasized that these\n",
      "diagrams do not represent probabilistic graphical models of the kind to be consid-\n",
      "ered in Chapter 8 because the internal nodes represent deterministic variables rather\n",
      "than stochastic ones. For this reason, we have adopted a slightly different graphical5.1. Feed-forward Network Functions 229\n",
      "notation for the two kinds of model. We shall see later how to give a probabilistic\n",
      "interpretation to a neural network.\n",
      "As discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into\n",
      "the set of weight parameters by deﬁning an additional input variable x0whose value\n",
      "is clamped at x0=1, so that (5.2) takes the form\n",
      "aj=D∑\n",
      "i=0w(1)\n",
      "jixi. (5.8)\n",
      "We can similarly absorb the second-layer biases into the second-layer weights, so\n",
      "that the overall network function becomes\n",
      "yk(x,w)=σ(M∑\n",
      "j=0w(2)\n",
      "kjh(D∑\n",
      "i=0w(1)\n",
      "jixi))\n",
      ". (5.9)\n",
      "As can be seen from Figure 5.1, the neural network model comprises two stages\n",
      "of processing, each of which resembles the perceptron model of Section 4.1.7, and\n",
      "for this reason the neural network is also known as the multilayer perceptron ,o r\n",
      "MLP. A key difference compared to the perceptron, however, is that the neural net-\n",
      "work uses continuous sigmoidal nonlinearities in the hidden units, whereas the per-\n",
      "ceptron uses step-function nonlinearities. This means that the neural network func-tion is differentiable with respect to the network parameters, and this property will\n",
      "play a central role in network training.\n",
      "If the activation functions of all the hidden units in a network are taken to be\n",
      "linear, then for any such network we can always ﬁnd an equivalent network without\n",
      "hidden units. This follows from the fact that the composition of successive linear\n",
      "transformations is itself a linear transformation. However, if the number of hiddenunits is smaller than either the number of input or output units, then the transforma-\n",
      "tions that the network can generate are not the most general possible linear trans-\n",
      "formations from inputs to outputs because information is lost in the dimensionalityreduction at the hidden units. In Section 12.4.2, we show that networks of linear\n",
      "units give rise to principal component analysis. In general, however, there is little\n",
      "interest in multilayer networks of linear units.\n",
      "The network architecture shown in Figure 5.1 is the most commonly used one\n",
      "in practice. However, it is easily generalized, for instance by considering additional\n",
      "layers of processing each consisting of a weighted linear combination of the form(5.4) followed by an element-wise transformation using a nonlinear activation func-\n",
      "tion. Note that there is some confusion in the literature regarding the terminology\n",
      "for counting the number of layers in such networks. Thus the network in Figure 5.1\n",
      "may be described as a 3-layer network (which counts the number of layers of units,\n",
      "and treats the inputs as units) or sometimes as a single-hidden-layer network (whichcounts the number of layers of hidden units). We recommend a terminology in which\n",
      "Figure 5.1 is called a two-layer network, because it is the number of layers of adap-\n",
      "tive weights that is important for determining the network properties.\n",
      "Another generalization of the network architecture is to include skip-layer con-\n",
      "nections, each of which is associated with a corresponding adaptive parameter. For230 5. NEURAL NETWORKS\n",
      "Figure 5.2 Example of a neural network having a\n",
      "general feed-forward topology. Note that\n",
      "each hidden and output unit has an\n",
      "associated bias parameter (omitted for\n",
      "clarity).\n",
      "x1x2\n",
      "z1\n",
      "z3z2\n",
      "y1y2\n",
      "inputs outputs\n",
      "instance, in a two-layer network these would go directly from inputs to outputs. In\n",
      "principle, a network with sigmoidal hidden units can always mimic skip layer con-\n",
      "nections (for bounded input values) by using a sufﬁciently small ﬁrst-layer weight\n",
      "that, over its operating range, the hidden unit is effectively linear, and then com-\n",
      "pensating with a large weight value from the hidden unit to the output. In practice,\n",
      "however, it may be advantageous to include skip-layer connections explicitly.\n",
      "Furthermore, the network can be sparse, with not all possible connections within\n",
      "a layer being present. We shall see an example of a sparse network architecture when\n",
      "we consider convolutional neural networks in Section 5.5.6.\n",
      "Because there is a direct correspondence between a network diagram and its\n",
      "mathematical function, we can develop more general network mappings by con-\n",
      "sidering more complex network diagrams. However, these must be restricted to a\n",
      "feed-forward architecture, in other words to one having no closed directed cycles, to\n",
      "ensure that the outputs are deterministic functions of the inputs. This is illustrated\n",
      "with a simple example in Figure 5.2. Each (hidden or output) unit in such a network\n",
      "computes a function given by\n",
      "zk=h(∑\n",
      "jwkjzj)\n",
      "(5.10)\n",
      "where the sum runs over all units that send connections to unit k(and a bias param-\n",
      "eter is included in the summation). For a given set of values applied to the inputs of\n",
      "the network, successive application of (5.10) allows the activations of all units in the\n",
      "network to be evaluated including those of the output units.\n",
      "The approximation properties of feed-forward networks have been widely stud-\n",
      "ied (Funahashi, 1989; Cybenko, 1989; Hornik et al. , 1989; Stinchecombe and White,\n",
      "1989; Cotter, 1990; Ito, 1991; Hornik, 1991; Kreinovich, 1991; Ripley, 1996) and\n",
      "found to be very general. Neural networks are therefore said to be universal ap-\n",
      "proximators . For example, a two-layer network with linear outputs can uniformly\n",
      "approximate any continuous function on a compact input domain to arbitrary accu-\n",
      "racy provided the network has a sufﬁciently large number of hidden units. This result\n",
      "holds for a wide range of hidden unit activation functions, but excluding polynomi-\n",
      "als. Although such theorems are reassuring, the key problem is how to ﬁnd suitable\n",
      "parameter values given a set of training data, and in later sections of this chapter we5.1. Feed-forward Network Functions 231\n",
      "Figure 5.3 Illustration of the ca-\n",
      "pability of a multilayer perceptron\n",
      "to approximate four different func-\n",
      "tions comprising (a) f(x)=x2, (b)\n",
      "f(x)=s i n ( x),( c ) , f(x)= |x|,\n",
      "and (d) f(x)=H(x)where H(x)\n",
      "is the Heaviside step function. Ineach case, N=5 0 data points,\n",
      "shown as blue dots, have been sam-pled uniformly in xover the interval\n",
      "(−1,1)and the corresponding val-\n",
      "ues of f(x)evaluated. These data\n",
      "points are then used to train a two-layer network having 3 hidden unitswith ‘tanh’ activation functions and\n",
      "linear output units. The resultingnetwork functions are shown by thered curves, and the outputs of thethree hidden units are shown by thethree dashed curves. (a) (b)\n",
      "(c) (d)\n",
      "will show that there exist effective solutions to this problem based on both maximum\n",
      "likelihood and Bayesian approaches.\n",
      "The capability of a two-layer network to model a broad range of functions is\n",
      "illustrated in Figure 5.3. This ﬁgure also shows how individual hidden units work\n",
      "collaboratively to approximate the ﬁnal function. The role of hidden units in a simple\n",
      "classiﬁcation problem is illustrated in Figure 5.4 using the synthetic classiﬁcationdata set described in Appendix A.\n",
      "5.1.1 Weight-space symmetries\n",
      "One property of feed-forward networks, which will play a role when we consider\n",
      "Bayesian model comparison, is that multiple distinct choices for the weight vector\n",
      "wcan all give rise to the same mapping function from inputs to outputs (Chen et al. ,\n",
      "1993). Consider a two-layer network of the form shown in Figure 5.1 with Mhidden\n",
      "units having ‘ tanh ’ activation functions and full connectivity in both layers. If we\n",
      "change the sign of all of the weights and the bias feeding into a particular hiddenunit, then, for a given input pattern, the sign of the activation of the hidden unit will\n",
      "be reversed, because ‘ tanh ’ is an odd function, so that tanh(−a)=−tanh(a). This\n",
      "transformation can be exactly compensated by changing the sign of all of the weightsleading out of that hidden unit. Thus, by changing the signs of a particular group of\n",
      "weights (and a bias), the input–output mapping function represented by the network\n",
      "is unchanged, and so we have found two different weight vectors that give rise tothe same mapping function. For Mhidden units, there will be Msuch ‘sign-ﬂip’232 5. NEURAL NETWORKS\n",
      "Figure 5.4 Example of the solution of a simple two-\n",
      "class classiﬁcation problem involving\n",
      "synthetic data using a neural network\n",
      "having two inputs, two hidden units with\n",
      "‘tanh’ activation functions, and a single\n",
      "output having a logistic sigmoid activa-\n",
      "tion function. The dashed blue lines\n",
      "show the z=0.5contours for each of\n",
      "the hidden units, and the red line shows\n",
      "they=0.5decision surface for the net-\n",
      "work. For comparison, the green line\n",
      "denotes the optimal decision boundary\n",
      "computed from the distributions used to\n",
      "generate the data.\n",
      "−2 −1 0 1 2−2−10123\n",
      "symmetries, and thus any given weight vector will be one of a set 2Mequivalent\n",
      "weight vectors .\n",
      "Similarly, imagine that we interchange the values of all of the weights (and the\n",
      "bias) leading both into and out of a particular hidden unit with the corresponding\n",
      "values of the weights (and bias) associated with a different hidden unit. Again, this\n",
      "clearly leaves the network input–output mapping function unchanged, but it corre-\n",
      "sponds to a different choice of weight vector. For Mhidden units, any given weight\n",
      "vector will belong to a set of M!equivalent weight vectors associated with this inter-\n",
      "change symmetry, corresponding to the M!different orderings of the hidden units.\n",
      "The network will therefore have an overall weight-space symmetry factor of M!2M.\n",
      "For networks with more than two layers of weights, the total level of symmetry will\n",
      "be given by the product of such factors, one for each layer of hidden units.\n",
      "It turns out that these factors account for all of the symmetries in weight space\n",
      "(except for possible accidental symmetries due to speciﬁc choices for the weight val-\n",
      "ues). Furthermore, the existence of these symmetries is not a particular property of\n",
      "the ‘tanh ’ function but applies to a wide range of activation functions (K ˙urkov ´a and\n",
      "Kainen, 1994). In many cases, these symmetries in weight space are of little practi-\n",
      "cal consequence, although in Section 5.7 we shall encounter a situation in which we\n",
      "need to take them into account.\n",
      "5.2. Network Training\n",
      "So far, we have viewed neural networks as a general class of parametric nonlinear\n",
      "functions from a vector xof input variables to a vector yof output variables. A\n",
      "simple approach to the problem of determining the network parameters is to make an\n",
      "analogy with the discussion of polynomial curve ﬁtting in Section 1.1, and therefore\n",
      "to minimize a sum-of-squares error function. Given a training set comprising a set\n",
      "of input vectors {xn}, where n=1,...,N , together with a corresponding set of5.2. Network Training 233\n",
      "target vectors {tn}, we minimize the error function\n",
      "E(w)=1\n",
      "2N∑\n",
      "n=1∥y(xn,w)−tn∥2. (5.11)\n",
      "However, we can provide a much more general view of network training by ﬁrst\n",
      "giving a probabilistic interpretation to the network outputs. We have already seenmany advantages of using probabilistic predictions in Section 1.5.4. Here it will also\n",
      "provide us with a clearer motivation both for the choice of output unit nonlinearity\n",
      "and the choice of error function.\n",
      "We start by discussing regression problems, and for the moment we consider\n",
      "a single target variable tthat can take any real value. Following the discussions\n",
      "in Section 1.2.5 and 3.1, we assume that thas a Gaussian distribution with an x-\n",
      "dependent mean, which is given by the output of the neural network, so that\n",
      "p(t|x,w)=N(\n",
      "t|y(x,w),β−1)\n",
      "(5.12)\n",
      "where βis the precision (inverse variance) of the Gaussian noise. Of course this\n",
      "is a somewhat restrictive assumption, and in Section 5.6 we shall see how to extend\n",
      "this approach to allow for more general conditional distributions. For the conditionaldistribution given by (5.12), it is sufﬁcient to take the output unit activation function\n",
      "to be the identity, because such a network can approximate any continuous function\n",
      "fromxtoy. Given a data set of Nindependent, identically distributed observations\n",
      "X={x\n",
      "1,...,xN}, along with corresponding target values t={t1,...,t N},w e\n",
      "can construct the corresponding likelihood function\n",
      "p(t|X,w,β)=N∏\n",
      "n=1p(tn|xn,w,β).\n",
      "Taking the negative logarithm, we obtain the error function\n",
      "β\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2−N\n",
      "2lnβ+N\n",
      "2ln(2π) (5.13)\n",
      "which can be used to learn the parameters wandβ. In Section 5.7, we shall dis-\n",
      "cuss the Bayesian treatment of neural networks, while here we consider a maximum\n",
      "likelihood approach. Note that in the neural networks literature, it is usual to con-sider the minimization of an error function rather than the maximization of the (log)\n",
      "likelihood, and so here we shall follow this convention. Consider ﬁrst the determi-\n",
      "nation of w. Maximizing the likelihood function is equivalent to minimizing the\n",
      "sum-of-squares error function given by\n",
      "E(w)=1\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2(5.14)234 5. NEURAL NETWORKS\n",
      "where we have discarded additive and multiplicative constants. The value of wfound\n",
      "by minimizing E(w)will be denoted wMLbecause it corresponds to the maximum\n",
      "likelihood solution. In practice, the nonlinearity of the network function y(xn,w)\n",
      "causes the error E(w)to be nonconvex, and so in practice local maxima of the\n",
      "likelihood may be found, corresponding to local minima of the error function, asdiscussed in Section 5.2.1.\n",
      "Having found w\n",
      "ML, the value of βcan be found by minimizing the negative log\n",
      "likelihood to give\n",
      "1\n",
      "βML=1\n",
      "NN∑\n",
      "n=1{y(xn,wML)−tn}2. (5.15)\n",
      "Note that this can be evaluated once the iterative optimization required to ﬁnd wML\n",
      "is completed. If we have multiple target variables, and we assume that they are inde-\n",
      "pendent conditional on xandwwith shared noise precision β, then the conditional\n",
      "distribution of the target values is given by\n",
      "p(t|x,w)=N(\n",
      "t|y(x,w),β−1I)\n",
      ". (5.16)\n",
      "Following the same argument as for a single target variable, we see that the maximum\n",
      "likelihood weights are determined by minimizing the sum-of-squares error function(5.11). The noise precision is then given by Exercise 5.2\n",
      "1\n",
      "βML=1\n",
      "NKN∑\n",
      "n=1∥y(xn,wML)−tn∥2(5.17)\n",
      "where Kis the number of target variables. The assumption of independence can be\n",
      "dropped at the expense of a slightly more complex optimization problem. Exercise 5.3\n",
      "Recall from Section 4.3.6 that there is a natural pairing of the error function\n",
      "(given by the negative log likelihood) and the output unit activation function. In the\n",
      "regression case, we can view the network as having an output activation function that\n",
      "is the identity, so that yk=ak. The corresponding sum-of-squares error function\n",
      "has the property\n",
      "∂E\n",
      "∂ak=yk−tk (5.18)\n",
      "which we shall make use of when discussing error backpropagation in Section 5.3.\n",
      "Now consider the case of binary classiﬁcation in which we have a single target\n",
      "variable tsuch that t=1 denotes class C1andt=0 denotes class C2. Following\n",
      "the discussion of canonical link functions in Section 4.3.6, we consider a network\n",
      "having a single output whose activation function is a logistic sigmoid\n",
      "y=σ(a)≡1\n",
      "1+e x p ( −a)(5.19)\n",
      "so that 0⩽y(x,w)⩽1. We can interpret y(x,w)as the conditional probability\n",
      "p(C1|x), with p(C2|x)given by 1−y(x,w). The conditional distribution of targets\n",
      "given inputs is then a Bernoulli distribution of the form\n",
      "p(t|x,w)=y(x,w)t{1−y(x,w)}1−t. (5.20)5.2. Network Training 235\n",
      "If we consider a training set of independent observations, then the error function,\n",
      "which is given by the negative log likelihood, is then a cross-entropy error function\n",
      "of the form\n",
      "E(w)=−N∑\n",
      "n=1{tnlnyn+( 1−tn)l n ( 1−yn)} (5.21)\n",
      "where yndenotes y(xn,w). Note that there is no analogue of the noise precision β\n",
      "because the target values are assumed to be correctly labelled. However, the model\n",
      "is easily extended to allow for labelling errors. Simard et al. (2003) found that using Exercise 5.4\n",
      "the cross-entropy error function instead of the sum-of-squares for a classiﬁcation\n",
      "problem leads to faster training as well as improved generalization.\n",
      "If we have Kseparate binary classiﬁcations to perform, then we can use a net-\n",
      "work having Koutputs each of which has a logistic sigmoid activation function.\n",
      "Associated with each output is a binary class label tk∈{0,1}, where k=1,...,K .\n",
      "If we assume that the class labels are independent, given the input vector, then theconditional distribution of the targets is\n",
      "p(t|x,w)=K∏\n",
      "k=1yk(x,w)tk[1−yk(x,w)]1−tk. (5.22)\n",
      "Taking the negative logarithm of the corresponding likelihood function then gives\n",
      "the following error function Exercise 5.5\n",
      "E(w)=−N∑\n",
      "n=1K∑\n",
      "k=1{tnklnynk+( 1−tnk)l n ( 1−ynk)} (5.23)\n",
      "where ynkdenotes yk(xn,w). Again, the derivative of the error function with re-\n",
      "spect to the activation for a particular output unit takes the form (5.18) just as in the Exercise 5.6\n",
      "regression case.\n",
      "It is interesting to contrast the neural network solution to this problem with the\n",
      "corresponding approach based on a linear classiﬁcation model of the kind discussed\n",
      "in Chapter 4. Suppose that we are using a standard two-layer network of the kindshown in Figure 5.1. We see that the weight parameters in the ﬁrst layer of the\n",
      "network are shared between the various outputs, whereas in the linear model each\n",
      "classiﬁcation problem is solved independently. The ﬁrst layer of the network canbe viewed as performing a nonlinear feature extraction, and the sharing of features\n",
      "between the different outputs can save on computation and can also lead to improved\n",
      "generalization.\n",
      "Finally, we consider the standard multiclass classiﬁcation problem in which each\n",
      "input is assigned to one of Kmutually exclusive classes. The binary target variables\n",
      "t\n",
      "k∈{0,1}have a 1-of- Kcoding scheme indicating the class, and the network\n",
      "outputs are interpreted as yk(x,w)=p(tk=1|x), leading to the following error\n",
      "function\n",
      "E(w)=−N∑\n",
      "n=1K∑\n",
      "k=1tknlnyk(xn,w). (5.24)236 5. NEURAL NETWORKS\n",
      "Figure 5.5 Geometrical view of the error function E(w)as\n",
      "a surface sitting over weight space. Point wAis\n",
      "a local minimum and wBis the global minimum.\n",
      "At any point wC, the local gradient of the error\n",
      "surface is given by the vector ∇E.\n",
      "w1\n",
      "w2E(w)\n",
      "wAwBwC\n",
      "∇E\n",
      "Following the discussion of Section 4.3.4, we see that the output unit activation\n",
      "function, which corresponds to the canonical link, is given by the softmax function\n",
      "yk(x,w)=exp(ak(x,w))∑\n",
      "jexp(aj(x,w))(5.25)\n",
      "which satisﬁes 0⩽yk⩽1and∑\n",
      "kyk=1. Note that the yk(x,w)are unchanged\n",
      "if a constant is added to all of the ak(x,w), causing the error function to be constant\n",
      "for some directions in weight space. This degeneracy is removed if an appropriate\n",
      "regularization term (Section 5.5) is added to the error function.\n",
      "Once again, the derivative of the error function with respect to the activation for\n",
      "a particular output unit takes the familiar form (5.18). Exercise 5.7\n",
      "In summary, there is a natural choice of both output unit activation function\n",
      "and matching error function, according to the type of problem being solved. For re-\n",
      "gression we use linear outputs and a sum-of-squares error, for (multiple independent)\n",
      "binary classiﬁcations we use logistic sigmoid outputs and a cross-entropy error func-\n",
      "tion, and for multiclass classiﬁcation we use softmax outputs with the corresponding\n",
      "multiclass cross-entropy error function. For classiﬁcation problems involving two\n",
      "classes, we can use a single logistic sigmoid output, or alternatively we can use a\n",
      "network with two outputs having a softmax output activation function.\n",
      "5.2.1 Parameter optimization\n",
      "We turn next to the task of ﬁnding a weight vector wwhich minimizes the\n",
      "chosen function E(w). At this point, it is useful to have a geometrical picture of the\n",
      "error function, which we can view as a surface sitting over weight space as shown in\n",
      "Figure 5.5. First note that if we make a small step in weight space from wtow+δw\n",
      "then the change in the error function is δE≃δwT∇E(w), where the vector ∇E(w)\n",
      "points in the direction of greatest rate of increase of the error function. Because the\n",
      "errorE(w)is a smooth continuous function of w, its smallest value will occur at a5.2. Network Training 237\n",
      "point in weight space such that the gradient of the error function vanishes, so that\n",
      "∇E(w)=0 (5.26)\n",
      "as otherwise we could make a small step in the direction of −∇E(w)and thereby\n",
      "further reduce the error. Points at which the gradient vanishes are called stationarypoints, and may be further classiﬁed into minima, maxima, and saddle points.\n",
      "Our goal is to ﬁnd a vector wsuch that E(w)takes its smallest value. How-\n",
      "ever, the error function typically has a highly nonlinear dependence on the weightsand bias parameters, and so there will be many points in weight space at which the\n",
      "gradient vanishes (or is numerically very small). Indeed, from the discussion in Sec-\n",
      "tion 5.1.1 we see that for any point wthat is a local minimum, there will be other\n",
      "points in weight space that are equivalent minima. For instance, in a two-layer net-\n",
      "work of the kind shown in Figure 5.1, with Mhidden units, each point in weight\n",
      "space is a member of a family of M!2\n",
      "Mequivalent points. Section 5.1.1\n",
      "Furthermore, there will typically be multiple inequivalent stationary points and\n",
      "in particular multiple inequivalent minima. A minimum that corresponds to thesmallest value of the error function for any weight vector is said to be a global\n",
      "minimum . Any other minima corresponding to higher values of the error function\n",
      "are said to be local minima . For a successful application of neural networks, it may\n",
      "not be necessary to ﬁnd the global minimum (and in general it will not be known\n",
      "whether the global minimum has been found) but it may be necessary to compare\n",
      "several local minima in order to ﬁnd a sufﬁciently good solution.\n",
      "Because there is clearly no hope of ﬁnding an analytical solution to the equa-\n",
      "tion∇E(w)=0 we resort to iterative numerical procedures. The optimization of\n",
      "continuous nonlinear functions is a widely studied problem and there exists an ex-tensive literature on how to solve it efﬁciently. Most techniques involve choosing\n",
      "some initial value w\n",
      "(0)for the weight vector and then moving through weight space\n",
      "in a succession of steps of the form\n",
      "w(τ+1)=w(τ)+∆w(τ)(5.27)\n",
      "where τlabels the iteration step. Different algorithms involve different choices for\n",
      "the weight vector update ∆w(τ). Many algorithms make use of gradient information\n",
      "and therefore require that, after each update, the value of ∇E(w)is evaluated at\n",
      "the new weight vector w(τ+1). In order to understand the importance of gradient\n",
      "information, it is useful to consider a local approximation to the error function based\n",
      "on a Taylor expansion.\n",
      "5.2.2 Local quadratic approximation\n",
      "Insight into the optimization problem, and into the various techniques for solv-\n",
      "ing it, can be obtained by considering a local quadratic approximation to the errorfunction.\n",
      "Consider the Taylor expansion of E(w)around some point\n",
      "ˆwin weight space\n",
      "E(w)≃E(ˆw)+(w−ˆw)Tb+1\n",
      "2(w−ˆw)TH(w−ˆw) (5.28)238 5. NEURAL NETWORKS\n",
      "where cubic and higher terms have been omitted. Here bis deﬁned to be the gradient\n",
      "ofEevaluated at ˆw\n",
      "b≡∇E|w=bw(5.29)\n",
      "and the Hessian matrix H=∇∇Ehas elements\n",
      "(H)ij≡∂E\n",
      "∂wi∂wj⏐⏐⏐⏐\n",
      "w=bw. (5.30)\n",
      "From (5.28), the corresponding local approximation to the gradient is given by\n",
      "∇E≃b+H(w−ˆw). (5.31)\n",
      "For points wthat are sufﬁciently close to ˆw, these expressions will give reasonable\n",
      "approximations for the error and its gradient.\n",
      "Consider the particular case of a local quadratic approximation around a point\n",
      "w⋆that is a minimum of the error function. In this case there is no linear term,\n",
      "because ∇E=0atw⋆, and (5.28) becomes\n",
      "E(w)=E(w⋆)+1\n",
      "2(w−w⋆)TH(w−w⋆) (5.32)\n",
      "where the Hessian His evaluated at w⋆. In order to interpret this geometrically,\n",
      "consider the eigenvalue equation for the Hessian matrix\n",
      "Hui=λiui (5.33)\n",
      "where the eigenvectors uiform a complete orthonormal set (Appendix C) so that\n",
      "uT\n",
      "iuj=δij. (5.34)\n",
      "We now expand (w−w⋆)as a linear combination of the eigenvectors in the form\n",
      "w−w⋆=∑\n",
      "iαiui. (5.35)\n",
      "This can be regarded as a transformation of the coordinate system in which the origin\n",
      "is translated to the point w⋆, and the axes are rotated to align with the eigenvectors\n",
      "(through the orthogonal matrix whose columns are the ui), and is discussed in more\n",
      "detail in Appendix C. Substituting (5.35) into (5.32), and using (5.33) and (5.34),\n",
      "allows the error function to be written in the form\n",
      "E(w)=E(w⋆)+1\n",
      "2∑\n",
      "iλiα2\n",
      "i. (5.36)\n",
      "A matrix His said to be positive deﬁnite if, and only if,\n",
      "vTHv>0 for allv. (5.37)5.2. Network Training 239\n",
      "Figure 5.6 In the neighbourhood of a min-\n",
      "imum w⋆, the error function\n",
      "can be approximated by a\n",
      "quadratic. Contours of con-\n",
      "stant error are then ellipses\n",
      "whose axes are aligned with\n",
      "the eigenvectors uiof the Hes-\n",
      "sian matrix, with lengths that\n",
      "are inversely proportional to the\n",
      "square roots of the correspond-\n",
      "ing eigenvectors λi.\n",
      "w1w2\n",
      "λ−1/2\n",
      "1λ−1/2\n",
      "2u1\n",
      "w⋆u2\n",
      "Because the eigenvectors {ui}form a complete set, an arbitrary vector vcan be\n",
      "written in the form\n",
      "v=∑\n",
      "iciui. (5.38)\n",
      "From (5.33) and (5.34), we then have\n",
      "vTHv=∑\n",
      "ic2\n",
      "iλi (5.39)\n",
      "and so Hwill be positive deﬁnite if, and only if, all of its eigenvalues are positive. Exercise 5.10\n",
      "In the new coordinate system, whose basis vectors are given by the eigenvectors\n",
      "{ui}, the contours of constant Eare ellipses centred on the origin, as illustrated Exercise 5.11\n",
      "in Figure 5.6. For a one-dimensional weight space, a stationary point w⋆will be a\n",
      "minimum if\n",
      "∂2E\n",
      "∂w2⏐⏐⏐⏐\n",
      "w⋆>0. (5.40)\n",
      "The corresponding result in D-dimensions is that the Hessian matrix, evaluated at\n",
      "w⋆, should be positive deﬁnite. Exercise 5.12\n",
      "5.2.3 Use of gradient information\n",
      "As we shall see in Section 5.3, it is possible to evaluate the gradient of an error\n",
      "function efﬁciently by means of the backpropagation procedure. The use of this\n",
      "gradient information can lead to signiﬁcant improvements in the speed with which\n",
      "the minima of the error function can be located. We can see why this is so, as follows.\n",
      "In the quadratic approximation to the error function, given in (5.28), the error\n",
      "surface is speciﬁed by the quantities bandH, which contain a total of W(W+\n",
      "3)/2independent elements (because the matrix His symmetric), where Wis the Exercise 5.13\n",
      "dimensionality of w(i.e., the total number of adaptive parameters in the network).\n",
      "The location of the minimum of this quadratic approximation therefore depends on\n",
      "O(W2)parameters, and we should not expect to be able to locate the minimum until\n",
      "we have gathered O(W2)independent pieces of information. If we do not make\n",
      "use of gradient information, we would expect to have to perform O(W2)function240 5. NEURAL NETWORKS\n",
      "evaluations, each of which would require O(W)steps. Thus, the computational\n",
      "effort needed to ﬁnd the minimum using such an approach would be O(W3).\n",
      "Now compare this with an algorithm that makes use of the gradient information.\n",
      "Because each evaluation of ∇Ebrings Witems of information, we might hope to\n",
      "ﬁnd the minimum of the function in O(W)gradient evaluations. As we shall see,\n",
      "by using error backpropagation, each such evaluation takes only O(W)steps and so\n",
      "the minimum can now be found in O(W2)steps. For this reason, the use of gradient\n",
      "information forms the basis of practical algorithms for training neural networks.\n",
      "5.2.4 Gradient descent optimization\n",
      "The simplest approach to using gradient information is to choose the weight\n",
      "update in (5.27) to comprise a small step in the direction of the negative gradient, sothat\n",
      "w\n",
      "(τ+1)=w(τ)−η∇E(w(τ)) (5.41)\n",
      "where the parameter η>0is known as the learning rate . After each such update, the\n",
      "gradient is re-evaluated for the new weight vector and the process repeated. Note that\n",
      "the error function is deﬁned with respect to a training set, and so each step requiresthat the entire training set be processed in order to evaluate ∇E. Techniques that\n",
      "use the whole data set at once are called batch methods. At each step the weight\n",
      "vector is moved in the direction of the greatest rate of decrease of the error function,and so this approach is known as gradient descent orsteepest descent . Although\n",
      "such an approach might intuitively seem reasonable, in fact it turns out to be a poor\n",
      "algorithm, for reasons discussed in Bishop and Nabney (2008).\n",
      "For batch optimization, there are more efﬁcient methods, such as conjugate gra-\n",
      "dients and quasi-Newton methods, which are much more robust and much faster\n",
      "than simple gradient descent (Gill et al. , 1981; Fletcher, 1987; Nocedal and Wright,\n",
      "1999). Unlike gradient descent, these algorithms have the property that the error\n",
      "function always decreases at each iteration unless the weight vector has arrived at a\n",
      "local or global minimum.\n",
      "In order to ﬁnd a sufﬁciently good minimum, it may be necessary to run a\n",
      "gradient-based algorithm multiple times, each time using a different randomly cho-\n",
      "sen starting point, and comparing the resulting performance on an independent vali-dation set.\n",
      "There is, however, an on-line version of gradient descent that has proved useful\n",
      "in practice for training neural networks on large data sets (Le Cun et al. , 1989).\n",
      "Error functions based on maximum likelihood for a set of independent observations\n",
      "comprise a sum of terms, one for each data point\n",
      "E(w)=\n",
      "N∑\n",
      "n=1En(w). (5.42)\n",
      "On-line gradient descent, also known as sequential gradient descent orstochastic\n",
      "gradient descent , makes an update to the weight vector based on one data point at a\n",
      "time, so that\n",
      "w(τ+1)=w(τ)−η∇En(w(τ)). (5.43)5.3. Error Backpropagation 241\n",
      "This update is repeated by cycling through the data either in sequence or by selecting\n",
      "points at random with replacement. There are of course intermediate scenarios inwhich the updates are based on batches of data points.\n",
      "One advantage of on-line methods compared to batch methods is that the former\n",
      "handle redundancy in the data much more efﬁciently. To see, this consider an ex-treme example in which we take a data set and double its size by duplicating every\n",
      "data point. Note that this simply multiplies the error function by a factor of 2 and so\n",
      "is equivalent to using the original error function. Batch methods will require doublethe computational effort to evaluate the batch error function gradient, whereas on-\n",
      "line methods will be unaffected. Another property of on-line gradient descent is the\n",
      "possibility of escaping from local minima, since a stationary point with respect to\n",
      "the error function for the whole data set will generally not be a stationary point for\n",
      "each data point individually.\n",
      "Nonlinear optimization algorithms, and their practical application to neural net-\n",
      "work training, are discussed in detail in Bishop and Nabney (2008).\n",
      "5.3. Error Backpropagation\n",
      "Our goal in this section is to ﬁnd an efﬁcient technique for evaluating the gradient\n",
      "of an error function E(w)for a feed-forward neural network. We shall see that\n",
      "this can be achieved using a local message passing scheme in which information is\n",
      "sent alternately forwards and backwards through the network and is known as error\n",
      "backpropagation , or sometimes simply as backprop .\n",
      "It should be noted that the term backpropagation is used in the neural com-\n",
      "puting literature to mean a variety of different things. For instance, the multilayer\n",
      "perceptron architecture is sometimes called a backpropagation network. The term\n",
      "backpropagation is also used to describe the training of a multilayer perceptron us-ing gradient descent applied to a sum-of-squares error function. In order to clarify\n",
      "the terminology, it is useful to consider the nature of the training process more care-\n",
      "fully. Most training algorithms involve an iterative procedure for minimization of anerror function, with adjustments to the weights being made in a sequence of steps. At\n",
      "each such step, we can distinguish between two distinct stages. In the ﬁrst stage, the\n",
      "derivatives of the error function with respect to the weights must be evaluated. Aswe shall see, the important contribution of the backpropagation technique is in pro-\n",
      "viding a computationally efﬁcient method for evaluating such derivatives. Because\n",
      "it is at this stage that errors are propagated backwards through the network, we shalluse the term backpropagation speciﬁcally to describe the evaluation of derivatives.\n",
      "In the second stage, the derivatives are then used to compute the adjustments to be\n",
      "made to the weights. The simplest such technique, and the one originally consideredby Rumelhart et al. (1986), involves gradient descent. It is important to recognize\n",
      "that the two stages are distinct. Thus, the ﬁrst stage, namely the propagation of er-\n",
      "rors backwards through the network in order to evaluate derivatives, can be applied\n",
      "to many other kinds of network and not just the multilayer perceptron. It can also be\n",
      "applied to error functions other that just the simple sum-of-squares, and to the eval-242 5. NEURAL NETWORKS\n",
      "uation of other derivatives such as the Jacobian and Hessian matrices, as we shall\n",
      "see later in this chapter. Similarly, the second stage of weight adjustment using thecalculated derivatives can be tackled using a variety of optimization schemes, many\n",
      "of which are substantially more powerful than simple gradient descent.\n",
      "5.3.1 Evaluation of error-function derivatives\n",
      "We now derive the backpropagation algorithm for a general network having ar-\n",
      "bitrary feed-forward topology, arbitrary differentiable nonlinear activation functions,\n",
      "and a broad class of error function. The resulting formulae will then be illustrated\n",
      "using a simple layered network structure having a single layer of sigmoidal hidden\n",
      "units together with a sum-of-squares error.\n",
      "Many error functions of practical interest, for instance those deﬁned by maxi-\n",
      "mum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data\n",
      "point in the training set, so that\n",
      "E(w)=N∑\n",
      "n=1En(w). (5.44)\n",
      "Here we shall consider the problem of evaluating ∇En(w)for one such term in the\n",
      "error function. This may be used directly for sequential optimization, or the results\n",
      "can be accumulated over the training set in the case of batch methods.\n",
      "Consider ﬁrst a simple linear model in which the outputs ykare linear combina-\n",
      "tions of the input variables xiso that\n",
      "yk=∑\n",
      "iwkixi (5.45)\n",
      "together with an error function that, for a particular input pattern n, takes the form\n",
      "En=1\n",
      "2∑\n",
      "k(ynk−tnk)2(5.46)\n",
      "where ynk=yk(xn,w). The gradient of this error function with respect to a weight\n",
      "wjiis given by\n",
      "∂En\n",
      "∂wji=(ynj−tnj)xni (5.47)\n",
      "which can be interpreted as a ‘local’ computation involving the product of an ‘error\n",
      "signal’ ynj−tnjassociated with the output end of the link wjiand the variable xni\n",
      "associated with the input end of the link. In Section 4.3.2, we saw how a similar\n",
      "formula arises with the logistic sigmoid activation function together with the crossentropy error function, and similarly for the softmax activation function together\n",
      "with its matching cross-entropy error function. We shall now see how this simple\n",
      "result extends to the more complex setting of multilayer feed-forward networks.\n",
      "In a general feed-forward network, each unit computes a weighted sum of its\n",
      "inputs of the form\n",
      "a\n",
      "j=∑\n",
      "iwjizi (5.48)5.3. Error Backpropagation 243\n",
      "where ziis the activation of a unit, or input, that sends a connection to unit j, andwji\n",
      "is the weight associated with that connection. In Section 5.1, we saw that biases can\n",
      "be included in this sum by introducing an extra unit, or input, with activation ﬁxed\n",
      "at+1. We therefore do not need to deal with biases explicitly. The sum in (5.48) is\n",
      "transformed by a nonlinear activation function h(·)to give the activation zjof unit j\n",
      "in the form\n",
      "zj=h(aj). (5.49)\n",
      "Note that one or more of the variables ziin the sum in (5.48) could be an input, and\n",
      "similarly, the unit jin (5.49) could be an output.\n",
      "For each pattern in the training set, we shall suppose that we have supplied the\n",
      "corresponding input vector to the network and calculated the activations of all of\n",
      "the hidden and output units in the network by successive application of (5.48) and\n",
      "(5.49). This process is often called forward propagation because it can be regarded\n",
      "as a forward ﬂow of information through the network.\n",
      "Now consider the evaluation of the derivative of Enwith respect to a weight\n",
      "wji. The outputs of the various units will depend on the particular input pattern n.\n",
      "However, in order to keep the notation uncluttered, we shall omit the subscript n\n",
      "from the network variables. First we note that Endepends on the weight wjionly\n",
      "via the summed input ajto unit j. We can therefore apply the chain rule for partial\n",
      "derivatives to give\n",
      "∂En\n",
      "∂wji=∂En\n",
      "∂aj∂aj\n",
      "∂wji. (5.50)\n",
      "We now introduce a useful notation\n",
      "δj≡∂En\n",
      "∂aj(5.51)\n",
      "where the δ’s are often referred to as errors for reasons we shall see shortly. Using\n",
      "(5.48), we can write\n",
      "∂aj\n",
      "∂wji=zi. (5.52)\n",
      "Substituting (5.51) and (5.52) into (5.50), we then obtain\n",
      "∂En\n",
      "∂wji=δjzi. (5.53)\n",
      "Equation (5.53) tells us that the required derivative is obtained simply by multiplying\n",
      "the value of δfor the unit at the output end of the weight by the value of zfor the unit\n",
      "at the input end of the weight (where z=1in the case of a bias). Note that this takes\n",
      "the same form as for the simple linear model considered at the start of this section.Thus, in order to evaluate the derivatives, we need only to calculate the value of δ\n",
      "j\n",
      "for each hidden and output unit in the network, and then apply (5.53).\n",
      "As we have seen already, for the output units, we have\n",
      "δk=yk−tk (5.54)244 5. NEURAL NETWORKS\n",
      "Figure 5.7 Illustration of the calculation of δjfor hidden unit jby\n",
      "backpropagation of the δ’s from those units kto which\n",
      "unitjsends connections. The blue arrow denotes the\n",
      "direction of information ﬂow during forward propagation,\n",
      "and the red arrows indicate the backward propagation\n",
      "of error information.zi\n",
      "zjδjδk\n",
      "δ1wji wkj\n",
      "provided we are using the canonical link as the output-unit activation function. To\n",
      "evaluate the δ’s for hidden units, we again make use of the chain rule for partial\n",
      "derivatives,\n",
      "δj≡∂En\n",
      "∂aj=∑\n",
      "k∂En\n",
      "∂ak∂ak\n",
      "∂aj(5.55)\n",
      "where the sum runs over all units kto which unit jsends connections. The arrange-\n",
      "ment of units and weights is illustrated in Figure 5.7. Note that the units labelled k\n",
      "could include other hidden units and/or output units. In writing down (5.55), we are\n",
      "making use of the fact that variations in ajgive rise to variations in the error func-\n",
      "tion only through variations in the variables ak. If we now substitute the deﬁnition\n",
      "ofδgiven by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the\n",
      "following backpropagation formula\n",
      "δj=h′(aj)∑\n",
      "kwkjδk (5.56)\n",
      "which tells us that the value of δfor a particular hidden unit can be obtained by\n",
      "propagating the δ’s backwards from units higher up in the network, as illustrated\n",
      "in Figure 5.7. Note that the summation in (5.56) is taken over the ﬁrst index on\n",
      "wkj(corresponding to backward propagation of information through the network),\n",
      "whereas in the forward propagation equation (5.10) it is taken over the second index.\n",
      "Because we already know the values of the δ’s for the output units, it follows that\n",
      "by recursively applying (5.56) we can evaluate the δ’s for all of the hidden units in a\n",
      "feed-forward network, regardless of its topology.\n",
      "The backpropagation procedure can therefore be summarized as follows.\n",
      "Error Backpropagation\n",
      "1. Apply an input vector xnto the network and forward propagate through\n",
      "the network using (5.48) and (5.49) to ﬁnd the activations of all the hidden\n",
      "and output units.\n",
      "2. Evaluate the δkfor all the output units using (5.54).\n",
      "3. Backpropagate the δ’s using (5.56) to obtain δjfor each hidden unit in the\n",
      "network.\n",
      "4. Use (5.53) to evaluate the required derivatives.5.3. Error Backpropagation 245\n",
      "For batch methods, the derivative of the total error Ecan then be obtained by\n",
      "repeating the above steps for each pattern in the training set and then summing overall patterns:\n",
      "∂E\n",
      "∂wji=∑\n",
      "n∂En\n",
      "∂wji. (5.57)\n",
      "In the above derivation we have implicitly assumed that each hidden or output unit in\n",
      "the network has the same activation function h(·). The derivation is easily general-\n",
      "ized, however, to allow different units to have individual activation functions, simply\n",
      "by keeping track of which form of h(·)goes with which unit.\n",
      "5.3.2 A simple example\n",
      "The above derivation of the backpropagation procedure allowed for general\n",
      "forms for the error function, the activation functions, and the network topology. In\n",
      "order to illustrate the application of this algorithm, we shall consider a particularexample. This is chosen both for its simplicity and for its practical importance, be-\n",
      "cause many applications of neural networks reported in the literature make use of\n",
      "this type of network. Speciﬁcally, we shall consider a two-layer network of the formillustrated in Figure 5.1, together with a sum-of-squares error, in which the output\n",
      "units have linear activation functions, so that y\n",
      "k=ak, while the hidden units have\n",
      "logistic sigmoid activation functions given by\n",
      "h(a)≡tanh(a) (5.58)\n",
      "where\n",
      "tanh(a)=ea−e−a\n",
      "ea+e−a. (5.59)\n",
      "A useful feature of this function is that its derivative can be expressed in a par-\n",
      "ticularly simple form:\n",
      "h′(a)=1−h(a)2. (5.60)\n",
      "We also consider a standard sum-of-squares error function, so that for pattern nthe\n",
      "error is given by\n",
      "En=1\n",
      "2K∑\n",
      "k=1(yk−tk)2(5.61)\n",
      "where ykis the activation of output unit k, andtkis the corresponding target, for a\n",
      "particular input pattern xn.\n",
      "For each pattern in the training set in turn, we ﬁrst perform a forward propagation\n",
      "using\n",
      "aj=D∑\n",
      "i=0w(1)\n",
      "jixi (5.62)\n",
      "zj=t a n h ( aj) (5.63)\n",
      "yk=M∑\n",
      "j=0w(2)\n",
      "kjzj. (5.64)246 5. NEURAL NETWORKS\n",
      "Next we compute the δ’s for each output unit using\n",
      "δk=yk−tk. (5.65)\n",
      "Then we backpropagate these to obtain δs for the hidden units using\n",
      "δj=( 1−z2\n",
      "j)K∑\n",
      "k=1wkjδk. (5.66)\n",
      "Finally, the derivatives with respect to the ﬁrst-layer and second-layer weights are\n",
      "given by\n",
      "∂En\n",
      "∂w(1)\n",
      "ji=δjxi,∂En\n",
      "∂w(2)\n",
      "kj=δkzj. (5.67)\n",
      "5.3.3 Efﬁciency of backpropagation\n",
      "One of the most important aspects of backpropagation is its computational efﬁ-\n",
      "ciency. To understand this, let us examine how the number of computer operations\n",
      "required to evaluate the derivatives of the error function scales with the total number\n",
      "Wof weights and biases in the network. A single evaluation of the error function\n",
      "(for a given input pattern) would require O(W)operations, for sufﬁciently large W.\n",
      "This follows from the fact that, except for a network with very sparse connections,the number of weights is typically much greater than the number of units, and so the\n",
      "bulk of the computational effort in forward propagation is concerned with evaluat-\n",
      "ing the sums in (5.48), with the evaluation of the activation functions representing asmall overhead. Each term in the sum in (5.48) requires one multiplication and one\n",
      "addition, leading to an overall computational cost that is O(W).\n",
      "An alternative approach to backpropagation for computing the derivatives of the\n",
      "error function is to use ﬁnite differences. This can be done by perturbing each weight\n",
      "in turn, and approximating the derivatives by the expression\n",
      "∂E\n",
      "n\n",
      "∂wji=En(wji+ϵ)−En(wji)\n",
      "ϵ+O(ϵ) (5.68)\n",
      "where ϵ≪1. In a software simulation, the accuracy of the approximation to the\n",
      "derivatives can be improved by making ϵsmaller, until numerical roundoff problems\n",
      "arise. The accuracy of the ﬁnite differences method can be improved signiﬁcantly\n",
      "by using symmetrical central differences of the form\n",
      "∂En\n",
      "∂wji=En(wji+ϵ)−En(wji−ϵ)\n",
      "2ϵ+O(ϵ2). (5.69)\n",
      "In this case, the O(ϵ)corrections cancel, as can be veriﬁed by Taylor expansion on Exercise 5.14\n",
      "the right-hand side of (5.69), and so the residual corrections are O(ϵ2). The number\n",
      "of computational steps is, however, roughly doubled compared with (5.68).\n",
      "The main problem with numerical differentiation is that the highly desirable\n",
      "O(W)scaling has been lost. Each forward propagation requires O(W)steps, and5.3. Error Backpropagation 247\n",
      "Figure 5.8 Illustration of a modular pattern\n",
      "recognition system in which the\n",
      "Jacobian matrix can be used\n",
      "to backpropagate error signals\n",
      "from the outputs through to ear-\n",
      "lier modules in the system.\n",
      "xu\n",
      "wy\n",
      "zv\n",
      "there are Wweights in the network each of which must be perturbed individually, so\n",
      "that the overall scaling is O(W2).\n",
      "However, numerical differentiation plays an important role in practice, because a\n",
      "comparison of the derivatives calculated by backpropagation with those obtained us-\n",
      "ing central differences provides a powerful check on the correctness of any software\n",
      "implementation of the backpropagation algorithm. When training networks in prac-\n",
      "tice, derivatives should be evaluated using backpropagation, because this gives the\n",
      "greatest accuracy and numerical efﬁciency. However, the results should be compared\n",
      "with numerical differentiation using (5.69) for some test cases in order to check the\n",
      "correctness of the implementation.\n",
      "5.3.4 The Jacobian matrix\n",
      "We have seen how the derivatives of an error function with respect to the weights\n",
      "can be obtained by the propagation of errors backwards through the network. The\n",
      "technique of backpropagation can also be applied to the calculation of other deriva-\n",
      "tives. Here we consider the evaluation of the Jacobian matrix, whose elements are\n",
      "given by the derivatives of the network outputs with respect to the inputs\n",
      "Jki≡∂yk\n",
      "∂xi(5.70)\n",
      "where each such derivative is evaluated with all other inputs held ﬁxed. Jacobian\n",
      "matrices play a useful role in systems built from a number of distinct modules, as\n",
      "illustrated in Figure 5.8. Each module can comprise a ﬁxed or adaptive function,\n",
      "which can be linear or nonlinear, so long as it is differentiable. Suppose we wish\n",
      "to minimize an error function Ewith respect to the parameter win Figure 5.8. The\n",
      "derivative of the error function is given by\n",
      "∂E\n",
      "∂w=∑\n",
      "k,j∂E\n",
      "∂yk∂yk\n",
      "∂zj∂zj\n",
      "∂w(5.71)\n",
      "in which the Jacobian matrix for the red module in Figure 5.8 appears in the middle\n",
      "term.\n",
      "Because the Jacobian matrix provides a measure of the local sensitivity of the\n",
      "outputs to changes in each of the input variables, it also allows any known errors ∆xi248 5. NEURAL NETWORKS\n",
      "associated with the inputs to be propagated through the trained network in order to\n",
      "estimate their contribution ∆ykto the errors at the outputs, through the relation\n",
      "∆yk≃∑\n",
      "i∂yk\n",
      "∂xi∆xi (5.72)\n",
      "which is valid provided the |∆xi|are small. In general, the network mapping rep-\n",
      "resented by a trained neural network will be nonlinear, and so the elements of theJacobian matrix will not be constants but will depend on the particular input vector\n",
      "used. Thus (5.72) is valid only for small perturbations of the inputs, and the Jacobian\n",
      "itself must be re-evaluated for each new input vector.\n",
      "The Jacobian matrix can be evaluated using a backpropagation procedure that is\n",
      "similar to the one derived earlier for evaluating the derivatives of an error function\n",
      "with respect to the weights. We start by writing the element J\n",
      "kiin the form\n",
      "Jki=∂yk\n",
      "∂xi=∑\n",
      "j∂yk\n",
      "∂aj∂aj\n",
      "∂xi\n",
      "=∑\n",
      "jwji∂yk\n",
      "∂aj(5.73)\n",
      "where we have made use of (5.48). The sum in (5.73) runs over all units jto which\n",
      "the input unit isends connections (for example, over all units in the ﬁrst hidden\n",
      "layer in the layered topology considered earlier). We now write down a recursivebackpropagation formula to determine the derivatives ∂y\n",
      "k/∂aj\n",
      "∂yk\n",
      "∂aj=∑\n",
      "l∂yk\n",
      "∂al∂al\n",
      "∂aj\n",
      "=h′(aj)∑\n",
      "lwlj∂yk\n",
      "∂al(5.74)\n",
      "where the sum runs over all units lto which unit jsends connections (corresponding\n",
      "to the ﬁrst index of wlj). Again, we have made use of (5.48) and (5.49). This\n",
      "backpropagation starts at the output units for which the required derivatives can be\n",
      "found directly from the functional form of the output-unit activation function. For\n",
      "instance, if we have individual sigmoidal activation functions at each output unit,then\n",
      "∂y\n",
      "k\n",
      "∂aj=δkjσ′(aj) (5.75)\n",
      "whereas for softmax outputs we have\n",
      "∂yk\n",
      "∂aj=δkjyk−ykyj. (5.76)\n",
      "We can summarize the procedure for evaluating the Jacobian matrix as follows.\n",
      "Apply the input vector corresponding to the point in input space at which the Ja-\n",
      "cobian matrix is to be found, and forward propagate in the usual way to obtain the5.4. The Hessian Matrix 249\n",
      "activations of all of the hidden and output units in the network. Next, for each row\n",
      "kof the Jacobian matrix, corresponding to the output unit k, backpropagate using\n",
      "the recursive relation (5.74), starting with (5.75) or (5.76), for all of the hidden units\n",
      "in the network. Finally, use (5.73) to do the backpropagation to the inputs. The\n",
      "Jacobian can also be evaluated using an alternative forward propagation formalism,\n",
      "which can be derived in an analogous way to the backpropagation approach given\n",
      "here. Exercise 5.15\n",
      "Again, the implementation of such algorithms can be checked by using numeri-\n",
      "cal differentiation in the form\n",
      "∂yk\n",
      "∂xi=yk(xi+ϵ)−yk(xi−ϵ)\n",
      "2ϵ+O(ϵ2) (5.77)\n",
      "which involves 2Dforward propagations for a network having Dinputs.\n",
      "5.4. The Hessian Matrix\n",
      "We have shown how the technique of backpropagation can be used to obtain the ﬁrst\n",
      "derivatives of an error function with respect to the weights in the network. Back-\n",
      "propagation can also be used to evaluate the second derivatives of the error, given\n",
      "by\n",
      "∂2E\n",
      "∂wji∂wlk. (5.78)\n",
      "Note that it is sometimes convenient to consider all of the weight and bias parameters\n",
      "as elements wiof a single vector, denoted w, in which case the second derivatives\n",
      "form the elements Hijof the Hessian matrixH, where i, j∈{1,...,W }andWis\n",
      "the total number of weights and biases. The Hessian plays an important role in manyaspects of neural computing, including the following:\n",
      "1. Several nonlinear optimization algorithms used for training neural networks\n",
      "are based on considerations of the second-order properties of the error surface,\n",
      "which are controlled by the Hessian matrix (Bishop and Nabney, 2008).\n",
      "2. The Hessian forms the basis of a fast procedure for re-training a feed-forward\n",
      "network following a small change in the training data (Bishop, 1991).\n",
      "3. The inverse of the Hessian has been used to identify the least signiﬁcant weights\n",
      "in a network as part of network ‘pruning’ algorithms (Le Cun et al. , 1990).\n",
      "4. The Hessian plays a central role in the Laplace approximation for a Bayesian\n",
      "neural network (see Section 5.7). Its inverse is used to determine the predic-\n",
      "tive distribution for a trained network, its eigenvalues determine the values of\n",
      "hyperparameters, and its determinant is used to evaluate the model evidence.\n",
      "Various approximation schemes have been used to evaluate the Hessian matrix\n",
      "for a neural network. However, the Hessian can also be calculated exactly using an\n",
      "extension of the backpropagation technique.250 5. NEURAL NETWORKS\n",
      "An important consideration for many applications of the Hessian is the efﬁciency\n",
      "with which it can be evaluated. If there are Wparameters (weights and biases) in the\n",
      "network, then the Hessian matrix has dimensions W×Wand so the computational\n",
      "effort needed to evaluate the Hessian will scale like O(W2)for each pattern in the\n",
      "data set. As we shall see, there are efﬁcient methods for evaluating the Hessianwhose scaling is indeed O(W\n",
      "2).\n",
      "5.4.1 Diagonal approximation\n",
      "Some of the applications for the Hessian matrix discussed above require the\n",
      "inverse of the Hessian, rather than the Hessian itself. For this reason, there has\n",
      "been some interest in using a diagonal approximation to the Hessian, in other wordsone that simply replaces the off-diagonal elements with zeros, because its inverse is\n",
      "trivial to evaluate. Again, we shall consider an error function that consists of a sum\n",
      "of terms, one for each pattern in the data set, so that E=∑\n",
      "nEn. The Hessian can\n",
      "then be obtained by considering one pattern at a time, and then summing the results\n",
      "over all patterns. From (5.48), the diagonal elements of the Hessian, for pattern n,\n",
      "can be written\n",
      "∂2En\n",
      "∂w2\n",
      "ji=∂2En\n",
      "∂a2\n",
      "jz2\n",
      "i. (5.79)\n",
      "Using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can\n",
      "be found recursively using the chain rule of differential calculus to give a backprop-\n",
      "agation equation of the form\n",
      "∂2En\n",
      "∂a2\n",
      "j=h′(aj)2∑\n",
      "k∑\n",
      "k′wkjwk′j∂2En\n",
      "∂ak∂ak′+h′′(aj)∑\n",
      "kwkj∂En\n",
      "∂ak. (5.80)\n",
      "If we now neglect off-diagonal elements in the second-derivative terms, we obtain\n",
      "(Becker and Le Cun, 1989; Le Cun et al. , 1990)\n",
      "∂2En\n",
      "∂a2\n",
      "j=h′(aj)2∑\n",
      "kw2\n",
      "kj∂2En\n",
      "∂a2\n",
      "k+h′′(aj)∑\n",
      "kwkj∂En\n",
      "∂ak. (5.81)\n",
      "Note that the number of computational steps required to evaluate this approximation\n",
      "isO(W), where Wis the total number of weight and bias parameters in the network,\n",
      "compared with O(W2)for the full Hessian.\n",
      "Ricotti et al. (1988) also used the diagonal approximation to the Hessian, but\n",
      "they retained all terms in the evaluation of ∂2En/∂a2\n",
      "jand so obtained exact expres-\n",
      "sions for the diagonal terms. Note that this no longer has O(W)scaling. The major\n",
      "problem with diagonal approximations, however, is that in practice the Hessian istypically found to be strongly nondiagonal, and so these approximations, which are\n",
      "driven mainly be computational convenience, must be treated with care.5.4. The Hessian Matrix 251\n",
      "5.4.2 Outer product approximation\n",
      "When neural networks are applied to regression problems, it is common to use\n",
      "a sum-of-squares error function of the form\n",
      "E=1\n",
      "2N∑\n",
      "n=1(yn−tn)2(5.82)\n",
      "where we have considered the case of a single output in order to keep the notation\n",
      "simple (the extension to several outputs is straightforward). We can then write the Exercise 5.16\n",
      "Hessian matrix in the form\n",
      "H=∇∇E=N∑\n",
      "n=1∇yn∇yn+N∑\n",
      "n=1(yn−tn)∇∇yn. (5.83)\n",
      "If the network has been trained on the data set, and its outputs ynhappen to be very\n",
      "close to the target values tn, then the second term in (5.83) will be small and can\n",
      "be neglected. More generally, however, it may be appropriate to neglect this term\n",
      "by the following argument. Recall from Section 1.5.5 that the optimal function thatminimizes a sum-of-squares loss is the conditional average of the target data. The\n",
      "quantity (y\n",
      "n−tn)is then a random variable with zero mean. If we assume that its\n",
      "value is uncorrelated with the value of the second derivative term on the right-handside of (5.83), then the whole term will average to zero in the summation over n. Exercise 5.17\n",
      "By neglecting the second term in (5.83), we arrive at the Levenberg–Marquardt\n",
      "approximation or outer product approximation (because the Hessian matrix is built\n",
      "up from a sum of outer products of vectors), given by\n",
      "H≃N∑\n",
      "n=1bnbT\n",
      "n (5.84)\n",
      "wherebn=∇yn=∇anbecause the activation function for the output units is\n",
      "simply the identity. Evaluation of the outer product approximation for the Hessian\n",
      "is straightforward as it only involves ﬁrst derivatives of the error function, whichcan be evaluated efﬁciently in O(W)steps using standard backpropagation. The\n",
      "elements of the matrix can then be found in O(W\n",
      "2)steps by simple multiplication.\n",
      "It is important to emphasize that this approximation is only likely to be valid for anetwork that has been trained appropriately, and that for a general network mapping\n",
      "the second derivative terms on the right-hand side of (5.83) will typically not be\n",
      "negligible.\n",
      "In the case of the cross-entropy error function for a network with logistic sigmoid\n",
      "output-unit activation functions, the corresponding approximation is given by Exercise 5.19\n",
      "H≃N∑\n",
      "n=1yn(1−yn)bnbT\n",
      "n. (5.85)\n",
      "An analogous result can be obtained for multiclass networks having softmax output-\n",
      "unit activation functions. Exercise 5.20252 5. NEURAL NETWORKS\n",
      "5.4.3 Inverse Hessian\n",
      "We can use the outer-product approximation to develop a computationally ef-\n",
      "ﬁcient procedure for approximating the inverse of the Hessian (Hassibi and Stork,\n",
      "1993). First we write the outer-product approximation in matrix notation as\n",
      "HN=N∑\n",
      "n=1bnbT\n",
      "n (5.86)\n",
      "wherebn≡∇wanis the contribution to the gradient of the output unit activation\n",
      "arising from data point n. We now derive a sequential procedure for building up the\n",
      "Hessian by including data points one at a time. Suppose we have already obtainedthe inverse Hessian using the ﬁrst Ldata points. By separating off the contribution\n",
      "from data point L+1, we obtain\n",
      "H\n",
      "L+1=HL+bL+1bT\n",
      "L+1. (5.87)\n",
      "In order to evaluate the inverse of the Hessian, we now consider the matrix identity\n",
      "(\n",
      "M+vvT)−1=M−1−(M−1v)(\n",
      "vTM−1)\n",
      "1+vTM−1v(5.88)\n",
      "whereIis the unit matrix, which is simply a special case of the Woodbury identity\n",
      "(C.7). If we now identify HLwithMandbL+1withv, we obtain\n",
      "H−1\n",
      "L+1=H−1\n",
      "L−H−1\n",
      "LbL+1bT\n",
      "L+1H−1\n",
      "L\n",
      "1+bT\n",
      "L+1H−1\n",
      "LbL+1. (5.89)\n",
      "In this way, data points are sequentially absorbed until L+1 = Nand the whole data\n",
      "set has been processed. This result therefore represents a procedure for evaluating\n",
      "the inverse of the Hessian using a single pass through the data set. The initial matrix\n",
      "H0is chosen to be αI, where αis a small quantity, so that the algorithm actually\n",
      "ﬁnds the inverse of H+αI. The results are not particularly sensitive to the precise\n",
      "value of α. Extension of this algorithm to networks having more than one output is\n",
      "straightforward. Exercise 5.21\n",
      "We note here that the Hessian matrix can sometimes be calculated indirectly as\n",
      "part of the network training algorithm. In particular, quasi-Newton nonlinear opti-mization algorithms gradually build up an approximation to the inverse of the Hes-\n",
      "sian during training. Such algorithms are discussed in detail in Bishop and Nabney\n",
      "(2008).\n",
      "5.4.4 Finite differences\n",
      "As in the case of the ﬁrst derivatives of the error function, we can ﬁnd the second\n",
      "derivatives by using ﬁnite differences, with accuracy limited by numerical precision.\n",
      "If we perturb each possible pair of weights in turn, we obtain\n",
      "∂2E\n",
      "∂wji∂wlk=1\n",
      "4ϵ2{E(wji+ϵ, wlk+ϵ)−E(wji+ϵ, wlk−ϵ)\n",
      "−E(wji−ϵ, wlk+ϵ)+E(wji−ϵ, wlk−ϵ)}+O(ϵ2). (5.90)5.4. The Hessian Matrix 253\n",
      "Again, by using a symmetrical central differences formulation, we ensure that the\n",
      "residual errors are O(ϵ2)rather than O(ϵ). Because there are W2elements in the\n",
      "Hessian matrix, and because the evaluation of each element requires four forward\n",
      "propagations each needing O(W)operations (per pattern), we see that this approach\n",
      "will require O(W3)operations to evaluate the complete Hessian. It therefore has\n",
      "poor scaling properties, although in practice it is very useful as a check on the soft-\n",
      "ware implementation of backpropagation methods.\n",
      "A more efﬁcient version of numerical differentiation can be found by applying\n",
      "central differences to the ﬁrst derivatives of the error function, which are themselves\n",
      "calculated using backpropagation. This gives\n",
      "∂2E\n",
      "∂wji∂wlk=1\n",
      "2ϵ{∂E\n",
      "∂wji(wlk+ϵ)−∂E\n",
      "∂wji(wlk−ϵ)}\n",
      "+O(ϵ2). (5.91)\n",
      "Because there are now only Wweights to be perturbed, and because the gradients\n",
      "can be evaluated in O(W)steps, we see that this method gives the Hessian in O(W2)\n",
      "operations.\n",
      "5.4.5 Exact evaluation of the Hessian\n",
      "So far, we have considered various approximation schemes for evaluating the\n",
      "Hessian matrix or its inverse. The Hessian can also be evaluated exactly, for a net-work of arbitrary feed-forward topology, using extension of the technique of back-\n",
      "propagation used to evaluate ﬁrst derivatives, which shares many of its desirable\n",
      "features including computational efﬁciency (Bishop, 1991; Bishop, 1992). It can beapplied to any differentiable error function that can be expressed as a function of\n",
      "the network outputs and to networks having arbitrary differentiable activation func-\n",
      "tions. The number of computational steps needed to evaluate the Hessian scaleslikeO(W\n",
      "2). Similar algorithms have also been considered by Buntine and Weigend\n",
      "(1993).\n",
      "Here we consider the speciﬁc case of a network having two layers of weights,\n",
      "for which the required equations are easily derived. We shall use indices iandi′Exercise 5.22\n",
      "to denote inputs, indices jandj′to denoted hidden units, and indices kandk′to\n",
      "denote outputs. We ﬁrst deﬁne\n",
      "δk=∂En\n",
      "∂ak,M kk′≡∂2En\n",
      "∂ak∂ak′(5.92)\n",
      "where Enis the contribution to the error from data point n. The Hessian matrix for\n",
      "this network can then be considered in three separate blocks as follows.\n",
      "1. Both weights in the second layer:\n",
      "∂2En\n",
      "∂w(2)\n",
      "kj∂w(2)\n",
      "k′j′=zjzj′Mkk′. (5.93)254 5. NEURAL NETWORKS\n",
      "2. Both weights in the ﬁrst layer:\n",
      "∂2En\n",
      "∂w(1)\n",
      "ji∂w(1)\n",
      "j′i′=xixi′h′′(aj′)Ijj′∑\n",
      "kw(2)\n",
      "kj′δk\n",
      "+xixi′h′(aj′)h′(aj)∑\n",
      "k∑\n",
      "k′w(2)\n",
      "k′j′w(2)\n",
      "kjMkk′. (5.94)\n",
      "3. One weight in each layer:\n",
      "∂2En\n",
      "∂w(1)\n",
      "ji∂w(2)\n",
      "kj′=xih′(aj′){\n",
      "δkIjj′+zj∑\n",
      "k′w(2)\n",
      "k′j′Hkk′}\n",
      ". (5.95)\n",
      "HereIjj′is thej,j′element of the identity matrix. If one or both of the weights is\n",
      "a bias term, then the corresponding expressions are obtained simply by setting theappropriate activation(s) to 1. Inclusion of skip-layer connections is straightforward. Exercise 5.23\n",
      "5.4.6 Fast multiplication by the Hessian\n",
      "For many applications of the Hessian, the quantity of interest is not the Hessian\n",
      "matrix Hitself but the product of Hwith some vector v. We have seen that the\n",
      "evaluation of the Hessian takes O(W2)operations, and it also requires storage that is\n",
      "O(W2). The vector vTHthat we wish to calculate, however, has only Welements,\n",
      "so instead of computing the Hessian as an intermediate step, we can instead try to\n",
      "ﬁnd an efﬁcient approach to evaluating vTHdirectly in a way that requires only\n",
      "O(W)operations.\n",
      "To do this, we ﬁrst note that\n",
      "vTH=vT∇(∇E) (5.96)\n",
      "where ∇denotes the gradient operator in weight space. We can then write down\n",
      "the standard forward-propagation and backpropagation equations for the evaluation\n",
      "of∇Eand apply (5.96) to these equations to give a set of forward-propagation and\n",
      "backpropagation equations for the evaluation of vTH(Møller, 1993; Pearlmutter,\n",
      "1994). This corresponds to acting on the original forward-propagation and back-\n",
      "propagation equations with a differential operator vT∇. Pearlmutter (1994) used the\n",
      "notation R{·} to denote the operator vT∇, and we shall follow this convention. The\n",
      "analysis is straightforward and makes use of the usual rules of differential calculus,\n",
      "together with the result\n",
      "R{w}=v. (5.97)\n",
      "The technique is best illustrated with a simple example, and again we choose a\n",
      "two-layer network of the form shown in Figure 5.1, with linear output units and a\n",
      "sum-of-squares error function. As before, we consider the contribution to the error\n",
      "function from one pattern in the data set. The required vector is then obtained as5.4. The Hessian Matrix 255\n",
      "usual by summing over the contributions from each of the patterns separately. For\n",
      "the two-layer network, the forward-propagation equations are given by\n",
      "aj=∑\n",
      "iwjixi (5.98)\n",
      "zj=h(aj) (5.99)\n",
      "yk=∑\n",
      "jwkjzj. (5.100)\n",
      "We now act on these equations using the R{·} operator to obtain a set of forward\n",
      "propagation equations in the form\n",
      "R{aj}=∑\n",
      "ivjixi (5.101)\n",
      "R{zj}=h′(aj)R{aj} (5.102)\n",
      "R{yk}=∑\n",
      "jwkjR{zj}+∑\n",
      "jvkjzj (5.103)\n",
      "where vjiis the element of the vector vthat corresponds to the weight wji. Quan-\n",
      "tities of the form R{zj},R{aj}andR{yk}are to be regarded as new variables\n",
      "whose values are found using the above equations.\n",
      "Because we are considering a sum-of-squares error function, we have the fol-\n",
      "lowing standard backpropagation expressions:\n",
      "δk=yk−tk (5.104)\n",
      "δj=h′(aj)∑\n",
      "kwkjδk. (5.105)\n",
      "Again, we act on these equations with the R{·} operator to obtain a set of backprop-\n",
      "agation equations in the form\n",
      "R{δk}=R{yk} (5.106)\n",
      "R{δj}=h′′(aj)R{aj}∑\n",
      "kwkjδk\n",
      "+h′(aj)∑\n",
      "kvkjδk+h′(aj)∑\n",
      "kwkjR{δk}. (5.107)\n",
      "Finally, we have the usual equations for the ﬁrst derivatives of the error\n",
      "∂E\n",
      "∂wkj=δkzj (5.108)\n",
      "∂E\n",
      "∂wji=δjxi (5.109)256 5. NEURAL NETWORKS\n",
      "and acting on these with the R{·} operator, we obtain expressions for the elements\n",
      "of the vector vTH\n",
      "R{∂E\n",
      "∂wkj}\n",
      "=R{δk}zj+δkR{zj} (5.110)\n",
      "R{∂E\n",
      "∂wji}\n",
      "=xiR{δj}. (5.111)\n",
      "The implementation of this algorithm involves the introduction of additional\n",
      "variables R{aj},R{zj}andR{δj}for the hidden units and R{δk}andR{yk}\n",
      "for the output units. For each input pattern, the values of these quantities can befound using the above results, and the elements of v\n",
      "THare then given by (5.110)\n",
      "and (5.111). An elegant aspect of this technique is that the equations for evaluating\n",
      "vTHmirror closely those for standard forward and backward propagation, and so the\n",
      "extension of existing software to compute this product is typically straightforward.\n",
      "If desired, the technique can be used to evaluate the full Hessian matrix by\n",
      "choosing the vector vto be given successively by a series of unit vectors of the\n",
      "form(0,0,...,1,...,0)each of which picks out one column of the Hessian. This\n",
      "leads to a formalism that is analytically equivalent to the backpropagation procedure\n",
      "of Bishop (1992), as described in Section 5.4.5, though with some loss of efﬁciency\n",
      "due to redundant calculations.\n",
      "5.5. Regularization in Neural Networks\n",
      "The number of input and outputs units in a neural network is generally determined\n",
      "by the dimensionality of the data set, whereas the number Mof hidden units is a free\n",
      "parameter that can be adjusted to give the best predictive performance. Note that M\n",
      "controls the number of parameters (weights and biases) in the network, and so we\n",
      "might expect that in a maximum likelihood setting there will be an optimum valueofMthat gives the best generalization performance, corresponding to the optimum\n",
      "balance between under-ﬁtting and over-ﬁtting. Figure 5.9 shows an example of the\n",
      "effect of different values of Mfor the sinusoidal regression problem.\n",
      "The generalization error, however, is not a simple function of Mdue to the\n",
      "presence of local minima in the error function, as illustrated in Figure 5.10. Herewe see the effect of choosing multiple random initializations for the weight vector\n",
      "for a range of values of M. The overall best validation set performance in this\n",
      "case occurred for a particular solution having M=8. In practice, one approach to\n",
      "choosing Mis in fact to plot a graph of the kind shown in Figure 5.10 and then to\n",
      "choose the speciﬁc solution having the smallest validation set error.\n",
      "There are, however, other ways to control the complexity of a neural network\n",
      "model in order to avoid over-ﬁtting. From our discussion of polynomial curve ﬁtting\n",
      "in Chapter 1, we see that an alternative approach is to choose a relatively large value\n",
      "forMand then to control complexity by the addition of a regularization term to the\n",
      "error function. The simplest regularizer is the quadratic, giving a regularized error5.5. Regularization in Neural Networks 257\n",
      "M=1\n",
      "0 1−101 M=3\n",
      "0 1−101 M=1 0\n",
      "0 1−101\n",
      "Figure 5.9 Examples of two-layer networks trained on 10data points drawn from the sinusoidal data set. The\n",
      "graphs show the result of ﬁtting networks having M=1,3and10hidden units, respectively, by minimizing a\n",
      "sum-of-squares error function using a scaled conjugate-gradient algorithm.\n",
      "of the form\n",
      "˜E(w)=E(w)+λ\n",
      "2wTw. (5.112)\n",
      "This regularizer is also known as weight decay and has been discussed at length\n",
      "in Chapter 3. The effective model complexity is then determined by the choice of\n",
      "the regularization coefﬁcient λ. As we have seen previously, this regularizer can be\n",
      "interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over\n",
      "the weight vector w.\n",
      "5.5.1 Consistent Gaussian priors\n",
      "One of the limitations of simple weight decay in the form (5.112) is that is\n",
      "inconsistent with certain scaling properties of network mappings. To illustrate this,\n",
      "consider a multilayer perceptron network having two layers of weights and linear\n",
      "output units, which performs a mapping from a set of input variables {xi}to a set\n",
      "of output variables {yk}. The activations of the hidden units in the ﬁrst hidden layer\n",
      "Figure 5.10 Plot of the sum-of-squares test-set\n",
      "error for the polynomial data set ver-\n",
      "sus the number of hidden units in the\n",
      "network, with 30random starts for\n",
      "each network size, showing the ef-\n",
      "fect of local minima. For each new\n",
      "start, the weight vector was initial-\n",
      "ized by sampling from an isotropic\n",
      "Gaussian distribution having a mean\n",
      "of zero and a variance of 10.\n",
      "0 2 4 6 8 106080100120140160258 5. NEURAL NETWORKS\n",
      "take the form\n",
      "zj=h(∑\n",
      "iwjixi+wj0)\n",
      "(5.113)\n",
      "while the activations of the output units are given by\n",
      "yk=∑\n",
      "jwkjzj+wk0. (5.114)\n",
      "Suppose we perform a linear transformation of the input data of the form\n",
      "xi→˜xi=axi+b. (5.115)\n",
      "Then we can arrange for the mapping performed by the network to be unchanged\n",
      "by making a corresponding linear transformation of the weights and biases from theinputs to the units in the hidden layer of the form Exercise 5.24\n",
      "w\n",
      "ji→˜wji=1\n",
      "awji (5.116)\n",
      "wj0→˜wj0=wj0−b\n",
      "a∑\n",
      "iwji. (5.117)\n",
      "Similarly, a linear transformation of the output variables of the network of the form\n",
      "yk→˜yk=cyk+d (5.118)\n",
      "can be achieved by making a transformation of the second-layer weights and biases\n",
      "using\n",
      "wkj→˜wkj=cwkj (5.119)\n",
      "wk0→˜wk0=cwk0+d. (5.120)\n",
      "If we train one network using the original data and one network using data for which\n",
      "the input and/or target variables are transformed by one of the above linear transfor-\n",
      "mations, then consistency requires that we should obtain equivalent networks that\n",
      "differ only by the linear transformation of the weights as given. Any regularizershould be consistent with this property, otherwise it arbitrarily favours one solution\n",
      "over another, equivalent one. Clearly, simple weight decay (5.112), that treats all\n",
      "weights and biases on an equal footing, does not satisfy this property.\n",
      "We therefore look for a regularizer which is invariant under the linear trans-\n",
      "formations (5.116), (5.117), (5.119) and (5.120). These require that the regularizer\n",
      "should be invariant to re-scaling of the weights and to shifts of the biases. Such aregularizer is given by\n",
      "λ\n",
      "1\n",
      "2∑\n",
      "w∈W 1w2+λ2\n",
      "2∑\n",
      "w∈W 2w2(5.121)\n",
      "where W1denotes the set of weights in the ﬁrst layer, W2denotes the set of weights\n",
      "in the second layer, and biases are excluded from the summations. This regularizer5.5. Regularization in Neural Networks 259\n",
      "will remain unchanged under the weight transformations provided the regularization\n",
      "parameters are re-scaled using λ1→a1/2λ1andλ2→c−1/2λ2.\n",
      "The regularizer (5.121) corresponds to a prior of the form\n",
      "p(w|α1,α2)∝exp(\n",
      "−α1\n",
      "2∑\n",
      "w∈W 1w2−α2\n",
      "2∑\n",
      "w∈W 2w2)\n",
      ". (5.122)\n",
      "Note that priors of this form are improper (they cannot be normalized) because the\n",
      "bias parameters are unconstrained. The use of improper priors can lead to difﬁculties\n",
      "in selecting regularization coefﬁcients and in model comparison within the Bayesianframework, because the corresponding evidence is zero. It is therefore common to\n",
      "include separate priors for the biases (which then break shift invariance) having their\n",
      "own hyperparameters. We can illustrate the effect of the resulting four hyperpa-rameters by drawing samples from the prior and plotting the corresponding network\n",
      "functions, as shown in Figure 5.11.\n",
      "More generally, we can consider priors in which the weights are divided into\n",
      "any number of groups W\n",
      "kso that\n",
      "p(w)∝exp(\n",
      "−1\n",
      "2∑\n",
      "kαk∥w∥2\n",
      "k)\n",
      "(5.123)\n",
      "where\n",
      "∥w∥2\n",
      "k=∑\n",
      "j∈W kw2\n",
      "j. (5.124)\n",
      "As a special case of this prior, if we choose the groups to correspond to the sets\n",
      "of weights associated with each of the input units, and we optimize the marginallikelihood with respect to the corresponding parameters α\n",
      "k, we obtain automatic\n",
      "relevance determination as discussed in Section 7.2.2.\n",
      "5.5.2 Early stopping\n",
      "An alternative to regularization as a way of controlling the effective complexity\n",
      "of a network is the procedure of early stopping . The training of nonlinear network\n",
      "models corresponds to an iterative reduction of the error function deﬁned with re-\n",
      "spect to a set of training data. For many of the optimization algorithms used for\n",
      "network training, such as conjugate gradients, the error is a nonincreasing functionof the iteration index. However, the error measured with respect to independent data,\n",
      "generally called a validation set, often shows a decrease at ﬁrst, followed by an in-\n",
      "crease as the network starts to over-ﬁt. Training can therefore be stopped at the pointof smallest error with respect to the validation data set, as indicated in Figure 5.12,\n",
      "in order to obtain a network having good generalization performance.\n",
      "The behaviour of the network in this case is sometimes explained qualitatively\n",
      "in terms of the effective number of degrees of freedom in the network, in which this\n",
      "number starts out small and then to grows during the training process, corresponding\n",
      "to a steady increase in the effective complexity of the model. Halting training before260 5. NEURAL NETWORKS\n",
      "αw\n",
      "1=1 ,αb\n",
      "1=1 ,αw\n",
      "2=1 ,αb\n",
      "2=1\n",
      "−1 −0.5 0 0.5 1−6−4−2024αw\n",
      "1=1 ,αb\n",
      "1=1 ,αw\n",
      "2= 10, αb\n",
      "2=1\n",
      "−1 −0.5 0 0.5 1−60−40−2002040\n",
      "αw\n",
      "1= 1000, αb\n",
      "1= 100, αw\n",
      "2=1 ,αb\n",
      "2=1\n",
      "−1 −0.5 0 0.5 1−10−505αw\n",
      "1= 1000, αb\n",
      "1= 1000, αw\n",
      "2=1 ,αb\n",
      "2=1\n",
      "−1 −0.5 0 0.5 1−10−505\n",
      "Figure 5.11 Illustration of the effect of the hyperparameters governing the prior distribution over weights and\n",
      "biases in a two-layer network having a single input, a single linear output, and 12hidden units having ‘ tanh’\n",
      "activation functions. The priors are governed by four hyperparameters αb\n",
      "1,αw\n",
      "1,αb\n",
      "2, and αw\n",
      "2, which represent\n",
      "the precisions of the Gaussian distributions of the ﬁrst-layer biases, ﬁrst-layer weights, second-layer biases, and\n",
      "second-layer weights, respectively. We see that the parameter αw\n",
      "2governs the vertical scale of functions (note\n",
      "the different vertical axis ranges on the top two diagrams), αw\n",
      "1governs the horizontal scale of variations in the\n",
      "function values, and αb\n",
      "1governs the horizontal range over which variations occur. The parameter αb\n",
      "2, whose\n",
      "effect is not illustrated here, governs the range of vertical offsets of the functions.\n",
      "a minimum of the training error has been reached then represents a way of limiting\n",
      "the effective network complexity.\n",
      "In the case of a quadratic error function, we can verify this insight, and show\n",
      "that early stopping should exhibit similar behaviour to regularization using a sim-\n",
      "ple weight-decay term. This can be understood from Figure 5.13, in which the axes\n",
      "in weight space have been rotated to be parallel to the eigenvectors of the Hessian\n",
      "matrix. If, in the absence of weight decay, the weight vector starts at the origin and\n",
      "proceeds during training along a path that follows the local negative gradient vec-\n",
      "tor, then the weight vector will move initially parallel to the w2axis through a point\n",
      "corresponding roughly to ˜wand then move towards the minimum of the error func-\n",
      "tionwML. This follows from the shape of the error surface and the widely differing\n",
      "eigenvalues of the Hessian. Stopping at a point near ˜wis therefore similar to weight\n",
      "decay. The relationship between early stopping and weight decay can be made quan-\n",
      "titative, thereby showing that the quantity τη(where τis the iteration index, and η Exercise 5.25\n",
      "is the learning rate parameter) plays the role of the reciprocal of the regularization5.5. Regularization in Neural Networks 261\n",
      "0 10 20 30 40 500.150.20.25\n",
      "0 10 20 30 40 500.350.40.45\n",
      "Figure 5.12 An illustration of the behaviour of training set error (left) and validation set error (right) during a\n",
      "typical training session, as a function of the iteration step, for the sinusoidal data set. The goal of achieving\n",
      "the best generalization performance suggests that training should be stopped at the point shown by the vertical\n",
      "dashed lines, corresponding to the minimum of the validation set error.\n",
      "parameter λ. The effective number of parameters in the network therefore grows\n",
      "during the course of training.\n",
      "5.5.3 Invariances\n",
      "In many applications of pattern recognition, it is known that predictions should\n",
      "be unchanged, or invariant , under one or more transformations of the input vari-\n",
      "ables. For example, in the classiﬁcation of objects in two-dimensional images, such\n",
      "as handwritten digits, a particular object should be assigned the same classiﬁcation\n",
      "irrespective of its position within the image ( translation invariance ) or of its size\n",
      "(scale invariance ). Such transformations produce signiﬁcant changes in the raw\n",
      "data, expressed in terms of the intensities at each of the pixels in the image, and\n",
      "yet should give rise to the same output from the classiﬁcation system. Similarly\n",
      "in speech recognition, small levels of nonlinear warping along the time axis, which\n",
      "preserve temporal ordering, should not change the interpretation of the signal.\n",
      "If sufﬁciently large numbers of training patterns are available, then an adaptive\n",
      "model such as a neural network can learn the invariance, at least approximately. This\n",
      "involves including within the training set a sufﬁciently large number of examples of\n",
      "the effects of the various transformations. Thus, for translation invariance in an im-\n",
      "age, the training set should include examples of objects at many different positions.\n",
      "This approach may be impractical, however, if the number of training examples\n",
      "is limited, or if there are several invariants (because the number of combinations of\n",
      "transformations grows exponentially with the number of such transformations). We\n",
      "therefore seek alternative approaches for encouraging an adaptive model to exhibit\n",
      "the required invariances. These can broadly be divided into four categories:\n",
      "1. The training set is augmented using replicas of the training patterns, trans-\n",
      "formed according to the desired invariances. For instance, in our digit recog-\n",
      "nition example, we could make multiple copies of each example in which the262 5. NEURAL NETWORKS\n",
      "Figure 5.13 A schematic illustration of why\n",
      "early stopping can give similar\n",
      "results to weight decay in the\n",
      "case of a quadratic error func-\n",
      "tion. The ellipse shows a con-\n",
      "tour of constant error, and wML\n",
      "denotes the minimum of the er-\n",
      "ror function. If the weight vector\n",
      "starts at the origin and moves ac-\n",
      "cording to the local negative gra-\n",
      "dient direction, then it will follow\n",
      "the path shown by the curve. By\n",
      "stopping training early, a weight\n",
      "vector ewis found that is qual-\n",
      "itatively similar to that obtained\n",
      "with a simple weight-decay reg-\n",
      "ularizer and training to the mini-\n",
      "mum of the regularized error, as\n",
      "can be seen by comparing with\n",
      "Figure 3.15.w1w2\n",
      "˜wwML\n",
      "digit is shifted to a different position in each image.\n",
      "2. A regularization term is added to the error function that penalizes changes in\n",
      "the model output when the input is transformed. This leads to the technique of\n",
      "tangent propagation , discussed in Section 5.5.4.\n",
      "3. Invariance is built into the pre-processing by extracting features that are invari-\n",
      "ant under the required transformations. Any subsequent regression or classi-\n",
      "ﬁcation system that uses such features as inputs will necessarily also respect\n",
      "these invariances.\n",
      "4. The ﬁnal option is to build the invariance properties into the structure of a neu-\n",
      "ral network (or into the deﬁnition of a kernel function in the case of techniques\n",
      "such as the relevance vector machine). One way to achieve this is through the\n",
      "use of local receptive ﬁelds and shared weights, as discussed in the context of\n",
      "convolutional neural networks in Section 5.5.6.\n",
      "Approach 1 is often relatively easy to implement and can be used to encourage com-\n",
      "plex invariances such as those illustrated in Figure 5.14. For sequential training\n",
      "algorithms, this can be done by transforming each input pattern before it is presented\n",
      "to the model so that, if the patterns are being recycled, a different transformation\n",
      "(drawn from an appropriate distribution) is added each time. For batch methods, a\n",
      "similar effect can be achieved by replicating each data point a number of times and\n",
      "transforming each copy independently. The use of such augmented data can lead to\n",
      "signiﬁcant improvements in generalization (Simard et al. , 2003), although it can also\n",
      "be computationally costly.\n",
      "Approach 2 leaves the data set unchanged but modiﬁes the error function through\n",
      "the addition of a regularizer. In Section 5.5.5, we shall show that this approach is\n",
      "closely related to approach 2.5.5. Regularization in Neural Networks 263\n",
      "Figure 5.14 Illustration of the synthetic warping of a handwritten digit. The original image is shown on the\n",
      "left. On the right, the top row shows three examples of warped digits, with the corresponding displacement\n",
      "ﬁelds shown on the bottom row. These displacement ﬁelds are generated by sampling random displacements\n",
      "∆x,∆y∈(0,1)at each pixel and then smoothing by convolution with Gaussians of width 0.01,30and60\n",
      "respectively.\n",
      "One advantage of approach 3 is that it can correctly extrapolate well beyond the\n",
      "range of transformations included in the training set. However, it can be difﬁcult\n",
      "to ﬁnd hand-crafted features with the required invariances that do not also discard\n",
      "information that can be useful for discrimination.\n",
      "5.5.4 Tangent propagation\n",
      "We can use regularization to encourage models to be invariant to transformations\n",
      "of the input through the technique of tangent propagation (Simard et al. , 1992).\n",
      "Consider the effect of a transformation on a particular input vector xn. Provided the\n",
      "transformation is continuous (such as translation or rotation, but not mirror reﬂection\n",
      "for instance), then the transformed pattern will sweep out a manifold Mwithin the\n",
      "D-dimensional input space. This is illustrated in Figure 5.15, for the case of D=\n",
      "2for simplicity. Suppose the transformation is governed by a single parameter ξ\n",
      "(which might be rotation angle for instance). Then the subspace Mswept out by xn\n",
      "Figure 5.15 Illustration of a two-dimensional input space\n",
      "showing the effect of a continuous transforma-\n",
      "tion on a particular input vector xn. A one-\n",
      "dimensional transformation, parameterized by\n",
      "the continuous variable ξ, applied to xncauses\n",
      "it to sweep out a one-dimensional manifold M.\n",
      "Locally, the effect of the transformation can be\n",
      "approximated by the tangent vector τn.\n",
      "x1x2\n",
      "xnτn\n",
      "ξM264 5. NEURAL NETWORKS\n",
      "will be one-dimensional, and will be parameterized by ξ. Let the vector that results\n",
      "from acting on xnby this transformation be denoted by s(xn,ξ), which is deﬁned\n",
      "so that s(x,0) =x. Then the tangent to the curve Mis given by the directional\n",
      "derivative τ=∂s/∂ξ, and the tangent vector at the point xnis given by\n",
      "τn=∂s(xn,ξ)\n",
      "∂ξ⏐⏐⏐⏐\n",
      "ξ=0. (5.125)\n",
      "Under a transformation of the input vector, the network output vector will, in general,\n",
      "change. The derivative of output kwith respect to ξis given by\n",
      "∂yk\n",
      "∂ξ⏐⏐⏐⏐\n",
      "ξ=0=D∑\n",
      "i=1∂yk\n",
      "∂xi∂xi\n",
      "∂ξ⏐⏐⏐⏐⏐\n",
      "ξ=0=D∑\n",
      "i=1Jkiτi (5.126)\n",
      "where Jkiis the(k,i)element of the Jacobian matrix J, as discussed in Section 5.3.4.\n",
      "The result (5.126) can be used to modify the standard error function, so as to encour-age local invariance in the neighbourhood of the data points, by the addition to the\n",
      "original error function Eof a regularization function Ωto give a total error function\n",
      "of the form\n",
      "˜E=E+λΩ (5.127)\n",
      "where λis a regularization coefﬁcient and\n",
      "Ω=1\n",
      "2∑\n",
      "n∑\n",
      "k(\n",
      "∂ynk\n",
      "∂ξ⏐⏐⏐⏐\n",
      "ξ=0)2\n",
      "=1\n",
      "2∑\n",
      "n∑\n",
      "k(D∑\n",
      "i=1Jnkiτni)2\n",
      ". (5.128)\n",
      "The regularization function will be zero when the network mapping function is in-\n",
      "variant under the transformation in the neighbourhood of each pattern vector, and\n",
      "the value of the parameter λdetermines the balance between ﬁtting the training data\n",
      "and learning the invariance property.\n",
      "In a practical implementation, the tangent vector τncan be approximated us-\n",
      "ing ﬁnite differences, by subtracting the original vector xnfrom the corresponding\n",
      "vector after transformation using a small value of ξ, and then dividing by ξ. This is\n",
      "illustrated in Figure 5.16.\n",
      "The regularization function depends on the network weights through the Jaco-\n",
      "bianJ. A backpropagation formalism for computing the derivatives of the regu-\n",
      "larizer with respect to the network weights is easily obtained by extension of the Exercise 5.26\n",
      "techniques introduced in Section 5.3.\n",
      "If the transformation is governed by Lparameters (e.g., L=3 for the case of\n",
      "translations combined with in-plane rotations in a two-dimensional image), then the\n",
      "manifold Mwill have dimensionality L, and the corresponding regularizer is given\n",
      "by the sum of terms of the form (5.128), one for each transformation. If severaltransformations are considered at the same time, and the network mapping is made\n",
      "invariant to each separately, then it will be (locally) invariant to combinations of the\n",
      "transformations (Simard et al. , 1992).5.5. Regularization in Neural Networks 265\n",
      "Figure 5.16 Illustration showing\n",
      "(a) the original image xof a hand-\n",
      "written digit, (b) the tangent vector\n",
      "τcorresponding to an inﬁnitesimal\n",
      "clockwise rotation, (c) the result of\n",
      "adding a small contribution from the\n",
      "tangent vector to the original image\n",
      "giving x+ϵτwithϵ=1 5 degrees,\n",
      "and (d) the true image rotated for\n",
      "comparison.\n",
      "(a)\n",
      " (b)\n",
      "(c)\n",
      " (d)\n",
      "A related technique, called tangent distance , can be used to build invariance\n",
      "properties into distance-based methods such as nearest-neighbour classiﬁers (Simard\n",
      "et al. , 1993).\n",
      "5.5.5 Training with transformed data\n",
      "We have seen that one way to encourage invariance of a model to a set of trans-\n",
      "formations is to expand the training set using transformed versions of the original\n",
      "input patterns. Here we show that this approach is closely related to the technique of\n",
      "tangent propagation (Bishop, 1995b; Leen, 1995).\n",
      "As in Section 5.5.4, we shall consider a transformation governed by a single\n",
      "parameter ξand described by the function s(x,ξ), withs(x,0) =x. We shall\n",
      "also consider a sum-of-squares error function. The error function for untransformed\n",
      "inputs can be written (in the inﬁnite data set limit) in the form\n",
      "E=1\n",
      "2∫∫\n",
      "{y(x)−t}2p(t|x)p(x)dxdt (5.129)\n",
      "as discussed in Section 1.5.5. Here we have considered a network having a single\n",
      "output, in order to keep the notation uncluttered. If we now consider an inﬁnite\n",
      "number of copies of each data point, each of which is perturbed by the transformation266 5. NEURAL NETWORKS\n",
      "in which the parameter ξis drawn from a distribution p(ξ), then the error function\n",
      "deﬁned over this expanded data set can be written as\n",
      "˜E=1\n",
      "2∫∫∫\n",
      "{y(s(x,ξ))−t}2p(t|x)p(x)p(ξ)dxdtdξ. (5.130)\n",
      "We now assume that the distribution p(ξ)has zero mean with small variance, so that\n",
      "we are only considering small transformations of the original input vectors. We can\n",
      "then expand the transformation function as a Taylor series in powers of ξto give\n",
      "s(x,ξ)= s(x,0) +ξ∂\n",
      "∂ξs(x,ξ)⏐⏐⏐⏐\n",
      "ξ=0+ξ2\n",
      "2∂2\n",
      "∂ξ2s(x,ξ)⏐⏐⏐⏐\n",
      "ξ=0+O(ξ3)\n",
      "=x+ξτ+1\n",
      "2ξ2τ′+O(ξ3)\n",
      "where τ′denotes the second derivative of s(x,ξ)with respect to ξevaluated at ξ=0.\n",
      "This allows us to expand the model function to give\n",
      "y(s(x,ξ)) =y(x)+ξτT∇y(x)+ξ2\n",
      "2[\n",
      "(τ′)T∇y(x)+τT∇∇y(x)τ]\n",
      "+O(ξ3).\n",
      "Substituting into the mean error function (5.130) and expanding, we then have\n",
      "˜E=1\n",
      "2∫∫\n",
      "{y(x)−t}2p(t|x)p(x)dxdt\n",
      "+ E[ξ]∫∫\n",
      "{y(x)−t}τT∇y(x)p(t|x)p(x)dxdt\n",
      "+ E[ξ2]∫∫[\n",
      "{y(x)−t}1\n",
      "2{\n",
      "(τ′)T∇y(x)+τT∇∇y(x)τ}\n",
      "+(\n",
      "τT∇y(x))2]\n",
      "p(t|x)p(x)dxdt+O(ξ3).\n",
      "Because the distribution of transformations has zero mean we have E[ξ]=0 . Also,\n",
      "we shall denote E[ξ2]byλ. Omitting terms of O(ξ3), the average error function then\n",
      "becomes\n",
      "˜E=E+λΩ (5.131)\n",
      "where Eis the original sum-of-squares error, and the regularization term Ωtakes the\n",
      "form\n",
      "Ω=∫[\n",
      "{y(x)−E[t|x]}1\n",
      "2{\n",
      "(τ′)T∇y(x)+τT∇∇y(x)τ}\n",
      "+(\n",
      "τT∇y(x))2]\n",
      "p(x)dx (5.132)\n",
      "in which we have performed the integration over t.5.5. Regularization in Neural Networks 267\n",
      "We can further simplify this regularization term as follows. In Section 1.5.5 we\n",
      "saw that the function that minimizes the sum-of-squares error is given by the condi-tional average E[t|x]of the target values t. From (5.131) we see that the regularized\n",
      "error will equal the unregularized sum-of-squares plus terms which are O(ξ), and so\n",
      "the network function that minimizes the total error will have the form\n",
      "y(x)= E[t|x]+O(ξ). (5.133)\n",
      "Thus, to leading order in ξ, the ﬁrst term in the regularizer vanishes and we are left\n",
      "with\n",
      "Ω=1\n",
      "2∫(\n",
      "τT∇y(x))2p(x)dx (5.134)\n",
      "which is equivalent to the tangent propagation regularizer (5.128).\n",
      "If we consider the special case in which the transformation of the inputs simply\n",
      "consists of the addition of random noise, so that x→x+ξ, then the regularizer\n",
      "takes the form Exercise 5.27\n",
      "Ω=1\n",
      "2∫\n",
      "∥∇y(x)∥2p(x)dx (5.135)\n",
      "which is known as Tikhonov regularization (Tikhonov and Arsenin, 1977; Bishop,\n",
      "1995b). Derivatives of this regularizer with respect to the network weights can be\n",
      "found using an extended backpropagation algorithm (Bishop, 1993). We see that, forsmall noise amplitudes, Tikhonov regularization is related to the addition of random\n",
      "noise to the inputs, which has been shown to improve generalization in appropriate\n",
      "circumstances (Sietsma and Dow, 1991).\n",
      "5.5.6 Convolutional networks\n",
      "Another approach to creating models that are invariant to certain transformation\n",
      "of the inputs is to build the invariance properties into the structure of a neural net-\n",
      "work. This is the basis for the convolutional neural network (Le Cun et al. , 1989;\n",
      "LeCun et al. , 1998), which has been widely applied to image data.\n",
      "Consider the speciﬁc task of recognizing handwritten digits. Each input image\n",
      "comprises a set of pixel intensity values, and the desired output is a posterior proba-\n",
      "bility distribution over the ten digit classes. We know that the identity of the digit isinvariant under translations and scaling as well as (small) rotations. Furthermore, the\n",
      "network must also exhibit invariance to more subtle transformations such as elastic\n",
      "deformations of the kind illustrated in Figure 5.14. One simple approach would be totreat the image as the input to a fully connected network, such as the kind shown in\n",
      "Figure 5.1. Given a sufﬁciently large training set, such a network could in principle\n",
      "yield a good solution to this problem and would learn the appropriate invariances by\n",
      "example.\n",
      "However, this approach ignores a key property of images, which is that nearby\n",
      "pixels are more strongly correlated than more distant pixels. Many of the modern\n",
      "approaches to computer vision exploit this property by extracting local features that\n",
      "depend only on small subregions of the image. Information from such features canthen be merged in later stages of processing in order to detect higher-order features268 5. NEURAL NETWORKS\n",
      "Input image Convolutional layerSub-sampling\n",
      "layer\n",
      "Figure 5.17 Diagram illustrating part of a convolutional neural network, showing a layer of convolu-\n",
      "tional units followed by a layer of subsampling units. Several successive pairs of such\n",
      "layers may be used.\n",
      "and ultimately to yield information about the image as whole. Also, local features\n",
      "that are useful in one region of the image are likely to be useful in other regions of\n",
      "the image, for instance if the object of interest is translated.\n",
      "These notions are incorporated into convolutional neural networks through three\n",
      "mechanisms: (i) local receptive ﬁelds, (ii) weight sharing, and (iii) subsampling. The\n",
      "structure of a convolutional network is illustrated in Figure 5.17. In the convolutional\n",
      "layer the units are organized into planes, each of which is called a feature map . Units\n",
      "in a feature map each take inputs only from a small subregion of the image, and all\n",
      "of the units in a feature map are constrained to share the same weight values. For\n",
      "instance, a feature map might consist of 100 units arranged in a 10×10grid, with\n",
      "each unit taking inputs from a 5×5pixel patch of the image. The whole feature map\n",
      "therefore has 25adjustable weight parameters plus one adjustable bias parameter.\n",
      "Input values from a patch are linearly combined using the weights and the bias, and\n",
      "the result transformed by a sigmoidal nonlinearity using (5.1). If we think of the units\n",
      "as feature detectors, then all of the units in a feature map detect the same pattern but\n",
      "at different locations in the input image. Due to the weight sharing, the evaluation\n",
      "of the activations of these units is equivalent to a convolution of the image pixel\n",
      "intensities with a ‘kernel’ comprising the weight parameters. If the input image is\n",
      "shifted, the activations of the feature map will be shifted by the same amount but will\n",
      "otherwise be unchanged. This provides the basis for the (approximate) invariance of5.5. Regularization in Neural Networks 269\n",
      "the network outputs to translations and distortions of the input image. Because we\n",
      "will typically need to detect multiple features in order to build an effective model,there will generally be multiple feature maps in the convolutional layer, each having\n",
      "its own set of weight and bias parameters.\n",
      "The outputs of the convolutional units form the inputs to the subsampling layer\n",
      "of the network. For each feature map in the convolutional layer, there is a plane of\n",
      "units in the subsampling layer and each unit takes inputs from a small receptive ﬁeld\n",
      "in the corresponding feature map of the convolutional layer. These units performsubsampling. For instance, each subsampling unit might take inputs from a 2×2\n",
      "unit region in the corresponding feature map and would compute the average of\n",
      "those inputs, multiplied by an adaptive weight with the addition of an adaptive bias\n",
      "parameter, and then transformed using a sigmoidal nonlinear activation function.\n",
      "The receptive ﬁelds are chosen to be contiguous and nonoverlapping so that thereare half the number of rows and columns in the subsampling layer compared with\n",
      "the convolutional layer. In this way, the response of a unit in the subsampling layer\n",
      "will be relatively insensitive to small shifts of the image in the corresponding regionsof the input space.\n",
      "In a practical architecture, there may be several pairs of convolutional and sub-\n",
      "sampling layers. At each stage there is a larger degree of invariance to input trans-formations compared to the previous layer. There may be several feature maps in a\n",
      "given convolutional layer for each plane of units in the previous subsampling layer,\n",
      "so that the gradual reduction in spatial resolution is then compensated by an increas-ing number of features. The ﬁnal layer of the network would typically be a fully\n",
      "connected, fully adaptive layer, with a softmax output nonlinearity in the case of\n",
      "multiclass classiﬁcation.\n",
      "The whole network can be trained by error minimization using backpropagation\n",
      "to evaluate the gradient of the error function. This involves a slight modiﬁcation\n",
      "of the usual backpropagation algorithm to ensure that the shared-weight constraints\n",
      "are satisﬁed. Due to the use of local receptive ﬁelds, the number of weights in Exercise 5.28\n",
      "the network is smaller than if the network were fully connected. Furthermore, thenumber of independent parameters to be learned from the data is much smaller still,\n",
      "due to the substantial numbers of constraints on the weights.\n",
      "5.5.7 Soft weight sharing\n",
      "One way to reduce the effective complexity of a network with a large number\n",
      "of weights is to constrain weights within certain groups to be equal. This is the\n",
      "technique of weight sharing that was discussed in Section 5.5.6 as a way of buildingtranslation invariance into networks used for image interpretation. It is only appli-\n",
      "cable, however, to particular problems in which the form of the constraints can be\n",
      "speciﬁed in advance. Here we consider a form of soft weight sharing (Nowlan and\n",
      "Hinton, 1992) in which the hard constraint of equal weights is replaced by a form\n",
      "of regularization in which groups of weights are encouraged to have similar values.Furthermore, the division of weights into groups, the mean weight value for each\n",
      "group, and the spread of values within the groups are all determined as part of the\n",
      "learning process.270 5. NEURAL NETWORKS\n",
      "Recall that the simple weight decay regularizer, given in (5.112), can be viewed\n",
      "as the negative log of a Gaussian prior distribution over the weights. We can encour-age the weight values to form several groups, rather than just one group, by consid-\n",
      "ering instead a probability distribution that is a mixture of Gaussians. The centres Section 2.3.9\n",
      "and variances of the Gaussian components, as well as the mixing coefﬁcients, will beconsidered as adjustable parameters to be determined as part of the learning process.\n",
      "Thus, we have a probability density of the form\n",
      "p(w)=\n",
      "∏\n",
      "ip(wi) (5.136)\n",
      "where\n",
      "p(wi)=M∑\n",
      "j=1πjN(wi|µj,σ2\n",
      "j) (5.137)\n",
      "andπjare the mixing coefﬁcients. Taking the negative logarithm then leads to a\n",
      "regularization function of the form\n",
      "Ω(w)=−∑\n",
      "iln(M∑\n",
      "j=1πjN(wi|µj,σ2\n",
      "j))\n",
      ". (5.138)\n",
      "The total error function is then given by\n",
      "˜E(w)=E(w)+λΩ(w) (5.139)\n",
      "where λis the regularization coefﬁcient. This error is minimized both with respect\n",
      "to the weights wiand with respect to the parameters {πj,µj,σj}of the mixture\n",
      "model. If the weights were constant, then the parameters of the mixture model could\n",
      "be determined by using the EM algorithm discussed in Chapter 9. However, the dis-tribution of weights is itself evolving during the learning process, and so to avoid nu-\n",
      "merical instability, a joint optimization is performed simultaneously over the weights\n",
      "and the mixture-model parameters. This can be done using a standard optimizationalgorithm such as conjugate gradients or quasi-Newton methods.\n",
      "In order to minimize the total error function, it is necessary to be able to evaluate\n",
      "its derivatives with respect to the various adjustable parameters. To do this it is con-venient to regard the {π\n",
      "j}asprior probabilities and to introduce the corresponding\n",
      "posterior probabilities which, following (2.192), are given by Bayes’ theorem in the\n",
      "form\n",
      "γj(w)=πjN(w|µj,σ2\n",
      "j)∑\n",
      "kπkN(w|µk,σ2\n",
      "k). (5.140)\n",
      "The derivatives of the total error function with respect to the weights are then given\n",
      "by Exercise 5.29\n",
      "∂˜E\n",
      "∂wi=∂E\n",
      "∂wi+λ∑\n",
      "jγj(wi)(wi−µj)\n",
      "σ2\n",
      "j. (5.141)5.5. Regularization in Neural Networks 271\n",
      "The effect of the regularization term is therefore to pull each weight towards the\n",
      "centre of the jthGaussian, with a force proportional to the posterior probability of\n",
      "that Gaussian for the given weight. This is precisely the kind of effect that we are\n",
      "seeking.\n",
      "Derivatives of the error with respect to the centres of the Gaussians are also\n",
      "easily computed to give Exercise 5.30\n",
      "∂˜E\n",
      "∂µj=λ∑\n",
      "iγj(wi)(µi−wj)\n",
      "σ2\n",
      "j(5.142)\n",
      "which has a simple intuitive interpretation, because it pushes µjtowards an aver-\n",
      "age of the weight values, weighted by the posterior probabilities that the respective\n",
      "weight parameters were generated by component j. Similarly, the derivatives with\n",
      "respect to the variances are given by Exercise 5.31\n",
      "∂˜E\n",
      "∂σj=λ∑\n",
      "iγj(wi)(1\n",
      "σj−(wi−µj)2\n",
      "σ3\n",
      "j)\n",
      "(5.143)\n",
      "which drives σjtowards the weighted average of the squared deviations of the weights\n",
      "around the corresponding centre µj, where the weighting coefﬁcients are again given\n",
      "by the posterior probability that each weight is generated by component j. Note that\n",
      "in a practical implementation, new variables ηjdeﬁned by\n",
      "σ2\n",
      "j= exp( ηj) (5.144)\n",
      "are introduced, and the minimization is performed with respect to the ηj. This en-\n",
      "sures that the parameters σjremain positive. It also has the effect of discouraging\n",
      "pathological solutions in which one or more of the σjgoes to zero, corresponding\n",
      "to a Gaussian component collapsing onto one of the weight parameter values. Such\n",
      "solutions are discussed in more detail in the context of Gaussian mixture models inSection 9.2.1.\n",
      "For the derivatives with respect to the mixing coefﬁcients π\n",
      "j, we need to take\n",
      "account of the constraints\n",
      "∑\n",
      "jπj=1, 0⩽πi⩽1 (5.145)\n",
      "which follow from the interpretation of the πjas prior probabilities. This can be\n",
      "done by expressing the mixing coefﬁcients in terms of a set of auxiliary variables\n",
      "{ηj}using the softmax function given by\n",
      "πj=exp(ηj)∑M\n",
      "k=1exp(ηk). (5.146)\n",
      "The derivatives of the regularized error function with respect to the {ηj}then take\n",
      "the form Exercise 5.32272 5. NEURAL NETWORKS\n",
      "Figure 5.18 The left ﬁgure shows a two-link robot arm,\n",
      "in which the Cartesian coordinates (x1,x2)of the end ef-\n",
      "fector are determined uniquely by the two joint angles θ1\n",
      "andθ2and the (ﬁxed) lengths L1andL2of the arms. This\n",
      "is know as the forward kinematics of the arm. In prac-\n",
      "tice, we have to ﬁnd the joint angles that will give rise to a\n",
      "desired end effector position and, as shown in the right ﬁg-\n",
      "ure, this inverse kinematics has two solutions correspond-\n",
      "ing to ‘elbow up’ and ‘elbow down’.L1L2\n",
      "θ1θ2(x1,x2) (x1,x2)\n",
      "elbow\n",
      "downelbow\n",
      "up\n",
      "∂˜E\n",
      "∂ηj=∑\n",
      "i{πj−γj(wi)}. (5.147)\n",
      "We see that πjis therefore driven towards the average posterior probability for com-\n",
      "ponent j.\n",
      "5.6. Mixture Density Networks\n",
      "The goal of supervised learning is to model a conditional distribution p(t|x), which\n",
      "for many simple regression problems is chosen to be Gaussian. However, practical\n",
      "machine learning problems can often have signiﬁcantly non-Gaussian distributions.\n",
      "These can arise, for example, with inverse problems in which the distribution can be\n",
      "multimodal, in which case the Gaussian assumption can lead to very poor predic-\n",
      "tions.\n",
      "As a simple example of an inverse problem, consider the kinematics of a robot\n",
      "arm, as illustrated in Figure 5.18. The forward problem involves ﬁnding the end ef- Exercise 5.33\n",
      "fector position given the joint angles and has a unique solution. However, in practice\n",
      "we wish to move the end effector of the robot to a speciﬁc position, and to do this we\n",
      "must set appropriate joint angles. We therefore need to solve the inverse problem,\n",
      "which has two solutions as seen in Figure 5.18.\n",
      "Forward problems often corresponds to causality in a physical system and gen-\n",
      "erally have a unique solution. For instance, a speciﬁc pattern of symptoms in the\n",
      "human body may be caused by the presence of a particular disease. In pattern recog-\n",
      "nition, however, we typically have to solve an inverse problem, such as trying to\n",
      "predict the presence of a disease given a set of symptoms. If the forward problem\n",
      "involves a many-to-one mapping, then the inverse problem will have multiple solu-\n",
      "tions. For instance, several different diseases may result in the same symptoms.\n",
      "In the robotics example, the kinematics is deﬁned by geometrical equations, and\n",
      "the multimodality is readily apparent. However, in many machine learning problems\n",
      "the presence of multimodality, particularly in problems involving spaces of high di-\n",
      "mensionality, can be less obvious. For tutorial purposes, however, we shall consider\n",
      "a simple toy problem for which we can easily visualize the multimodality. Data for\n",
      "this problem is generated by sampling a variable xuniformly over the interval (0,1),\n",
      "to give a set of values {xn}, and the corresponding target values tnare obtained5.6. Mixture Density Networks 273\n",
      "Figure 5.19 On the left is the data\n",
      "set for a simple ‘forward problem’ in\n",
      "which the red curve shows the result\n",
      "of ﬁtting a two-layer neural network\n",
      "by minimizing the sum-of-squares\n",
      "error function. The corresponding\n",
      "inverse problem, shown on the right,\n",
      "is obtained by exchanging the roles\n",
      "ofxandt. Here the same net-\n",
      "work trained again by minimizing the\n",
      "sum-of-squares error function gives\n",
      "a very poor ﬁt to the data due to the\n",
      "multimodality of the data set.0 101\n",
      "0 101\n",
      "by computing the function xn+0.3 sin(2 πxn)and then adding uniform noise over\n",
      "the interval (−0.1,0.1). The inverse problem is then obtained by keeping the same\n",
      "data points but exchanging the roles of xandt. Figure 5.19 shows the data sets for\n",
      "the forward and inverse problems, along with the results of ﬁtting two-layer neural\n",
      "networks having 6hidden units and a single linear output unit by minimizing a sum-\n",
      "of-squares error function. Least squares corresponds to maximum likelihood under\n",
      "a Gaussian assumption. We see that this leads to a very poor model for the highly\n",
      "non-Gaussian inverse problem.\n",
      "We therefore seek a general framework for modelling conditional probability\n",
      "distributions. This can be achieved by using a mixture model for p(t|x)in which\n",
      "both the mixing coefﬁcients as well as the component densities are ﬂexible functions\n",
      "of the input vector x, giving rise to the mixture density network . For any given value\n",
      "ofx, the mixture model provides a general formalism for modelling an arbitrary\n",
      "conditional density function p(t|x). Provided we consider a sufﬁciently ﬂexible\n",
      "network, we then have a framework for approximating arbitrary conditional distri-\n",
      "butions.\n",
      "Here we shall develop the model explicitly for Gaussian components, so that\n",
      "p(t|x)=K∑\n",
      "k=1πk(x)N(\n",
      "t|µk(x),σ2\n",
      "k(x))\n",
      ". (5.148)\n",
      "This is an example of a heteroscedastic model since the noise variance on the data\n",
      "is a function of the input vector x. Instead of Gaussians, we can use other distribu-\n",
      "tions for the components, such as Bernoulli distributions if the target variables are\n",
      "binary rather than continuous. We have also specialized to the case of isotropic co-\n",
      "variances for the components, although the mixture density network can readily be\n",
      "extended to allow for general covariance matrices by representing the covariances\n",
      "using a Cholesky factorization (Williams, 1996). Even with isotropic components,\n",
      "the conditional distribution p(t|x)does not assume factorization with respect to the\n",
      "components of t(in contrast to the standard sum-of-squares regression model) as a\n",
      "consequence of the mixture distribution.\n",
      "We now take the various parameters of the mixture model, namely the mixing\n",
      "coefﬁcients πk(x), the means µk(x), and the variances σ2\n",
      "k(x), to be governed by274 5. NEURAL NETWORKS\n",
      "x1xD\n",
      "θ1θM\n",
      "θ\n",
      "tp(t|x)\n",
      "Figure 5.20 Themixture density network can represent general conditional probability densities p(t|x)\n",
      "by considering a parametric mixture model for the distribution of twhose parameters are\n",
      "determined by the outputs of a neural network that takes xas its input vector.\n",
      "the outputs of a conventional neural network that takes xas its input. The structure\n",
      "of this mixture density network is illustrated in Figure 5.20. The mixture density\n",
      "network is closely related to the mixture of experts discussed in Section 14.5.3. The\n",
      "principle difference is that in the mixture density network the same function is used\n",
      "to predict the parameters of all of the component densities as well as the mixing co-\n",
      "efﬁcients, and so the nonlinear hidden units are shared amongst the input-dependent\n",
      "functions.\n",
      "The neural network in Figure 5.20 can, for example, be a two-layer network\n",
      "having sigmoidal (‘ tanh ’) hidden units. If there are Lcomponents in the mixture\n",
      "model (5.148), and if thasKcomponents, then the network will have Loutput unit\n",
      "activations denoted by aπ\n",
      "kthat determine the mixing coefﬁcients πk(x),Koutputs\n",
      "denoted by aσ\n",
      "kthat determine the kernel widths σk(x), andL×Koutputs denoted\n",
      "byaµ\n",
      "kjthat determine the components µkj(x)of the kernel centres µk(x). The total\n",
      "number of network outputs is given by (K+2 )L, as compared with the usual K\n",
      "outputs for a network, which simply predicts the conditional means of the target\n",
      "variables.\n",
      "The mixing coefﬁcients must satisfy the constraints\n",
      "K∑\n",
      "k=1πk(x)=1, 0⩽πk(x)⩽1 (5.149)\n",
      "which can be achieved using a set of softmax outputs\n",
      "πk(x)=exp(aπ\n",
      "k)∑K\n",
      "l=1exp(aπ\n",
      "l). (5.150)\n",
      "Similarly, the variances must satisfy σ2\n",
      "k(x)⩾0and so can be represented in terms\n",
      "of the exponentials of the corresponding network activations using\n",
      "σk(x)=e x p ( aσ\n",
      "k). (5.151)\n",
      "Finally, because the means µk(x)have real components, they can be represented5.6. Mixture Density Networks 275\n",
      "directly by the network output activations\n",
      "µkj(x)=aµ\n",
      "kj. (5.152)\n",
      "The adaptive parameters of the mixture density network comprise the vector w\n",
      "of weights and biases in the neural network, that can be set by maximum likelihood,\n",
      "or equivalently by minimizing an error function deﬁned to be the negative logarithmof the likelihood. For independent data, this error function takes the form\n",
      "E(w)=−N∑\n",
      "n=1ln{k∑\n",
      "k=1πk(xn,w)N(\n",
      "tn|µk(xn,w),σ2\n",
      "k(xn,w))}\n",
      "(5.153)\n",
      "where we have made the dependencies on wexplicit.\n",
      "In order to minimize the error function, we need to calculate the derivatives of\n",
      "the error E(w)with respect to the components of w. These can be evaluated by\n",
      "using the standard backpropagation procedure, provided we obtain suitable expres-\n",
      "sions for the derivatives of the error with respect to the output-unit activations. These\n",
      "represent error signals δfor each pattern and for each output unit, and can be back-\n",
      "propagated to the hidden units and the error function derivatives evaluated in the\n",
      "usual way. Because the error function (5.153) is composed of a sum of terms, one\n",
      "for each training data point, we can consider the derivatives for a particular patternnand then ﬁnd the derivatives of Eby summing over all patterns.\n",
      "Because we are dealing with mixture distributions, it is convenient to view the\n",
      "mixing coefﬁcients π\n",
      "k(x)asx-dependent prior probabilities and to introduce the\n",
      "corresponding posterior probabilities given by\n",
      "γk(t|x)=πkNnk∑K\n",
      "l=1πlNnl(5.154)\n",
      "where Nnkdenotes N(tn|µk(xn),σ2\n",
      "k(xn)).\n",
      "The derivatives with respect to the network output activations governing the mix-\n",
      "ing coefﬁcients are given by Exercise 5.34\n",
      "∂En\n",
      "∂aπ\n",
      "k=πk−γk. (5.155)\n",
      "Similarly, the derivatives with respect to the output activations controlling the com-\n",
      "ponent means are given by Exercise 5.35\n",
      "∂En\n",
      "∂aµ\n",
      "kl=γk{µkl−tl\n",
      "σ2\n",
      "k}\n",
      ". (5.156)\n",
      "Finally, the derivatives with respect to the output activations controlling the compo-\n",
      "nent variances are given by Exercise 5.36\n",
      "∂En\n",
      "∂aσ\n",
      "k=−γk{∥t−µk∥2\n",
      "σ3\n",
      "k−1\n",
      "σk}\n",
      ". (5.157)276 5. NEURAL NETWORKS\n",
      "Figure 5.21 (a) Plot of the mixing\n",
      "coefﬁcients πk(x)as a function of\n",
      "xfor the three kernel functions in a\n",
      "mixture density network trained on\n",
      "the data shown in Figure 5.19. The\n",
      "model has three Gaussian compo-\n",
      "nents, and uses a two-layer multi-\n",
      "layer perceptron with ﬁve ‘ tanh’ sig-\n",
      "moidal units in the hidden layer, and\n",
      "nine outputs (corresponding to the 3\n",
      "means and 3 variances of the Gaus-\n",
      "sian components and the 3 mixing\n",
      "coefﬁcients). At both small and large\n",
      "values of x, where the conditional\n",
      "probability density of the target data\n",
      "is unimodal, only one of the ker-\n",
      "nels has a high value for its prior\n",
      "probability, while at intermediate val-\n",
      "ues of x, where the conditional den-\n",
      "sity is trimodal, the three mixing co-\n",
      "efﬁcients have comparable values.\n",
      "(b) Plots of the means µk(x)using\n",
      "the same colour coding as for the\n",
      "mixing coefﬁcients. (c) Plot of the\n",
      "contours of the corresponding con-\n",
      "ditional probability density of the tar-\n",
      "get data for the same mixture den-\n",
      "sity network. (d) Plot of the ap-\n",
      "proximate conditional mode, shown\n",
      "by the red points, of the conditional\n",
      "density.0 101\n",
      "(a)0 101\n",
      "(b)\n",
      "(c)0 101\n",
      "0 101\n",
      "(d)\n",
      "We illustrate the use of a mixture density network by returning to the toy ex-\n",
      "ample of an inverse problem shown in Figure 5.19. Plots of the mixing coefﬁ-\n",
      "cients πk(x), the means µk(x), and the conditional density contours corresponding\n",
      "top(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the\n",
      "parameters in the mixture model, are necessarily continuous single-valued functions\n",
      "of the input variables. However, we see from Figure 5.21(c) that the model is able to\n",
      "produce a conditional density that is unimodal for some values of xand trimodal for\n",
      "other values by modulating the amplitudes of the mixing components πk(x).\n",
      "Once a mixture density network has been trained, it can predict the conditional\n",
      "density function of the target data for any given value of the input vector. This\n",
      "conditional density represents a complete description of the generator of the data, so\n",
      "far as the problem of predicting the value of the output vector is concerned. From\n",
      "this density function we can calculate more speciﬁc quantities that may be of interest\n",
      "in different applications. One of the simplest of these is the mean, corresponding to\n",
      "the conditional average of the target data, and is given by\n",
      "E[t|x]=∫\n",
      "tp(t|x)dt=K∑\n",
      "k=1πk(x)µk(x) (5.158)5.7. Bayesian Neural Networks 277\n",
      "where we have used (5.148). Because a standard network trained by least squares\n",
      "is approximating the conditional mean, we see that a mixture density network canreproduce the conventional least-squares result as a special case. Of course, as we\n",
      "have already noted, for a multimodal distribution the conditional mean is of limited\n",
      "value.\n",
      "We can similarly evaluate the variance of the density function about the condi-\n",
      "tional average, to give Exercise 5.37\n",
      "s\n",
      "2(x)= E[\n",
      "∥t−E[t|x]∥2|x]\n",
      "(5.159)\n",
      "=K∑\n",
      "k=1πk(x)⎧\n",
      "⎨\n",
      "⎩σ2\n",
      "k(x)+µk(x)−K∑\n",
      "l=1πl(x)µl(x)2⎫\n",
      "⎬\n",
      "⎭(5.160)\n",
      "where we have used (5.148) and (5.158). This is more general than the corresponding\n",
      "least-squares result because the variance is a function of x.\n",
      "We have seen that for multimodal distributions, the conditional mean can give\n",
      "a poor representation of the data. For instance, in controlling the simple robot arm\n",
      "shown in Figure 5.18, we need to pick one of the two possible joint angle settings\n",
      "in order to achieve the desired end-effector location, whereas the average of the twosolutions is not itself a solution. In such cases, the conditional mode may be of\n",
      "more value. Because the conditional mode for the mixture density network does not\n",
      "have a simple analytical solution, this would require numerical iteration. A simplealternative is to take the mean of the most probable component (i.e., the one with the\n",
      "largest mixing coefﬁcient) at each value of x. This is shown for the toy data set in\n",
      "Figure 5.21(d).\n",
      "5.7. Bayesian Neural Networks\n",
      "So far, our discussion of neural networks has focussed on the use of maximum like-lihood to determine the network parameters (weights and biases). Regularized max-\n",
      "imum likelihood can be interpreted as a MAP (maximum posterior) approach in\n",
      "which the regularizer can be viewed as the logarithm of a prior parameter distribu-tion. However, in a Bayesian treatment we need to marginalize over the distribution\n",
      "of parameters in order to make predictions.\n",
      "In Section 3.3, we developed a Bayesian solution for a simple linear regression\n",
      "model under the assumption of Gaussian noise. We saw that the posterior distribu-\n",
      "tion, which is Gaussian, could be evaluated exactly and that the predictive distribu-\n",
      "tion could also be found in closed form. In the case of a multilayered network, the\n",
      "highly nonlinear dependence of the network function on the parameter values means\n",
      "that an exact Bayesian treatment can no longer be found. In fact, the log of the pos-terior distribution will be nonconvex, corresponding to the multiple local minima in\n",
      "the error function.\n",
      "The technique of variational inference, to be discussed in Chapter 10, has been\n",
      "applied to Bayesian neural networks using a factorized Gaussian approximation278 5. NEURAL NETWORKS\n",
      "to the posterior distribution (Hinton and van Camp, 1993) and also using a full-\n",
      "covariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b). Themost complete treatment, however, has been based on the Laplace approximation\n",
      "(MacKay, 1992c; MacKay, 1992b) and forms the basis for the discussion given here.\n",
      "We will approximate the posterior distribution by a Gaussian, centred at a mode ofthe true posterior. Furthermore, we shall assume that the covariance of this Gaus-\n",
      "sian is small so that the network function is approximately linear with respect to the\n",
      "parameters over the region of parameter space for which the posterior probability issigniﬁcantly nonzero. With these two approximations, we will obtain models that\n",
      "are analogous to the linear regression and classiﬁcation models discussed in earlier\n",
      "chapters and so we can exploit the results obtained there. We can then make use of\n",
      "the evidence framework to provide point estimates for the hyperparameters and to\n",
      "compare alternative models (for example, networks having different numbers of hid-den units). To start with, we shall discuss the regression case and then later consider\n",
      "the modiﬁcations needed for solving classiﬁcation tasks.\n",
      "5.7.1 Posterior parameter distribution\n",
      "Consider the problem of predicting a single continuous target variable tfrom\n",
      "a vector xof inputs (the extension to multiple targets is straightforward). We shall\n",
      "suppose that the conditional distribution p(t|x)is Gaussian, with an x-dependent\n",
      "mean given by the output of a neural network model y(x,w), and with precision\n",
      "(inverse variance) β\n",
      "p(t|x,w,β)=N(t|y(x,w),β−1). (5.161)\n",
      "Similarly, we shall choose a prior distribution over the weights wthat is Gaussian of\n",
      "the form\n",
      "p(w|α)=N(w|0,α−1I). (5.162)\n",
      "For an i.i.d. data set of Nobservations x1,...,xN, with a corresponding set of target\n",
      "values D={t1,...,t N}, the likelihood function is given by\n",
      "p(D|w,β)=N∏\n",
      "n=1N(tn|y(xn,w),β−1) (5.163)\n",
      "and so the resulting posterior distribution is then\n",
      "p(w|D,α,β)∝p(w|α)p(D|w,β). (5.164)\n",
      "which, as a consequence of the nonlinear dependence of y(x,w)onw, will be non-\n",
      "Gaussian.\n",
      "We can ﬁnd a Gaussian approximation to the posterior distribution by using the\n",
      "Laplace approximation. To do this, we must ﬁrst ﬁnd a (local) maximum of the\n",
      "posterior, and this must be done using iterative numerical optimization. As usual, itis convenient to maximize the logarithm of the posterior, which can be written in the5.7. Bayesian Neural Networks 279\n",
      "form\n",
      "lnp(w|D)=−α\n",
      "2wTw−β\n",
      "2N∑\n",
      "n=1{y(xn,w)−tn}2+c o n s t (5.165)\n",
      "which corresponds to a regularized sum-of-squares error function. Assuming for\n",
      "the moment that αandβare ﬁxed, we can ﬁnd a maximum of the posterior, which\n",
      "we denote wMAP, by standard nonlinear optimization algorithms such as conjugate\n",
      "gradients, using error backpropagation to evaluate the required derivatives.\n",
      "Having found a mode wMAP, we can then build a local Gaussian approximation\n",
      "by evaluating the matrix of second derivatives of the negative log posterior distribu-\n",
      "tion. From (5.165), this is given by\n",
      "A=−∇∇ lnp(w|D,α,β)=αI+βH (5.166)\n",
      "whereHis the Hessian matrix comprising the second derivatives of the sum-of-\n",
      "squares error function with respect to the components of w. Algorithms for comput-\n",
      "ing and approximating the Hessian were discussed in Section 5.4. The correspondingGaussian approximation to the posterior is then given from (4.134) by\n",
      "q(w|D)=N(w|w\n",
      "MAP,A−1). (5.167)\n",
      "Similarly, the predictive distribution is obtained by marginalizing with respect\n",
      "to this posterior distribution\n",
      "p(t|x,D)=∫\n",
      "p(t|x,w)q(w|D)dw. (5.168)\n",
      "However, even with the Gaussian approximation to the posterior, this integration is\n",
      "still analytically intractable due to the nonlinearity of the network function y(x,w)\n",
      "as a function of w. To make progress, we now assume that the posterior distribution\n",
      "has small variance compared with the characteristic scales of wover which y(x,w)\n",
      "is varying. This allows us to make a Taylor series expansion of the network function\n",
      "around wMAP and retain only the linear terms\n",
      "y(x,w)≃y(x,wMAP)+gT(w−wMAP) (5.169)\n",
      "where we have deﬁned\n",
      "g=∇wy(x,w)|w=wMAP. (5.170)\n",
      "With this approximation, we now have a linear-Gaussian model with a Gaussian\n",
      "distribution for p(w)and a Gaussian for p(t|w)whose mean is a linear function of\n",
      "wof the form\n",
      "p(t|x,w,β)≃N(\n",
      "t|y(x,wMAP)+gT(w−wMAP),β−1)\n",
      ". (5.171)\n",
      "We can therefore make use of the general result (2.115) for the marginal p(t)to give Exercise 5.38\n",
      "p(t|x,D,α,β)=N(\n",
      "t|y(x,wMAP),σ2(x))\n",
      "(5.172)280 5. NEURAL NETWORKS\n",
      "where the input-dependent variance is given by\n",
      "σ2(x)=β−1+gTA−1g. (5.173)\n",
      "We see that the predictive distribution p(t|x,D)is a Gaussian whose mean is given\n",
      "by the network function y(x,wMAP)with the parameter set to their MAP value. The\n",
      "variance has two terms, the ﬁrst of which arises from the intrinsic noise on the target\n",
      "variable, whereas the second is an x-dependent term that expresses the uncertainty\n",
      "in the interpolant due to the uncertainty in the model parameters w. This should\n",
      "be compared with the corresponding predictive distribution for the linear regression\n",
      "model, given by (3.58) and (3.59).\n",
      "5.7.2 Hyperparameter optimization\n",
      "So far, we have assumed that the hyperparameters αandβare ﬁxed and known.\n",
      "We can make use of the evidence framework, discussed in Section 3.5, together with\n",
      "the Gaussian approximation to the posterior obtained using the Laplace approxima-tion, to obtain a practical procedure for choosing the values of such hyperparameters.\n",
      "The marginal likelihood, or evidence, for the hyperparameters is obtained by\n",
      "integrating over the network weights\n",
      "p(D|α,β)=∫\n",
      "p(D|w,β)p(w|α)dw. (5.174)\n",
      "This is easily evaluated by making use of the Laplace approximation result (4.135). Exercise 5.39\n",
      "Taking logarithms then gives\n",
      "lnp(D|α,β)≃−E(wMAP)−1\n",
      "2ln|A|+W\n",
      "2lnα+N\n",
      "2lnβ−N\n",
      "2ln(2π)(5.175)\n",
      "where Wis the total number of parameters in w, and the regularized error function\n",
      "is deﬁned by\n",
      "E(wMAP)=β\n",
      "2N∑\n",
      "n=1{y(xn,wMAP)−tn}2+α\n",
      "2wT\n",
      "MAPwMAP. (5.176)\n",
      "We see that this takes the same form as the corresponding result (3.86) for the linear\n",
      "regression model.\n",
      "In the evidence framework, we make point estimates for αandβby maximizing\n",
      "lnp(D|α,β). Consider ﬁrst the maximization with respect to α, which can be done\n",
      "by analogy with the linear regression case discussed in Section 3.5.2. We ﬁrst deﬁnethe eigenvalue equation\n",
      "βHu\n",
      "i=λiui (5.177)\n",
      "whereHis the Hessian matrix comprising the second derivatives of the sum-of-\n",
      "squares error function, evaluated at w=wMAP. By analogy with (3.92), we obtain\n",
      "α=γ\n",
      "wT\n",
      "MAPwMAP(5.178)5.7. Bayesian Neural Networks 281\n",
      "where γrepresents the effective number of parameters and is deﬁned by Section 3.5.3\n",
      "γ=W∑\n",
      "i=1λi\n",
      "α+λi. (5.179)\n",
      "Note that this result was exact for the linear regression case. For the nonlinear neural\n",
      "network, however, it ignores the fact that changes in αwill cause changes in the\n",
      "Hessian H, which in turn will change the eigenvalues. We have therefore implicitly\n",
      "ignored terms involving the derivatives of λiwith respect to α.\n",
      "Similarly, from (3.95) we see that maximizing the evidence with respect to β\n",
      "gives the re-estimation formula\n",
      "1\n",
      "β=1\n",
      "N−γN∑\n",
      "n=1{y(xn,wMAP)−tn}2. (5.180)\n",
      "As with the linear model, we need to alternate between re-estimation of the hyper-\n",
      "parameters αandβand updating of the posterior distribution. The situation with\n",
      "a neural network model is more complex, however, due to the multimodality of the\n",
      "posterior distribution. As a consequence, the solution for wMAP found by maximiz-\n",
      "ing the log posterior will depend on the initialization of w. Solutions that differ only\n",
      "as a consequence of the interchange and sign reversal symmetries in the hidden units Section 5.1.1\n",
      "are identical so far as predictions are concerned, and it is irrelevant which of theequivalent solutions is found. However, there may be inequivalent solutions as well,\n",
      "and these will generally yield different values for the optimized hyperparameters.\n",
      "In order to compare different models, for example neural networks having differ-\n",
      "ent numbers of hidden units, we need to evaluate the model evidence p(D). This can\n",
      "be approximated by taking (5.175) and substituting the values of αandβobtained\n",
      "from the iterative optimization of these hyperparameters. A more careful evaluationis obtained by marginalizing over αandβ, again by making a Gaussian approxima-\n",
      "tion (MacKay, 1992c; Bishop, 1995a). In either case, it is necessary to evaluate the\n",
      "determinant |A|of the Hessian matrix. This can be problematic in practice because\n",
      "the determinant, unlike the trace, is sensitive to the small eigenvalues that are often\n",
      "difﬁcult to determine accurately.\n",
      "The Laplace approximation is based on a local quadratic expansion around a\n",
      "mode of the posterior distribution over weights. We have seen in Section 5.1.1 that\n",
      "any given mode in a two-layer network is a member of a set of M!2\n",
      "Mequivalent\n",
      "modes that differ by interchange and sign-change symmetries, where Mis the num-\n",
      "ber of hidden units. When comparing networks having different numbers of hid-\n",
      "den units, this can be taken into account by multiplying the evidence by a factor ofM!2\n",
      "M.\n",
      "5.7.3 Bayesian neural networks for classiﬁcation\n",
      "So far, we have used the Laplace approximation to develop a Bayesian treat-\n",
      "ment of neural network regression models. We now discuss the modiﬁcations to282 5. NEURAL NETWORKS\n",
      "this framework that arise when it is applied to classiﬁcation. Here we shall con-\n",
      "sider a network having a single logistic sigmoid output corresponding to a two-classclassiﬁcation problem. The extension to networks with multiclass softmax outputs\n",
      "is straightforward. We shall build extensively on the analogous results for linear Exercise 5.40\n",
      "classiﬁcation models discussed in Section 4.5, and so we encourage the reader tofamiliarize themselves with that material before studying this section.\n",
      "The log likelihood function for this model is given by\n",
      "lnp(D|w)=\n",
      "∑\n",
      "n=1N{tnlnyn+( 1−tn)l n ( 1−yn)} (5.181)\n",
      "where tn∈{0,1}are the target values, and yn≡y(xn,w). Note that there is no\n",
      "hyperparameter β, because the data points are assumed to be correctly labelled. As\n",
      "before, the prior is taken to be an isotropic Gaussian of the form (5.162).\n",
      "The ﬁrst stage in applying the Laplace framework to this model is to initialize\n",
      "the hyperparameter α, and then to determine the parameter vector wby maximizing\n",
      "the log posterior distribution. This is equivalent to minimizing the regularized errorfunction\n",
      "E(w)=−lnp(D|w)+α\n",
      "2wTw (5.182)\n",
      "and can be achieved using error backpropagation combined with standard optimiza-\n",
      "tion algorithms, as discussed in Section 5.3.\n",
      "Having found a solution wMAP for the weight vector, the next step is to eval-\n",
      "uate the Hessian matrix Hcomprising the second derivatives of the negative log\n",
      "likelihood function. This can be done, for instance, using the exact method of Sec-tion 5.4.5, or using the outer product approximation given by (5.85). The second\n",
      "derivatives of the negative log posterior can again be written in the form (5.166), and\n",
      "the Gaussian approximation to the posterior is then given by (5.167).\n",
      "To optimize the hyperparameter α, we again maximize the marginal likelihood,\n",
      "which is easily shown to take the form Exercise 5.41\n",
      "lnp(D|α)≃−E(w\n",
      "MAP)−1\n",
      "2ln|A|+W\n",
      "2lnα+c o n s t (5.183)\n",
      "where the regularized error function is deﬁned by\n",
      "E(wMAP)=−N∑\n",
      "n=1{tnlnyn+( 1−tn)l n ( 1−yn)}+α\n",
      "2wT\n",
      "MAPwMAP (5.184)\n",
      "in which yn≡y(xn,wMAP). Maximizing this evidence function with respect to α\n",
      "again leads to the re-estimation equation given by (5.178).\n",
      "The use of the evidence procedure to determine αis illustrated in Figure 5.22\n",
      "for the synthetic two-dimensional data discussed in Appendix A.\n",
      "Finally, we need the predictive distribution, which is deﬁned by (5.168). Again,\n",
      "this integration is intractable due to the nonlinearity of the network function. The5.7. Bayesian Neural Networks 283\n",
      "Figure 5.22 Illustration of the evidence framework\n",
      "applied to a synthetic two-class data set.\n",
      "The green curve shows the optimal de-\n",
      "cision boundary, the black curve shows\n",
      "the result of ﬁtting a two-layer network\n",
      "with 8 hidden units by maximum likeli-\n",
      "hood, and the red curve shows the re-\n",
      "sult of including a regularizer in which\n",
      "αis optimized using the evidence pro-\n",
      "cedure, starting from the initial value\n",
      "α=0. Note that the evidence proce-\n",
      "dure greatly reduces the over-ﬁtting of\n",
      "the network.\n",
      "−2 −1 0 1 2−2−10123\n",
      "simplest approximation is to assume that the posterior distribution is very narrow\n",
      "and hence make the approximation\n",
      "p(t|x,D)≃p(t|x,wMAP). (5.185)\n",
      "We can improve on this, however, by taking account of the variance of the posterior\n",
      "distribution. In this case, a linear approximation for the network outputs, as was used\n",
      "in the case of regression, would be inappropriate due to the logistic sigmoid output-\n",
      "unit activation function that constrains the output to lie in the range (0,1). Instead,\n",
      "we make a linear approximation for the output unit activation in the form\n",
      "a(x,w)≃aMAP(x)+bT(w−wMAP) (5.186)\n",
      "where aMAP(x)=a(x,wMAP), and the vector b≡∇a(x,wMAP)can be found by\n",
      "backpropagation.\n",
      "Because we now have a Gaussian approximation for the posterior distribution\n",
      "overw, and a model for athat is a linear function of w, we can now appeal to the\n",
      "results of Section 4.5.2. The distribution of output unit activation values, induced by\n",
      "the distribution over network weights, is given by\n",
      "p(a|x,D)=∫\n",
      "δ(\n",
      "a−aMAP(x)−bT(x)(w−wMAP))\n",
      "q(w|D)dw (5.187)\n",
      "where q(w|D)is the Gaussian approximation to the posterior distribution given by\n",
      "(5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean\n",
      "aMAP≡a(x,wMAP), and variance\n",
      "σ2\n",
      "a(x)=bT(x)A−1b(x). (5.188)\n",
      "Finally, to obtain the predictive distribution, we must marginalize over ausing\n",
      "p(t=1|x,D)=∫\n",
      "σ(a)p(a|x,D)da. (5.189)284 5. NEURAL NETWORKS\n",
      "−2 −1 0 1 2−2−10123\n",
      "−2 −1 0 1 2−2−10123\n",
      "Figure 5.23 An illustration of the Laplace approximation for a Bayesian neural network having 8 hidden units\n",
      "with ‘tanh’ activation functions and a single logistic-sigmoid output unit. The weight parameters were found using\n",
      "scaled conjugate gradients, and the hyperparameter αwas optimized using the evidence framework. On the left\n",
      "is the result of using the simple approximation (5.185) based on a point estimate wMAP of the parameters,\n",
      "in which the green curve shows the y=0.5decision boundary, and the other contours correspond to output\n",
      "probabilities of y=0.1,0.3,0.7, and 0.9. On the right is the corresponding result obtained using (5.190). Note\n",
      "that the effect of marginalization is to spread out the contours and to make the predictions less conﬁdent, so\n",
      "that at each input point x, the posterior probabilities are shifted towards 0.5, while the y=0.5contour itself is\n",
      "unaffected.\n",
      "The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore\n",
      "apply the approximation (4.153) to (5.189) giving\n",
      "p(t=1|x,D)=σ(\n",
      "κ(σ2\n",
      "a)bTwMAP)\n",
      "(5.190)\n",
      "where κ(·)is deﬁned by (4.154). Recall that both σ2\n",
      "aandbare functions of x.\n",
      "Figure 5.23 shows an example of this framework applied to the synthetic classi-\n",
      "ﬁcation data set described in Appendix A.\n",
      "Exercises\n",
      "5.1 (⋆⋆)Consider a two-layer network function of the form (5.7) in which the hidden-\n",
      "unit nonlinear activation functions g(·)are given by logistic sigmoid functions of the\n",
      "form\n",
      "σ(a)={1+e x p ( −a)}−1. (5.191)\n",
      "Show that there exists an equivalent network, which computes exactly the same func-\n",
      "tion, but with hidden unit activation functions given by tanh(a)where the tanh func-\n",
      "tion is deﬁned by (5.59). Hint: ﬁrst ﬁnd the relation between σ(a)andtanh(a), and\n",
      "then show that the parameters of the two networks differ by linear transformations.\n",
      "5.2 (⋆)www Show that maximizing the likelihood function under the conditional\n",
      "distribution (5.16) for a multioutput neural network is equivalent to minimizing the\n",
      "sum-of-squares error function (5.11).Exercises 285\n",
      "5.3 (⋆⋆)Consider a regression problem involving multiple target variables in which it\n",
      "is assumed that the distribution of the targets, conditioned on the input vector x,i sa\n",
      "Gaussian of the form\n",
      "p(t|x,w)=N(t|y(x,w),Σ) (5.192)\n",
      "wherey(x,w)is the output of a neural network with input vector xand weight\n",
      "vectorw, andΣis the covariance of the assumed Gaussian noise on the targets.\n",
      "Given a set of independent observations of xandt, write down the error function\n",
      "that must be minimized in order to ﬁnd the maximum likelihood solution for w,i f\n",
      "we assume that Σis ﬁxed and known. Now assume that Σis also to be determined\n",
      "from the data, and write down an expression for the maximum likelihood solutionforΣ. Note that the optimizations of wandΣare now coupled, in contrast to the\n",
      "case of independent target variables discussed in Section 5.2.\n",
      "5.4 (⋆⋆)Consider a binary classiﬁcation problem in which the target values are t∈\n",
      "{0,1}, with a network output y(x,w)that represents p(t=1|x), and suppose that\n",
      "there is a probability ϵthat the class label on a training data point has been incorrectly\n",
      "set. Assuming independent and identically distributed data, write down the error\n",
      "function corresponding to the negative log likelihood. Verify that the error function\n",
      "(5.21) is obtained when ϵ=0. Note that this error function makes the model robust\n",
      "to incorrectly labelled data, in contrast to the usual error function.\n",
      "5.5 (⋆)www Show that maximizing likelihood for a multiclass neural network model\n",
      "in which the network outputs have the interpretation yk(x,w)=p(tk=1|x)is\n",
      "equivalent to the minimization of the cross-entropy error function (5.24).\n",
      "5.6 (⋆)www Show the derivative of the error function (5.21) with respect to the\n",
      "activation akfor an output unit having a logistic sigmoid activation function satisﬁes\n",
      "(5.18).\n",
      "5.7 (⋆)Show the derivative of the error function (5.24) with respect to the activation ak\n",
      "for output units having a softmax activation function satisﬁes (5.18).\n",
      "5.8 (⋆)We saw in (4.88) that the derivative of the logistic sigmoid activation function\n",
      "can be expressed in terms of the function value itself. Derive the corresponding result\n",
      "for the ‘ tanh ’ activation function deﬁned by (5.59).\n",
      "5.9 (⋆)www The error function (5.21) for binary classiﬁcation problems was de-\n",
      "rived for a network having a logistic-sigmoid output activation function, so that\n",
      "0⩽y(x,w)⩽1, and data having target values t∈{0,1}. Derive the correspond-\n",
      "ing error function if we consider a network having an output −1⩽y(x,w)⩽1\n",
      "and target values t=1 for class C1andt=−1for class C2. What would be the\n",
      "appropriate choice of output unit activation function?\n",
      "5.10 (⋆)www Consider a Hessian matrix Hwith eigenvector equation (5.33). By\n",
      "setting the vector vin (5.39) equal to each of the eigenvectors uiin turn, show that\n",
      "His positive deﬁnite if, and only if, all of its eigenvalues are positive.286 5. NEURAL NETWORKS\n",
      "5.11 (⋆⋆)www Consider a quadratic error function deﬁned by (5.32), in which the\n",
      "Hessian matrix Hhas an eigenvalue equation given by (5.33). Show that the con-\n",
      "tours of constant error are ellipses whose axes are aligned with the eigenvectors ui,\n",
      "with lengths that are inversely proportional to the square root of the corresponding\n",
      "eigenvalues λi.\n",
      "5.12 (⋆⋆)www By considering the local Taylor expansion (5.32) of an error function\n",
      "about a stationary point w⋆, show that the necessary and sufﬁcient condition for the\n",
      "stationary point to be a local minimum of the error function is that the Hessian matrixH, deﬁned by (5.30) with\n",
      "ˆw=w⋆, be positive deﬁnite.\n",
      "5.13 (⋆)Show that as a consequence of the symmetry of the Hessian matrix H, the\n",
      "number of independent elements in the quadratic error function (5.28) is given byW(W+3 )/2.\n",
      "5.14 (⋆)By making a Taylor expansion, verify that the terms that are O(ϵ)cancel on the\n",
      "right-hand side of (5.69).\n",
      "5.15 (⋆⋆)In Section 5.3.4, we derived a procedure for evaluating the Jacobian matrix of a\n",
      "neural network using a backpropagation procedure. Derive an alternative formalism\n",
      "for ﬁnding the Jacobian based on forward propagation equations.\n",
      "5.16 (⋆)The outer product approximation to the Hessian matrix for a neural network\n",
      "using a sum-of-squares error function is given by (5.84). Extend this result to the\n",
      "case of multiple outputs.\n",
      "5.17 (⋆)Consider a squared loss function of the form\n",
      "E=1\n",
      "2∫∫\n",
      "{y(x,w)−t}2p(x,t)dxdt (5.193)\n",
      "where y(x,w)is a parametric function such as a neural network. The result (1.89)\n",
      "shows that the function y(x,w)that minimizes this error is given by the conditional\n",
      "expectation of tgivenx. Use this result to show that the second derivative of Ewith\n",
      "respect to two elements wrandwsof the vector w, is given by\n",
      "∂2E\n",
      "∂wr∂ws=∫∂y\n",
      "∂wr∂y\n",
      "∂wsp(x)dx. (5.194)\n",
      "Note that, for a ﬁnite sample from p(x), we obtain (5.84).\n",
      "5.18 (⋆)Consider a two-layer network of the form shown in Figure 5.1 with the addition\n",
      "of extra parameters corresponding to skip-layer connections that go directly from\n",
      "the inputs to the outputs. By extending the discussion of Section 5.3.2, write down\n",
      "the equations for the derivatives of the error function with respect to these additionalparameters.\n",
      "5.19 (⋆)\n",
      "www Derive the expression (5.85) for the outer product approximation to\n",
      "the Hessian matrix for a network having a single output with a logistic sigmoidoutput-unit activation function and a cross-entropy error function, corresponding to\n",
      "the result (5.84) for the sum-of-squares error function.Exercises 287\n",
      "5.20 (⋆)Derive an expression for the outer product approximation to the Hessian matrix\n",
      "for a network having Koutputs with a softmax output-unit activation function and\n",
      "a cross-entropy error function, corresponding to the result (5.84) for the sum-of-\n",
      "squares error function.\n",
      "5.21 (⋆⋆⋆ )Extend the expression (5.86) for the outer product approximation of the Hes-\n",
      "sian matrix to the case of K>1output units. Hence, derive a recursive expression\n",
      "analogous to (5.87) for incrementing the number Nof patterns and a similar expres-\n",
      "sion for incrementing the number Kof outputs. Use these results, together with the\n",
      "identity (5.88), to ﬁnd sequential update expressions analogous to (5.89) for ﬁnding\n",
      "the inverse of the Hessian by incrementally including both extra patterns and extraoutputs.\n",
      "5.22 (⋆⋆)Derive the results (5.93), (5.94), and (5.95) for the elements of the Hessian\n",
      "matrix of a two-layer feed-forward network by application of the chain rule of cal-\n",
      "culus.\n",
      "5.23 (⋆⋆)Extend the results of Section 5.4.5 for the exact Hessian of a two-layer network\n",
      "to include skip-layer connections that go directly from inputs to outputs.\n",
      "5.24 (⋆)Verify that the network function deﬁned by (5.113) and (5.114) is invariant un-\n",
      "der the transformation (5.115) applied to the inputs, provided the weights and biases\n",
      "are simultaneously transformed using (5.116) and (5.117). Similarly, show that the\n",
      "network outputs can be transformed according (5.118) by applying the transforma-tion (5.119) and (5.120) to the second-layer weights and biases.\n",
      "5.25 (⋆⋆⋆ )\n",
      "www Consider a quadratic error function of the form\n",
      "E=E0+1\n",
      "2(w−w⋆)TH(w−w⋆) (5.195)\n",
      "wherew⋆represents the minimum, and the Hessian matrix His positive deﬁnite and\n",
      "constant. Suppose the initial weight vector w(0)is chosen to be at the origin and is\n",
      "updated using simple gradient descent\n",
      "w(τ)=w(τ−1)−ρ∇E (5.196)\n",
      "where τdenotes the step number, and ρis the learning rate (which is assumed to be\n",
      "small). Show that, after τsteps, the components of the weight vector parallel to the\n",
      "eigenvectors of Hcan be written\n",
      "w(τ)\n",
      "j={1−(1−ρηj)τ}w⋆\n",
      "j (5.197)\n",
      "where wj=wTuj, andujandηjare the eigenvectors and eigenvalues, respectively,\n",
      "ofHso that\n",
      "Huj=ηjuj. (5.198)\n",
      "Show that as τ→∞ , this gives w(τ)→w⋆as expected, provided |1−ρηj|<1.\n",
      "Now suppose that training is halted after a ﬁnite number τof steps. Show that the288 5. NEURAL NETWORKS\n",
      "components of the weight vector parallel to the eigenvectors of the Hessian satisfy\n",
      "w(τ)\n",
      "j≃w⋆\n",
      "jwhen ηj≫(ρτ)−1(5.199)\n",
      "|w(τ)\n",
      "j|≪|w⋆\n",
      "j|when ηj≪(ρτ)−1. (5.200)\n",
      "Compare this result with the discussion in Section 3.5.3 of regularization with simple\n",
      "weight decay, and hence show that (ρτ)−1is analogous to the regularization param-\n",
      "eterλ. The above results also show that the effective number of parameters in the\n",
      "network, as deﬁned by (3.91), grows as the training progresses.\n",
      "5.26 (⋆⋆)Consider a multilayer perceptron with arbitrary feed-forward topology, which\n",
      "is to be trained by minimizing the tangent propagation error function (5.127) in\n",
      "which the regularizing function is given by (5.128). Show that the regularization\n",
      "termΩcan be written as a sum over patterns of terms of the form\n",
      "Ωn=1\n",
      "2∑\n",
      "k(Gyk)2(5.201)\n",
      "where Gis a differential operator deﬁned by\n",
      "G≡∑\n",
      "iτi∂\n",
      "∂xi. (5.202)\n",
      "By acting on the forward propagation equations\n",
      "zj=h(aj),a j=∑\n",
      "iwjizi (5.203)\n",
      "with the operator G, show that Ωncan be evaluated by forward propagation using\n",
      "the following equations:\n",
      "αj=h′(aj)βj,β j=∑\n",
      "iwjiαi. (5.204)\n",
      "where we have deﬁned the new variables\n",
      "αj≡Gzj,β j≡Gaj. (5.205)\n",
      "Now show that the derivatives of Ωnwith respect to a weight wrsin the network can\n",
      "be written in the form\n",
      "∂Ωn\n",
      "∂wrs=∑\n",
      "kαk{φkrzs+δkrαs} (5.206)\n",
      "where we have deﬁned\n",
      "δkr≡∂yk\n",
      "∂ar,φ kr≡Gδkr. (5.207)\n",
      "Write down the backpropagation equations for δkr, and hence derive a set of back-\n",
      "propagation equations for the evaluation of the φkr.Exercises 289\n",
      "5.27 (⋆⋆)www Consider the framework for training with transformed data in the\n",
      "special case in which the transformation consists simply of the addition of randomnoisex→x+ξwhere ξhas a Gaussian distribution with zero mean and unit\n",
      "covariance. By following an argument analogous to that of Section 5.5.5, show that\n",
      "the resulting regularizer reduces to the Tikhonov form (5.135).\n",
      "5.28 (⋆)\n",
      "www Consider a neural network, such as the convolutional network discussed\n",
      "in Section 5.5.6, in which multiple weights are constrained to have the same value.\n",
      "Discuss how the standard backpropagation algorithm must be modiﬁed in order to\n",
      "ensure that such constraints are satisﬁed when evaluating the derivatives of an errorfunction with respect to the adjustable parameters in the network.\n",
      "5.29 (⋆)\n",
      "www Verify the result (5.141).\n",
      "5.30 (⋆)Verify the result (5.142).\n",
      "5.31 (⋆)Verify the result (5.143).\n",
      "5.32 (⋆⋆)Show that the derivatives of the mixing coefﬁcients {πk}, deﬁned by (5.146),\n",
      "with respect to the auxiliary parameters {ηj}are given by\n",
      "∂πk\n",
      "∂ηj=δjkπj−πjπk. (5.208)\n",
      "Hence, by making use of the constraint∑\n",
      "kπk=1, derive the result (5.147).\n",
      "5.33 (⋆)Write down a pair of equations that express the Cartesian coordinates (x1,x2)\n",
      "for the robot arm shown in Figure 5.18 in terms of the joint angles θ1andθ2and\n",
      "the lengths L1andL2of the links. Assume the origin of the coordinate system is\n",
      "given by the attachment point of the lower arm. These equations deﬁne the ‘forward\n",
      "kinematics’ of the robot arm.\n",
      "5.34 (⋆)www Derive the result (5.155) for the derivative of the error function with\n",
      "respect to the network output activations controlling the mixing coefﬁcients in the\n",
      "mixture density network.\n",
      "5.35 (⋆)Derive the result (5.156) for the derivative of the error function with respect\n",
      "to the network output activations controlling the component means in the mixture\n",
      "density network.\n",
      "5.36 (⋆)Derive the result (5.157) for the derivative of the error function with respect to\n",
      "the network output activations controlling the component variances in the mixturedensity network.\n",
      "5.37 (⋆)Verify the results (5.158) and (5.160) for the conditional mean and variance of\n",
      "the mixture density network model.\n",
      "5.38 (⋆)Using the general result (2.115), derive the predictive distribution (5.172) for\n",
      "the Laplace approximation to the Bayesian neural network model.290 5. NEURAL NETWORKS\n",
      "5.39 (⋆)www Make use of the Laplace approximation result (4.135) to show that the\n",
      "evidence function for the hyperparameters αandβin the Bayesian neural network\n",
      "model can be approximated by (5.175).\n",
      "5.40 (⋆)www Outline the modiﬁcations needed to the framework for Bayesian neural\n",
      "networks, discussed in Section 5.7.3, to handle multiclass problems using networks\n",
      "having softmax output-unit activation functions.\n",
      "5.41 (⋆⋆)By following analogous steps to those given in Section 5.7.1 for regression\n",
      "networks, derive the result (5.183) for the marginal likelihood in the case of a net-\n",
      "work having a cross-entropy error function and logistic-sigmoid output-unit activa-\n",
      "tion function.6\n",
      "Kernel\n",
      "Methods\n",
      "In Chapters 3 and 4, we considered linear parametric models for regression and\n",
      "classiﬁcation in which the form of the mapping y(x,w)from input xto output y\n",
      "is governed by a vector wof adaptive parameters. During the learning phase, a\n",
      "set of training data is used either to obtain a point estimate of the parameter vector\n",
      "or to determine a posterior distribution over this vector. The training data is thendiscarded, and predictions for new inputs are based purely on the learned parameter\n",
      "vectorw. This approach is also used in nonlinear parametric models such as neural\n",
      "networks. Chapter 5\n",
      "However, there is a class of pattern recognition techniques, in which the training\n",
      "data points, or a subset of them, are kept and used also during the prediction phase.\n",
      "For instance, the Parzen probability density model comprised a linear combination Section 2.5.1\n",
      "of ‘kernel’ functions each one centred on one of the training data points. Similarly,\n",
      "in Section 2.5.2 we introduced a simple technique for classiﬁcation called nearest\n",
      "neighbours, which involved assigning to each new test vector the same label as the\n",
      "291292 6. KERNEL METHODS\n",
      "closest example from the training set. These are examples of memory-based methods\n",
      "that involve storing the entire training set in order to make predictions for future datapoints. They typically require a metric to be deﬁned that measures the similarity of\n",
      "any two vectors in input space, and are generally fast to ‘train’ but slow at making\n",
      "predictions for test data points.\n",
      "Many linear parametric models can be re-cast into an equivalent ‘dual represen-\n",
      "tation’ in which the predictions are also based on linear combinations of a kernel\n",
      "function evaluated at the training data points. As we shall see, for models which are\n",
      "based on a ﬁxed nonlinear feature space mapping φ(x), the kernel function is given\n",
      "by the relation\n",
      "k(x,x\n",
      "′)=φ(x)Tφ(x′). (6.1)\n",
      "From this deﬁnition, we see that the kernel is a symmetric function of its arguments\n",
      "so that k(x,x′)=k(x′,x). The kernel concept was introduced into the ﬁeld of pat-\n",
      "tern recognition by Aizerman et al. (1964) in the context of the method of potential\n",
      "functions, so-called because of an analogy with electrostatics. Although neglected\n",
      "for many years, it was re-introduced into machine learning in the context of large-margin classiﬁers by Boser et al. (1992) giving rise to the technique of support\n",
      "vector machines . Since then, there has been considerable interest in this topic, both Chapter 7\n",
      "in terms of theory and applications. One of the most signiﬁcant developments has\n",
      "been the extension of kernels to handle symbolic objects, thereby greatly expanding\n",
      "the range of problems that can be addressed.\n",
      "The simplest example of a kernel function is obtained by considering the identity\n",
      "mapping for the feature space in (6.1) so that φ(x)=x, in which case k(x,x\n",
      "′)=\n",
      "xTx′. We shall refer to this as the linear kernel.\n",
      "The concept of a kernel formulated as an inner product in a feature space allows\n",
      "us to build interesting extensions of many well-known algorithms by making use of\n",
      "thekernel trick , also known as kernel substitution . The general idea is that, if we have\n",
      "an algorithm formulated in such a way that the input vector xenters only in the form\n",
      "of scalar products, then we can replace that scalar product with some other choice of\n",
      "kernel. For instance, the technique of kernel substitution can be applied to principalcomponent analysis in order to develop a nonlinear variant of PCA (Sch ¨olkopf et al. , Section 12.3\n",
      "1998). Other examples of kernel substitution include nearest-neighbour classiﬁers\n",
      "and the kernel Fisher discriminant (Mika et al. , 1999; Roth and Steinhage, 2000;\n",
      "Baudat and Anouar, 2000).\n",
      "There are numerous forms of kernel functions in common use, and we shall en-\n",
      "counter several examples in this chapter. Many have the property of being a function\n",
      "only of the difference between the arguments, so that k(x,x\n",
      "′)=k(x−x′), which\n",
      "are known as stationary kernels because they are invariant to translations in input\n",
      "space. A further specialization involves homogeneous kernels, also known as ra-\n",
      "dial basis functions , which depend only on the magnitude of the distance (typically Section 6.3\n",
      "Euclidean) between the arguments so that k(x,x′)=k(∥x−x′∥).\n",
      "For recent textbooks on kernel methods, see Sch ¨olkopf and Smola (2002), Her-\n",
      "brich (2002), and Shawe-Taylor and Cristianini (2004).6.1. Dual Representations 293\n",
      "6.1. Dual Representations\n",
      "Many linear models for regression and classiﬁcation can be reformulated in terms of\n",
      "a dual representation in which the kernel function arises naturally. This concept will\n",
      "play an important role when we consider support vector machines in the next chapter.Here we consider a linear regression model whose parameters are determined by\n",
      "minimizing a regularized sum-of-squares error function given by\n",
      "J(w)=1\n",
      "2N∑\n",
      "n=1{\n",
      "wTφ(xn)−tn}2+λ\n",
      "2wTw (6.2)\n",
      "where λ⩾0. If we set the gradient of J(w)with respect to wequal to zero, we see\n",
      "that the solution for wtakes the form of a linear combination of the vectors φ(xn),\n",
      "with coefﬁcients that are functions of w, of the form\n",
      "w=−1\n",
      "λN∑\n",
      "n=1{\n",
      "wTφ(xn)−tn}\n",
      "φ(xn)=N∑\n",
      "n=1anφ(xn)=ΦTa (6.3)\n",
      "whereΦis the design matrix, whose nthrow is given by φ(xn)T. Here the vector\n",
      "a=(a1,...,a N)T, and we have deﬁned\n",
      "an=−1\n",
      "λ{\n",
      "wTφ(xn)−tn}\n",
      ". (6.4)\n",
      "Instead of working with the parameter vector w, we can now reformulate the least-\n",
      "squares algorithm in terms of the parameter vector a, giving rise to a dual represen-\n",
      "tation . If we substitute w=ΦTaintoJ(w), we obtain\n",
      "J(a)=1\n",
      "2aTΦΦTΦΦTa−aTΦΦTt+1\n",
      "2tTt+λ\n",
      "2aTΦΦTa (6.5)\n",
      "where t=(t1,...,t N)T. We now deﬁne the Gram matrix K=ΦΦT, which is an\n",
      "N×Nsymmetric matrix with elements\n",
      "Knm=φ(xn)Tφ(xm)=k(xn,xm) (6.6)\n",
      "where we have introduced the kernel function k(x,x′)deﬁned by (6.1). In terms of\n",
      "the Gram matrix, the sum-of-squares error function can be written as\n",
      "J(a)=1\n",
      "2aTKKa−aTKt+1\n",
      "2tTt+λ\n",
      "2aTKa. (6.7)\n",
      "Setting the gradient of J(a)with respect to ato zero, we obtain the following solu-\n",
      "tion\n",
      "a=(K+λIN)−1t. (6.8)294 6. KERNEL METHODS\n",
      "If we substitute this back into the linear regression model, we obtain the following\n",
      "prediction for a new input x\n",
      "y(x)=wTφ(x)=aTΦφ(x)=k(x)T(K+λIN)−1t (6.9)\n",
      "where we have deﬁned the vector k(x)with elements kn(x)=k(xn,x). Thus we\n",
      "see that the dual formulation allows the solution to the least-squares problem to beexpressed entirely in terms of the kernel function k(x,x\n",
      "′). This is known as a dual\n",
      "formulation because, by noting that the solution for acan be expressed as a linear\n",
      "combination of the elements of φ(x), we recover the original formulation in terms of\n",
      "the parameter vector w. Note that the prediction at xis given by a linear combination Exercise 6.1\n",
      "of the target values from the training set. In fact, we have already obtained this result,\n",
      "using a slightly different notation, in Section 3.3.3.\n",
      "In the dual formulation, we determine the parameter vector aby inverting an\n",
      "N×Nmatrix, whereas in the original parameter space formulation we had to invert\n",
      "anM×Mmatrix in order to determine w. Because Nis typically much larger\n",
      "thanM, the dual formulation does not seem to be particularly useful. However, the\n",
      "advantage of the dual formulation, as we shall see, is that it is expressed entirely interms of the kernel function k(x,x\n",
      "′). We can therefore work directly in terms of\n",
      "kernels and avoid the explicit introduction of the feature vector φ(x), which allows\n",
      "us implicitly to use feature spaces of high, even inﬁnite, dimensionality.\n",
      "The existence of a dual representation based on the Gram matrix is a property of\n",
      "many linear models, including the perceptron. In Section 6.4, we will develop a dual- Exercise 6.2\n",
      "ity between probabilistic linear models for regression and the technique of Gaussianprocesses. Duality will also play an important role when we discuss support vector\n",
      "machines in Chapter 7.\n",
      "6.2. Constructing Kernels\n",
      "In order to exploit kernel substitution, we need to be able to construct valid kernelfunctions. One approach is to choose a feature space mapping φ(x)and then use\n",
      "this to ﬁnd the corresponding kernel, as is illustrated in Figure 6.1. Here the kernelfunction is deﬁned for a one-dimensional input space by\n",
      "k(x, x\n",
      "′)=φ(x)Tφ(x′)=M∑\n",
      "i=1φi(x)φi(x′) (6.10)\n",
      "where φi(x)are the basis functions.\n",
      "An alternative approach is to construct kernel functions directly. In this case,\n",
      "we must ensure that the function we choose is a valid kernel, in other words that it\n",
      "corresponds to a scalar product in some (perhaps inﬁnite dimensional) feature space.\n",
      "As a simple example, consider a kernel function given by\n",
      "k(x,z)=(\n",
      "xTz)2. (6.11)6.2. Constructing Kernels 295\n",
      "−1 0 1−1−0.500.51\n",
      "−1 0 100.250.50.751\n",
      "−1 0 100.250.50.751\n",
      "−1 0 100.020.04\n",
      "−1 0 100.020.04\n",
      "−1 0 100.020.04\n",
      "Figure 6.1 Illustration of the construction of kernel functions starting from a corresponding set of basis func-\n",
      "tions. In each column the lower plot shows the kernel function k(x, x′)deﬁned by (6.10) plotted as a function of\n",
      "xforx′=0, while the upper plot shows the corresponding basis functions given by polynomials (left column),\n",
      "‘Gaussians’ (centre column), and logistic sigmoids (right column).\n",
      "If we take the particular case of a two-dimensional input space x=(x1,x2)we\n",
      "can expand out the terms and thereby identify the corresponding nonlinear feature\n",
      "mapping\n",
      "k(x,z)=(\n",
      "xTz)2=(x1z1+x2z2)2\n",
      "=x2\n",
      "1z2\n",
      "1+2x1z1x2z2+x2\n",
      "2z2\n",
      "2\n",
      "=(x2\n",
      "1,√\n",
      "2x1x2,x2\n",
      "2)(z2\n",
      "1,√\n",
      "2z1z2,z2\n",
      "2)T\n",
      "=φ(x)Tφ(z). (6.12)\n",
      "We see that the feature mapping takes the form φ(x)=( x2\n",
      "1,√\n",
      "2x1x2,x2\n",
      "2)Tand\n",
      "therefore comprises all possible second order terms, with a speciﬁc weighting be-\n",
      "tween them.\n",
      "More generally, however, we need a simple way to test whether a function con-\n",
      "stitutes a valid kernel without having to construct the function φ(x)explicitly. A\n",
      "necessary and sufﬁcient condition for a function k(x,x′)to be a valid kernel (Shawe-\n",
      "Taylor and Cristianini, 2004) is that the Gram matrix K, whose elements are given by\n",
      "k(xn,xm), should be positive semideﬁnite for all possible choices of the set {xn}.\n",
      "Note that a positive semideﬁnite matrix is not the same thing as a matrix whose\n",
      "elements are nonnegative. Appendix C\n",
      "One powerful technique for constructing new kernels is to build them out of\n",
      "simpler kernels as building blocks. This can be done using the following properties:296 6. KERNEL METHODS\n",
      "Techniques for Constructing New Kernels.\n",
      "Given valid kernels k1(x,x′)andk2(x,x′), the following new kernels will also\n",
      "be valid:\n",
      "k(x,x′)= ck1(x,x′) (6.13)\n",
      "k(x,x′)= f(x)k1(x,x′)f(x′) (6.14)\n",
      "k(x,x′)= q(k1(x,x′)) (6.15)\n",
      "k(x,x′)=e x p ( k1(x,x′)) (6.16)\n",
      "k(x,x′)= k1(x,x′)+k2(x,x′) (6.17)\n",
      "k(x,x′)= k1(x,x′)k2(x,x′) (6.18)\n",
      "k(x,x′)= k3(φ(x),φ(x′)) (6.19)\n",
      "k(x,x′)= xTAx′(6.20)\n",
      "k(x,x′)= ka(xa,x′\n",
      "a)+kb(xb,x′\n",
      "b) (6.21)\n",
      "k(x,x′)= ka(xa,x′\n",
      "a)kb(xb,x′\n",
      "b) (6.22)\n",
      "where c>0is a constant, f(·)is any function, q(·)is a polynomial with nonneg-\n",
      "ative coefﬁcients, φ(x)is a function from xtoRM,k3(·,·)is a valid kernel in\n",
      "RM,Ais a symmetric positive semideﬁnite matrix, xaandxbare variables (not\n",
      "necessarily disjoint) with x=(xa,xb), andkaandkbare valid kernel functions\n",
      "over their respective spaces.\n",
      "Equipped with these properties, we can now embark on the construction of more\n",
      "complex kernels appropriate to speciﬁc applications. We require that the kernel\n",
      "k(x,x′)be symmetric and positive semideﬁnite and that it expresses the appropriate\n",
      "form of similarity between xandx′according to the intended application. Here we\n",
      "consider a few common examples of kernel functions. For a more extensive discus-sion of ‘kernel engineering’, see Shawe-Taylor and Cristianini (2004).\n",
      "We saw that the simple polynomial kernel k(x,x\n",
      "′)=(\n",
      "xTx′)2contains only\n",
      "terms of degree two. If we consider the slightly generalized kernel k(x,x′)=(\n",
      "xTx′+c)2withc>0, then the corresponding feature mapping φ(x)contains con-\n",
      "stant and linear terms as well as terms of order two. Similarly, k(x,x′)=(\n",
      "xTx′)M\n",
      "contains all monomials of order M. For instance, if xandx′are two images, then\n",
      "the kernel represents a particular weighted sum of all possible products of Mpixels\n",
      "in the ﬁrst image with Mpixels in the second image. This can similarly be gener-\n",
      "alized to include all terms up to degree Mby considering k(x,x′)=(\n",
      "xTx′+c)M\n",
      "withc>0. Using the results (6.17) and (6.18) for combining kernels we see that\n",
      "these will all be valid kernel functions.\n",
      "Another commonly used kernel takes the form\n",
      "k(x,x′) = exp(\n",
      "−∥x−x′∥2/2σ2)\n",
      "(6.23)\n",
      "and is often called a ‘Gaussian’ kernel. Note, however, that in this context it is\n",
      "not interpreted as a probability density, and hence the normalization coefﬁcient is6.2. Constructing Kernels 297\n",
      "omitted. We can see that this is a valid kernel by expanding the square\n",
      "∥x−x′∥2=xTx+(x′)Tx′−2xTx′(6.24)\n",
      "to give\n",
      "k(x,x′)=e x p(\n",
      "−xTx/2σ2)\n",
      "exp(\n",
      "xTx′/σ2)\n",
      "exp(\n",
      "−(x′)Tx′/2σ2)\n",
      "(6.25)\n",
      "and then making use of (6.14) and (6.16), together with the validity of the linear\n",
      "kernel k(x,x′)=xTx′. Note that the feature vector that corresponds to the Gaussian\n",
      "kernel has inﬁnite dimensionality. Exercise 6.11\n",
      "The Gaussian kernel is not restricted to the use of Euclidean distance. If we use\n",
      "kernel substitution in (6.24) to replace xTx′with a nonlinear kernel κ(x,x′),w e\n",
      "obtain\n",
      "k(x,x′) = exp{\n",
      "−1\n",
      "2σ2(κ(x,x)+κ(x′,x′)−2κ(x,x′))}\n",
      ". (6.26)\n",
      "An important contribution to arise from the kernel viewpoint has been the exten-\n",
      "sion to inputs that are symbolic, rather than simply vectors of real numbers. Kernel\n",
      "functions can be deﬁned over objects as diverse as graphs, sets, strings, and text doc-\n",
      "uments. Consider, for instance, a ﬁxed set and deﬁne a nonvectorial space consisting\n",
      "of all possible subsets of this set. If A1andA2are two such subsets then one simple\n",
      "choice of kernel would be\n",
      "k(A1,A2)=2|A1∩A2|(6.27)\n",
      "where A1∩A2denotes the intersection of sets A1andA2, and|A|denotes the\n",
      "number of subsets in A. This is a valid kernel function because it can be shown to\n",
      "correspond to an inner product in a feature space. Exercise 6.12\n",
      "One powerful approach to the construction of kernels starts from a probabilistic\n",
      "generative model (Haussler, 1999), which allows us to apply generative models in a\n",
      "discriminative setting. Generative models can deal naturally with missing data andin the case of hidden Markov models can handle sequences of varying length. By\n",
      "contrast, discriminative models generally give better performance on discriminative\n",
      "tasks than generative models. It is therefore of some interest to combine these twoapproaches (Lasserre et al. , 2006). One way to combine them is to use a generative\n",
      "model to deﬁne a kernel, and then use this kernel in a discriminative approach.\n",
      "Given a generative model p(x)we can deﬁne a kernel by\n",
      "k(x,x\n",
      "′)=p(x)p(x′). (6.28)\n",
      "This is clearly a valid kernel function because we can interpret it as an inner product\n",
      "in the one-dimensional feature space deﬁned by the mapping p(x). It says that two\n",
      "inputsxandx′are similar if they both have high probabilities. We can use (6.13) and\n",
      "(6.17) to extend this class of kernels by considering sums over products of different\n",
      "probability distributions, with positive weighting coefﬁcients p(i), of the form\n",
      "k(x,x′)=∑\n",
      "ip(x|i)p(x′|i)p(i). (6.29)298 6. KERNEL METHODS\n",
      "This is equivalent, up to an overall multiplicative constant, to a mixture distribution\n",
      "in which the components factorize, with the index iplaying the role of a ‘latent’\n",
      "variable. Two inputs xandx′will give a large value for the kernel function, and Section 9.2\n",
      "hence appear similar, if they have signiﬁcant probability under a range of different\n",
      "components. Taking the limit of an inﬁnite sum, we can also consider kernels of theform\n",
      "k(x,x\n",
      "′)=∫\n",
      "p(x|z)p(x′|z)p(z)dz (6.30)\n",
      "wherezis a continuous latent variable.\n",
      "Now suppose that our data consists of ordered sequences of length Lso that\n",
      "an observation is given by X={x1,...,xL}. A popular generative model for\n",
      "sequences is the hidden Markov model, which expresses the distribution p(X)as a Section 13.2\n",
      "marginalization over a corresponding sequence of hidden states Z={z1,...,zL}.\n",
      "We can use this approach to deﬁne a kernel function measuring the similarity of twosequences XandX\n",
      "′by extending the mixture representation (6.29) to give\n",
      "k(X,X′)=∑\n",
      "Zp(X|Z)p(X′|Z)p(Z) (6.31)\n",
      "so that both observed sequences are generated by the same hidden sequence Z. This\n",
      "model can easily be extended to allow sequences of differing length to be compared.\n",
      "An alternative technique for using generative models to deﬁne kernel functions\n",
      "is known as the Fisher kernel (Jaakkola and Haussler, 1999). Consider a parametric\n",
      "generative model p(x|θ)where θdenotes the vector of parameters. The goal is to\n",
      "ﬁnd a kernel that measures the similarity of two input vectors xandx′induced by the\n",
      "generative model. Jaakkola and Haussler (1999) consider the gradient with respect\n",
      "toθ, which deﬁnes a vector in a ‘feature’ space having the same dimensionality as\n",
      "θ. In particular, they consider the Fisher score\n",
      "g(θ,x)=∇θlnp(x|θ) (6.32)\n",
      "from which the Fisher kernel is deﬁned by\n",
      "k(x,x′)=g(θ,x)TF−1g(θ,x′). (6.33)\n",
      "HereFis the Fisher information matrix , given by\n",
      "F=Ex[\n",
      "g(θ,x)g(θ,x)T]\n",
      "(6.34)\n",
      "where the expectation is with respect to xunder the distribution p(x|θ). This can\n",
      "be motivated from the perspective of information geometry (Amari, 1998), which\n",
      "considers the differential geometry of the space of model parameters. Here we sim-\n",
      "ply note that the presence of the Fisher information matrix causes this kernel to beinvariant under a nonlinear re-parameterization of the density model θ→ψ(θ). Exercise 6.13\n",
      "In practice, it is often infeasible to evaluate the Fisher information matrix. One\n",
      "approach is simply to replace the expectation in the deﬁnition of the Fisher informa-tion with the sample average, giving\n",
      "F≃1\n",
      "NN∑\n",
      "n=1g(θ,xn)g(θ,xn)T. (6.35)6.3. Radial Basis Function Networks 299\n",
      "This is the covariance matrix of the Fisher scores, and so the Fisher kernel corre-\n",
      "sponds to a whitening of these scores. More simply, we can just omit the Fisher Section 12.1.3\n",
      "information matrix altogether and use the noninvariant kernel\n",
      "k(x,x′)=g(θ,x)Tg(θ,x′). (6.36)\n",
      "An application of Fisher kernels to document retrieval is given by Hofmann (2000).\n",
      "A ﬁnal example of a kernel function is the sigmoidal kernel given by\n",
      "k(x,x′) = tanh(\n",
      "axTx′+b)\n",
      "(6.37)\n",
      "whose Gram matrix in general is not positive semideﬁnite. This form of kernel\n",
      "has, however, been used in practice (Vapnik, 1995), possibly because it gives kernel\n",
      "expansions such as the support vector machine a superﬁcial resemblance to neural\n",
      "network models. As we shall see, in the limit of an inﬁnite number of basis functions,\n",
      "a Bayesian neural network with an appropriate prior reduces to a Gaussian process,thereby providing a deeper link between neural networks and kernel methods. Section 6.4.7\n",
      "6.3. Radial Basis Function Networks\n",
      "In Chapter 3, we discussed regression models based on linear combinations of ﬁxed\n",
      "basis functions, although we did not discuss in detail what form those basis functions\n",
      "might take. One choice that has been widely used is that of radial basis functions ,\n",
      "which have the property that each basis function depends only on the radial distance(typically Euclidean) from a centre µ\n",
      "j, so that φj(x)=h(∥x−µj∥).\n",
      "Historically, radial basis functions were introduced for the purpose of exact func-\n",
      "tion interpolation (Powell, 1987). Given a set of input vectors {x1,...,xN}along\n",
      "with corresponding target values {t1,...,t N}, the goal is to ﬁnd a smooth function\n",
      "f(x)that ﬁts every target value exactly, so that f(xn)=tnforn=1,...,N . This\n",
      "is achieved by expressing f(x)as a linear combination of radial basis functions, one\n",
      "centred on every data point\n",
      "f(x)=N∑\n",
      "n=1wnh(∥x−xn∥). (6.38)\n",
      "The values of the coefﬁcients {wn}are found by least squares, and because there\n",
      "are the same number of coefﬁcients as there are constraints, the result is a function\n",
      "that ﬁts every target value exactly. In pattern recognition applications, however, the\n",
      "target values are generally noisy, and exact interpolation is undesirable because this\n",
      "corresponds to an over-ﬁtted solution.\n",
      "Expansions in radial basis functions also arise from regularization theory (Pog-\n",
      "gio and Girosi, 1990; Bishop, 1995a). For a sum-of-squares error function with a\n",
      "regularizer deﬁned in terms of a differential operator, the optimal solution is givenby an expansion in the Green’s functions of the operator (which are analogous to the\n",
      "eigenvectors of a discrete matrix), again with one basis function centred on each data300 6. KERNEL METHODS\n",
      "point. If the differential operator is isotropic then the Green’s functions depend only\n",
      "on the radial distance from the corresponding data point. Due to the presence of theregularizer, the solution no longer interpolates the training data exactly.\n",
      "Another motivation for radial basis functions comes from a consideration of\n",
      "the interpolation problem when the input (rather than the target) variables are noisy(Webb, 1994; Bishop, 1995a). If the noise on the input variable xis described\n",
      "by a variable ξhaving a distribution ν(ξ), then the sum-of-squares error function\n",
      "becomes\n",
      "E=1\n",
      "2N∑\n",
      "n=1∫\n",
      "{y(xn+ξ)−tn}2ν(ξ)dξ. (6.39)\n",
      "Using the calculus of variations, we can optimize with respect to the function f(x) Appendix D\n",
      "to give Exercise 6.17\n",
      "y(xn)=N∑\n",
      "n=1tnh(x−xn) (6.40)\n",
      "where the basis functions are given by\n",
      "h(x−xn)=ν(x−xn)\n",
      "N∑\n",
      "n=1ν(x−xn). (6.41)\n",
      "We see that there is one basis function centred on every data point. This is known as\n",
      "theNadaraya-Watson model and will be derived again from a different perspective\n",
      "in Section 6.3.1. If the noise distribution ν(ξ)is isotropic, so that it is a function\n",
      "only of ∥ξ∥, then the basis functions will be radial.\n",
      "Note that the basis functions (6.41) are normalized, so that∑\n",
      "nh(x−xn)=1\n",
      "for any value of x. The effect of such normalization is shown in Figure 6.2. Normal-\n",
      "ization is sometimes used in practice as it avoids having regions of input space where\n",
      "all of the basis functions take small values, which would necessarily lead to predic-\n",
      "tions in such regions that are either small or controlled purely by the bias parameter.\n",
      "Another situation in which expansions in normalized radial basis functions arise\n",
      "is in the application of kernel density estimation to the problem of regression, as weshall discuss in Section 6.3.1.\n",
      "Because there is one basis function associated with every data point, the corre-\n",
      "sponding model can be computationally costly to evaluate when making predictions\n",
      "for new data points. Models have therefore been proposed (Broomhead and Lowe,\n",
      "1988; Moody and Darken, 1989; Poggio and Girosi, 1990), which retain the expan-sion in radial basis functions but where the number Mof basis functions is smaller\n",
      "than the number Nof data points. Typically, the number of basis functions, and the\n",
      "locations µ\n",
      "iof their centres, are determined based on the input data {xn}alone. The\n",
      "basis functions are then kept ﬁxed and the coefﬁcients {wi}are determined by least\n",
      "squares by solving the usual set of linear equations, as discussed in Section 3.1.1.6.3. Radial Basis Function Networks 301\n",
      "−1 −0.5 0 0.5 100.20.40.60.81\n",
      "−1 −0.5 0 0.5 100.20.40.60.81\n",
      "Figure 6.2 Plot of a set of Gaussian basis functions on the left, together with the corresponding normalized\n",
      "basis functions on the right.\n",
      "One of the simplest ways of choosing basis function centres is to use a randomly\n",
      "chosen subset of the data points. A more systematic approach is called orthogonal\n",
      "least squares (Chen et al. , 1991). This is a sequential selection process in which at\n",
      "each step the next data point to be chosen as a basis function centre corresponds to\n",
      "the one that gives the greatest reduction in the sum-of-squares error. Values for the\n",
      "expansion coefﬁcients are determined as part of the algorithm. Clustering algorithms\n",
      "such as K-means have also been used, which give a set of basis function centres that Section 9.1\n",
      "no longer coincide with training data points.\n",
      "6.3.1 Nadaraya-Watson model\n",
      "In Section 3.3.3, we saw that the prediction of a linear regression model for a\n",
      "new input xtakes the form of a linear combination of the training set target values\n",
      "with coefﬁcients given by the ‘equivalent kernel’ (3.62) where the equivalent kernel\n",
      "satisﬁes the summation constraint (3.64).\n",
      "We can motivate the kernel regression model (3.61) from a different perspective,\n",
      "starting with kernel density estimation. Suppose we have a training set {xn,tn}and\n",
      "we use a Parzen density estimator to model the joint distribution p(x,t), so that Section 2.5.1\n",
      "p(x,t)=1\n",
      "NN∑\n",
      "n=1f(x−xn,t−tn) (6.42)\n",
      "where f(x,t)is the component density function, and there is one such component\n",
      "centred on each data point. We now ﬁnd an expression for the regression function\n",
      "y(x), corresponding to the conditional average of the target variable conditioned on302 6. KERNEL METHODS\n",
      "the input variable, which is given by\n",
      "y(x)= E[t|x]=∫∞\n",
      "−∞tp(t|x)dt\n",
      "=∫\n",
      "tp(x,t)dt\n",
      "∫\n",
      "p(x,t)dt\n",
      "=∑\n",
      "n∫\n",
      "tf(x−xn,t−tn)dt\n",
      "∑\n",
      "m∫\n",
      "f(x−xm,t−tm)dt. (6.43)\n",
      "We now assume for simplicity that the component density functions have zero mean\n",
      "so that ∫∞\n",
      "−∞f(x,t)tdt=0 (6.44)\n",
      "for all values of x. Using a simple change of variable, we then obtain\n",
      "y(x)=∑\n",
      "ng(x−xn)tn\n",
      "∑\n",
      "mg(x−xm)\n",
      "=∑\n",
      "nk(x,xn)tn (6.45)\n",
      "where n, m=1,...,N and the kernel function k(x,xn)is given by\n",
      "k(x,xn)=g(x−xn)∑\n",
      "mg(x−xm)(6.46)\n",
      "and we have deﬁned\n",
      "g(x)=∫∞\n",
      "−∞f(x,t)dt. (6.47)\n",
      "The result (6.45) is known as the Nadaraya-Watson model, or kernel regression\n",
      "(Nadaraya, 1964; Watson, 1964). For a localized kernel function, it has the prop-\n",
      "erty of giving more weight to the data points xnthat are close to x. Note that the\n",
      "kernel (6.46) satisﬁes the summation constraint\n",
      "N∑\n",
      "n=1k(x,xn)=1.6.4. Gaussian Processes 303\n",
      "Figure 6.3 Illustration of the Nadaraya-Watson kernel\n",
      "regression model using isotropic Gaussian kernels, for the\n",
      "sinusoidal data set. The original sine function is shown\n",
      "by the green curve, the data points are shown in blue,\n",
      "and each is the centre of an isotropic Gaussian kernel.\n",
      "The resulting regression function, given by the condi-\n",
      "tional mean, is shown by the red line, along with the two-\n",
      "standard-deviation region for the conditional distribution\n",
      "p(t|x)shown by the red shading. The blue ellipse around\n",
      "each data point shows one standard deviation contour for\n",
      "the corresponding kernel. These appear noncircular due\n",
      "to the different scales on the horizontal and vertical axes.\n",
      "0 0.2 0.4 0.6 0.8 1−1.5−1−0.500.511.5\n",
      "In fact, this model deﬁnes not only a conditional expectation but also a full\n",
      "conditional distribution given by\n",
      "p(t|x)=p(t,x)∫\n",
      "p(t,x)dt=∑\n",
      "nf(x−xn,t−tn)\n",
      "∑\n",
      "m∫\n",
      "f(x−xm,t−tm)dt(6.48)\n",
      "from which other expectations can be evaluated.\n",
      "As an illustration we consider the case of a single input variable xin which\n",
      "f(x, t)is given by a zero-mean isotropic Gaussian over the variable z=(x, t)with\n",
      "variance σ2. The corresponding conditional distribution (6.48) is given by a Gaus-\n",
      "sian mixture, and is shown, together with the conditional mean, for the sinusoidal Exercise 6.18\n",
      "synthetic data set in Figure 6.3.\n",
      "An obvious extension of this model is to allow for more ﬂexible forms of Gaus-\n",
      "sian components, for instance having different variance parameters for the input and\n",
      "target variables. More generally, we could model the joint distribution p(t,x)using\n",
      "a Gaussian mixture model, trained using techniques discussed in Chapter 9 (Ghahra-\n",
      "mani and Jordan, 1994), and then ﬁnd the corresponding conditional distribution\n",
      "p(t|x). In this latter case we no longer have a representation in terms of kernel func-\n",
      "tions evaluated at the training set data points. However, the number of components\n",
      "in the mixture model can be smaller than the number of training set points, resulting\n",
      "in a model that is faster to evaluate for test data points. We have thereby accepted an\n",
      "increased computational cost during the training phase in order to have a model that\n",
      "is faster at making predictions.\n",
      "6.4. Gaussian Processes\n",
      "In Section 6.1, we introduced kernels by applying the concept of duality to a non-\n",
      "probabilistic model for regression. Here we extend the role of kernels to probabilis-304 6. KERNEL METHODS\n",
      "tic discriminative models, leading to the framework of Gaussian processes. We shall\n",
      "thereby see how kernels arise naturally in a Bayesian setting.\n",
      "In Chapter 3, we considered linear regression models of the form y(x,w)=\n",
      "wTφ(x)in which wis a vector of parameters and φ(x)is a vector of ﬁxed nonlinear\n",
      "basis functions that depend on the input vector x. We showed that a prior distribution\n",
      "overwinduced a corresponding prior distribution over functions y(x,w). Given a\n",
      "training data set, we then evaluated the posterior distribution over wand thereby\n",
      "obtained the corresponding posterior distribution over regression functions, whichin turn (with the addition of noise) implies a predictive distribution p(t|x)for new\n",
      "input vectors x.\n",
      "In the Gaussian process viewpoint, we dispense with the parametric model and\n",
      "instead deﬁne a prior probability distribution over functions directly. At ﬁrst sight, it\n",
      "might seem difﬁcult to work with a distribution over the uncountably inﬁnite space offunctions. However, as we shall see, for a ﬁnite training set we only need to consider\n",
      "the values of the function at the discrete set of input values x\n",
      "ncorresponding to the\n",
      "training set and test set data points, and so in practice we can work in a ﬁnite space.\n",
      "Models equivalent to Gaussian processes have been widely studied in many dif-\n",
      "ferent ﬁelds. For instance, in the geostatistics literature Gaussian process regression\n",
      "is known as kriging (Cressie, 1993). Similarly, ARMA (autoregressive moving aver-\n",
      "age) models, Kalman ﬁlters, and radial basis function networks can all be viewed as\n",
      "forms of Gaussian process models. Reviews of Gaussian processes from a machine\n",
      "learning perspective can be found in MacKay (1998), Williams (1999), and MacKay(2003), and a comparison of Gaussian process models with alternative approaches is\n",
      "given in Rasmussen (1996). See also Rasmussen and Williams (2006) for a recent\n",
      "textbook on Gaussian processes.\n",
      "6.4.1 Linear regression revisited\n",
      "In order to motivate the Gaussian process viewpoint, let us return to the linear\n",
      "regression example and re-derive the predictive distribution by working in terms\n",
      "of distributions over functions y(x,w). This will provide a speciﬁc example of a\n",
      "Gaussian process.\n",
      "Consider a model deﬁned in terms of a linear combination of Mﬁxed basis\n",
      "functions given by the elements of the vector φ(x)so that\n",
      "y(x)=wTφ(x) (6.49)\n",
      "wherexis the input vector and wis theM-dimensional weight vector. Now consider\n",
      "a prior distribution over wgiven by an isotropic Gaussian of the form\n",
      "p(w)=N(w|0,α−1I) (6.50)\n",
      "governed by the hyperparameter α, which represents the precision (inverse variance)\n",
      "of the distribution. For any given value of w, the deﬁnition (6.49) deﬁnes a partic-\n",
      "ular function of x. The probability distribution over wdeﬁned by (6.50) therefore\n",
      "induces a probability distribution over functions y(x). In practice, we wish to eval-\n",
      "uate this function at speciﬁc values of x, for example at the training data points6.4. Gaussian Processes 305\n",
      "x1,...,xN. We are therefore interested in the joint distribution of the function val-\n",
      "uesy(x1),...,y (xN), which we denote by the vector ywith elements yn=y(xn)\n",
      "forn=1,...,N . From (6.49), this vector is given by\n",
      "y=Φw (6.51)\n",
      "whereΦis the design matrix with elements Φnk=φk(xn). We can ﬁnd the proba-\n",
      "bility distribution of yas follows. First of all we note that yis a linear combination of\n",
      "Gaussian distributed variables given by the elements of wand hence is itself Gaus-\n",
      "sian. We therefore need only to ﬁnd its mean and covariance, which are given from Exercise 2.31\n",
      "(6.50) by\n",
      "E[y]= ΦE[w]=0 (6.52)\n",
      "cov[ y]= E[\n",
      "yyT]\n",
      "=ΦE[\n",
      "wwT]\n",
      "ΦT=1\n",
      "αΦΦT=K (6.53)\n",
      "whereKis the Gram matrix with elements\n",
      "Knm=k(xn,xm)=1\n",
      "αφ(xn)Tφ(xm) (6.54)\n",
      "andk(x,x′)is the kernel function.\n",
      "This model provides us with a particular example of a Gaussian process. In gen-\n",
      "eral, a Gaussian process is deﬁned as a probability distribution over functions y(x)\n",
      "such that the set of values of y(x)evaluated at an arbitrary set of points x1,...,xN\n",
      "jointly have a Gaussian distribution. In cases where the input vector xis two di-\n",
      "mensional, this may also be known as a Gaussian random ﬁeld . More generally, a\n",
      "stochastic process y(x)is speciﬁed by giving the joint probability distribution for\n",
      "any ﬁnite set of values y(x1),...,y (xN)in a consistent manner.\n",
      "A key point about Gaussian stochastic processes is that the joint distribution\n",
      "overNvariables y1,...,y Nis speciﬁed completely by the second-order statistics,\n",
      "namely the mean and the covariance. In most applications, we will not have anyprior knowledge about the mean of y(x)and so by symmetry we take it to be zero.\n",
      "This is equivalent to choosing the mean of the prior over weight values p(w|α)to\n",
      "be zero in the basis function viewpoint. The speciﬁcation of the Gaussian process is\n",
      "then completed by giving the covariance of y(x)evaluated at any two values of x,\n",
      "which is given by the kernel function\n",
      "E[y(x\n",
      "n)y(xm)] =k(xn,xm). (6.55)\n",
      "For the speciﬁc case of a Gaussian process deﬁned by the linear regression model\n",
      "(6.49) with a weight prior (6.50), the kernel function is given by (6.54).\n",
      "We can also deﬁne the kernel function directly, rather than indirectly through a\n",
      "choice of basis function. Figure 6.4 shows samples of functions drawn from Gaus-\n",
      "sian processes for two different choices of kernel function. The ﬁrst of these is a‘Gaussian’ kernel of the form (6.23), and the second is the exponential kernel given\n",
      "by\n",
      "k(x, x\n",
      "′)=e x p( −θ|x−x′|) (6.56)\n",
      "which corresponds to the Ornstein-Uhlenbeck process originally introduced by Uh-\n",
      "lenbeck and Ornstein (1930) to describe Brownian motion.306 6. KERNEL METHODS\n",
      "Figure 6.4 Samples from Gaus-\n",
      "sian processes for a ‘Gaussian’ ker-\n",
      "nel (left) and an exponential kernel\n",
      "(right).\n",
      "−1 −0.5 0 0.5 1−3−1.501.53\n",
      "−1 −0.5 0 0.5 1−3−1.501.53\n",
      "6.4.2 Gaussian processes for regression\n",
      "In order to apply Gaussian process models to the problem of regression, we need\n",
      "to take account of the noise on the observed target values, which are given by\n",
      "tn=yn+ϵn (6.57)\n",
      "where yn=y(xn), andϵnis a random noise variable whose value is chosen inde-\n",
      "pendently for each observation n. Here we shall consider noise processes that have\n",
      "a Gaussian distribution, so that\n",
      "p(tn|yn)=N(tn|yn,β−1) (6.58)\n",
      "where βis a hyperparameter representing the precision of the noise. Because the\n",
      "noise is independent for each data point, the joint distribution of the target values\n",
      "t=(t1,...,t N)Tconditioned on the values of y=(y1,...,y N)Tis given by an\n",
      "isotropic Gaussian of the form\n",
      "p(t|y)=N(t|y,β−1IN) (6.59)\n",
      "whereINdenotes the N×Nunit matrix. From the deﬁnition of a Gaussian process,\n",
      "the marginal distribution p(y)is given by a Gaussian whose mean is zero and whose\n",
      "covariance is deﬁned by a Gram matrix Kso that\n",
      "p(y)=N(y|0,K). (6.60)\n",
      "The kernel function that determines Kis typically chosen to express the property\n",
      "that, for points xnandxmthat are similar, the corresponding values y(xn)and\n",
      "y(xm)will be more strongly correlated than for dissimilar points. Here the notion\n",
      "of similarity will depend on the application.\n",
      "In order to ﬁnd the marginal distribution p(t), conditioned on the input values\n",
      "x1,...,xN, we need to integrate over y. This can be done by making use of the\n",
      "results from Section 2.3.3 for the linear-Gaussian model. Using (2.115), we see that\n",
      "the marginal distribution of tis given by\n",
      "p(t)=∫\n",
      "p(t|y)p(y)dy=N(t|0,C) (6.61)6.4. Gaussian Processes 307\n",
      "where the covariance matrix Chas elements\n",
      "C(xn,xm)=k(xn,xm)+β−1δnm. (6.62)\n",
      "This result reﬂects the fact that the two Gaussian sources of randomness, namely\n",
      "that associated with y(x)and that associated with ϵ, are independent and so their\n",
      "covariances simply add.\n",
      "One widely used kernel function for Gaussian process regression is given by the\n",
      "exponential of a quadratic form, with the addition of constant and linear terms to\n",
      "give\n",
      "k(xn,xm)=θ0exp{\n",
      "−θ1\n",
      "2∥xn−xm∥2}\n",
      "+θ2+θ3xT\n",
      "nxm. (6.63)\n",
      "Note that the term involving θ3corresponds to a parametric model that is a linear\n",
      "function of the input variables. Samples from this prior are plotted for various values\n",
      "of the parameters θ0,...,θ 3in Figure 6.5, and Figure 6.6 shows a set of points sam-\n",
      "pled from the joint distribution (6.60) along with the corresponding values deﬁned\n",
      "by (6.61).\n",
      "So far, we have used the Gaussian process viewpoint to build a model of the\n",
      "joint distribution over sets of data points. Our goal in regression, however, is to\n",
      "make predictions of the target variables for new inputs, given a set of training data.\n",
      "Let us suppose that tN=(t1,...,t N)T, corresponding to input values x1,...,xN,\n",
      "comprise the observed training set, and our goal is to predict the target variable tN+1\n",
      "for a new input vector xN+1. This requires that we evaluate the predictive distri-\n",
      "bution p(tN+1|tN). Note that this distribution is conditioned also on the variables\n",
      "x1,...,xNandxN+1. However, to keep the notation simple we will not show these\n",
      "conditioning variables explicitly.\n",
      "To ﬁnd the conditional distribution p(tN+1|t), we begin by writing down the\n",
      "joint distribution p(tN+1), where tN+1denotes the vector (t1,...,t N,tN+1)T.W e\n",
      "then apply the results from Section 2.3.1 to obtain the required conditional distribu-tion, as illustrated in Figure 6.7.\n",
      "From (6.61), the joint distribution over t\n",
      "1,...,t N+1will be given by\n",
      "p(tN+1)=N(tN+1|0,CN+1) (6.64)\n",
      "whereCN+1is an(N+1 )×(N+1 ) covariance matrix with elements given by\n",
      "(6.62). Because this joint distribution is Gaussian, we can apply the results from\n",
      "Section 2.3.1 to ﬁnd the conditional Gaussian distribution. To do this, we partitionthe covariance matrix as follows\n",
      "C\n",
      "N+1=(\n",
      "CNk\n",
      "kTc)\n",
      "(6.65)\n",
      "whereCNis theN×Ncovariance matrix with elements given by (6.62) for n, m=\n",
      "1,...,N , the vector khas elements k(xn,xN+1)forn=1,...,N , and the scalar308 6. KERNEL METHODS\n",
      "(1.00,4.00,0.00,0.00)\n",
      "−1 −0.5 0 0.5 1−3−1.501.53(9.00,4.00,0.00,0.00)\n",
      "−1 −0.5 0 0.5 1−9−4.504.59(1.00,64.00,0.00,0.00)\n",
      "−1 −0.5 0 0.5 1−3−1.501.53\n",
      "(1.00,0.25,0.00,0.00)\n",
      "−1 −0.5 0 0.5 1−3−1.501.53(1.00,4.00,10.00,0.00)\n",
      "−1 −0.5 0 0.5 1−9−4.504.59(1.00,4.00,0.00,5.00)\n",
      "−1 −0.5 0 0.5 1−4−2024\n",
      "Figure 6.5 Samples from a Gaussian process prior deﬁned by the covariance function (6.63). The title above\n",
      "each plot denotes (θ0,θ1,θ2,θ3).\n",
      "c=k(xN+1,xN+1)+β−1. Using the results (2.81) and (2.82), we see that the con-\n",
      "ditional distribution p(tN+1|t)is a Gaussian distribution with mean and covariance\n",
      "given by\n",
      "m(xN+1)= kTC−1\n",
      "Nt (6.66)\n",
      "σ2(xN+1)= c−kTC−1\n",
      "Nk. (6.67)\n",
      "These are the key results that deﬁne Gaussian process regression. Because the vector\n",
      "kis a function of the test point input value xN+1, we see that the predictive distribu-\n",
      "tion is a Gaussian whose mean and variance both depend on xN+1. An example of\n",
      "Gaussian process regression is shown in Figure 6.8.\n",
      "The only restriction on the kernel function is that the covariance matrix given by\n",
      "(6.62) must be positive deﬁnite. If λiis an eigenvalue of K, then the corresponding\n",
      "eigenvalue of Cwill be λi+β−1. It is therefore sufﬁcient that the kernel matrix\n",
      "k(xn,xm)be positive semideﬁnite for any pair of points xnandxm, so that λi⩾0,\n",
      "because any eigenvalue λithat is zero will still give rise to a positive eigenvalue\n",
      "forCbecause β>0. This is the same restriction on the kernel function discussed\n",
      "earlier, and so we can again exploit all of the techniques in Section 6.2 to construct6.4. Gaussian Processes 309\n",
      "Figure 6.6 Illustration of the sampling of data\n",
      "points {tn}from a Gaussian process.\n",
      "The blue curve shows a sample func-\n",
      "tion from the Gaussian process prior\n",
      "over functions, and the red points\n",
      "show the values of ynobtained by\n",
      "evaluating the function at a set of in-\n",
      "put values {xn}. The correspond-\n",
      "ing values of {tn}, shown in green,\n",
      "are obtained by adding independent\n",
      "Gaussian noise to each of the {yn}.\n",
      "xt\n",
      "−1 0 1−303\n",
      "suitable kernels.\n",
      "Note that the mean (6.66) of the predictive distribution can be written, as a func-\n",
      "tion of xN+1, in the form\n",
      "m(xN+1)=N∑\n",
      "n=1ank(xn,xN+1) (6.68)\n",
      "where anis the nthcomponent of C−1\n",
      "Nt. Thus, if the kernel function k(xn,xm)\n",
      "depends only on the distance ∥xn−xm∥, then we obtain an expansion in radial\n",
      "basis functions.\n",
      "The results (6.66) and (6.67) deﬁne the predictive distribution for Gaussian pro-\n",
      "cess regression with an arbitrary kernel function k(xn,xm). In the particular case in\n",
      "which the kernel function k(x,x′)is deﬁned in terms of a ﬁnite set of basis functions,\n",
      "we can derive the results obtained previously in Section 3.3.2 for linear regression\n",
      "starting from the Gaussian process viewpoint. Exercise 6.21\n",
      "For such models, we can therefore obtain the predictive distribution either by\n",
      "taking a parameter space viewpoint and using the linear regression result or by taking\n",
      "a function space viewpoint and using the Gaussian process result.\n",
      "The central computational operation in using Gaussian processes will involve\n",
      "the inversion of a matrix of size N×N, for which standard methods require O(N3)\n",
      "computations. By contrast, in the basis function model we have to invert a matrix\n",
      "SNof size M×M, which has O(M3)computational complexity. Note that for\n",
      "both viewpoints, the matrix inversion must be performed once for the given training\n",
      "set. For each new test point, both methods require a vector-matrix multiply, which\n",
      "has cost O(N2)in the Gaussian process case and O(M2)for the linear basis func-\n",
      "tion model. If the number Mof basis functions is smaller than the number Nof\n",
      "data points, it will be computationally more efﬁcient to work in the basis function310 6. KERNEL METHODS\n",
      "Figure 6.7 Illustration of the mechanism of\n",
      "Gaussian process regression for\n",
      "the case of one training point and\n",
      "one test point, in which the red el-\n",
      "lipses show contours of the joint dis-\n",
      "tribution p(t1,t2). Here t1is the\n",
      "training data point, and condition-\n",
      "i n go nt h ev a l u eo f t1, correspond-\n",
      "ing to the vertical blue line, we ob-\n",
      "tainp(t2|t1)shown as a function of\n",
      "t2by the green curve. t1t2\n",
      "m(x2)\n",
      "−1 0 1−101\n",
      "framework. However, an advantage of a Gaussian processes viewpoint is that we\n",
      "can consider covariance functions that can only be expressed in terms of an inﬁnite\n",
      "number of basis functions.\n",
      "For large training data sets, however, the direct application of Gaussian process\n",
      "methods can become infeasible, and so a range of approximation schemes have been\n",
      "developed that have better scaling with training set size than the exact approach\n",
      "(Gibbs, 1997; Tresp, 2001; Smola and Bartlett, 2001; Williams and Seeger, 2001;\n",
      "Csat´o and Opper, 2002; Seeger et al. , 2003). Practical issues in the application of\n",
      "Gaussian processes are discussed in Bishop and Nabney (2008).\n",
      "We have introduced Gaussian process regression for the case of a single tar-\n",
      "get variable. The extension of this formalism to multiple target variables, known\n",
      "as co-kriging (Cressie, 1993), is straightforward. Various other extensions of Gaus- Exercise 6.23\n",
      "Figure 6.8 Illustration of Gaussian process re-\n",
      "gression applied to the sinusoidal\n",
      "data set in Figure A.6 in which the\n",
      "three right-most data points have\n",
      "been omitted. The green curve\n",
      "shows the sinusoidal function from\n",
      "which the data points, shown in\n",
      "blue, are obtained by sampling and\n",
      "addition of Gaussian noise. The\n",
      "red line shows the mean of the\n",
      "Gaussian process predictive distri-\n",
      "bution, and the shaded region cor-\n",
      "responds to plus and minus two\n",
      "standard deviations. Notice how\n",
      "the uncertainty increases in the re-\n",
      "gion to the right of the data points.0 0.2 0.4 0.6 0.8 1−1−0.500.516.4. Gaussian Processes 311\n",
      "sian process regression have also been considered, for purposes such as modelling\n",
      "the distribution over low-dimensional manifolds for unsupervised learning (Bishopet al. , 1998a) and the solution of stochastic differential equations (Graepel, 2003).\n",
      "6.4.3 Learning the hyperparameters\n",
      "The predictions of a Gaussian process model will depend, in part, on the choice\n",
      "of covariance function. In practice, rather than ﬁxing the covariance function, we\n",
      "may prefer to use a parametric family of functions and then infer the parametervalues from the data. These parameters govern such things as the length scale of the\n",
      "correlations and the precision of the noise and correspond to the hyperparameters in\n",
      "a standard parametric model.\n",
      "Techniques for learning the hyperparameters are based on the evaluation of the\n",
      "likelihood function p(t|θ)where θdenotes the hyperparameters of the Gaussian pro-\n",
      "cess model. The simplest approach is to make a point estimate of θby maximizing\n",
      "the log likelihood function. Because θrepresents a set of hyperparameters for the\n",
      "regression problem, this can be viewed as analogous to the type 2 maximum like-\n",
      "lihood procedure for linear regression models. Maximization of the log likelihood Section 3.5\n",
      "can be done using efﬁcient gradient-based optimization algorithms such as conjugate\n",
      "gradients (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008).\n",
      "The log likelihood function for a Gaussian process regression model is easily\n",
      "evaluated using the standard form for a multivariate Gaussian distribution, giving\n",
      "lnp(t|θ)=−1\n",
      "2ln|CN|−1\n",
      "2tTC−1\n",
      "Nt−N\n",
      "2ln(2π). (6.69)\n",
      "For nonlinear optimization, we also need the gradient of the log likelihood func-\n",
      "tion with respect to the parameter vector θ. We shall assume that evaluation of the\n",
      "derivatives of CNis straightforward, as would be the case for the covariance func-\n",
      "tions considered in this chapter. Making use of the result (C.21) for the derivative ofC\n",
      "−1\n",
      "N, together with the result (C.22) for the derivative of ln|CN|, we obtain\n",
      "∂\n",
      "∂θilnp(t|θ)=−1\n",
      "2Tr(\n",
      "C−1\n",
      "N∂CN\n",
      "∂θi)\n",
      "+1\n",
      "2tTC−1\n",
      "N∂CN\n",
      "∂θiC−1\n",
      "Nt. (6.70)\n",
      "Because lnp(t|θ)will in general be a nonconvex function, it can have multiple max-\n",
      "ima.\n",
      "It is straightforward to introduce a prior over θand to maximize the log poste-\n",
      "rior using gradient-based methods. In a fully Bayesian treatment, we need to evaluatemarginals over θweighted by the product of the prior p(θ)and the likelihood func-\n",
      "tionp(t|θ). In general, however, exact marginalization will be intractable, and we\n",
      "must resort to approximations.\n",
      "The Gaussian process regression model gives a predictive distribution whose\n",
      "mean and variance are functions of the input vector x. However, we have assumed\n",
      "that the contribution to the predictive variance arising from the additive noise, gov-\n",
      "erned by the parameter β, is a constant. For some problems, known as heteroscedas-\n",
      "tic, the noise variance itself will also depend on x. To model this, we can extend the312 6. KERNEL METHODS\n",
      "Figure 6.9 Samples from the ARD\n",
      "prior for Gaussian processes, in\n",
      "which the kernel function is given by\n",
      "(6.71). The left plot corresponds toη\n",
      "1=η2=1, and the right plot cor-\n",
      "responds to η1=1,η2=0.01.\n",
      "Gaussian process framework by introducing a second Gaussian process to represent\n",
      "the dependence of βon the input x(Goldberg et al. , 1998). Because βis a variance,\n",
      "and hence nonnegative, we use the Gaussian process to model lnβ(x).\n",
      "6.4.4 Automatic relevance determination\n",
      "In the previous section, we saw how maximum likelihood could be used to de-\n",
      "termine a value for the correlation length-scale parameter in a Gaussian process.This technique can usefully be extended by incorporating a separate parameter for\n",
      "each input variable (Rasmussen and Williams, 2006). The result, as we shall see, is\n",
      "that the optimization of these parameters by maximum likelihood allows the relative\n",
      "importance of different inputs to be inferred from the data. This represents an exam-\n",
      "ple in the Gaussian process context of automatic relevance determination ,o r ARD ,\n",
      "which was originally formulated in the framework of neural networks (MacKay,\n",
      "1994; Neal, 1996). The mechanism by which appropriate inputs are preferred is\n",
      "discussed in Section 7.2.2.\n",
      "Consider a Gaussian process with a two-dimensional input space x=(x\n",
      "1,x2),\n",
      "having a kernel function of the form\n",
      "k(x,x′)=θ0exp{\n",
      "−1\n",
      "22∑\n",
      "i=1ηi(xi−x′\n",
      "i)2}\n",
      ". (6.71)\n",
      "Samples from the resulting prior over functions y(x)are shown for two different\n",
      "settings of the precision parameters ηiin Figure 6.9. We see that, as a particu-\n",
      "lar parameter ηibecomes small, the function becomes relatively insensitive to the\n",
      "corresponding input variable xi. By adapting these parameters to a data set using\n",
      "maximum likelihood, it becomes possible to detect input variables that have little\n",
      "effect on the predictive distribution, because the corresponding values of ηiwill be\n",
      "small. This can be useful in practice because it allows such inputs to be discarded.ARD is illustrated using a simple synthetic data set having three inputs x\n",
      "1,x2andx3\n",
      "(Nabney, 2002) in Figure 6.10. The target variable t, is generated by sampling 100\n",
      "values of x1from a Gaussian, evaluating the function sin(2πx1), and then adding\n",
      "6.4. Gaussian Processes 313\n",
      "Figure 6.10 Illustration of automatic rele-\n",
      "vance determination in a Gaus-\n",
      "sian process for a synthetic prob-\n",
      "lem having three inputs x1,x2,\n",
      "andx3, for which the curves\n",
      "show the corresponding values of\n",
      "the hyperparameters η1(red), η2\n",
      "(green), and η3(blue) as a func-\n",
      "tion of the number of iterations\n",
      "when optimizing the marginal\n",
      "likelihood. Details are given in\n",
      "the text. Note the logarithmic\n",
      "scale on the vertical axis.\n",
      "0 20 40 60 80 10010−410−2100102\n",
      "Gaussian noise. Values of x2are given by copying the corresponding values of x1\n",
      "and adding noise, and values of x3are sampled from an independent Gaussian dis-\n",
      "tribution. Thus x1is a good predictor of t,x2is a more noisy predictor of t, andx3\n",
      "has only chance correlations with t. The marginal likelihood for a Gaussian process\n",
      "with ARD parameters η1,η2,η3is optimized using the scaled conjugate gradients\n",
      "algorithm. We see from Figure 6.10 that η1converges to a relatively large value, η2\n",
      "converges to a much smaller value, and η3becomes very small indicating that x3is\n",
      "irrelevant for predicting t.\n",
      "The ARD framework is easily incorporated into the exponential-quadratic kernel\n",
      "(6.63) to give the following form of kernel function, which has been found useful for\n",
      "applications of Gaussian processes to a range of regression problems\n",
      "k(xn,xm)=θ0exp{\n",
      "−1\n",
      "2D∑\n",
      "i=1ηi(xni−xmi)2}\n",
      "+θ2+θ3D∑\n",
      "i=1xnixmi (6.72)\n",
      "where Dis the dimensionality of the input space.\n",
      "6.4.5 Gaussian processes for classiﬁcation\n",
      "In a probabilistic approach to classiﬁcation, our goal is to model the posterior\n",
      "probabilities of the target variable for a new input vector, given a set of training\n",
      "data. These probabilities must lie in the interval (0,1), whereas a Gaussian process\n",
      "model makes predictions that lie on the entire real axis. However, we can easily\n",
      "adapt Gaussian processes to classiﬁcation problems by transforming the output of\n",
      "the Gaussian process using an appropriate nonlinear activation function.\n",
      "Consider ﬁrst the two-class problem with a target variable t∈{0,1}. If we de-\n",
      "ﬁne a Gaussian process over a function a(x)and then transform the function using\n",
      "a logistic sigmoid y=σ(a), given by (4.59), then we will obtain a non-Gaussian\n",
      "stochastic process over functions y(x)where y∈(0,1). This is illustrated for the\n",
      "case of a one-dimensional input space in Figure 6.11 in which the probability distri-314 6. KERNEL METHODS\n",
      "−1 −0.5 0 0.5 1−10−50510\n",
      "−1 −0.5 0 0.5 100.250.50.751\n",
      "Figure 6.11 The left plot shows a sample from a Gaussian process prior over functions a(x), and the right plot\n",
      "shows the result of transforming this sample using a logistic sigmoid function.\n",
      "bution over the target variable tis then given by the Bernoulli distribution\n",
      "p(t|a)=σ(a)t(1−σ(a))1−t. (6.73)\n",
      "As usual, we denote the training set inputs by x1,...,xNwith corresponding\n",
      "observed target variables t=(t1,...,t N)T. We also consider a single test point\n",
      "xN+1with target value tN+1. Our goal is to determine the predictive distribution\n",
      "p(tN+1|t), where we have left the conditioning on the input variables implicit. To do\n",
      "this we introduce a Gaussian process prior over the vector aN+1, which has compo-\n",
      "nentsa(x1),...,a (xN+1). This in turn deﬁnes a non-Gaussian process over tN+1,\n",
      "and by conditioning on the training data tNwe obtain the required predictive distri-\n",
      "bution. The Gaussian process prior for aN+1takes the form\n",
      "p(aN+1)=N(aN+1|0,CN+1). (6.74)\n",
      "Unlike the regression case, the covariance matrix no longer includes a noise term\n",
      "because we assume that all of the training data points are correctly labelled. How-\n",
      "ever, for numerical reasons it is convenient to introduce a noise-like term governed\n",
      "by a parameter νthat ensures that the covariance matrix is positive deﬁnite. Thus\n",
      "the covariance matrix CN+1has elements given by\n",
      "C(xn,xm)=k(xn,xm)+νδnm (6.75)\n",
      "where k(xn,xm)is any positive semideﬁnite kernel function of the kind considered\n",
      "in Section 6.2, and the value of νis typically ﬁxed in advance. We shall assume that\n",
      "the kernel function k(x,x′)is governed by a vector θof parameters, and we shall\n",
      "later discuss how θmay be learned from the training data.\n",
      "For two-class problems, it is sufﬁcient to predict p(tN+1=1|tN)because the\n",
      "value of p(tN+1=0|tN)is then given by 1−p(tN+1=1|tN). The required6.4. Gaussian Processes 315\n",
      "predictive distribution is given by\n",
      "p(tN+1=1|tN)=∫\n",
      "p(tN+1=1|aN+1)p(aN+1|tN)daN+1 (6.76)\n",
      "where p(tN+1=1|aN+1)=σ(aN+1).\n",
      "This integral is analytically intractable, and so may be approximated using sam-\n",
      "pling methods (Neal, 1997). Alternatively, we can consider techniques based onan analytical approximation. In Section 4.5.2, we derived the approximate formula\n",
      "(4.153) for the convolution of a logistic sigmoid with a Gaussian distribution. We\n",
      "can use this result to evaluate the integral in (6.76) provided we have a Gaussianapproximation to the posterior distribution p(a\n",
      "N+1|tN). The usual justiﬁcation for a\n",
      "Gaussian approximation to a posterior distribution is that the true posterior will tend\n",
      "to a Gaussian as the number of data points increases as a consequence of the centrallimit theorem. In the case of Gaussian processes, the number of variables grows with Section 2.3\n",
      "the number of data points, and so this argument does not apply directly. However, if\n",
      "we consider increasing the number of data points falling in a ﬁxed region of xspace,\n",
      "then the corresponding uncertainty in the function a(x)will decrease, again leading\n",
      "asymptotically to a Gaussian (Williams and Barber, 1998).\n",
      "Three different approaches to obtaining a Gaussian approximation have been\n",
      "considered. One technique is based on variational inference (Gibbs and MacKay, Section 10.1\n",
      "2000) and makes use of the local variational bound (10.144) on the logistic sigmoid.This allows the product of sigmoid functions to be approximated by a product of\n",
      "Gaussians thereby allowing the marginalization over a\n",
      "Nto be performed analyti-\n",
      "cally. The approach also yields a lower bound on the likelihood function p(tN|θ).\n",
      "The variational framework for Gaussian process classiﬁcation can also be extended\n",
      "to multiclass ( K>2) problems by using a Gaussian approximation to the softmax\n",
      "function (Gibbs, 1997).\n",
      "A second approach uses expectation propagation (Opper and Winther, 2000b; Section 10.7\n",
      "Minka, 2001b; Seeger, 2003). Because the true posterior distribution is unimodal, as\n",
      "we shall see shortly, the expectation propagation approach can give good results.\n",
      "6.4.6 Laplace approximation\n",
      "The third approach to Gaussian process classiﬁcation is based on the Laplace\n",
      "approximation, which we now consider in detail. In order to evaluate the predictive Section 4.4\n",
      "distribution (6.76), we seek a Gaussian approximation to the posterior distribution\n",
      "overaN+1, which, using Bayes’ theorem, is given by\n",
      "p(aN+1|tN)=∫\n",
      "p(aN+1,aN|tN)daN\n",
      "=1\n",
      "p(tN)∫\n",
      "p(aN+1,aN)p(tN|aN+1,aN)daN\n",
      "=1\n",
      "p(tN)∫\n",
      "p(aN+1|aN)p(aN)p(tN|aN)daN\n",
      "=∫\n",
      "p(aN+1|aN)p(aN|tN)daN (6.77)316 6. KERNEL METHODS\n",
      "where we have used p(tN|aN+1,aN)=p(tN|aN). The conditional distribution\n",
      "p(aN+1|aN)is obtained by invoking the results (6.66) and (6.67) for Gaussian pro-\n",
      "cess regression, to give\n",
      "p(aN+1|aN)=N(aN+1|kTC−1\n",
      "NaN,c−kTC−1\n",
      "Nk). (6.78)\n",
      "We can therefore evaluate the integral in (6.77) by ﬁnding a Laplace approximation\n",
      "for the posterior distribution p(aN|tN), and then using the standard result for the\n",
      "convolution of two Gaussian distributions.\n",
      "The prior p(aN)is given by a zero-mean Gaussian process with covariance ma-\n",
      "trixCN, and the data term (assuming independence of the data points) is given by\n",
      "p(tN|aN)=N∏\n",
      "n=1σ(an)tn(1−σ(an))1−tn=N∏\n",
      "n=1eantnσ(−an). (6.79)\n",
      "We then obtain the Laplace approximation by Taylor expanding the logarithm of\n",
      "p(aN|tN), which up to an additive normalization constant is given by the quantity\n",
      "Ψ(aN)=l n p(aN)+l n p(tN|aN)\n",
      "=−1\n",
      "2aT\n",
      "NC−1\n",
      "NaN−N\n",
      "2ln(2π)−1\n",
      "2ln|CN|+tT\n",
      "NaN\n",
      "−N∑\n",
      "n=1ln(1 + ean) + const . (6.80)\n",
      "First we need to ﬁnd the mode of the posterior distribution, and this requires that we\n",
      "evaluate the gradient of Ψ(aN), which is given by\n",
      "∇Ψ(aN)=tN−σN−C−1\n",
      "NaN (6.81)\n",
      "where σNis a vector with elements σ(an). We cannot simply ﬁnd the mode by\n",
      "setting this gradient to zero, because σNdepends nonlinearly on aN, and so we\n",
      "resort to an iterative scheme based on the Newton-Raphson method, which gives rise\n",
      "to an iterative reweighted least squares (IRLS) algorithm. This requires the second Section 4.3.3\n",
      "derivatives of Ψ(aN), which we also require for the Laplace approximation anyway,\n",
      "and which are given by\n",
      "∇∇Ψ(aN)=−WN−C−1\n",
      "N(6.82)\n",
      "whereWNis a diagonal matrix with elements σ(an)(1−σ(an)), and we have used\n",
      "the result (4.88) for the derivative of the logistic sigmoid function. Note that these\n",
      "diagonal elements lie in the range (0,1/4), and hence WNis a positive deﬁnite\n",
      "matrix. Because CN(and hence its inverse) is positive deﬁnite by construction, and\n",
      "because the sum of two positive deﬁnite matrices is also positive deﬁnite, we see Exercise 6.24\n",
      "that the Hessian matrix A=−∇∇Ψ(aN)is positive deﬁnite and so the posterior\n",
      "distribution p(aN|tN)is log convex and therefore has a single mode that is the global6.4. Gaussian Processes 317\n",
      "maximum. The posterior distribution is not Gaussian, however, because the Hessian\n",
      "is a function of aN.\n",
      "Using the Newton-Raphson formula (4.92), the iterative update equation for aN\n",
      "is given by Exercise 6.25\n",
      "anew\n",
      "N=CN(I+WNCN)−1{tN−σN+WNaN}. (6.83)\n",
      "These equations are iterated until they converge to the mode which we denote by\n",
      "a⋆\n",
      "N. At the mode, the gradient ∇Ψ(aN)will vanish, and hence a⋆\n",
      "Nwill satisfy\n",
      "a⋆\n",
      "N=CN(tN−σN). (6.84)\n",
      "Once we have found the mode a⋆\n",
      "Nof the posterior, we can evaluate the Hessian\n",
      "matrix given by\n",
      "H=−∇∇Ψ(aN)=WN+C−1\n",
      "N(6.85)\n",
      "where the elements of WNare evaluated using a⋆\n",
      "N. This deﬁnes our Gaussian ap-\n",
      "proximation to the posterior distribution p(aN|tN)given by\n",
      "q(aN)=N(aN|a⋆\n",
      "N,H−1). (6.86)\n",
      "We can now combine this with (6.78) and hence evaluate the integral (6.77). Because\n",
      "this corresponds to a linear-Gaussian model, we can use the general result (2.115) togive Exercise 6.26\n",
      "E[a\n",
      "N+1|tN]= kT(tN−σN) (6.87)\n",
      "var[aN+1|tN]= c−kT(W−1\n",
      "N+CN)−1k. (6.88)\n",
      "Now that we have a Gaussian distribution for p(aN+1|tN), we can approximate\n",
      "the integral (6.76) using the result (4.153). As with the Bayesian logistic regression\n",
      "model of Section 4.5, if we are only interested in the decision boundary correspond-ing to p(t\n",
      "N+1|tN)=0.5, then we need only consider the mean and we can ignore\n",
      "the effect of the variance.\n",
      "We also need to determine the parameters θof the covariance function. One\n",
      "approach is to maximize the likelihood function given by p(tN|θ)for which we need\n",
      "expressions for the log likelihood and its gradient. If desired, suitable regularization\n",
      "terms can also be added, leading to a penalized maximum likelihood solution. Thelikelihood function is deﬁned by\n",
      "p(t\n",
      "N|θ)=∫\n",
      "p(tN|aN)p(aN|θ)daN. (6.89)\n",
      "This integral is analytically intractable, so again we make use of the Laplace approx-\n",
      "imation. Using the result (4.135), we obtain the following approximation for the logof the likelihood function\n",
      "lnp(t\n",
      "N|θ)=Ψ ( a⋆\n",
      "N)−1\n",
      "2ln|WN+C−1\n",
      "N|+N\n",
      "2ln(2π) (6.90)318 6. KERNEL METHODS\n",
      "where Ψ(a⋆\n",
      "N)=l n p(a⋆\n",
      "N|θ)+l n p(tN|a⋆\n",
      "N). We also need to evaluate the gradient\n",
      "oflnp(tN|θ)with respect to the parameter vector θ. Note that changes in θwill\n",
      "cause changes in a⋆\n",
      "N, leading to additional terms in the gradient. Thus, when we\n",
      "differentiate (6.90) with respect to θ, we obtain two sets of terms, the ﬁrst arising\n",
      "from the dependence of the covariance matrix CNonθ, and the rest arising from\n",
      "dependence of a⋆\n",
      "Nonθ.\n",
      "The terms arising from the explicit dependence on θcan be found by using\n",
      "(6.80) together with the results (C.21) and (C.22), and are given by\n",
      "∂lnp(tN|θ)\n",
      "∂θj=1\n",
      "2a⋆T\n",
      "NC−1\n",
      "N∂CN\n",
      "∂θjC−1\n",
      "Na⋆\n",
      "N\n",
      "−1\n",
      "2Tr[\n",
      "(I+CNWN)−1WN∂CN\n",
      "∂θj]\n",
      ". (6.91)\n",
      "To compute the terms arising from the dependence of a⋆\n",
      "Nonθ, we note that\n",
      "the Laplace approximation has been constructed such that Ψ(aN)has zero gradient\n",
      "ataN=a⋆\n",
      "N, and so Ψ(a⋆\n",
      "N)gives no contribution to the gradient as a result of its\n",
      "dependence on a⋆\n",
      "N. This leaves the following contribution to the derivative with\n",
      "respect to a component θjofθ\n",
      "−1\n",
      "2N∑\n",
      "n=1∂ln|WN+C−1\n",
      "N|\n",
      "∂a⋆n∂a⋆\n",
      "n\n",
      "∂θj\n",
      "=−1\n",
      "2N∑\n",
      "n=1[\n",
      "(I+CNWN)−1CN]\n",
      "nnσ⋆\n",
      "n(1−σ⋆\n",
      "n)(1−2σ⋆\n",
      "n)∂a⋆\n",
      "n\n",
      "∂θj(6.92)\n",
      "where σ⋆\n",
      "n=σ(a⋆\n",
      "n), and again we have used the result (C.22) together with the\n",
      "deﬁnition of WN. We can evaluate the derivative of a⋆\n",
      "Nwith respect to θjby differ-\n",
      "entiating the relation (6.84) with respect to θjto give\n",
      "∂a⋆\n",
      "n\n",
      "∂θj=∂CN\n",
      "∂θj(tN−σN)−CNWN∂a⋆\n",
      "n\n",
      "∂θj. (6.93)\n",
      "Rearranging then gives\n",
      "∂a⋆\n",
      "n\n",
      "∂θj=(I+WNCN)−1∂CN\n",
      "∂θj(tN−σN). (6.94)\n",
      "Combining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log\n",
      "likelihood function, which can be used with standard nonlinear optimization algo-\n",
      "rithms in order to determine a value for θ.\n",
      "We can illustrate the application of the Laplace approximation for Gaussian pro-\n",
      "cesses using the synthetic two-class data set shown in Figure 6.12. Extension of the Appendix A\n",
      "Laplace approximation to Gaussian processes involving K> 2classes, using the\n",
      "softmax activation function, is straightforward (Williams and Barber, 1998).6.4. Gaussian Processes 319\n",
      "−2 0 2−202\n",
      "Figure 6.12 Illustration of the use of a Gaussian process for classiﬁcation, showing the data on the left together\n",
      "with the optimal decision boundary from the true distribution in green, and the decision boundary from the\n",
      "Gaussian process classiﬁer in black. On the right is the predicted posterior probability for the blue and red\n",
      "classes together with the Gaussian process decision boundary.\n",
      "6.4.7 Connection to neural networks\n",
      "We have seen that the range of functions which can be represented by a neural\n",
      "network is governed by the number Mof hidden units, and that, for sufﬁciently\n",
      "largeM, a two-layer network can approximate any given function with arbitrary\n",
      "accuracy. In the framework of maximum likelihood, the number of hidden units\n",
      "needs to be limited (to a level dependent on the size of the training set) in order\n",
      "to avoid over-ﬁtting. However, from a Bayesian perspective it makes little sense to\n",
      "limit the number of parameters in the network according to the size of the training\n",
      "set.\n",
      "In a Bayesian neural network, the prior distribution over the parameter vector\n",
      "w, in conjunction with the network function f(x,w), produces a prior distribution\n",
      "over functions from y(x)whereyis the vector of network outputs. Neal (1996)\n",
      "has shown that, for a broad class of prior distributions over w, the distribution of\n",
      "functions generated by a neural network will tend to a Gaussian process in the limit\n",
      "M→∞ . It should be noted, however, that in this limit the output variables of the\n",
      "neural network become independent. One of the great merits of neural networks is\n",
      "that the outputs share the hidden units and so they can ‘borrow statistical strength’\n",
      "from each other, that is, the weights associated with each hidden unit are inﬂuenced\n",
      "by all of the output variables not just by one of them. This property is therefore lost\n",
      "in the Gaussian process limit.\n",
      "We have seen that a Gaussian process is determined by its covariance (kernel)\n",
      "function. Williams (1998) has given explicit forms for the covariance in the case of\n",
      "two speciﬁc choices for the hidden unit activation function (probit and Gaussian).\n",
      "These kernel functions k(x,x′)are nonstationary, i.e. they cannot be expressed as\n",
      "a function of the difference x−x′, as a consequence of the Gaussian weight prior\n",
      "being centred on zero which breaks translation invariance in weight space.320 6. KERNEL METHODS\n",
      "By working directly with the covariance function we have implicitly marginal-\n",
      "ized over the distribution of weights. If the weight prior is governed by hyperpa-rameters, then their values will determine the length scales of the distribution over\n",
      "functions, as can be understood by studying the examples in Figure 5.11 for the case\n",
      "of a ﬁnite number of hidden units. Note that we cannot marginalize out the hyperpa-rameters analytically, and must instead resort to techniques of the kind discussed in\n",
      "Section 6.4.\n",
      "Exercises\n",
      "6.1 (⋆⋆)www Consider the dual formulation of the least squares linear regression\n",
      "problem given in Section 6.1. Show that the solution for the components anof\n",
      "the vector acan be expressed as a linear combination of the elements of the vector\n",
      "φ(xn). Denoting these coefﬁcients by the vector w, show that the dual of the dual\n",
      "formulation is given by the original representation in terms of the parameter vector\n",
      "w.\n",
      "6.2 (⋆⋆)In this exercise, we develop a dual formulation of the perceptron learning\n",
      "algorithm. Using the perceptron learning rule (4.55), show that the learned weightvectorwcan be written as a linear combination of the vectors t\n",
      "nφ(xn)where tn∈\n",
      "{−1,+1}. Denote the coefﬁcients of this linear combination by αnand derive a\n",
      "formulation of the perceptron learning algorithm, and the predictive function for theperceptron, in terms of the α\n",
      "n. Show that the feature vector φ(x)enters only in the\n",
      "form of the kernel function k(x,x′)=φ(x)Tφ(x′).\n",
      "6.3 (⋆)The nearest-neighbour classiﬁer (Section 2.5.2) assigns a new input vector x\n",
      "to the same class as that of the nearest input vector xnfrom the training set, where\n",
      "in the simplest case, the distance is deﬁned by the Euclidean metric ∥x−xn∥2.B y\n",
      "expressing this rule in terms of scalar products and then making use of kernel sub-\n",
      "stitution, formulate the nearest-neighbour classiﬁer for a general nonlinear kernel.\n",
      "6.4 (⋆)In Appendix C, we give an example of a matrix that has positive elements but\n",
      "that has a negative eigenvalue and hence that is not positive deﬁnite. Find an exampleof the converse property, namely a 2×2matrix with positive eigenvalues yet that\n",
      "has at least one negative element.\n",
      "6.5 (⋆)\n",
      "www Verify the results (6.13) and (6.14) for constructing valid kernels.\n",
      "6.6 (⋆)Verify the results (6.15) and (6.16) for constructing valid kernels.\n",
      "6.7 (⋆)www Verify the results (6.17) and (6.18) for constructing valid kernels.\n",
      "6.8 (⋆)Verify the results (6.19) and (6.20) for constructing valid kernels.\n",
      "6.9 (⋆)Verify the results (6.21) and (6.22) for constructing valid kernels.\n",
      "6.10 (⋆)Show that an excellent choice of kernel for learning a function f(x)is given\n",
      "byk(x,x′)=f(x)f(x′)by showing that a linear learning machine based on this\n",
      "kernel will always ﬁnd a solution proportional to f(x).Exercises 321\n",
      "6.11 (⋆)By making use of the expansion (6.25), and then expanding the middle factor\n",
      "as a power series, show that the Gaussian kernel (6.23) can be expressed as the innerproduct of an inﬁnite-dimensional feature vector.\n",
      "6.12 (⋆⋆)\n",
      "www Consider the space of all possible subsets Aof a given ﬁxed set D.\n",
      "Show that the kernel function (6.27) corresponds to an inner product in a feature\n",
      "space of dimensionality 2|D|deﬁned by the mapping φ(A)where Ais a subset of D\n",
      "and the element φU(A), indexed by the subset U, is given by\n",
      "φU(A)={\n",
      "1,ifU⊆A;\n",
      "0,otherwise.(6.95)\n",
      "HereU⊆Adenotes that Uis either a subset of Aor is equal to A.\n",
      "6.13 (⋆)Show that the Fisher kernel, deﬁned by (6.33), remains invariant if we make\n",
      "a nonlinear transformation of the parameter vector θ→ψ(θ), where the function\n",
      "ψ(·)is invertible and differentiable.\n",
      "6.14 (⋆)www Write down the form of the Fisher kernel, deﬁned by (6.33), for the\n",
      "case of a distribution p(x|µ)=N(x|µ,S)that is Gaussian with mean µand ﬁxed\n",
      "covariance S.\n",
      "6.15 (⋆)By considering the determinant of a 2×2Gram matrix, show that a positive-\n",
      "deﬁnite kernel function k(x, x′)satisﬁes the Cauchy-Schwartz inequality\n",
      "k(x1,x2)2⩽k(x1,x1)k(x2,x2). (6.96)\n",
      "6.16 (⋆⋆)Consider a parametric model governed by the parameter vector wtogether\n",
      "with a data set of input values x1,...,xNand a nonlinear feature mapping φ(x).\n",
      "Suppose that the dependence of the error function on wtakes the form\n",
      "J(w)=f(wTφ(x1),...,wTφ(xN)) +g(wTw) (6.97)\n",
      "where g(·)is a monotonically increasing function. By writing win the form\n",
      "w=N∑\n",
      "n=1αnφ(xn)+w⊥ (6.98)\n",
      "show that the value of wthat minimizes J(w)takes the form of a linear combination\n",
      "of the basis functions φ(xn)forn=1,...,N .\n",
      "6.17 (⋆⋆)www Consider the sum-of-squares error function (6.39) for data having\n",
      "noisy inputs, where ν(ξ)is the distribution of the noise. Use the calculus of vari-\n",
      "ations to minimize this error function with respect to the function y(x), and hence\n",
      "show that the optimal solution is given by an expansion of the form (6.40) in which\n",
      "the basis functions are given by (6.41).322 6. KERNEL METHODS\n",
      "6.18 (⋆)Consider a Nadaraya-Watson model with one input variable xand one target\n",
      "variable thaving Gaussian components with isotropic covariances, so that the co-\n",
      "variance matrix is given by σ2IwhereIis the unit matrix. Write down expressions\n",
      "for the conditional density p(t|x)and for the conditional mean E[t|x]and variance\n",
      "var[t|x], in terms of the kernel function k(x, xn).\n",
      "6.19 (⋆⋆)Another viewpoint on kernel regression comes from a consideration of re-\n",
      "gression problems in which the input variables as well as the target variables are\n",
      "corrupted with additive noise. Suppose each target value tnis generated as usual\n",
      "by taking a function y(zn)evaluated at a point zn, and adding Gaussian noise. The\n",
      "value of znis not directly observed, however, but only a noise corrupted version\n",
      "xn=zn+ξnwhere the random variable ξis governed by some distribution g(ξ).\n",
      "Consider a set of observations {xn,tn}, where n=1,...,N , together with a cor-\n",
      "responding sum-of-squares error function deﬁned by averaging over the distribution\n",
      "of input noise to give\n",
      "E=1\n",
      "2N∑\n",
      "n=1∫\n",
      "{y(xn−ξn)−tn}2g(ξn)dξn. (6.99)\n",
      "By minimizing Ewith respect to the function y(z)using the calculus of variations\n",
      "(Appendix D), show that optimal solution for y(x)is given by a Nadaraya-Watson\n",
      "kernel regression solution of the form (6.45) with a kernel of the form (6.46).\n",
      "6.20 (⋆⋆)www Verify the results (6.66) and (6.67).\n",
      "6.21 (⋆⋆)www Consider a Gaussian process regression model in which the kernel\n",
      "function is deﬁned in terms of a ﬁxed set of nonlinear basis functions. Show that thepredictive distribution is identical to the result (3.58) obtained in Section 3.3.2 for the\n",
      "Bayesian linear regression model. To do this, note that both models have Gaussian\n",
      "predictive distributions, and so it is only necessary to show that the conditional meanand variance are the same. For the mean, make use of the matrix identity (C.6), and\n",
      "for the variance, make use of the matrix identity (C.7).\n",
      "6.22 (⋆⋆)Consider a regression problem with Ntraining set input vectors x\n",
      "1,...,xN\n",
      "andLtest set input vectors xN+1,...,xN+L, and suppose we deﬁne a Gaussian\n",
      "process prior over functions t(x). Derive an expression for the joint predictive dis-\n",
      "tribution for t(xN+1),...,t (xN+L), given the values of t(x1),...,t (xN). Show the\n",
      "marginal of this distribution for one of the test observations tjwhere N+1⩽j⩽\n",
      "N+Lis given by the usual Gaussian process regression result (6.66) and (6.67).\n",
      "6.23 (⋆⋆)www Consider a Gaussian process regression model in which the target\n",
      "variable thas dimensionality D. Write down the conditional distribution of tN+1\n",
      "for a test input vector xN+1, given a training set of input vectors x1,...,xN+1and\n",
      "corresponding target observations t1,...,tN.\n",
      "6.24 (⋆)Show that a diagonal matrix Wwhose elements satisfy 0<W ii<1is positive\n",
      "deﬁnite. Show that the sum of two positive deﬁnite matrices is itself positive deﬁnite.Exercises 323\n",
      "6.25 (⋆)www Using the Newton-Raphson formula (4.92), derive the iterative update\n",
      "formula (6.83) for ﬁnding the mode a⋆\n",
      "Nof the posterior distribution in the Gaussian\n",
      "process classiﬁcation model.\n",
      "6.26 (⋆)Using the result (2.115), derive the expressions (6.87) and (6.88) for the mean\n",
      "and variance of the posterior distribution p(aN+1|tN)in the Gaussian process clas-\n",
      "siﬁcation model.\n",
      "6.27 (⋆⋆⋆ )Derive the result (6.90) for the log likelihood function in the Laplace approx-\n",
      "imation framework for Gaussian process classiﬁcation. Similarly, derive the results\n",
      "(6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood.7\n",
      "Sparse Kernel\n",
      "Machines\n",
      "In the previous chapter, we explored a variety of learning algorithms based on non-\n",
      "linear kernels. One of the signiﬁcant limitations of many such algorithms is thatthe kernel function k(x\n",
      "n,xm)must be evaluated for all possible pairs xnandxm\n",
      "of training points, which can be computationally infeasible during training and can\n",
      "lead to excessive computation times when making predictions for new data points.In this chapter we shall look at kernel-based algorithms that have sparse solutions,\n",
      "so that predictions for new inputs depend only on the kernel function evaluated at a\n",
      "subset of the training data points.\n",
      "We begin by looking in some detail at the support vector machine (SVM), which\n",
      "became popular in some years ago for solving problems in classiﬁcation, regression,\n",
      "and novelty detection. An important property of support vector machines is that thedetermination of the model parameters corresponds to a convex optimization prob-\n",
      "lem, and so any local solution is also a global optimum. Because the discussion of\n",
      "support vector machines makes extensive use of Lagrange multipliers, the reader is\n",
      "325326 7. SPARSE KERNEL MACHINES\n",
      "encouraged to review the key concepts covered in Appendix E. Additional infor-\n",
      "mation on support vector machines can be found in Vapnik (1995), Burges (1998),Cristianini and Shawe-Taylor (2000), M ¨uller et al. (2001), Sch ¨olkopf and Smola\n",
      "(2002), and Herbrich (2002).\n",
      "The SVM is a decision machine and so does not provide posterior probabilities.\n",
      "We have already discussed some of the beneﬁts of determining probabilities in Sec-\n",
      "tion 1.5.4. An alternative sparse kernel technique, known as the relevance vector\n",
      "machine (RVM), is based on a Bayesian formulation and provides posterior proba- Section 7.2\n",
      "bilistic outputs, as well as having typically much sparser solutions than the SVM.\n",
      "7.1. Maximum Margin Classiﬁers\n",
      "We begin our discussion of support vector machines by returning to the two-classclassiﬁcation problem using linear models of the form\n",
      "y(x)=w\n",
      "Tφ(x)+b (7.1)\n",
      "where φ(x)denotes a ﬁxed feature-space transformation, and we have made the\n",
      "bias parameter bexplicit. Note that we shall shortly introduce a dual representation\n",
      "expressed in terms of kernel functions, which avoids having to work explicitly infeature space. The training data set comprises Ninput vectors x\n",
      "1,...,xN, with\n",
      "corresponding target values t1,...,t Nwhere tn∈{ −1,1}, and new data points x\n",
      "are classiﬁed according to the sign of y(x).\n",
      "We shall assume for the moment that the training data set is linearly separable in\n",
      "feature space, so that by deﬁnition there exists at least one choice of the parameters\n",
      "wandbsuch that a function of the form (7.1) satisﬁes y(xn)>0for points having\n",
      "tn=+ 1 andy(xn)<0for points having tn=−1, so that tny(xn)>0for all\n",
      "training data points.\n",
      "There may of course exist many such solutions that separate the classes exactly.\n",
      "In Section 4.1.7, we described the perceptron algorithm that is guaranteed to ﬁnd\n",
      "a solution in a ﬁnite number of steps. The solution that it ﬁnds, however, will be\n",
      "dependent on the (arbitrary) initial values chosen for wandbas well as on the\n",
      "order in which the data points are presented. If there are multiple solutions all of\n",
      "which classify the training data set exactly, then we should try to ﬁnd the one that\n",
      "will give the smallest generalization error. The support vector machine approachesthis problem through the concept of the margin , which is deﬁned to be the smallest\n",
      "distance between the decision boundary and any of the samples, as illustrated in\n",
      "Figure 7.1.\n",
      "In support vector machines the decision boundary is chosen to be the one for\n",
      "which the margin is maximized. The maximum margin solution can be motivated us-ingcomputational learning theory , also known as statistical learning theory .H o w - Section 7.1.5\n",
      "ever, a simple insight into the origins of maximum margin has been given by Tong\n",
      "and Koller (2000) who consider a framework for classiﬁcation based on a hybrid ofgenerative and discriminative approaches. They ﬁrst model the distribution over in-\n",
      "put vectors xfor each class using a Parzen density estimator with Gaussian kernels7.1. Maximum Margin Classiﬁers 327\n",
      "y=1\n",
      "y=0\n",
      "y=−1\n",
      "marginy=1y=0y=−1\n",
      "Figure 7.1 The margin is deﬁned as the perpendicular distance between the decision boundary and the closest\n",
      "of the data points, as shown on the left ﬁgure. Maximizing the margin leads to a particular choice of decision\n",
      "boundary, as shown on the right. The location of this boundary is determined by a subset of the data points,\n",
      "known as support vectors, which are indicated by the circles.\n",
      "having a common parameter σ2. Together with the class priors, this deﬁnes an opti-\n",
      "mal misclassiﬁcation-rate decision boundary. However, instead of using this optimal\n",
      "boundary, they determine the best hyperplane by minimizing the probability of error\n",
      "relative to the learned density model. In the limit σ2→0, the optimal hyperplane\n",
      "is shown to be the one having maximum margin. The intuition behind this result is\n",
      "that as σ2is reduced, the hyperplane is increasingly dominated by nearby data points\n",
      "relative to more distant ones. In the limit, the hyperplane becomes independent of\n",
      "data points that are not support vectors.\n",
      "We shall see in Figure 10.13 that marginalization with respect to the prior distri-\n",
      "bution of the parameters in a Bayesian approach for a simple linearly separable data\n",
      "set leads to a decision boundary that lies in the middle of the region separating the\n",
      "data points. The large margin solution has similar behaviour.\n",
      "Recall from Figure 4.1 that the perpendicular distance of a point xfrom a hyper-\n",
      "plane deﬁned by y(x)=0 where y(x)takes the form (7.1) is given by |y(x)|/∥w∥.\n",
      "Furthermore, we are only interested in solutions for which all data points are cor-\n",
      "rectly classiﬁed, so that tny(xn)>0for all n. Thus the distance of a point xnto the\n",
      "decision surface is given by\n",
      "tny(xn)\n",
      "∥w∥=tn(wTφ(xn)+b)\n",
      "∥w∥. (7.2)\n",
      "The margin is given by the perpendicular distance to the closest point xnfrom the\n",
      "data set, and we wish to optimize the parameters wandbin order to maximize this\n",
      "distance. Thus the maximum margin solution is found by solving\n",
      "arg max\n",
      "w,b{1\n",
      "∥w∥min\n",
      "n[\n",
      "tn(\n",
      "wTφ(xn)+b)]}\n",
      "(7.3)\n",
      "where we have taken the factor 1/∥w∥outside the optimization over nbecause w328 7. SPARSE KERNEL MACHINES\n",
      "does not depend on n. Direct solution of this optimization problem would be very\n",
      "complex, and so we shall convert it into an equivalent problem that is much easierto solve. To do this we note that if we make the rescaling w→κwandb→κb,\n",
      "then the distance from any point x\n",
      "nto the decision surface, given by tny(xn)/∥w∥,\n",
      "is unchanged. We can use this freedom to set\n",
      "tn(\n",
      "wTφ(xn)+b)\n",
      "=1 (7.4)\n",
      "for the point that is closest to the surface. In this case, all data points will satisfy the\n",
      "constraints\n",
      "tn(\n",
      "wTφ(xn)+b)\n",
      "⩾1,n =1,...,N. (7.5)\n",
      "This is known as the canonical representation of the decision hyperplane. In the\n",
      "case of data points for which the equality holds, the constraints are said to be active ,\n",
      "whereas for the remainder they are said to be inactive . By deﬁnition, there will\n",
      "always be at least one active constraint, because there will always be a closest point,\n",
      "and once the margin has been maximized there will be at least two active constraints.\n",
      "The optimization problem then simply requires that we maximize ∥w∥−1, which is\n",
      "equivalent to minimizing ∥w∥2, and so we have to solve the optimization problem\n",
      "arg min\n",
      "w,b1\n",
      "2∥w∥2(7.6)\n",
      "subject to the constraints given by (7.5). The factor of 1/2in (7.6) is included for\n",
      "later convenience. This is an example of a quadratic programming problem in which\n",
      "we are trying to minimize a quadratic function subject to a set of linear inequalityconstraints. It appears that the bias parameter bhas disappeared from the optimiza-\n",
      "tion. However, it is determined implicitly via the constraints, because these require\n",
      "that changes to ∥w∥be compensated by changes to b. We shall see how this works\n",
      "shortly.\n",
      "In order to solve this constrained optimization problem, we introduce Lagrange\n",
      "multipliers a\n",
      "n⩾0, with one multiplier anfor each of the constraints in (7.5), giving Appendix E\n",
      "the Lagrangian function\n",
      "L(w,b ,a)=1\n",
      "2∥w∥2−N∑\n",
      "n=1an{\n",
      "tn(wTφ(xn)+b)−1}\n",
      "(7.7)\n",
      "wherea=(a1,...,a N)T. Note the minus sign in front of the Lagrange multiplier\n",
      "term, because we are minimizing with respect to wandb, and maximizing with\n",
      "respect to a. Setting the derivatives of L(w,b ,a)with respect to wandbequal to\n",
      "zero, we obtain the following two conditions\n",
      "w=N∑\n",
      "n=1antnφ(xn) (7.8)\n",
      "0=N∑\n",
      "n=1antn. (7.9)7.1. Maximum Margin Classiﬁers 329\n",
      "Eliminating wandbfromL(w,b ,a)using these conditions then gives the dual\n",
      "representation of the maximum margin problem in which we maximize\n",
      "˜L(a)=N∑\n",
      "n=1an−1\n",
      "2N∑\n",
      "n=1N∑\n",
      "m=1anamtntmk(xn,xm) (7.10)\n",
      "with respect to asubject to the constraints\n",
      "an⩾0,n =1,...,N, (7.11)\n",
      "N∑\n",
      "n=1antn=0. (7.12)\n",
      "Here the kernel function is deﬁned by k(x,x′)=φ(x)Tφ(x′). Again, this takes the\n",
      "form of a quadratic programming problem in which we optimize a quadratic function\n",
      "ofasubject to a set of inequality constraints. We shall discuss techniques for solving\n",
      "such quadratic programming problems in Section 7.1.1.\n",
      "The solution to a quadratic programming problem in Mvariables in general has\n",
      "computational complexity that is O(M3). In going to the dual formulation we have\n",
      "turned the original optimization problem, which involved minimizing (7.6) over M\n",
      "variables, into the dual problem (7.10), which has Nvariables. For a ﬁxed set of\n",
      "basis functions whose number Mis smaller than the number Nof data points, the\n",
      "move to the dual problem appears disadvantageous. However, it allows the model tobe reformulated using kernels, and so the maximum margin classiﬁer can be applied\n",
      "efﬁciently to feature spaces whose dimensionality exceeds the number of data points,\n",
      "including inﬁnite feature spaces. The kernel formulation also makes clear the roleof the constraint that the kernel function k(x,x\n",
      "′)be positive deﬁnite, because this\n",
      "ensures that the Lagrangian function ˜L(a)is bounded below, giving rise to a well-\n",
      "deﬁned optimization problem.\n",
      "In order to classify new data points using the trained model, we evaluate the sign\n",
      "ofy(x)deﬁned by (7.1). This can be expressed in terms of the parameters {an}and\n",
      "the kernel function by substituting for wusing (7.8) to give\n",
      "y(x)=N∑\n",
      "n=1antnk(x,xn)+b. (7.13)\n",
      "Joseph-Louis Lagrange\n",
      "1736–1813\n",
      "Although widely considered to be\n",
      "a French mathematician, Lagrangewas born in Turin in Italy. By the ageof nineteen, he had already madeimportant contributions mathemat-ics and had been appointed as Pro-\n",
      "fessor at the Royal Artillery School in Turin. For manyyears, Euler worked hard to persuade Lagrange to\n",
      "move to Berlin, which he eventually did in 1766 wherehe succeeded Euler as Director of Mathematics atthe Berlin Academy. Later he moved to Paris, nar-rowly escaping with his life during the French revo-lution thanks to the personal intervention of Lavoisier(the French chemist who discovered oxygen) who him-self was later executed at the guillotine. Lagrangemade key contributions to the calculus of variationsand the foundations of dynamics.330 7. SPARSE KERNEL MACHINES\n",
      "In Appendix E, we show that a constrained optimization of this form satisﬁes the\n",
      "Karush-Kuhn-Tucker (KKT) conditions, which in this case require that the following\n",
      "three properties hold\n",
      "an⩾0 (7.14)\n",
      "tny(xn)−1⩾0 (7.15)\n",
      "an{tny(xn)−1}=0. (7.16)\n",
      "Thus for every data point, either an=0 ortny(xn)=1 . Any data point for\n",
      "which an=0will not appear in the sum in (7.13) and hence plays no role in making\n",
      "predictions for new data points. The remaining data points are called support vectors ,\n",
      "and because they satisfy tny(xn)=1 , they correspond to points that lie on the\n",
      "maximum margin hyperplanes in feature space, as illustrated in Figure 7.1. This\n",
      "property is central to the practical applicability of support vector machines. Once\n",
      "the model is trained, a signiﬁcant proportion of the data points can be discarded andonly the support vectors retained.\n",
      "Having solved the quadratic programming problem and found a value for a,w e\n",
      "can then determine the value of the threshold parameter bby noting that any support\n",
      "vectorx\n",
      "nsatisﬁes tny(xn)=1 . Using (7.13) this gives\n",
      "tn(∑\n",
      "m∈Samtmk(xn,xm)+b)\n",
      "=1 (7.17)\n",
      "where Sdenotes the set of indices of the support vectors. Although we can solve\n",
      "this equation for busing an arbitrarily chosen support vector xn, a numerically more\n",
      "stable solution is obtained by ﬁrst multiplying through by tn, making use of t2\n",
      "n=1,\n",
      "and then averaging these equations over all support vectors and solving for bto give\n",
      "b=1\n",
      "NS∑\n",
      "n∈S(\n",
      "tn−∑\n",
      "m∈Samtmk(xn,xm))\n",
      "(7.18)\n",
      "where NSis the total number of support vectors.\n",
      "For later comparison with alternative models, we can express the maximum-\n",
      "margin classiﬁer in terms of the minimization of an error function, with a simple\n",
      "quadratic regularizer, in the form\n",
      "N∑\n",
      "n=1E∞(y(xn)tn−1) +λ∥w∥2(7.19)\n",
      "where E∞(z)is a function that is zero if z⩾0and∞otherwise and ensures that\n",
      "the constraints (7.5) are satisﬁed. Note that as long as the regularization parameter\n",
      "satisﬁes λ>0, its precise value plays no role.\n",
      "Figure 7.2 shows an example of the classiﬁcation resulting from training a sup-\n",
      "port vector machine on a simple synthetic data set using a Gaussian kernel of the7.1. Maximum Margin Classiﬁers 331\n",
      "Figure 7.2 Example of synthetic data from\n",
      "two classes in two dimensions\n",
      "showing contours of constant\n",
      "y(x)obtained from a support\n",
      "vector machine having a Gaus-sian kernel function. Also shownare the decision boundary, themargin boundaries, and the sup-port vectors.\n",
      "form (6.23). Although the data set is not linearly separable in the two-dimensional\n",
      "data space x, it is linearly separable in the nonlinear feature space deﬁned implicitly\n",
      "by the nonlinear kernel function. Thus the training data points are perfectly separated\n",
      "in the original data space.\n",
      "This example also provides a geometrical insight into the origin of sparsity in\n",
      "the SVM. The maximum margin hyperplane is deﬁned by the location of the supportvectors. Other data points can be moved around freely (so long as they remain out-\n",
      "side the margin region) without changing the decision boundary, and so the solution\n",
      "will be independent of such data points.\n",
      "7.1.1 Overlapping class distributions\n",
      "So far, we have assumed that the training data points are linearly separable in the\n",
      "feature space φ(x). The resulting support vector machine will give exact separation\n",
      "of the training data in the original input space x, although the corresponding decision\n",
      "boundary will be nonlinear. In practice, however, the class-conditional distributions\n",
      "may overlap, in which case exact separation of the training data can lead to poor\n",
      "generalization.\n",
      "We therefore need a way to modify the support vector machine so as to allow\n",
      "some of the training points to be misclassiﬁed. From (7.19) we see that in the case\n",
      "of separable classes, we implicitly used an error function that gave inﬁnite error\n",
      "if a data point was misclassiﬁed and zero error if it was classiﬁed correctly, andthen optimized the model parameters to maximize the margin. We now modify this\n",
      "approach so that data points are allowed to be on the ‘wrong side’ of the margin\n",
      "boundary, but with a penalty that increases with the distance from that boundary. Forthe subsequent optimization problem, it is convenient to make this penalty a linear\n",
      "function of this distance. To do this, we introduce slack variables ,ξ\n",
      "n⩾0where\n",
      "n=1,...,N , with one slack variable for each training data point (Bennett, 1992;\n",
      "Cortes and Vapnik, 1995). These are deﬁned by ξn=0for data points that are on or\n",
      "inside the correct margin boundary and ξn=|tn−y(xn)|for other points. Thus a\n",
      "data point that is on the decision boundary y(xn)=0 will have ξn=1, and points332 7. SPARSE KERNEL MACHINES\n",
      "Figure 7.3 Illustration of the slack variables ξn⩾0.\n",
      "Data points with circles around them are\n",
      "support vectors.\n",
      "y=1y=0y=−1\n",
      "ξ> 1\n",
      "ξ< 1\n",
      "ξ=0ξ=0\n",
      "withξn>1will be misclassiﬁed. The exact classiﬁcation constraints (7.5) are then\n",
      "replaced with\n",
      "tny(xn)⩾1−ξn,n =1,...,N (7.20)\n",
      "in which the slack variables are constrained to satisfy ξn⩾0. Data points for which\n",
      "ξn=0 are correctly classiﬁed and are either on the margin or on the correct side\n",
      "of the margin. Points for which 0<ξn⩽1lie inside the margin, but on the cor-\n",
      "rect side of the decision boundary, and those data points for which ξn>1lie on\n",
      "the wrong side of the decision boundary and are misclassiﬁed, as illustrated in Fig-\n",
      "ure 7.3. This is sometimes described as relaxing the hard margin constraint to give a\n",
      "soft margin and allows some of the training set data points to be misclassiﬁed. Note\n",
      "that while slack variables allow for overlapping class distributions, this framework is\n",
      "still sensitive to outliers because the penalty for misclassiﬁcation increases linearly\n",
      "withξ.\n",
      "Our goal is now to maximize the margin while softly penalizing points that lie\n",
      "on the wrong side of the margin boundary. We therefore minimize\n",
      "CN∑\n",
      "n=1ξn+1\n",
      "2∥w∥2(7.21)\n",
      "where the parameter C>0controls the trade-off between the slack variable penalty\n",
      "and the margin. Because any point that is misclassiﬁed has ξn>1, it follows that∑\n",
      "nξnis an upper bound on the number of misclassiﬁed points. The parameter Cis\n",
      "therefore analogous to (the inverse of) a regularization coefﬁcient because it controls\n",
      "the trade-off between minimizing training errors and controlling model complexity.\n",
      "In the limit C→∞ , we will recover the earlier support vector machine for separable\n",
      "data.\n",
      "We now wish to minimize (7.21) subject to the constraints (7.20) together with\n",
      "ξn⩾0. The corresponding Lagrangian is given by\n",
      "L(w,b ,a)=1\n",
      "2∥w∥2+CN∑\n",
      "n=1ξn−N∑\n",
      "n=1an{tny(xn)−1+ξn}−N∑\n",
      "n=1µnξn(7.22)7.1. Maximum Margin Classiﬁers 333\n",
      "where {an⩾0}and{µn⩾0}are Lagrange multipliers. The corresponding set of\n",
      "KKT conditions are given by Appendix E\n",
      "an⩾0 (7.23)\n",
      "tny(xn)−1+ξn⩾0 (7.24)\n",
      "an(tny(xn)−1+ξn)=0 (7.25)\n",
      "µn⩾0 (7.26)\n",
      "ξn⩾0 (7.27)\n",
      "µnξn=0 (7.28)\n",
      "where n=1,...,N .\n",
      "We now optimize out w,b, and{ξn}making use of the deﬁnition (7.1) of y(x)\n",
      "to give\n",
      "∂L\n",
      "∂w=0⇒w=N∑\n",
      "n=1antnφ(xn) (7.29)\n",
      "∂L\n",
      "∂b=0⇒N∑\n",
      "n=1antn=0 (7.30)\n",
      "∂L\n",
      "∂ξn=0⇒an=C−µn. (7.31)\n",
      "Using these results to eliminate w,b, and{ξn}from the Lagrangian, we obtain the\n",
      "dual Lagrangian in the form\n",
      "˜L(a)=N∑\n",
      "n=1an−1\n",
      "2N∑\n",
      "n=1N∑\n",
      "m=1anamtntmk(xn,xm) (7.32)\n",
      "which is identical to the separable case, except that the constraints are somewhat\n",
      "different. To see what these constraints are, we note that an⩾0is required because\n",
      "these are Lagrange multipliers. Furthermore, (7.31) together with µn⩾0implies\n",
      "an⩽C. We therefore have to minimize (7.32) with respect to the dual variables\n",
      "{an}subject to\n",
      "0⩽an⩽C (7.33)\n",
      "N∑\n",
      "n=1antn=0 (7.34)\n",
      "forn=1,...,N , where (7.33) are known as box constraints . This again represents\n",
      "a quadratic programming problem. If we substitute (7.29) into (7.1), we see that\n",
      "predictions for new data points are again made by using (7.13).\n",
      "We can now interpret the resulting solution. As before, a subset of the data\n",
      "points may have an=0, in which case they do not contribute to the predictive334 7. SPARSE KERNEL MACHINES\n",
      "model (7.13). The remaining data points constitute the support vectors. These have\n",
      "an>0and hence from (7.25) must satisfy\n",
      "tny(xn)=1−ξn. (7.35)\n",
      "Ifan<C, then (7.31) implies that µn>0, which from (7.28) requires ξn=0and\n",
      "hence such points lie on the margin. Points with an=Ccan lie inside the margin\n",
      "and can either be correctly classiﬁed if ξn⩽1or misclassiﬁed if ξn>1.\n",
      "To determine the parameter bin (7.1), we note that those support vectors for\n",
      "which 0<an<C haveξn=0so that tny(xn)=1 and hence will satisfy\n",
      "tn(∑\n",
      "m∈Samtmk(xn,xm)+b)\n",
      "=1. (7.36)\n",
      "Again, a numerically stable solution is obtained by averaging to give\n",
      "b=1\n",
      "NM∑\n",
      "n∈M(\n",
      "tn−∑\n",
      "m∈Samtmk(xn,xm))\n",
      "(7.37)\n",
      "where Mdenotes the set of indices of data points having 0<an<C.\n",
      "An alternative, equivalent formulation of the support vector machine, known as\n",
      "theν-SVM , has been proposed by Sch ¨olkopf et al. (2000). This involves maximizing\n",
      "˜L(a)=−1\n",
      "2N∑\n",
      "n=1N∑\n",
      "m=1anamtntmk(xn,xm) (7.38)\n",
      "subject to the constraints\n",
      "0⩽an⩽1/N (7.39)\n",
      "N∑\n",
      "n=1antn=0 (7.40)\n",
      "N∑\n",
      "n=1an⩾ν. (7.41)\n",
      "This approach has the advantage that the parameter ν, which replaces C, can be\n",
      "interpreted as both an upper bound on the fraction of margin errors (points for which\n",
      "ξn>0and hence which lie on the wrong side of the margin boundary and which may\n",
      "or may not be misclassiﬁed) and a lower bound on the fraction of support vectors. Anexample of the ν-SVM applied to a synthetic data set is shown in Figure 7.4. Here\n",
      "Gaussian kernels of the form exp (−γ∥x−x\n",
      "′∥2)have been used, with γ=0.45.\n",
      "Although predictions for new inputs are made using only the support vectors,\n",
      "the training phase (i.e., the determination of the parameters aandb) makes use of\n",
      "the whole data set, and so it is important to have efﬁcient algorithms for solving7.1. Maximum Margin Classiﬁers 335\n",
      "Figure 7.4 Illustration of the ν-SVM applied\n",
      "to a nonseparable data set in two\n",
      "dimensions. The support vectors\n",
      "are indicated by circles.\n",
      "−2 0 2−202\n",
      "the quadratic programming problem. We ﬁrst note that the objective function ˜L(a)\n",
      "given by (7.10) or (7.32) is quadratic and so any local optimum will also be a global\n",
      "optimum provided the constraints deﬁne a convex region (which they do as a conse-\n",
      "quence of being linear). Direct solution of the quadratic programming problem us-\n",
      "ing traditional techniques is often infeasible due to the demanding computation and\n",
      "memory requirements, and so more practical approaches need to be found. The tech-\n",
      "nique of chunking (Vapnik, 1982) exploits the fact that the value of the Lagrangian\n",
      "is unchanged if we remove the rows and columns of the kernel matrix corresponding\n",
      "to Lagrange multipliers that have value zero. This allows the full quadratic pro-\n",
      "gramming problem to be broken down into a series of smaller ones, whose goal is\n",
      "eventually to identify all of the nonzero Lagrange multipliers and discard the others.\n",
      "Chunking can be implemented using protected conjugate gradients (Burges, 1998).\n",
      "Although chunking reduces the size of the matrix in the quadratic function from the\n",
      "number of data points squared to approximately the number of nonzero Lagrange\n",
      "multipliers squared, even this may be too big to ﬁt in memory for large-scale appli-\n",
      "cations. Decomposition methods (Osuna et al. , 1996) also solve a series of smaller\n",
      "quadratic programming problems but are designed so that each of these is of a ﬁxed\n",
      "size, and so the technique can be applied to arbitrarily large data sets. However, it\n",
      "still involves numerical solution of quadratic programming subproblems and these\n",
      "can be problematic and expensive. One of the most popular approaches to training\n",
      "support vector machines is called sequential minimal optimization ,o r SMO (Platt,\n",
      "1999). It takes the concept of chunking to the extreme limit and considers just two\n",
      "Lagrange multipliers at a time. In this case, the subproblem can be solved analyti-\n",
      "cally, thereby avoiding numerical quadratic programming altogether. Heuristics are\n",
      "given for choosing the pair of Lagrange multipliers to be considered at each step.\n",
      "In practice, SMO is found to have a scaling with the number of data points that is\n",
      "somewhere between linear and quadratic depending on the particular application.\n",
      "We have seen that kernel functions correspond to inner products in feature spaces\n",
      "that can have high, or even inﬁnite, dimensionality. By working directly in terms of\n",
      "the kernel function, without introducing the feature space explicitly, it might there-\n",
      "fore seem that support vector machines somehow manage to avoid the curse of di-336 7. SPARSE KERNEL MACHINES\n",
      "mensionality. This is not the case, however, because there are constraints amongst Section 1.4\n",
      "the feature values that restrict the effective dimensionality of feature space. To seethis consider a simple second-order polynomial kernel that we can expand in terms\n",
      "of its components\n",
      "k(x,z)=(\n",
      "1+xTz)2=( 1+ x1z1+x2z2)2\n",
      "=1 + 2 x1z1+2x2z2+x2\n",
      "1z2\n",
      "1+2x1z1x2z2+x2\n",
      "2z2\n",
      "2\n",
      "=( 1 ,√\n",
      "2x1,√\n",
      "2x2,x2\n",
      "1,√\n",
      "2x1x2,x2\n",
      "2)(1,√\n",
      "2z1,√\n",
      "2z2,z2\n",
      "1,√\n",
      "2z1z2,z2\n",
      "2)T\n",
      "=φ(x)Tφ(z). (7.42)\n",
      "This kernel function therefore represents an inner product in a feature space having\n",
      "six dimensions, in which the mapping from input space to feature space is describedby the vector function φ(x). However, the coefﬁcients weighting these different\n",
      "features are constrained to have speciﬁc forms. Thus any set of points in the original\n",
      "two-dimensional space xwould be constrained to lie exactly on a two-dimensional\n",
      "nonlinear manifold embedded in the six-dimensional feature space.\n",
      "We have already highlighted the fact that the support vector machine does not\n",
      "provide probabilistic outputs but instead makes classiﬁcation decisions for new in-\n",
      "put vectors. Veropoulos et al. (1999) discuss modiﬁcations to the SVM to allow\n",
      "the trade-off between false positive and false negative errors to be controlled. How-ever, if we wish to use the SVM as a module in a larger probabilistic system, then\n",
      "probabilistic predictions of the class label tfor new inputs xare required.\n",
      "To address this issue, Platt (2000) has proposed ﬁtting a logistic sigmoid to the\n",
      "outputs of a previously trained support vector machine. Speciﬁcally, the required\n",
      "conditional probability is assumed to be of the form\n",
      "p(t=1|x)=σ(Ay(x)+B) (7.43)\n",
      "where y(x)is deﬁned by (7.1). Values for the parameters AandBare found by\n",
      "minimizing the cross-entropy error function deﬁned by a training set consisting of\n",
      "pairs of values y(x\n",
      "n)andtn. The data used to ﬁt the sigmoid needs to be independent\n",
      "of that used to train the original SVM in order to avoid severe over-ﬁtting. This two-\n",
      "stage approach is equivalent to assuming that the output y(x)of the support vector\n",
      "machine represents the log-odds of xbelonging to class t=1. Because the SVM\n",
      "training procedure is not speciﬁcally intended to encourage this, the SVM can give\n",
      "a poor approximation to the posterior probabilities (Tipping, 2001).\n",
      "7.1.2 Relation to logistic regression\n",
      "As with the separable case, we can re-cast the SVM for nonseparable distri-\n",
      "butions in terms of the minimization of a regularized error function. This will alsoallow us to highlight similarities, and differences, compared to the logistic regression\n",
      "model. Section 4.3.2\n",
      "We have seen that for data points that are on the correct side of the margin\n",
      "boundary, and which therefore satisfy y\n",
      "ntn⩾1,w eh a v e ξn=0, and for the7.1. Maximum Margin Classiﬁers 337\n",
      "Figure 7.5 Plot of the ‘hinge’ error function used\n",
      "in support vector machines, shown\n",
      "in blue, along with the error function\n",
      "for logistic regression, rescaled by a\n",
      "factor of 1/ln(2) so that it passes\n",
      "through the point (0,1), shown in red.\n",
      "Also shown are the misclassiﬁcation\n",
      "error in black and the squared error\n",
      "in green.\n",
      "−2 −1 012zE(z)\n",
      "remaining points we have ξn=1−yntn. Thus the objective function (7.21) can be\n",
      "written (up to an overall multiplicative constant) in the form\n",
      "N∑\n",
      "n=1ESV(yntn)+λ∥w∥2(7.44)\n",
      "where λ=( 2C)−1, andESV(·)is the hinge error function deﬁned by\n",
      "ESV(yntn)=[ 1 −yntn]+ (7.45)\n",
      "where [·]+denotes the positive part. The hinge error function, so-called because\n",
      "of its shape, is plotted in Figure 7.5. It can be viewed as an approximation to the\n",
      "misclassiﬁcation error, i.e., the error function that ideally we would like to minimize,\n",
      "which is also shown in Figure 7.5.\n",
      "When we considered the logistic regression model in Section 4.3.2, we found it\n",
      "convenient to work with target variable t∈{0,1}. For comparison with the support\n",
      "vector machine, we ﬁrst reformulate maximum likelihood logistic regression using\n",
      "the target variable t∈{ −1,1}. To do this, we note that p(t=1|y)=σ(y)where\n",
      "y(x)is given by (7.1), and σ(y)is the logistic sigmoid function deﬁned by (4.59). It\n",
      "follows that p(t=−1|y)=1−σ(y)=σ(−y), where we have used the properties\n",
      "of the logistic sigmoid function, and so we can write\n",
      "p(t|y)=σ(yt). (7.46)\n",
      "From this we can construct an error function by taking the negative logarithm of the\n",
      "likelihood function that, with a quadratic regularizer, takes the form Exercise 7.6\n",
      "N∑\n",
      "n=1ELR(yntn)+λ∥w∥2. (7.47)\n",
      "where\n",
      "ELR(yt)=l n( 1+e x p ( −yt)). (7.48)338 7. SPARSE KERNEL MACHINES\n",
      "For comparison with other error functions, we can divide by ln(2) so that the error\n",
      "function passes through the point (0,1). This rescaled error function is also plotted\n",
      "in Figure 7.5 and we see that it has a similar form to the support vector error function.\n",
      "The key difference is that the ﬂat region in ESV(yt)leads to sparse solutions.\n",
      "Both the logistic error and the hinge loss can be viewed as continuous approx-\n",
      "imations to the misclassiﬁcation error. Another continuous error function that has\n",
      "sometimes been used to solve classiﬁcation problems is the squared error, which\n",
      "is again plotted in Figure 7.5. It has the property, however, of placing increasingemphasis on data points that are correctly classiﬁed but that are a long way from\n",
      "the decision boundary on the correct side. Such points will be strongly weighted at\n",
      "the expense of misclassiﬁed points, and so if the objective is to minimize the mis-\n",
      "classiﬁcation rate, then a monotonically decreasing error function would be a better\n",
      "choice.\n",
      "7.1.3 Multiclass SVMs\n",
      "The support vector machine is fundamentally a two-class classiﬁer. In practice,\n",
      "however, we often have to tackle problems involving K>2classes. Various meth-\n",
      "ods have therefore been proposed for combining multiple two-class SVMs in order\n",
      "to build a multiclass classiﬁer.\n",
      "One commonly used approach (Vapnik, 1998) is to construct Kseparate SVMs,\n",
      "in which the kthmodel yk(x)is trained using the data from class Ckas the positive\n",
      "examples and the data from the remaining K−1classes as the negative examples.\n",
      "This is known as the one-versus-the-rest approach. However, in Figure 4.2 we saw\n",
      "that using the decisions of the individual classiﬁers can lead to inconsistent results\n",
      "in which an input is assigned to multiple classes simultaneously. This problem issometimes addressed by making predictions for new inputs xusing\n",
      "y(x)=m a x\n",
      "kyk(x). (7.49)\n",
      "Unfortunately, this heuristic approach suffers from the problem that the different\n",
      "classiﬁers were trained on different tasks, and there is no guarantee that the real-\n",
      "valued quantities yk(x)for different classiﬁers will have appropriate scales.\n",
      "Another problem with the one-versus-the-rest approach is that the training sets\n",
      "are imbalanced. For instance, if we have ten classes each with equal numbers of\n",
      "training data points, then the individual classiﬁers are trained on data sets comprising90% negative examples and only 10% positive examples, and the symmetry of the\n",
      "original problem is lost. A variant of the one-versus-the-rest scheme was proposed\n",
      "by Lee et al. (2001) who modify the target values so that the positive class has target\n",
      "+1and the negative class has target −1/(K−1).\n",
      "Weston and Watkins (1999) deﬁne a single objective function for training all\n",
      "KSVMs simultaneously, based on maximizing the margin from each to remaining\n",
      "classes. However, this can result in much slower training because, instead of solving\n",
      "Kseparate optimization problems each over Ndata points with an overall cost of\n",
      "O(KN\n",
      "2), a single optimization problem of size (K−1)Nmust be solved giving an\n",
      "overall cost of O(K2N2).7.1. Maximum Margin Classiﬁers 339\n",
      "Another approach is to train K(K−1)/2different 2-class SVMs on all possible\n",
      "pairs of classes, and then to classify test points according to which class has the high-est number of ‘votes’, an approach that is sometimes called one-versus-one . Again,\n",
      "we saw in Figure 4.2 that this can lead to ambiguities in the resulting classiﬁcation.\n",
      "Also, for large Kthis approach requires signiﬁcantly more training time than the\n",
      "one-versus-the-rest approach. Similarly, to evaluate test points, signiﬁcantly more\n",
      "computation is required.\n",
      "The latter problem can be alleviated by organizing the pairwise classiﬁers into\n",
      "a directed acyclic graph (not to be confused with a probabilistic graphical model)\n",
      "leading to the DAGSVM (Platt et al. , 2000). For Kclasses, the DAGSVM has a total\n",
      "ofK(K−1)/2classiﬁers, and to classify a new test point only K−1pairwise\n",
      "classiﬁers need to be evaluated, with the particular classiﬁers used depending on\n",
      "which path through the graph is traversed.\n",
      "A different approach to multiclass classiﬁcation, based on error-correcting out-\n",
      "put codes, was developed by Dietterich and Bakiri (1995) and applied to support\n",
      "vector machines by Allwein et al. (2000). This can be viewed as a generalization of\n",
      "the voting scheme of the one-versus-one approach in which more general partitions\n",
      "of the classes are used to train the individual classiﬁers. The Kclasses themselves\n",
      "are represented as particular sets of responses from the two-class classiﬁers chosen,and together with a suitable decoding scheme, this gives robustness to errors and to\n",
      "ambiguity in the outputs of the individual classiﬁers. Although the application of\n",
      "SVMs to multiclass classiﬁcation problems remains an open issue, in practice theone-versus-the-rest approach is the most widely used in spite of its ad-hoc formula-\n",
      "tion and its practical limitations.\n",
      "There are also single-class support vector machines, which solve an unsuper-\n",
      "vised learning problem related to probability density estimation. Instead of mod-\n",
      "elling the density of data, however, these methods aim to ﬁnd a smooth boundary\n",
      "enclosing a region of high density. The boundary is chosen to represent a quantile of\n",
      "the density, that is, the probability that a data point drawn from the distribution will\n",
      "land inside that region is given by a ﬁxed number between 0 and 1 that is speciﬁed inadvance. This is a more restricted problem than estimating the full density but may\n",
      "be sufﬁcient in speciﬁc applications. Two approaches to this problem using support\n",
      "vector machines have been proposed. The algorithm of Sch ¨olkopf et al. (2001) tries\n",
      "to ﬁnd a hyperplane that separates all but a ﬁxed fraction νof the training data from\n",
      "the origin while at the same time maximizing the distance (margin) of the hyperplane\n",
      "from the origin, while Tax and Duin (1999) look for the smallest sphere in featurespace that contains all but a fraction νof the data points. For kernels k(x,x\n",
      "′)that\n",
      "are functions only of x−x′, the two algorithms are equivalent.\n",
      "7.1.4 SVMs for regression\n",
      "We now extend support vector machines to regression problems while at the\n",
      "same time preserving the property of sparseness. In simple linear regression, we Section 3.1.4340 7. SPARSE KERNEL MACHINES\n",
      "Figure 7.6 Plot of an ϵ-insensitive error function (in\n",
      "red) in which the error increases lin-\n",
      "early with distance beyond the insen-\n",
      "sitive region. Also shown for compar-\n",
      "ison is the quadratic error function (in\n",
      "green).\n",
      "0 zE(z)\n",
      "−ϵ ϵ\n",
      "minimize a regularized error function given by\n",
      "1\n",
      "2N∑\n",
      "n=1{yn−tn}2+λ\n",
      "2∥w∥2. (7.50)\n",
      "To obtain sparse solutions, the quadratic error function is replaced by an ϵ-insensitive\n",
      "error function (Vapnik, 1995), which gives zero error if the absolute difference be-\n",
      "tween the prediction y(x)and the target tis less than ϵwhere ϵ>0. A simple\n",
      "example of an ϵ-insensitive error function, having a linear cost associated with errors\n",
      "outside the insensitive region, is given by\n",
      "Eϵ(y(x)−t)={\n",
      "0, if|y(x)−t|<ϵ;\n",
      "|y(x)−t|−ϵ,otherwise(7.51)\n",
      "and is illustrated in Figure 7.6.\n",
      "We therefore minimize a regularized error function given by\n",
      "CN∑\n",
      "n=1Eϵ(y(xn)−tn)+1\n",
      "2∥w∥2(7.52)\n",
      "where y(x)is given by (7.1). By convention the (inverse) regularization parameter,\n",
      "denoted C, appears in front of the error term.\n",
      "As before, we can re-express the optimization problem by introducing slack\n",
      "variables. For each data point xn, we now need two slack variables ξn⩾0and\n",
      "ˆξn⩾0, where ξn>0corresponds to a point for which tn>y(xn)+ϵ, andˆξn>0\n",
      "corresponds to a point for which tn<y(xn)−ϵ, as illustrated in Figure 7.7.\n",
      "The condition for a target point to lie inside the ϵ-tube is that yn−ϵ⩽tn⩽\n",
      "yn+ϵ, where yn=y(xn). Introducing the slack variables allows points to lie outside\n",
      "the tube provided the slack variables are nonzero, and the corresponding conditions\n",
      "are\n",
      "tn⩽y(xn)+ϵ+ξn (7.53)\n",
      "tn⩾y(xn)−ϵ−ˆξn. (7.54)7.1. Maximum Margin Classiﬁers 341\n",
      "Figure 7.7 Illustration of SVM regression, showing\n",
      "the regression curve together with the ϵ-\n",
      "insensitive ‘tube’. Also shown are exam-\n",
      "ples of the slack variables ξand bξ. Points\n",
      "above the ϵ-tube have ξ>0and bξ=0,\n",
      "points below the ϵ-tube have ξ=0 and\n",
      "bξ>0, and points inside the ϵ-tube have\n",
      "ξ=bξ=0.yy+ϵ\n",
      "y−ϵy(x)\n",
      "xˆξ>0ξ>0\n",
      "The error function for support vector regression can then be written as\n",
      "CN∑\n",
      "n=1(ξn+ˆξn)+1\n",
      "2∥w∥2(7.55)\n",
      "which must be minimized subject to the constraints ξn⩾0andˆξn⩾0as well as\n",
      "(7.53) and (7.54). This can be achieved by introducing Lagrange multipliers an⩾0,\n",
      "ˆan⩾0,µn⩾0, andˆµn⩾0and optimizing the Lagrangian\n",
      "L=CN∑\n",
      "n=1(ξn+ˆξn)+1\n",
      "2∥w∥2−N∑\n",
      "n=1(µnξn+ˆµnˆξn)\n",
      "−N∑\n",
      "n=1an(ϵ+ξn+yn−tn)−N∑\n",
      "n=1ˆan(ϵ+ˆξn−yn+tn).(7.56)\n",
      "We now substitute for y(x)using (7.1) and then set the derivatives of the La-\n",
      "grangian with respect to w,b,ξn, andˆξnto zero, giving\n",
      "∂L\n",
      "∂w=0⇒w=N∑\n",
      "n=1(an−ˆan)φ(xn) (7.57)\n",
      "∂L\n",
      "∂b=0⇒N∑\n",
      "n=1(an−ˆan)=0 (7.58)\n",
      "∂L\n",
      "∂ξn=0⇒an+µn=C (7.59)\n",
      "∂L\n",
      "∂ˆξn=0⇒ˆan+ˆµn=C. (7.60)\n",
      "Using these results to eliminate the corresponding variables from the Lagrangian, we\n",
      "see that the dual problem involves maximizing Exercise 7.7342 7. SPARSE KERNEL MACHINES\n",
      "˜L(a,ˆa)= −1\n",
      "2N∑\n",
      "n=1N∑\n",
      "m=1(an−ˆan)(am−ˆam)k(xn,xm)\n",
      "−ϵN∑\n",
      "n=1(an+ˆan)+N∑\n",
      "n=1(an−ˆan)tn (7.61)\n",
      "with respect to {an}and{ˆan}, where we have introduced the kernel k(x,x′)=\n",
      "φ(x)Tφ(x′). Again, this is a constrained maximization, and to ﬁnd the constraints\n",
      "we note that an⩾0andˆan⩾0are both required because these are Lagrange\n",
      "multipliers. Also µn⩾0andˆµn⩾0together with (7.59) and (7.60), require\n",
      "an⩽Candˆan⩽C, and so again we have the box constraints\n",
      "0⩽an⩽C (7.62)\n",
      "0⩽ˆan⩽C (7.63)\n",
      "together with the condition (7.58).\n",
      "Substituting (7.57) into (7.1), we see that predictions for new inputs can be made\n",
      "using\n",
      "y(x)=N∑\n",
      "n=1(an−ˆan)k(x,xn)+b (7.64)\n",
      "which is again expressed in terms of the kernel function.\n",
      "The corresponding Karush-Kuhn-Tucker (KKT) conditions, which state that at\n",
      "the solution the product of the dual variables and the constraints must vanish, are\n",
      "given by\n",
      "an(ϵ+ξn+yn−tn)=0 (7.65)\n",
      "ˆan(ϵ+ˆξn−yn+tn)=0 (7.66)\n",
      "(C−an)ξn=0 (7.67)\n",
      "(C−ˆan)ˆξn=0. (7.68)\n",
      "From these we can obtain several useful results. First of all, we note that a coefﬁcient\n",
      "ancan only be nonzero if ϵ+ξn+yn−tn=0, which implies that the data point\n",
      "either lies on the upper boundary of the ϵ-tube ( ξn=0) or lies above the upper\n",
      "boundary ( ξn>0). Similarly, a nonzero value for ˆanimplies ϵ+ˆξn−yn+tn=0,\n",
      "and such points must lie either on or below the lower boundary of the ϵ-tube.\n",
      "Furthermore, the two constraints ϵ+ξn+yn−tn=0andϵ+ˆξn−yn+tn=0\n",
      "are incompatible, as is easily seen by adding them together and noting that ξnand\n",
      "ˆξnare nonnegative while ϵis strictly positive, and so for every data point xn, either\n",
      "anorˆan(or both) must be zero.\n",
      "The support vectors are those data points that contribute to predictions given by\n",
      "(7.64), in other words those for which either an̸=0orˆan̸=0. These are points that\n",
      "lie on the boundary of the ϵ-tube or outside the tube. All points within the tube have7.1. Maximum Margin Classiﬁers 343\n",
      "an=ˆan=0. We again have a sparse solution, and the only terms that have to be\n",
      "evaluated in the predictive model (7.64) are those that involve the support vectors.\n",
      "The parameter bcan be found by considering a data point for which 0<an<\n",
      "C, which from (7.67) must have ξn=0, and from (7.65) must therefore satisfy\n",
      "ϵ+yn−tn=0. Using (7.1) and solving for b, we obtain\n",
      "b=tn−ϵ−wTφ(xn)\n",
      "=tn−ϵ−N∑\n",
      "m=1(am−ˆam)k(xn,xm) (7.69)\n",
      "where we have used (7.57). We can obtain an analogous result by considering a point\n",
      "for which 0<ˆan<C. In practice, it is better to average over all such estimates of\n",
      "b.\n",
      "As with the classiﬁcation case, there is an alternative formulation of the SVM\n",
      "for regression in which the parameter governing complexity has a more intuitiveinterpretation (Sch ¨olkopf et al. , 2000). In particular, instead of ﬁxing the width ϵof\n",
      "the insensitive region, we ﬁx instead a parameter νthat bounds the fraction of points\n",
      "lying outside the tube. This involves maximizing\n",
      "˜L(a,ˆa)= −1\n",
      "2N∑\n",
      "n=1N∑\n",
      "m=1(an−ˆan)(am−ˆam)k(xn,xm)\n",
      "+N∑\n",
      "n=1(an−ˆan)tn (7.70)\n",
      "subject to the constraints\n",
      "0⩽an⩽C/N (7.71)\n",
      "0⩽ˆan⩽C/N (7.72)\n",
      "N∑\n",
      "n=1(an−ˆan)=0 (7.73)\n",
      "N∑\n",
      "n=1(an+ˆan)⩽νC. (7.74)\n",
      "It can be shown that there are at most νNdata points falling outside the insensitive\n",
      "tube, while at least νNdata points are support vectors and so lie either on the tube\n",
      "or outside it.\n",
      "The use of a support vector machine to solve a regression problem is illustrated\n",
      "using the sinusoidal data set in Figure 7.8. Here the parameters νandChave been Appendix A\n",
      "chosen by hand. In practice, their values would typically be determined by cross-validation.344 7. SPARSE KERNEL MACHINES\n",
      "Figure 7.8 Illustration of the ν-SVM for re-\n",
      "gression applied to the sinusoidal\n",
      "synthetic data set using Gaussian\n",
      "kernels. The predicted regression\n",
      "curve is shown by the red line, and\n",
      "theϵ-insensitive tube corresponds\n",
      "to the shaded region. Also, the\n",
      "data points are shown in green,\n",
      "and those with support vectors\n",
      "are indicated by blue circles.\n",
      "xt\n",
      "0 1−101\n",
      "7.1.5 Computational learning theory\n",
      "Historically, support vector machines have largely been motivated and analysed\n",
      "using a theoretical framework known as computational learning theory , also some-\n",
      "times called statistical learning theory (Anthony and Biggs, 1992; Kearns and Vazi-\n",
      "rani, 1994; Vapnik, 1995; Vapnik, 1998). This has its origins with Valiant (1984)\n",
      "who formulated the probably approximately correct , or PAC, learning framework.\n",
      "The goal of the PAC framework is to understand how large a data set needs to be in\n",
      "order to give good generalization. It also gives bounds for the computational cost of\n",
      "learning, although we do not consider these here.\n",
      "Suppose that a data set Dof size Nis drawn from some joint distribution p(x,t)\n",
      "wherexis the input variable and trepresents the class label, and that we restrict\n",
      "attention to ‘noise free’ situations in which the class labels are determined by some\n",
      "(unknown) deterministic function t=g(x). In PAC learning we say that a function\n",
      "f(x;D), drawn from a space Fof such functions on the basis of the training set\n",
      "D, has good generalization if its expected error rate is below some pre-speciﬁed\n",
      "threshold ϵ, so that\n",
      "Ex,t[I(f(x;D)̸=t)]<ϵ (7.75)\n",
      "where I(·)is the indicator function, and the expectation is with respect to the dis-\n",
      "tribution p(x,t). The quantity on the left-hand side is a random variable, because\n",
      "it depends on the training set D, and the PAC framework requires that (7.75) holds,\n",
      "with probability greater than 1−δ, for a data set Ddrawn randomly from p(x,t).\n",
      "Hereδis another pre-speciﬁed parameter, and the terminology ‘probably approxi-\n",
      "mately correct’ comes from the requirement that with high probability (greater than\n",
      "1−δ), the error rate be small (less than ϵ). For a given choice of model space F, and\n",
      "for given parameters ϵandδ, PAC learning aims to provide bounds on the minimum\n",
      "sizeNof data set needed to meet this criterion. A key quantity in PAC learning is\n",
      "theV apnik-Chervonenkis dimension , or VC dimension, which provides a measure of\n",
      "the complexity of a space of functions, and which allows the PAC framework to be\n",
      "extended to spaces containing an inﬁnite number of functions.\n",
      "The bounds derived within the PAC framework are often described as worst-7.2. Relevance Vector Machines 345\n",
      "case, because they apply to anychoice for the distribution p(x,t), so long as both\n",
      "the training and the test examples are drawn (independently) from the same distribu-tion, and for anychoice for the function f(x)so long as it belongs to F. In real-world\n",
      "applications of machine learning, we deal with distributions that have signiﬁcant reg-\n",
      "ularity, for example in which large regions of input space carry the same class label.As a consequence of the lack of any assumptions about the form of the distribution,\n",
      "the PAC bounds are very conservative, in other words they strongly over-estimate\n",
      "the size of data sets required to achieve a given generalization performance. For thisreason, PAC bounds have found few, if any, practical applications.\n",
      "One attempt to improve the tightness of the PAC bounds is the PAC-Bayesian\n",
      "framework (McAllester, 2003), which considers a distribution over the space Fof\n",
      "functions, somewhat analogous to the prior in a Bayesian treatment. This still con-\n",
      "siders any possible choice for p(x,t), and so although the bounds are tighter, they\n",
      "are still very conservative.\n",
      "7.2. Relevance Vector Machines\n",
      "Support vector machines have been used in a variety of classiﬁcation and regres-sion applications. Nevertheless, they suffer from a number of limitations, severalof which have been highlighted already in this chapter. In particular, the outputs of\n",
      "an SVM represent decisions rather than posterior probabilities. Also, the SVM was\n",
      "originally formulated for two classes, and the extension to K>2classes is prob-\n",
      "lematic. There is a complexity parameter C,o rν(as well as a parameter ϵin the case\n",
      "of regression), that must be found using a hold-out method such as cross-validation.\n",
      "Finally, predictions are expressed as linear combinations of kernel functions that are\n",
      "centred on training data points and that are required to be positive deﬁnite.\n",
      "The relevance vector machine or RVM (Tipping, 2001) is a Bayesian sparse ker-\n",
      "nel technique for regression and classiﬁcation that shares many of the characteristics\n",
      "of the SVM whilst avoiding its principal limitations. Additionally, it typically leads\n",
      "to much sparser models resulting in correspondingly faster performance on test datawhilst maintaining comparable generalization error.\n",
      "In contrast to the SVM we shall ﬁnd it more convenient to introduce the regres-\n",
      "sion form of the RVM ﬁrst and then consider the extension to classiﬁcation tasks.\n",
      "7.2.1 RVM for regression\n",
      "The relevance vector machine for regression is a linear model of the form studied\n",
      "in Chapter 3 but with a modiﬁed prior that results in sparse solutions. The model\n",
      "deﬁnes a conditional distribution for a real-valued target variable t, given an input\n",
      "vectorx, which takes the form\n",
      "p(t|x,w,β)=N(t|y(x),β−1) (7.76)346 7. SPARSE KERNEL MACHINES\n",
      "where β=σ−2is the noise precision (inverse noise variance), and the mean is given\n",
      "by a linear model of the form\n",
      "y(x)=M∑\n",
      "i=1wiφi(x)=wTφ(x) (7.77)\n",
      "with ﬁxed nonlinear basis functions φi(x), which will typically include a constant\n",
      "term so that the corresponding weight parameter represents a ‘bias’.\n",
      "The relevance vector machine is a speciﬁc instance of this model, which is in-\n",
      "tended to mirror the structure of the support vector machine. In particular, the basis\n",
      "functions are given by kernels, with one kernel associated with each of the data\n",
      "points from the training set. The general expression (7.77) then takes the SVM-likeform\n",
      "y(x)=N∑\n",
      "n=1wnk(x,xn)+b (7.78)\n",
      "where bis a bias parameter. The number of parameters in this case is M=N+1,\n",
      "andy(x)has the same form as the predictive model (7.64) for the SVM, except that\n",
      "the coefﬁcients anare here denoted wn. It should be emphasized that the subsequent\n",
      "analysis is valid for arbitrary choices of basis function, and for generality we shallwork with the form (7.77). In contrast to the SVM, there is no restriction to positive-\n",
      "deﬁnite kernels, nor are the basis functions tied in either number or location to the\n",
      "training data points.\n",
      "Suppose we are given a set of Nobservations of the input vector x, which we\n",
      "denote collectively by a data matrix Xwhose n\n",
      "throw isxT\n",
      "nwithn=1,...,N . The\n",
      "corresponding target values are given by t=(t1,...,t N)T. Thus, the likelihood\n",
      "function is given by\n",
      "p(t|X,w,β)=N∏\n",
      "n=1p(tn|xn,w,β−1). (7.79)\n",
      "Next we introduce a prior distribution over the parameter vector wand as in\n",
      "Chapter 3, we shall consider a zero-mean Gaussian prior. However, the key differ-ence in the RVM is that we introduce a separate hyperparameter α\n",
      "ifor each of the\n",
      "weight parameters wiinstead of a single shared hyperparameter. Thus the weight\n",
      "prior takes the form\n",
      "p(w|α)=M∏\n",
      "i=1N(wi|0,α−1\n",
      "i) (7.80)\n",
      "where αirepresents the precision of the corresponding parameter wi, andαdenotes\n",
      "(α1,...,α M)T. We shall see that, when we maximize the evidence with respect\n",
      "to these hyperparameters, a signiﬁcant proportion of them go to inﬁnity, and the\n",
      "corresponding weight parameters have posterior distributions that are concentrated\n",
      "at zero. The basis functions associated with these parameters therefore play no role7.2. Relevance Vector Machines 347\n",
      "in the predictions made by the model and so are effectively pruned out, resulting in\n",
      "a sparse model.\n",
      "Using the result (3.49) for linear regression models, we see that the posterior\n",
      "distribution for the weights is again Gaussian and takes the form\n",
      "p(w|t,X,α,β)=N(w|m,Σ) (7.81)\n",
      "where the mean and covariance are given by\n",
      "m=βΣΦTt (7.82)\n",
      "Σ=(\n",
      "A+βΦTΦ)−1(7.83)\n",
      "whereΦis the N×Mdesign matrix with elements Φni=φi(xn), andA=\n",
      "diag(αi). Note that in the speciﬁc case of the model (7.78), we have Φ=K, where\n",
      "Kis the symmetric (N+1 )×(N+1 ) kernel matrix with elements k(xn,xm).\n",
      "The values of αandβare determined using type-2 maximum likelihood, also\n",
      "known as the evidence approximation , in which we maximize the marginal likeli- Section 3.5\n",
      "hood function obtained by integrating out the weight parameters\n",
      "p(t|X,α,β)=∫\n",
      "p(t|X,w,β)p(w|α)dw. (7.84)\n",
      "Because this represents the convolution of two Gaussians, it is readily evaluated to Exercise 7.10\n",
      "give the log marginal likelihood in the form\n",
      "lnp(t|X,α,β)=l n N(t|0,C)\n",
      "=−1\n",
      "2{\n",
      "Nln(2π)+l n|C|+tTC−1t}\n",
      "(7.85)\n",
      "where t=(t1,...,t N)T, and we have deﬁned the N×NmatrixCgiven by\n",
      "C=β−1I+ΦA−1ΦT. (7.86)\n",
      "Our goal is now to maximize (7.85) with respect to the hyperparameters αand\n",
      "β. This requires only a small modiﬁcation to the results obtained in Section 3.5 for\n",
      "the evidence approximation in the linear regression model. Again, we can identify\n",
      "two approaches. In the ﬁrst, we simply set the required derivatives of the marginal\n",
      "likelihood to zero and obtain the following re-estimation equations Exercise 7.12\n",
      "αnew\n",
      "i=γi\n",
      "m2\n",
      "i(7.87)\n",
      "(βnew)−1=∥t−Φm∥2\n",
      "N−∑\n",
      "iγi(7.88)\n",
      "where miis the ithcomponent of the posterior mean mdeﬁned by (7.82). The\n",
      "quantity γimeasures how well the corresponding parameter wiis determined by the\n",
      "data and is deﬁned by Section 3.5.3348 7. SPARSE KERNEL MACHINES\n",
      "γi=1−αiΣii (7.89)\n",
      "in which Σiiis theithdiagonal component of the posterior covariance Σgiven by\n",
      "(7.83). Learning therefore proceeds by choosing initial values for αandβ, evalu-\n",
      "ating the mean and covariance of the posterior using (7.82) and (7.83), respectively,and then alternately re-estimating the hyperparameters, using (7.87) and (7.88), and\n",
      "re-estimating the posterior mean and covariance, using (7.82) and (7.83), until a suit-\n",
      "able convergence criterion is satisﬁed.\n",
      "The second approach is to use the EM algorithm, and is discussed in Sec-\n",
      "tion 9.3.4. These two approaches to ﬁnding the values of the hyperparameters that\n",
      "maximize the evidence are formally equivalent. Numerically, however, it is found Exercise 9.23\n",
      "that the direct optimization approach corresponding to (7.87) and (7.88) gives some-\n",
      "what faster convergence (Tipping, 2001).\n",
      "As a result of the optimization, we ﬁnd that a proportion of the hyperparameters\n",
      "{α\n",
      "i}are driven to large (in principle inﬁnite) values, and so the weight parameters Section 7.2.2\n",
      "wicorresponding to these hyperparameters have posterior distributions with mean\n",
      "and variance both zero. Thus those parameters, and the corresponding basis func-tionsφ\n",
      "i(x), are removed from the model and play no role in making predictions for\n",
      "new inputs. In the case of models of the form (7.78), the inputs xncorresponding to\n",
      "the remaining nonzero weights are called relevance vectors , because they are iden-\n",
      "tiﬁed through the mechanism of automatic relevance determination, and are analo-\n",
      "gous to the support vectors of an SVM. It is worth emphasizing, however, that thismechanism for achieving sparsity in probabilistic models through automatic rele-\n",
      "vance determination is quite general and can be applied to any model expressed as\n",
      "an adaptive linear combination of basis functions.\n",
      "Having found values α\n",
      "⋆andβ⋆for the hyperparameters that maximize the\n",
      "marginal likelihood, we can evaluate the predictive distribution over tfor a new\n",
      "inputx. Using (7.76) and (7.81), this is given by Exercise 7.14\n",
      "p(t|x,X,t,α⋆,β⋆)=∫\n",
      "p(t|x,w,β⋆)p(w|X,t,α⋆,β⋆)dw\n",
      "=N(\n",
      "t|mTφ(x),σ2(x))\n",
      ". (7.90)\n",
      "Thus the predictive mean is given by (7.76) with wset equal to the posterior mean\n",
      "m, and the variance of the predictive distribution is given by\n",
      "σ2(x)=(β⋆)−1+φ(x)TΣφ(x) (7.91)\n",
      "whereΣis given by (7.83) in which αandβare set to their optimized values α⋆and\n",
      "β⋆. This is just the familiar result (3.59) obtained in the context of linear regression.\n",
      "Recall that for localized basis functions, the predictive variance for linear regression\n",
      "models becomes small in regions of input space where there are no basis functions.In the case of an RVM with the basis functions centred on data points, the model will\n",
      "therefore become increasingly certain of its predictions when extrapolating outside\n",
      "the domain of the data (Rasmussen and Qui ˜nonero-Candela, 2005), which of course\n",
      "is undesirable. The predictive distribution in Gaussian process regression does not Section 6.4.27.2. Relevance Vector Machines 349\n",
      "Figure 7.9 Illustration of RVM regression us-\n",
      "ing the same data set, and the\n",
      "same Gaussian kernel functions,\n",
      "as used in Figure 7.8 for the\n",
      "ν-SVM regression model. The\n",
      "mean of the predictive distribu-\n",
      "tion for the RVM is shown by the\n",
      "red line, and the one standard-\n",
      "deviation predictive distribution is\n",
      "shown by the shaded region.\n",
      "Also, the data points are shown\n",
      "in green, and the relevance vec-\n",
      "tors are indicated by blue circles.\n",
      "Note that there are only 3 rele-\n",
      "vance vectors compared to 7 sup-\n",
      "port vectors for the ν-SVM in Fig-\n",
      "ure 7.8.xt\n",
      "0 1−101\n",
      "suffer from this problem. However, the computational cost of making predictions\n",
      "with a Gaussian processes is typically much higher than with an RVM.\n",
      "Figure 7.9 shows an example of the RVM applied to the sinusoidal regression\n",
      "data set. Here the noise precision parameter βis also determined through evidence\n",
      "maximization. We see that the number of relevance vectors in the RVM is signif-\n",
      "icantly smaller than the number of support vectors used by the SVM. For a wide\n",
      "range of regression and classiﬁcation tasks, the RVM is found to give models that\n",
      "are typically an order of magnitude more compact than the corresponding support\n",
      "vector machine, resulting in a signiﬁcant improvement in the speed of processing on\n",
      "test data. Remarkably, this greater sparsity is achieved with little or no reduction in\n",
      "generalization error compared with the corresponding SVM.\n",
      "The principal disadvantage of the RVM compared to the SVM is that training\n",
      "involves optimizing a nonconvex function, and training times can be longer than for a\n",
      "comparable SVM. For a model with Mbasis functions, the RVM requires inversion\n",
      "of a matrix of size M×M, which in general requires O(M3)computation. In the\n",
      "speciﬁc case of the SVM-like model (7.78), we have M=N+1. As we have noted,\n",
      "there are techniques for training SVMs whose cost is roughly quadratic in N.O f\n",
      "course, in the case of the RVM we always have the option of starting with a smaller\n",
      "number of basis functions than N+1. More signiﬁcantly, in the relevance vector\n",
      "machine the parameters governing complexity and noise variance are determined\n",
      "automatically from a single training run, whereas in the support vector machine the\n",
      "parameters Candϵ(orν) are generally found using cross-validation, which involves\n",
      "multiple training runs. Furthermore, in the next section we shall derive an alternative\n",
      "procedure for training the relevance vector machine that improves training speed\n",
      "signiﬁcantly.\n",
      "7.2.2 Analysis of sparsity\n",
      "We have noted earlier that the mechanism of automatic relevance determination\n",
      "causes a subset of parameters to be driven to zero. We now examine in more detail350 7. SPARSE KERNEL MACHINES\n",
      "t1t2\n",
      "t\n",
      "C\n",
      "t1t2\n",
      "t\n",
      "Cϕ\n",
      "Figure 7.10 Illustration of the mechanism for sparsity in a Bayesian linear regression model, showing a training\n",
      "set vector of target values given by t=(t1,t2)T, indicated by the cross, for a model with one basis vector\n",
      "ϕ=(φ(x1),φ(x2))T, which is poorly aligned with the target data vector t. On the left we see a model having\n",
      "only isotropic noise, so that C=β−1I, corresponding to α=∞, with βset to its most probable value. On\n",
      "the right we see the same model but with a ﬁnite value of α. In each case the red ellipse corresponds to unit\n",
      "Mahalanobis distance, with |C|taking the same value for both plots, while the dashed green circle shows the\n",
      "contrition arising from the noise term β−1. We see that any ﬁnite value of αreduces the probability of the\n",
      "observed data, and so for the most probable solution the basis vector is removed.\n",
      "the mechanism of sparsity in the context of the relevance vector machine. In the\n",
      "process, we will arrive at a signiﬁcantly faster procedure for optimizing the hyper-\n",
      "parameters compared to the direct techniques given above.\n",
      "Before proceeding with a mathematical analysis, we ﬁrst give some informal\n",
      "insight into the origin of sparsity in Bayesian linear models. Consider a data set\n",
      "comprising N=2 observations t1andt2, together with a model having a single\n",
      "basis function φ(x), with hyperparameter α, along with isotropic noise having pre-\n",
      "cision β. From (7.85), the marginal likelihood is given by p(t|α,β)=N(t|0,C)in\n",
      "which the covariance matrix takes the form\n",
      "C=1\n",
      "βI+1\n",
      "αϕϕT(7.92)\n",
      "where ϕdenotes the N-dimensional vector (φ(x1),φ(x2))T, and similarly t=\n",
      "(t1,t2)T. Notice that this is just a zero-mean Gaussian process model over twith\n",
      "covariance C. Given a particular observation for t, our goal is to ﬁnd α⋆andβ⋆by\n",
      "maximizing the marginal likelihood. We see from Figure 7.10 that, if there is a poor\n",
      "alignment between the direction of ϕand that of the training data vector t, then the\n",
      "corresponding hyperparameter αwill be driven to ∞, and the basis vector will be\n",
      "pruned from the model. This arises because any ﬁnite value for αwill always assign\n",
      "a lower probability to the data, thereby decreasing the value of the density at t, pro-\n",
      "vided that βis set to its optimal value. We see that any ﬁnite value for αwould cause\n",
      "the distribution to be elongated in a direction away from the data, thereby increasing\n",
      "the probability mass in regions away from the observed data and hence reducing the\n",
      "value of the density at the target data vector itself. For the more general case of M7.2. Relevance Vector Machines 351\n",
      "basis vectors ϕ1,...,ϕMa similar intuition holds, namely that if a particular basis\n",
      "vector is poorly aligned with the data vector t, then it is likely to be pruned from the\n",
      "model.\n",
      "We now investigate the mechanism for sparsity from a more mathematical per-\n",
      "spective, for a general case involving Mbasis functions. To motivate this analysis\n",
      "we ﬁrst note that, in the result (7.87) for re-estimating the parameter αi, the terms on\n",
      "the right-hand side are themselves also functions of αi. These results therefore rep-\n",
      "resent implicit solutions, and iteration would be required even to determine a singleα\n",
      "iwith all other αjforj̸=iﬁxed.\n",
      "This suggests a different approach to solving the optimization problem for the\n",
      "RVM, in which we make explicit all of the dependence of the marginal likelihood\n",
      "(7.85) on a particular αiand then determine its stationary points explicitly (Faul and\n",
      "Tipping, 2002; Tipping and Faul, 2003). To do this, we ﬁrst pull out the contributionfromα\n",
      "iin the matrix Cdeﬁned by (7.86) to give\n",
      "C=β−1I+∑\n",
      "j̸=iα−1\n",
      "jϕjϕT\n",
      "j+α−1\n",
      "iϕiϕT\n",
      "i\n",
      "=C−i+α−1\n",
      "iϕiϕT\n",
      "i (7.93)\n",
      "where ϕidenotes the ithcolumn of Φ, in other words the N-dimensional vector with\n",
      "elements (φi(x1),...,φ i(xN)), in contrast to φn, which denotes the nthrow of Φ.\n",
      "The matrix C−irepresents the matrix Cwith the contribution from basis function i\n",
      "removed. Using the matrix identities (C.7) and (C.15), the determinant and inverse\n",
      "ofCcan then be written\n",
      "|C|=|C−i||1+α−1\n",
      "iϕT\n",
      "iC−1\n",
      "−iϕi| (7.94)\n",
      "C−1=C−1\n",
      "−i−C−1\n",
      "−iϕiϕT\n",
      "iC−1\n",
      "−i\n",
      "αi+ϕT\n",
      "iC−1\n",
      "−iϕi. (7.95)\n",
      "Using these results, we can then write the log marginal likelihood function (7.85) in\n",
      "the form Exercise 7.15\n",
      "L(α)=L(α−i)+λ(αi) (7.96)\n",
      "where L(α−i)is simply the log marginal likelihood with basis function ϕiomitted,\n",
      "and the quantity λ(αi)is deﬁned by\n",
      "λ(αi)=1\n",
      "2[\n",
      "lnαi−ln(αi+si)+q2\n",
      "i\n",
      "αi+si]\n",
      "(7.97)\n",
      "and contains all of the dependence on αi. Here we have introduced the two quantities\n",
      "si=ϕT\n",
      "iC−1\n",
      "−iϕi (7.98)\n",
      "qi=ϕT\n",
      "iC−1\n",
      "−it. (7.99)\n",
      "Heresiis called the sparsity andqiis known as the quality ofϕi, and as we shall\n",
      "see, a large value of sirelative to the value of qimeans that the basis function ϕi352 7. SPARSE KERNEL MACHINES\n",
      "Figure 7.11 Plots of the log\n",
      "marginal likelihood λ(αi)versus\n",
      "lnαishowing on the left, the single\n",
      "maximum at a ﬁnite αiforq2\n",
      "i=4\n",
      "andsi=1(so that q2\n",
      "i>si) and on\n",
      "the right, the maximum at αi=∞\n",
      "forq2\n",
      "i=1 andsi=2 (so that\n",
      "q2\n",
      "i<si).\n",
      "−5 0 5−4−202\n",
      "−5 0 5−4−202\n",
      "is more likely to be pruned from the model. The ‘sparsity’ measures the extent to\n",
      "which basis function ϕioverlaps with the other basis vectors in the model, and the\n",
      "‘quality’ represents a measure of the alignment of the basis vector ϕnwith the error\n",
      "between the training set values t=(t1,...,t N)Tand the vector y−iof predictions\n",
      "that would result from the model with the vector ϕiexcluded (Tipping and Faul,\n",
      "2003).\n",
      "The stationary points of the marginal likelihood with respect to αioccur when\n",
      "the derivative\n",
      "dλ(αi)\n",
      "dαi=α−1\n",
      "is2\n",
      "i−(q2\n",
      "i−si)\n",
      "2(αi+si)2(7.100)\n",
      "is equal to zero. There are two possible forms for the solution. Recalling that αi⩾0,\n",
      "we see that if q2\n",
      "i<si, thenαi→∞ provides a solution. Conversely, if q2\n",
      "i>si,w e\n",
      "can solve for αito obtain\n",
      "αi=s2\n",
      "i\n",
      "q2\n",
      "i−si. (7.101)\n",
      "These two solutions are illustrated in Figure 7.11. We see that the relative size of\n",
      "the quality and sparsity terms determines whether a particular basis vector will be\n",
      "pruned from the model or not. A more complete analysis (Faul and Tipping, 2002),\n",
      "based on the second derivatives of the marginal likelihood, conﬁrms these solutions\n",
      "are indeed the unique maxima of λ(αi). Exercise 7.16\n",
      "Note that this approach has yielded a closed-form solution for αi, for given\n",
      "values of the other hyperparameters. As well as providing insight into the origin of\n",
      "sparsity in the RVM, this analysis also leads to a practical algorithm for optimizing\n",
      "the hyperparameters that has signiﬁcant speed advantages. This uses a ﬁxed set\n",
      "of candidate basis vectors, and then cycles through them in turn to decide whether\n",
      "each vector should be included in the model or not. The resulting sequential sparse\n",
      "Bayesian learning algorithm is described below.\n",
      "Sequential Sparse Bayesian Learning Algorithm\n",
      "1. If solving a regression problem, initialize β.\n",
      "2. Initialize using one basis function ϕ1, with hyperparameter α1set using\n",
      "(7.101), with the remaining hyperparameters αjforj̸=iinitialized to\n",
      "inﬁnity, so that only ϕ1is included in the model.7.2. Relevance Vector Machines 353\n",
      "3. Evaluate Σandm, along with qiandsifor all basis functions.\n",
      "4. Select a candidate basis function ϕi.\n",
      "5. Ifq2\n",
      "i>si, andαi<∞, so that the basis vector ϕiis already included in\n",
      "the model, then update αiusing (7.101).\n",
      "6. Ifq2\n",
      "i>si, andαi=∞, then add ϕito the model, and evaluate hyperpa-\n",
      "rameter αiusing (7.101).\n",
      "7. Ifq2\n",
      "i⩽si, andαi<∞then remove basis function ϕifrom the model,\n",
      "and set αi=∞.\n",
      "8. If solving a regression problem, update β.\n",
      "9. If converged terminate, otherwise go to 3.\n",
      "Note that if q2\n",
      "i⩽siandαi=∞, then the basis function ϕiis already excluded\n",
      "from the model and no action is required.\n",
      "In practice, it is convenient to evaluate the quantities\n",
      "Qi=ϕT\n",
      "iC−1t (7.102)\n",
      "Si=ϕT\n",
      "iC−1ϕi. (7.103)\n",
      "The quality and sparseness variables can then be expressed in the form\n",
      "qi=αiQi\n",
      "αi−Si(7.104)\n",
      "si=αiSi\n",
      "αi−Si. (7.105)\n",
      "Note that when αi=∞,w eh a v e qi=Qiandsi=Si. Using (C.7), we can write Exercise 7.17\n",
      "Qi=βϕT\n",
      "it−β2ϕT\n",
      "iΦΣΦTt (7.106)\n",
      "Si=βϕT\n",
      "iϕi−β2ϕT\n",
      "iΦΣΦTϕi (7.107)\n",
      "whereΦandΣinvolve only those basis vectors that correspond to ﬁnite hyperpa-\n",
      "rameters αi. At each stage the required computations therefore scale like O(M3),\n",
      "where Mis the number of active basis vectors in the model and is typically much\n",
      "smaller than the number Nof training patterns.\n",
      "7.2.3 RVM for classiﬁcation\n",
      "We can extend the relevance vector machine framework to classiﬁcation prob-\n",
      "lems by applying the ARD prior over weights to a probabilistic linear classiﬁcation\n",
      "model of the kind studied in Chapter 4. To start with, we consider two-class prob-\n",
      "lems with a binary target variable t∈{0,1}. The model now takes the form of a\n",
      "linear combination of basis functions transformed by a logistic sigmoid function\n",
      "y(x,w)=σ(\n",
      "wTφ(x))\n",
      "(7.108)354 7. SPARSE KERNEL MACHINES\n",
      "where σ(·)is the logistic sigmoid function deﬁned by (4.59). If we introduce a\n",
      "Gaussian prior over the weight vector w, then we obtain the model that has been\n",
      "considered already in Chapter 4. The difference here is that in the RVM, this model\n",
      "uses the ARD prior (7.80) in which there is a separate precision hyperparameter\n",
      "associated with each weight parameter.\n",
      "In contrast to the regression model, we can no longer integrate analytically over\n",
      "the parameter vector w. Here we follow Tipping (2001) and use the Laplace ap-\n",
      "proximation, which was applied to the closely related problem of Bayesian logistic Section 4.4\n",
      "regression in Section 4.5.1.\n",
      "We begin by initializing the hyperparameter vector α. For this given value of\n",
      "α, we then build a Gaussian approximation to the posterior distribution and thereby\n",
      "obtain an approximation to the marginal likelihood. Maximization of this approxi-\n",
      "mate marginal likelihood then leads to a re-estimated value for α, and the process is\n",
      "repeated until convergence.\n",
      "Let us consider the Laplace approximation for this model in more detail. For\n",
      "a ﬁxed value of α, the mode of the posterior distribution over wis obtained by\n",
      "maximizing\n",
      "lnp(w|t,α)=l n {p(t|w)p(w|α)}−lnp(t|α)\n",
      "=N∑\n",
      "n=1{tnlnyn+( 1−tn)l n ( 1−yn)}−1\n",
      "2wTAw+c o n s t (7.109)\n",
      "whereA=d i a g ( αi). This can be done using iterative reweighted least squares\n",
      "(IRLS) as discussed in Section 4.3.3. For this, we need the gradient vector and\n",
      "Hessian matrix of the log posterior distribution, which from (7.109) are given by Exercise 7.18\n",
      "∇lnp(w|t,α)= ΦT(t−y)−Aw (7.110)\n",
      "∇∇lnp(w|t,α)= −(\n",
      "ΦTBΦ+A)\n",
      "(7.111)\n",
      "whereBis anN×Ndiagonal matrix with elements bn=yn(1−yn), the vector\n",
      "y=(y1,...,y N)T, andΦis the design matrix with elements Φni=φi(xn). Here\n",
      "we have used the property (4.88) for the derivative of the logistic sigmoid function.At convergence of the IRLS algorithm, the negative Hessian represents the inverse\n",
      "covariance matrix for the Gaussian approximation to the posterior distribution.\n",
      "The mode of the resulting approximation to the posterior distribution, corre-\n",
      "sponding to the mean of the Gaussian approximation, is obtained setting (7.110) to\n",
      "zero, giving the mean and covariance of the Laplace approximation in the form\n",
      "w\n",
      "⋆=A−1ΦT(t−y) (7.112)\n",
      "Σ=(\n",
      "ΦTBΦ+A)−1. (7.113)\n",
      "We can now use this Laplace approximation to evaluate the marginal likelihood.\n",
      "Using the general result (4.135) for an integral evaluated using the Laplace approxi-7.2. Relevance Vector Machines 355\n",
      "mation, we have\n",
      "p(t|α)=∫\n",
      "p(t|w)p(w|α)dw\n",
      "≃p(t|w⋆)p(w⋆|α)(2π)M/2|Σ|1/2. (7.114)\n",
      "If we substitute for p(t|w⋆)andp(w⋆|α)and then set the derivative of the marginal\n",
      "likelihood with respect to αiequal to zero, we obtain Exercise 7.19\n",
      "−1\n",
      "2(w⋆\n",
      "i)2+1\n",
      "2αi−1\n",
      "2Σii=0. (7.115)\n",
      "Deﬁning γi=1−αiΣiiand rearranging then gives\n",
      "αnew\n",
      "i=γi\n",
      "(w⋆\n",
      "i)2(7.116)\n",
      "which is identical to the re-estimation formula (7.87) obtained for the regression\n",
      "RVM.\n",
      "If we deﬁne\n",
      "ˆt=Φw⋆+B−1(t−y) (7.117)\n",
      "we can write the approximate log marginal likelihood in the form\n",
      "lnp(t|α,β)=−1\n",
      "2{\n",
      "Nln(2π)+l n|C|+(ˆt)TC−1ˆt}\n",
      "(7.118)\n",
      "where\n",
      "C=B+ΦAΦT. (7.119)\n",
      "This takes the same form as (7.85) in the regression case, and so we can apply the\n",
      "same analysis of sparsity and obtain the same fast learning algorithm in which we\n",
      "fully optimize a single hyperparameter αiat each step.\n",
      "Figure 7.12 shows the relevance vector machine applied to a synthetic classiﬁ-\n",
      "cation data set. We see that the relevance vectors tend not to lie in the region of the Appendix A\n",
      "decision boundary, in contrast to the support vector machine. This is consistent with\n",
      "our earlier discussion of sparsity in the RVM, because a basis function φi(x)centred\n",
      "on a data point near the boundary will have a vector ϕithat is poorly aligned with\n",
      "the training data vector t.\n",
      "One of the potential advantages of the relevance vector machine compared with\n",
      "the SVM is that it makes probabilistic predictions. For example, this allows the RVM\n",
      "to be used to help construct an emission density in a nonlinear extension of the linear\n",
      "dynamical system for tracking faces in video sequences (Williams et al. , 2005). Section 13.3\n",
      "So far, we have considered the RVM for binary classiﬁcation problems. For\n",
      "K>2classes, we again make use of the probabilistic approach in Section 4.3.4 in\n",
      "which there are Klinear models of the form\n",
      "ak=wT\n",
      "kx (7.120)356 7. SPARSE KERNEL MACHINES\n",
      "−2 0 2−202\n",
      "Figure 7.12 Example of the relevance vector machine applied to a synthetic data set, in which the left-hand plot\n",
      "shows the decision boundary and the data points, with the relevance vectors indicated by circles. Comparison\n",
      "with the results shown in Figure 7.4 for the corresponding support vector machine shows that the RVM gives a\n",
      "much sparser model. The right-hand plot shows the posterior probability given by the RVM output in which the\n",
      "proportion of red (blue) ink indicates the probability of that point belonging to the red (blue) class.\n",
      "which are combined using a softmax function to give outputs\n",
      "yk(x)=exp(ak)∑\n",
      "jexp(aj). (7.121)\n",
      "The log likelihood function is then given by\n",
      "lnp(T|w1,...,wK)=N∏\n",
      "n=1K∏\n",
      "k=1ytnk\n",
      "nk(7.122)\n",
      "where the target values tnkhave a 1-of- Kcoding for each data point n, andTis a\n",
      "matrix with elements tnk. Again, the Laplace approximation can be used to optimize\n",
      "the hyperparameters (Tipping, 2001), in which the model and its Hessian are found\n",
      "using IRLS. This gives a more principled approach to multiclass classiﬁcation than\n",
      "the pairwise method used in the support vector machine and also provides probabilis-\n",
      "tic predictions for new data points. The principal disadvantage is that the Hessian\n",
      "matrix has size MK×MK , where Mis the number of active basis functions, which\n",
      "gives an additional factor of K3in the computational cost of training compared with\n",
      "the two-class RVM.\n",
      "The principal disadvantage of the relevance vector machine is the relatively long\n",
      "training times compared with the SVM. This is offset, however, by the avoidance of\n",
      "cross-validation runs to set the model complexity parameters. Furthermore, because\n",
      "it yields sparser models, the computation time on test points, which is usually the\n",
      "more important consideration in practice, is typically much less.Exercises 357\n",
      "Exercises\n",
      "7.1 (⋆⋆)www Suppose we have a data set of input vectors {xn}with corresponding\n",
      "target values tn∈{ −1,1}, and suppose that we model the density of input vec-\n",
      "tors within each class separately using a Parzen kernel density estimator (see Sec-\n",
      "tion 2.5.1) with a kernel k(x,x′). Write down the minimum misclassiﬁcation-rate\n",
      "decision rule assuming the two classes have equal prior probability. Show also that,if the kernel is chosen to be k(x,x\n",
      "′)=xTx′, then the classiﬁcation rule reduces to\n",
      "simply assigning a new input vector to the class having the closest mean. Finally,\n",
      "show that, if the kernel takes the form k(x,x′)=φ(x)Tφ(x′), that the classiﬁcation\n",
      "is based on the closest mean in the feature space φ(x).\n",
      "7.2 (⋆)Show that, if the 1on the right-hand side of the constraint (7.5) is replaced by\n",
      "some arbitrary constant γ>0, the solution for the maximum margin hyperplane is\n",
      "unchanged.\n",
      "7.3 (⋆⋆)Show that, irrespective of the dimensionality of the data space, a data set\n",
      "consisting of just two data points, one from each class, is sufﬁcient to determine the\n",
      "location of the maximum-margin hyperplane.\n",
      "7.4 (⋆⋆)www Show that the value ρof the margin for the maximum-margin hyper-\n",
      "plane is given by\n",
      "1\n",
      "ρ2=N∑\n",
      "n=1an (7.123)\n",
      "where {an}are given by maximizing (7.10) subject to the constraints (7.11) and\n",
      "(7.12).\n",
      "7.5 (⋆⋆)Show that the values of ρand{an}in the previous exercise also satisfy\n",
      "1\n",
      "ρ2=2˜L(a) (7.124)\n",
      "where˜L(a)is deﬁned by (7.10). Similarly, show that\n",
      "1\n",
      "ρ2=∥w∥2. (7.125)\n",
      "7.6 (⋆)Consider the logistic regression model with a target variable t∈{ −1,1}.I f\n",
      "we deﬁne p(t=1|y)=σ(y)where y(x)is given by (7.1), show that the negative\n",
      "log likelihood, with the addition of a quadratic regularization term, takes the form(7.47).\n",
      "7.7 (⋆)Consider the Lagrangian (7.56) for the regression support vector machine. By\n",
      "setting the derivatives of the Lagrangian with respect to w,b,ξ\n",
      "n, andˆξnto zero and\n",
      "then back substituting to eliminate the corresponding variables, show that the dual\n",
      "Lagrangian is given by (7.61).358 7. SPARSE KERNEL MACHINES\n",
      "7.8 (⋆)www For the regression support vector machine considered in Section 7.1.4,\n",
      "show that all training data points for which ξn>0will have an=C, and similarly\n",
      "all points for which ˆξn>0will haveˆan=C.\n",
      "7.9 (⋆)Verify the results (7.82) and (7.83) for the mean and covariance of the posterior\n",
      "distribution over weights in the regression RVM.\n",
      "7.10 (⋆⋆)www Derive the result (7.85) for the marginal likelihood function in the\n",
      "regression RVM, by performing the Gaussian integral over win (7.84) using the\n",
      "technique of completing the square in the exponential.\n",
      "7.11 (⋆⋆)Repeat the above exercise, but this time make use of the general result (2.115).\n",
      "7.12 (⋆⋆)www Show that direct maximization of the log marginal likelihood (7.85) for\n",
      "the regression relevance vector machine leads to the re-estimation equations (7.87)and (7.88) where γ\n",
      "iis deﬁned by (7.89).\n",
      "7.13 (⋆⋆)In the evidence framework for RVM regression, we obtained the re-estimation\n",
      "formulae (7.87) and (7.88) by maximizing the marginal likelihood given by (7.85).\n",
      "Extend this approach by inclusion of hyperpriors given by gamma distributions ofthe form (B.26) and obtain the corresponding re-estimation formulae for αandβby\n",
      "maximizing the corresponding posterior probability p(t,α,β|X)with respect to α\n",
      "andβ.\n",
      "7.14 (⋆⋆)Derive the result (7.90) for the predictive distribution in the relevance vector\n",
      "machine for regression. Show that the predictive variance is given by (7.91).\n",
      "7.15 (⋆⋆)\n",
      "www Using the results (7.94) and (7.95), show that the marginal likelihood\n",
      "(7.85) can be written in the form (7.96), where λ(αn)is deﬁned by (7.97) and the\n",
      "sparsity and quality factors are deﬁned by (7.98) and (7.99), respectively.\n",
      "7.16 (⋆)By taking the second derivative of the log marginal likelihood (7.97) for the\n",
      "regression RVM with respect to the hyperparameter αi, show that the stationary\n",
      "point given by (7.101) is a maximum of the marginal likelihood.\n",
      "7.17 (⋆⋆)Using (7.83) and (7.86), together with the matrix identity (C.7), show that\n",
      "the quantities SnandQndeﬁned by (7.102) and (7.103) can be written in the form\n",
      "(7.106) and (7.107).\n",
      "7.18 (⋆)www Show that the gradient vector and Hessian matrix of the log poste-\n",
      "rior distribution (7.109) for the classiﬁcation relevance vector machine are given by\n",
      "(7.110) and (7.111).\n",
      "7.19 (⋆⋆)Verify that maximization of the approximate log marginal likelihood function\n",
      "(7.114) for the classiﬁcation relevance vector machine leads to the result (7.116) for\n",
      "re-estimation of the hyperparameters.8\n",
      "Graphical\n",
      "Models\n",
      "Probabilities play a central role in modern pattern recognition. We have seen in\n",
      "Chapter 1 that probability theory can be expressed in terms of two simple equationscorresponding to the sum rule and the product rule. All of the probabilistic infer-\n",
      "ence and learning manipulations discussed in this book, no matter how complex,\n",
      "amount to repeated application of these two equations. We could therefore proceedto formulate and solve complicated probabilistic models purely by algebraic ma-\n",
      "nipulation. However, we shall ﬁnd it highly advantageous to augment the analysis\n",
      "using diagrammatic representations of probability distributions, called probabilistic\n",
      "graphical models . These offer several useful properties:\n",
      "1. They provide a simple way to visualize the structure of a probabilistic model\n",
      "and can be used to design and motivate new models.\n",
      "2. Insights into the properties of the model, including conditional independence\n",
      "properties, can be obtained by inspection of the graph.\n",
      "359360 8. GRAPHICAL MODELS\n",
      "3. Complex computations, required to perform inference and learning in sophis-\n",
      "ticated models, can be expressed in terms of graphical manipulations, in whichunderlying mathematical expressions are carried along implicitly.\n",
      "A graph comprises nodes (also called vertices ) connected by links (also known\n",
      "asedges orarcs). In a probabilistic graphical model, each node represents a random\n",
      "variable (or group of random variables), and the links express probabilistic relation-\n",
      "ships between these variables. The graph then captures the way in which the jointdistribution over all of the random variables can be decomposed into a product of\n",
      "factors each depending only on a subset of the variables. We shall begin by dis-\n",
      "cussing Bayesian networks , also known as directed graphical models , in which the\n",
      "links of the graphs have a particular directionality indicated by arrows. The other\n",
      "major class of graphical models are Markov random ﬁelds , also known as undirected\n",
      "graphical models , in which the links do not carry arrows and have no directional\n",
      "signiﬁcance. Directed graphs are useful for expressing causal relationships between\n",
      "random variables, whereas undirected graphs are better suited to expressing soft con-\n",
      "straints between random variables. For the purposes of solving inference problems,it is often convenient to convert both directed and undirected graphs into a different\n",
      "representation called a factor graph .\n",
      "In this chapter, we shall focus on the key aspects of graphical models as needed\n",
      "for applications in pattern recognition and machine learning. More general treat-\n",
      "ments of graphical models can be found in the books by Whittaker (1990), Lauritzen\n",
      "(1996), Jensen (1996), Castillo et al. (1997), Jordan (1999), Cowell et al. (1999),\n",
      "and Jordan (2007).\n",
      "8.1. Bayesian Networks\n",
      "In order to motivate the use of directed graphs to describe probability distributions,consider ﬁrst an arbitrary joint distribution p(a, b, c)over three variables a,b, andc.\n",
      "Note that at this stage, we do not need to specify anything further about these vari-ables, such as whether they are discrete or continuous. Indeed, one of the powerful\n",
      "aspects of graphical models is that a speciﬁc graph can make probabilistic statements\n",
      "for a broad class of distributions. By application of the product rule of probability(1.11), we can write the joint distribution in the form\n",
      "p(a, b, c)=p(c|a, b)p(a, b). (8.1)\n",
      "A second application of the product rule, this time to the second term on the right-\n",
      "hand side of (8.1), gives\n",
      "p(a, b, c)=p(c|a, b)p(b|a)p(a). (8.2)\n",
      "Note that this decomposition holds for any choice of the joint distribution. We now\n",
      "represent the right-hand side of (8.2) in terms of a simple graphical model as follows.First we introduce a node for each of the random variables a,b, andcand associate\n",
      "each node with the corresponding conditional distribution on the right-hand side of8.1. Bayesian Networks 361\n",
      "Figure 8.1 A directed graphical model representing the joint probabil-\n",
      "ity distribution over three variables a,b, andc, correspond-\n",
      "ing to the decomposition on the right-hand side of (8.2).a\n",
      "b\n",
      "c\n",
      "(8.2). Then, for each conditional distribution we add directed links (arrows) to the\n",
      "graph from the nodes corresponding to the variables on which the distribution is\n",
      "conditioned. Thus for the factor p(c|a, b), there will be links from nodes aandbto\n",
      "nodec, whereas for the factor p(a)there will be no incoming links. The result is the\n",
      "graph shown in Figure 8.1. If there is a link going from a node ato a node b, then we\n",
      "say that node ais the parent of node b, and we say that node bis the child of node a.\n",
      "Note that we shall not make any formal distinction between a node and the variable\n",
      "to which it corresponds but will simply use the same symbol to refer to both.\n",
      "An interesting point to note about (8.2) is that the left-hand side is symmetrical\n",
      "with respect to the three variables a,b, andc, whereas the right-hand side is not.\n",
      "Indeed, in making the decomposition in (8.2), we have implicitly chosen a particular\n",
      "ordering, namely a, b, c , and had we chosen a different ordering we would have\n",
      "obtained a different decomposition and hence a different graphical representation.\n",
      "We shall return to this point later.\n",
      "For the moment let us extend the example of Figure 8.1 by considering the joint\n",
      "distribution over Kvariables given by p(x1,...,x K). By repeated application of\n",
      "the product rule of probability, this joint distribution can be written as a product of\n",
      "conditional distributions, one for each of the variables\n",
      "p(x1,...,x K)=p(xK|x1,...,x K−1)...p(x2|x1)p(x1). (8.3)\n",
      "For a given choice of K, we can again represent this as a directed graph having K\n",
      "nodes, one for each conditional distribution on the right-hand side of (8.3), with each\n",
      "node having incoming links from all lower numbered nodes. We say that this graph\n",
      "isfully connected because there is a link between every pair of nodes.\n",
      "So far, we have worked with completely general joint distributions, so that the\n",
      "decompositions, and their representations as fully connected graphs, will be applica-\n",
      "ble to any choice of distribution. As we shall see shortly, it is the absence of links\n",
      "in the graph that conveys interesting information about the properties of the class of\n",
      "distributions that the graph represents. Consider the graph shown in Figure 8.2. This\n",
      "is not a fully connected graph because, for instance, there is no link from x1tox2or\n",
      "fromx3tox7.\n",
      "We shall now go from this graph to the corresponding representation of the joint\n",
      "probability distribution written in terms of the product of a set of conditional dis-\n",
      "tributions, one for each node in the graph. Each such conditional distribution will\n",
      "be conditioned only on the parents of the corresponding node in the graph. For in-\n",
      "stance, x5will be conditioned on x1andx3. The joint distribution of all 7variables362 8. GRAPHICAL MODELS\n",
      "Figure 8.2 Example of a directed acyclic graph describing the joint\n",
      "distribution over variables x1,...,x 7. The corresponding\n",
      "decomposition of the joint distribution is given by (8.4).x1\n",
      "x2 x3\n",
      "x4 x5\n",
      "x6 x7\n",
      "is therefore given by\n",
      "p(x1)p(x2)p(x3)p(x4|x1,x2,x3)p(x5|x1,x3)p(x6|x4)p(x7|x4,x5). (8.4)\n",
      "The reader should take a moment to study carefully the correspondence between\n",
      "(8.4) and Figure 8.2.\n",
      "We can now state in general terms the relationship between a given directed\n",
      "graph and the corresponding distribution over the variables. The joint distribution\n",
      "deﬁned by a graph is given by the product, over all of the nodes of the graph, of\n",
      "a conditional distribution for each node conditioned on the variables corresponding\n",
      "to the parents of that node in the graph. Thus, for a graph with Knodes, the joint\n",
      "distribution is given by\n",
      "p(x)=K∏\n",
      "k=1p(xk|pak) (8.5)\n",
      "where pakdenotes the set of parents of xk, andx={x1,...,x K}. This key\n",
      "equation expresses the factorization properties of the joint distribution for a directed\n",
      "graphical model. Although we have considered each node to correspond to a single\n",
      "variable, we can equally well associate sets of variables and vector-valued variables\n",
      "with the nodes of a graph. It is easy to show that the representation on the right-\n",
      "hand side of (8.5) is always correctly normalized provided the individual conditional\n",
      "distributions are normalized. Exercise 8.1\n",
      "The directed graphs that we are considering are subject to an important restric-\n",
      "tion namely that there must be no directed cycles , in other words there are no closed\n",
      "paths within the graph such that we can move from node to node along links follow-\n",
      "ing the direction of the arrows and end up back at the starting node. Such graphs are\n",
      "also called directed acyclic graphs ,o r DAGs . This is equivalent to the statement that Exercise 8.2\n",
      "there exists an ordering of the nodes such that there are no links that go from any\n",
      "node to any lower numbered node.\n",
      "8.1.1 Example: Polynomial regression\n",
      "As an illustration of the use of directed graphs to describe probability distri-\n",
      "butions, we consider the Bayesian polynomial regression model introduced in Sec-8.1. Bayesian Networks 363\n",
      "Figure 8.3 Directed graphical model representing the joint\n",
      "distribution (8.6) corresponding to the Bayesian\n",
      "polynomial regression model introduced in Sec-\n",
      "tion 1.2.6.w\n",
      "t1 tN\n",
      "tion 1.2.6. The random variables in this model are the vector of polynomial coefﬁ-\n",
      "cientswand the observed data t=(t1,...,t N)T. In addition, this model contains\n",
      "the input data x=(x1,...,x N)T, the noise variance σ2, and the hyperparameter α\n",
      "representing the precision of the Gaussian prior over w, all of which are parameters\n",
      "of the model rather than random variables. Focussing just on the random variables\n",
      "for the moment, we see that the joint distribution is given by the product of the prior\n",
      "p(w)andNconditional distributions p(tn|w)forn=1,...,N so that\n",
      "p(t,w)=p(w)N∏\n",
      "n=1p(tn|w). (8.6)\n",
      "This joint distribution can be represented by a graphical model shown in Figure 8.3.\n",
      "When we start to deal with more complex models later in the book, we shall ﬁnd\n",
      "it inconvenient to have to write out multiple nodes of the form t1,...,t Nexplicitly as\n",
      "in Figure 8.3. We therefore introduce a graphical notation that allows such multiple\n",
      "nodes to be expressed more compactly, in which we draw a single representative\n",
      "nodetnand then surround this with a box, called a plate , labelled with Nindicating\n",
      "that there are Nnodes of this kind. Re-writing the graph of Figure 8.3 in this way,\n",
      "we obtain the graph shown in Figure 8.4.\n",
      "We shall sometimes ﬁnd it helpful to make the parameters of a model, as well as\n",
      "its stochastic variables, explicit. In this case, (8.6) becomes\n",
      "p(t,w|x,α,σ2)=p(w|α)N∏\n",
      "n=1p(tn|w,xn,σ2).\n",
      "Correspondingly, we can make xandαexplicit in the graphical representation. To\n",
      "do this, we shall adopt the convention that random variables will be denoted by open\n",
      "circles, and deterministic parameters will be denoted by smaller solid circles. If we\n",
      "take the graph of Figure 8.4 and include the deterministic parameters, we obtain the\n",
      "graph shown in Figure 8.5.\n",
      "When we apply a graphical model to a problem in machine learning or pattern\n",
      "recognition, we will typically set some of the random variables to speciﬁc observed\n",
      "Figure 8.4 An alternative, more compact, representation of the graph\n",
      "shown in Figure 8.3 in which we have introduced a plate\n",
      "(the box labelled N) that represents Nnodes of which only\n",
      "a single example tnis shown explicitly.tn\n",
      "Nw364 8. GRAPHICAL MODELS\n",
      "Figure 8.5 This shows the same model as in Figure 8.4 but\n",
      "with the deterministic parameters shown explicitly\n",
      "by the smaller solid nodes.\n",
      "tnxn\n",
      "Nwα\n",
      "σ2\n",
      "values, for example the variables {tn}from the training set in the case of polynomial\n",
      "curve ﬁtting. In a graphical model, we will denote such observed variables by shad-\n",
      "ing the corresponding nodes. Thus the graph corresponding to Figure 8.5 in which\n",
      "the variables {tn}are observed is shown in Figure 8.6. Note that the value of wis\n",
      "not observed, and so wis an example of a latent variable, also known as a hidden\n",
      "variable. Such variables play a crucial role in many probabilistic models and will\n",
      "form the focus of Chapters 9 and 12.\n",
      "Having observed the values {tn}we can, if desired, evaluate the posterior dis-\n",
      "tribution of the polynomial coefﬁcients was discussed in Section 1.2.5. For the\n",
      "moment, we note that this involves a straightforward application of Bayes’ theorem\n",
      "p(w|T)∝p(w)N∏\n",
      "n=1p(tn|w) (8.7)\n",
      "where again we have omitted the deterministic parameters in order to keep the nota-\n",
      "tion uncluttered.\n",
      "In general, model parameters such as ware of little direct interest in themselves,\n",
      "because our ultimate goal is to make predictions for new input values. Suppose we\n",
      "are given a new input value ˆxand we wish to ﬁnd the corresponding probability dis-\n",
      "tribution for ˆtconditioned on the observed data. The graphical model that describes\n",
      "this problem is shown in Figure 8.7, and the corresponding joint distribution of all\n",
      "of the random variables in this model, conditioned on the deterministic parameters,\n",
      "is then given by\n",
      "p(ˆt,t,w|ˆx,x,α,σ2)=[N∏\n",
      "n=1p(tn|xn,w,σ2)]\n",
      "p(w|α)p(ˆt|ˆx,w,σ2). (8.8)\n",
      "Figure 8.6 As in Figure 8.5 but with the nodes {tn}shaded\n",
      "to indicate that the corresponding random vari-\n",
      "ables have been set to their observed (training set)\n",
      "values.\n",
      "tnxn\n",
      "Nwα\n",
      "σ28.1. Bayesian Networks 365\n",
      "Figure 8.7 The polynomial regression model, corresponding\n",
      "to Figure 8.6, showing also a new input value bx\n",
      "together with the corresponding model prediction\n",
      "bt.\n",
      "tnxn\n",
      "Nwα\n",
      "ˆtσ2ˆx\n",
      "The required predictive distribution for ˆtis then obtained, from the sum rule of\n",
      "probability, by integrating out the model parameters wso that\n",
      "p(ˆt|ˆx,x,t,α,σ2)∝∫\n",
      "p(ˆt,t,w|ˆx,x,α,σ2)dw\n",
      "where we are implicitly setting the random variables in tto the speciﬁc values ob-\n",
      "served in the data set. The details of this calculation were discussed in Chapter 3.\n",
      "8.1.2 Generative models\n",
      "There are many situations in which we wish to draw samples from a given prob-\n",
      "ability distribution. Although we shall devote the whole of Chapter 11 to a detailed\n",
      "discussion of sampling methods, it is instructive to outline here one technique, called\n",
      "ancestral sampling , which is particularly relevant to graphical models. Consider a\n",
      "joint distribution p(x1,...,x K)overKvariables that factorizes according to (8.5)\n",
      "corresponding to a directed acyclic graph. We shall suppose that the variables have\n",
      "been ordered such that there are no links from any node to any lower numbered node,\n",
      "in other words each node has a higher number than any of its parents. Our goal is to\n",
      "draw a sample ˆx1,...,ˆxKfrom the joint distribution.\n",
      "To do this, we start with the lowest-numbered node and draw a sample from the\n",
      "distribution p(x1), which we call ˆx1. We then work through each of the nodes in or-\n",
      "der, so that for node nwe draw a sample from the conditional distribution p(xn|pan)\n",
      "in which the parent variables have been set to their sampled values. Note that at each\n",
      "stage, these parent values will always be available because they correspond to lower-\n",
      "numbered nodes that have already been sampled. Techniques for sampling from\n",
      "speciﬁc distributions will be discussed in detail in Chapter 11. Once we have sam-\n",
      "pled from the ﬁnal variable xK, we will have achieved our objective of obtaining a\n",
      "sample from the joint distribution. To obtain a sample from some marginal distribu-\n",
      "tion corresponding to a subset of the variables, we simply take the sampled values\n",
      "for the required nodes and ignore the sampled values for the remaining nodes. For\n",
      "example, to draw a sample from the distribution p(x2,x4), we simply sample from\n",
      "the full joint distribution and then retain the values ˆx2,ˆx4and discard the remaining\n",
      "values {ˆxj̸=2,4}.366 8. GRAPHICAL MODELS\n",
      "Figure 8.8 A graphical model representing the process by which\n",
      "images of objects are created, in which the identity\n",
      "of an object (a discrete variable) and the position and\n",
      "orientation of that object (continuous variables) have\n",
      "independent prior probabilities. The image (a vector\n",
      "of pixel intensities) has a probability distribution that\n",
      "is dependent on the identity of the object as well as\n",
      "on its position and orientation.\n",
      "ImageObject Orientation Position\n",
      "For practical applications of probabilistic models, it will typically be the higher-\n",
      "numbered variables corresponding to terminal nodes of the graph that represent the\n",
      "observations, with lower-numbered nodes corresponding to latent variables. The\n",
      "primary role of the latent variables is to allow a complicated distribution over the\n",
      "observed variables to be represented in terms of a model constructed from simpler\n",
      "(typically exponential family) conditional distributions.\n",
      "We can interpret such models as expressing the processes by which the observed\n",
      "data arose. For instance, consider an object recognition task in which each observed\n",
      "data point corresponds to an image (comprising a vector of pixel intensities) of one\n",
      "of the objects. In this case, the latent variables might have an interpretation as the\n",
      "position and orientation of the object. Given a particular observed image, our goal is\n",
      "to ﬁnd the posterior distribution over objects, in which we integrate over all possible\n",
      "positions and orientations. We can represent this problem using a graphical model\n",
      "of the form show in Figure 8.8.\n",
      "The graphical model captures the causal process (Pearl, 1988) by which the ob-\n",
      "served data was generated. For this reason, such models are often called generative\n",
      "models. By contrast, the polynomial regression model described by Figure 8.5 is\n",
      "not generative because there is no probability distribution associated with the input\n",
      "variable x, and so it is not possible to generate synthetic data points from this model.\n",
      "We could make it generative by introducing a suitable prior distribution p(x), at the\n",
      "expense of a more complex model.\n",
      "The hidden variables in a probabilistic model need not, however, have any ex-\n",
      "plicit physical interpretation but may be introduced simply to allow a more complex\n",
      "joint distribution to be constructed from simpler components. In either case, the\n",
      "technique of ancestral sampling applied to a generative model mimics the creation\n",
      "of the observed data and would therefore give rise to ‘fantasy’ data whose probability\n",
      "distribution (if the model were a perfect representation of reality) would be the same\n",
      "as that of the observed data. In practice, producing synthetic observations from a\n",
      "generative model can prove informative in understanding the form of the probability\n",
      "distribution represented by that model.\n",
      "8.1.3 Discrete variables\n",
      "We have discussed the importance of probability distributions that are members\n",
      "of the exponential family, and we have seen that this family includes many well- Section 2.4\n",
      "known distributions as particular cases. Although such distributions are relatively\n",
      "simple, they form useful building blocks for constructing more complex probability8.1. Bayesian Networks 367\n",
      "Figure 8.9 (a) This fully-connected graph describes a general distribu-\n",
      "tion over two K-state discrete variables having a total of\n",
      "K2−1parameters. (b) By dropping the link between the\n",
      "nodes, the number of parameters is reduced to 2(K−1).(a)x1 x2\n",
      "(b)x1 x2\n",
      "distributions, and the framework of graphical models is very useful in expressing the\n",
      "way in which these building blocks are linked together.\n",
      "Such models have particularly nice properties if we choose the relationship be-\n",
      "tween each parent-child pair in a directed graph to be conjugate, and we shall ex-\n",
      "plore several examples of this shortly. Two cases are particularly worthy of note,\n",
      "namely when the parent and child node each correspond to discrete variables and\n",
      "when they each correspond to Gaussian variables, because in these two cases the\n",
      "relationship can be extended hierarchically to construct arbitrarily complex directed\n",
      "acyclic graphs. We begin by examining the discrete case.\n",
      "The probability distribution p(x|µ)for a single discrete variable xhaving K\n",
      "possible states (using the 1-of- Krepresentation) is given by\n",
      "p(x|µ)=K∏\n",
      "k=1µxk\n",
      "k(8.9)\n",
      "and is governed by the parameters µ=(µ1,...,µ K)T. Due to the constraint∑\n",
      "kµk=1, only K−1values for µkneed to be speciﬁed in order to deﬁne the\n",
      "distribution.\n",
      "Now suppose that we have two discrete variables, x1andx2, each of which has\n",
      "Kstates, and we wish to model their joint distribution. We denote the probability of\n",
      "observing both x1k=1 andx2l=1 by the parameter µkl, where x1kdenotes the\n",
      "kthcomponent of x1, and similarly for x2l. The joint distribution can be written\n",
      "p(x1,x2|µ)=K∏\n",
      "k=1K∏\n",
      "l=1µx1kx2l\n",
      "kl.\n",
      "Because the parameters µklare subject to the constraint∑\n",
      "k∑\n",
      "lµkl=1, this distri-\n",
      "bution is governed by K2−1parameters. It is easily seen that the total number of\n",
      "parameters that must be speciﬁed for an arbitrary joint distribution over Mvariables\n",
      "isKM−1and therefore grows exponentially with the number Mof variables.\n",
      "Using the product rule, we can factor the joint distribution p(x1,x2)in the form\n",
      "p(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the\n",
      "x1node to the x2node as shown in Figure 8.9(a). The marginal distribution p(x1)\n",
      "is governed by K−1parameters, as before, Similarly, the conditional distribution\n",
      "p(x2|x1)requires the speciﬁcation of K−1parameters for each of the Kpossible\n",
      "values of x1. The total number of parameters that must be speciﬁed in the joint\n",
      "distribution is therefore (K−1) +K(K−1) =K2−1as before.\n",
      "Now suppose that the variables x1andx2were independent, corresponding to\n",
      "the graphical model shown in Figure 8.9(b). Each variable is then described by368 8. GRAPHICAL MODELS\n",
      "Figure 8.10 This chain of Mdiscrete nodes, each\n",
      "having Kstates, requires the speciﬁcation of K−1+\n",
      "(M−1)K(K−1)parameters, which grows linearly\n",
      "with the length Mof the chain. In contrast, a fully con-\n",
      "nected graph of Mnodes would have KM−1param-\n",
      "eters, which grows exponentially with M.x1 x2 xM\n",
      "a separate multinomial distribution, and the total number of parameters would be\n",
      "2(K−1). For a distribution over Mindependent discrete variables, each having K\n",
      "states, the total number of parameters would be M(K−1), which therefore grows\n",
      "linearly with the number of variables. From a graphical perspective, we have reduced\n",
      "the number of parameters by dropping links in the graph, at the expense of having a\n",
      "restricted class of distributions.\n",
      "More generally, if we have Mdiscrete variables x1,...,xM, we can model\n",
      "the joint distribution using a directed graph with one variable corresponding to each\n",
      "node. The conditional distribution at each node is given by a set of nonnegative pa-\n",
      "rameters subject to the usual normalization constraint. If the graph is fully connected\n",
      "then we have a completely general distribution having KM−1parameters, whereas\n",
      "if there are no links in the graph the joint distribution factorizes into the product of\n",
      "the marginals, and the total number of parameters is M(K−1). Graphs having in-\n",
      "termediate levels of connectivity allow for more general distributions than the fully\n",
      "factorized one while requiring fewer parameters than the general joint distribution.\n",
      "As an illustration, consider the chain of nodes shown in Figure 8.10. The marginal\n",
      "distribution p(x1)requires K−1parameters, whereas each of the M−1condi-\n",
      "tional distributions p(xi|xi−1), fori=2,...,M , requires K(K−1)parameters.\n",
      "This gives a total parameter count of K−1+(M−1)K(K−1), which is quadratic\n",
      "inKand which grows linearly (rather than exponentially) with the length Mof the\n",
      "chain.\n",
      "An alternative way to reduce the number of independent parameters in a model\n",
      "is by sharing parameters (also known as tying of parameters). For instance, in the\n",
      "chain example of Figure 8.10, we can arrange that all of the conditional distributions\n",
      "p(xi|xi−1), fori=2,...,M , are governed by the same set of K(K−1)parameters.\n",
      "Together with the K−1parameters governing the distribution of x1, this gives a total\n",
      "ofK2−1parameters that must be speciﬁed in order to deﬁne the joint distribution.\n",
      "We can turn a graph over discrete variables into a Bayesian model by introduc-\n",
      "ing Dirichlet priors for the parameters. From a graphical point of view, each node\n",
      "then acquires an additional parent representing the Dirichlet distribution over the pa-\n",
      "rameters associated with the corresponding discrete node. This is illustrated for the\n",
      "chain model in Figure 8.11. The corresponding model in which we tie the parame-\n",
      "ters governing the conditional distributions p(xi|xi−1), fori=2,...,M , is shown\n",
      "in Figure 8.12.\n",
      "Another way of controlling the exponential growth in the number of parameters\n",
      "in models of discrete variables is to use parameterized models for the conditional\n",
      "distributions instead of complete tables of conditional probability values. To illus-\n",
      "trate this idea, consider the graph in Figure 8.13 in which all of the nodes represent\n",
      "binary variables. Each of the parent variables xiis governed by a single parame-8.1. Bayesian Networks 369\n",
      "Figure 8.11 An extension of the model of\n",
      "Figure 8.10 to include Dirich-\n",
      "let priors over the param-\n",
      "eters governing the discrete\n",
      "distributions.\n",
      "x1 x2 xMµ1 µ2 µM\n",
      "Figure 8.12 As in Figure 8.11 but with a sin-\n",
      "gle set of parameters µshared\n",
      "amongst all of the conditional\n",
      "distributions p(xi|xi−1).\n",
      "x1 x2 xMµ1 µ\n",
      "terµirepresenting the probability p(xi=1 ) , giving Mparameters in total for the\n",
      "parent nodes. The conditional distribution p(y|x1,...,x M), however, would require\n",
      "2Mparameters representing the probability p(y=1 ) for each of the 2Mpossible\n",
      "settings of the parent variables. Thus in general the number of parameters required\n",
      "to specify this conditional distribution will grow exponentially with M. We can ob-\n",
      "tain a more parsimonious form for the conditional distribution by using a logistic\n",
      "sigmoid function acting on a linear combination of the parent variables, giving Section 2.4\n",
      "p(y=1|x1,...,x M)=σ(\n",
      "w0+M∑\n",
      "i=1wixi)\n",
      "=σ(wTx) (8.10)\n",
      "where σ(a) = (1+exp( −a))−1is the logistic sigmoid, x=(x0,x1,...,x M)Tis an\n",
      "(M+1 ) -dimensional vector of parent states augmented with an additional variable\n",
      "x0whose value is clamped to 1, and w=(w0,w1,...,w M)Tis a vector of M+1\n",
      "parameters. This is a more restricted form of conditional distribution than the general\n",
      "case but is now governed by a number of parameters that grows linearly with M.I n\n",
      "this sense, it is analogous to the choice of a restrictive form of covariance matrix (for\n",
      "example, a diagonal matrix) in a multivariate Gaussian distribution. The motivation\n",
      "for the logistic sigmoid representation was discussed in Section 4.2.\n",
      "Figure 8.13 A graph comprising Mparents x1,...,x Mand a sin-\n",
      "gle child y, used to illustrate the idea of parameterized\n",
      "conditional distributions for discrete variables.\n",
      "yx1 xM370 8. GRAPHICAL MODELS\n",
      "8.1.4 Linear-Gaussian models\n",
      "In the previous section, we saw how to construct joint probability distributions\n",
      "over a set of discrete variables by expressing the variables as nodes in a directed\n",
      "acyclic graph. Here we show how a multivariate Gaussian can be expressed as a\n",
      "directed graph corresponding to a linear-Gaussian model over the component vari-ables. This allows us to impose interesting structure on the distribution, with the\n",
      "general Gaussian and the diagonal covariance Gaussian representing opposite ex-\n",
      "tremes. Several widely used techniques are examples of linear-Gaussian models,\n",
      "such as probabilistic principal component analysis, factor analysis, and linear dy-\n",
      "namical systems (Roweis and Ghahramani, 1999). We shall make extensive use ofthe results of this section in later chapters when we consider some of these techniques\n",
      "in detail.\n",
      "Consider an arbitrary directed acyclic graph over Dvariables in which node i\n",
      "represents a single continuous random variable x\n",
      "ihaving a Gaussian distribution.\n",
      "The mean of this distribution is taken to be a linear combination of the states of its\n",
      "parent nodes paiof node i\n",
      "p(xi|pai)=N⎛\n",
      "⎝xi⏐⏐⏐⏐⏐⏐∑\n",
      "j∈paiwijxj+bi,vi⎞\n",
      "⎠ (8.11)\n",
      "where wijandbiare parameters governing the mean, and viis the variance of the\n",
      "conditional distribution for xi. The log of the joint distribution is then the log of the\n",
      "product of these conditionals over all nodes in the graph and hence takes the form\n",
      "lnp(x)=D∑\n",
      "i=1lnp(xi|pai) (8.12)\n",
      "=−D∑\n",
      "i=11\n",
      "2vi⎛\n",
      "⎝xi−∑\n",
      "j∈paiwijxj−bi⎞\n",
      "⎠2\n",
      "+c o n s t (8.13)\n",
      "wherex=(x1,...,x D)Tand ‘const’ denotes terms independent of x. We see that\n",
      "this is a quadratic function of the components of x, and hence the joint distribution\n",
      "p(x)is a multivariate Gaussian.\n",
      "We can determine the mean and covariance of the joint distribution recursively\n",
      "as follows. Each variable xihas (conditional on the states of its parents) a Gaussian\n",
      "distribution of the form (8.11) and so\n",
      "xi=∑\n",
      "j∈paiwijxj+bi+√viϵi (8.14)\n",
      "where ϵiis a zero mean, unit variance Gaussian random variable satisfying E[ϵi]=0\n",
      "and E[ϵiϵj]=Iij, where Iijis the i, jelement of the identity matrix. Taking the\n",
      "expectation of (8.14), we have\n",
      "E[xi]=∑\n",
      "j∈paiwijE[xj]+bi. (8.15)8.1. Bayesian Networks 371\n",
      "Figure 8.14 A directed graph over three Gaussian variables,\n",
      "with one missing link.x1 x2 x3\n",
      "Thus we can ﬁnd the components of E[x]=( E[x1],..., E[xD])Tby starting at the\n",
      "lowest numbered node and working recursively through the graph (here we again\n",
      "assume that the nodes are numbered such that each node has a higher number than\n",
      "its parents). Similarly, we can use (8.14) and (8.15) to obtain the i, jelement of the\n",
      "covariance matrix for p(x)in the form of a recursion relation\n",
      "cov[xi,xj]= E[(xi−E[xi])(xj−E[xj])]\n",
      "= E⎡\n",
      "⎣(xi−E[xi])⎧\n",
      "⎨\n",
      "⎩∑\n",
      "k∈pajwjk(xk−E[xk]) +√vjϵj⎫\n",
      "⎬\n",
      "⎭⎤\n",
      "⎦\n",
      "=∑\n",
      "k∈pajwjkcov[xi,xk]+Iijvj (8.16)\n",
      "and so the covariance can similarly be evaluated recursively starting from the lowest\n",
      "numbered node.\n",
      "Let us consider two extreme cases. First of all, suppose that there are no links\n",
      "in the graph, which therefore comprises Disolated nodes. In this case, there are no\n",
      "parameters wijand so there are just Dparameters biandDparameters vi. From\n",
      "the recursion relations (8.15) and (8.16), we see that the mean of p(x)is given by\n",
      "(b1,...,b D)Tand the covariance matrix is diagonal of the form diag(v1,...,v D).\n",
      "The joint distribution has a total of 2Dparameters and represents a set of Dinde-\n",
      "pendent univariate Gaussian distributions.\n",
      "Now consider a fully connected graph in which each node has all lower num-\n",
      "bered nodes as parents. The matrix wijthen has i−1entries on the ithrow and\n",
      "hence is a lower triangular matrix (with no entries on the leading diagonal). Then\n",
      "the total number of parameters wijis obtained by taking the number D2of elements\n",
      "in aD×Dmatrix, subtracting Dto account for the absence of elements on the lead-\n",
      "ing diagonal, and then dividing by 2because the matrix has elements only below the\n",
      "diagonal, giving a total of D(D−1)/2. The total number of independent parameters\n",
      "{wij}and{vi}in the covariance matrix is therefore D(D+1 )/2corresponding to\n",
      "a general symmetric covariance matrix. Section 2.3\n",
      "Graphs having some intermediate level of complexity correspond to joint Gaus-\n",
      "sian distributions with partially constrained covariance matrices. Consider for ex-\n",
      "ample the graph shown in Figure 8.14, which has a link missing between variables\n",
      "x1andx3. Using the recursion relations (8.15) and (8.16), we see that the mean and\n",
      "covariance of the joint distribution are given by Exercise 8.7\n",
      "µ=(b1,b2+w21b1,b3+w32b2+w32w21b1)T(8.17)\n",
      "Σ=(v1 w21v1 w32w21v1\n",
      "w21v1 v2+w2\n",
      "21v1 w32(v2+w2\n",
      "21v1)\n",
      "w32w21v1w32(v2+w2\n",
      "21v1)v3+w2\n",
      "32(v2+w2\n",
      "21v1))\n",
      ".(8.18)372 8. GRAPHICAL MODELS\n",
      "We can readily extend the linear-Gaussian graphical model to the case in which\n",
      "the nodes of the graph represent multivariate Gaussian variables. In this case, we canwrite the conditional distribution for node iin the form\n",
      "p(x\n",
      "i|pai)=N⎛\n",
      "⎝xi⏐⏐⏐⏐⏐⏐∑\n",
      "j∈paiWijxj+bi,Σi⎞\n",
      "⎠ (8.19)\n",
      "where now Wijis a matrix (which is nonsquare if xiandxjhave different dimen-\n",
      "sionalities). Again it is easy to verify that the joint distribution over all variables is\n",
      "Gaussian.\n",
      "Note that we have already encountered a speciﬁc example of the linear-Gaussian\n",
      "relationship when we saw that the conjugate prior for the mean µof a Gaussian Section 2.3.6\n",
      "variable xis itself a Gaussian distribution over µ. The joint distribution over xand\n",
      "µis therefore Gaussian. This corresponds to a simple two-node graph in which\n",
      "the node representing µis the parent of the node representing x. The mean of the\n",
      "distribution over µis a parameter controlling a prior, and so it can be viewed as a\n",
      "hyperparameter. Because the value of this hyperparameter may itself be unknown,\n",
      "we can again treat it from a Bayesian perspective by introducing a prior over thehyperparameter, sometimes called a hyperprior , which is again given by a Gaussian\n",
      "distribution. This type of construction can be extended in principle to any level and is\n",
      "an illustration of a hierarchical Bayesian model , of which we shall encounter further\n",
      "examples in later chapters.\n",
      "8.2. Conditional Independence\n",
      "An important concept for probability distributions over multiple variables is that ofconditional independence (Dawid, 1980). Consider three variables a,b, andc, and\n",
      "suppose that the conditional distribution of a,g i v e n bandc, is such that it does not\n",
      "depend on the value of b, so that\n",
      "p(a|b, c)=p(a|c). (8.20)\n",
      "We say that ais conditionally independent of bgivenc. This can be expressed in a\n",
      "slightly different way if we consider the joint distribution of aandbconditioned on\n",
      "c, which we can write in the form\n",
      "p(a, b|c)= p(a|b, c)p(b|c)\n",
      "=p(a|c)p(b|c). (8.21)\n",
      "where we have used the product rule of probability together with (8.20). Thus we\n",
      "see that, conditioned on c, the joint distribution of aandbfactorizes into the prod-\n",
      "uct of the marginal distribution of aand the marginal distribution of b(again both\n",
      "conditioned on c). This says that the variables aandbare statistically independent,\n",
      "givenc\n",
      ". Note that our deﬁnition of conditional independence will require that (8.20),8.2. Conditional Independence 373\n",
      "Figure 8.15 The ﬁrst of three examples of graphs over three variables\n",
      "a,b, and cused to discuss conditional independence\n",
      "properties of directed graphical models.c\n",
      "ab\n",
      "or equivalently (8.21), must hold for every possible value of c, and not just for some\n",
      "values. We shall sometimes use a shorthand notation for conditional independence\n",
      "(Dawid, 1979) in which\n",
      "a⊥⊥b|c (8.22)\n",
      "denotes that ais conditionally independent of bgivencand is equivalent to (8.20).\n",
      "Conditional independence properties play an important role in using probabilis-\n",
      "tic models for pattern recognition by simplifying both the structure of a model and\n",
      "the computations needed to perform inference and learning under that model. We\n",
      "shall see examples of this shortly.\n",
      "If we are given an expression for the joint distribution over a set of variables in\n",
      "terms of a product of conditional distributions (i.e., the mathematical representation\n",
      "underlying a directed graph), then we could in principle test whether any poten-\n",
      "tial conditional independence property holds by repeated application of the sum and\n",
      "product rules of probability. In practice, such an approach would be very time con-\n",
      "suming. An important and elegant feature of graphical models is that conditional\n",
      "independence properties of the joint distribution can be read directly from the graph\n",
      "without having to perform any analytical manipulations. The general framework\n",
      "for achieving this is called d-separation , where the ‘d’ stands for ‘directed’ (Pearl,\n",
      "1988). Here we shall motivate the concept of d-separation and give a general state-\n",
      "ment of the d-separation criterion. A formal proof can be found in Lauritzen (1996).\n",
      "8.2.1 Three example graphs\n",
      "We begin our discussion of the conditional independence properties of directed\n",
      "graphs by considering three simple examples each involving graphs having just three\n",
      "nodes. Together, these will motivate and illustrate the key concepts of d-separation.\n",
      "The ﬁrst of the three examples is shown in Figure 8.15, and the joint distribution\n",
      "corresponding to this graph is easily written down using the general result (8.5) to\n",
      "give\n",
      "p(a, b, c)=p(a|c)p(b|c)p(c). (8.23)\n",
      "If none of the variables are observed, then we can investigate whether aandbare\n",
      "independent by marginalizing both sides of (8.23) with respect to cto give\n",
      "p(a, b)=∑\n",
      "cp(a|c)p(b|c)p(c). (8.24)\n",
      "In general, this does not factorize into the product p(a)p(b), and so\n",
      "a̸⊥⊥b|∅ (8.25)374 8. GRAPHICAL MODELS\n",
      "Figure 8.16 As in Figure 8.15 but where we have conditioned on the\n",
      "value of variable c.c\n",
      "ab\n",
      "where ∅denotes the empty set, and the symbol ̸⊥⊥means that the conditional inde-\n",
      "pendence property does not hold in general. Of course, it may hold for a particular\n",
      "distribution by virtue of the speciﬁc numerical values associated with the various\n",
      "conditional probabilities, but it does not follow in general from the structure of the\n",
      "graph.\n",
      "Now suppose we condition on the variable c, as represented by the graph of\n",
      "Figure 8.16. From (8.23), we can easily write down the conditional distribution of a\n",
      "andb,g i v e n c, in the form\n",
      "p(a, b|c)=p(a, b, c)\n",
      "p(c)\n",
      "=p(a|c)p(b|c)\n",
      "and so we obtain the conditional independence property\n",
      "a⊥⊥b|c.\n",
      "We can provide a simple graphical interpretation of this result by considering\n",
      "the path from node ato node bviac. The node cis said to be tail-to-tail with re-\n",
      "spect to this path because the node is connected to the tails of the two arrows, and\n",
      "the presence of such a path connecting nodes aandbcauses these nodes to be de-\n",
      "pendent. However, when we condition on node c, as in Figure 8.16, the conditioned\n",
      "node ‘blocks’ the path from atoband causes aandbto become (conditionally)\n",
      "independent.\n",
      "We can similarly consider the graph shown in Figure 8.17. The joint distribution\n",
      "corresponding to this graph is again obtained from our general formula (8.5) to give\n",
      "p(a, b, c)=p(a)p(c|a)p(b|c). (8.26)\n",
      "First of all, suppose that none of the variables are observed. Again, we can test to\n",
      "see ifaandbare independent by marginalizing over cto give\n",
      "p(a, b)=p(a)∑\n",
      "cp(c|a)p(b|c)=p(a)p(b|a).\n",
      "Figure 8.17 The second of our three examples of 3-node\n",
      "graphs used to motivate the conditional indepen-\n",
      "dence framework for directed graphical models.ac b8.2. Conditional Independence 375\n",
      "Figure 8.18 As in Figure 8.17 but now conditioning on node c.ac b\n",
      "which in general does not factorize into p(a)p(b), and so\n",
      "a̸⊥⊥b|∅ (8.27)\n",
      "as before.\n",
      "Now suppose we condition on node c, as shown in Figure 8.18. Using Bayes’\n",
      "theorem, together with (8.26), we obtain\n",
      "p(a, b|c)=p(a, b, c)\n",
      "p(c)\n",
      "=p(a)p(c|a)p(b|c)\n",
      "p(c)\n",
      "=p(a|c)p(b|c)\n",
      "and so again we obtain the conditional independence property\n",
      "a⊥⊥b|c.\n",
      "As before, we can interpret these results graphically. The node cis said to be\n",
      "head-to-tail with respect to the path from node ato node b. Such a path connects\n",
      "nodes aandband renders them dependent. If we now observe c, as in Figure 8.18,\n",
      "then this observation ‘blocks’ the path from atoband so we obtain the conditional\n",
      "independence property a⊥⊥b|c.\n",
      "Finally, we consider the third of our 3-node examples, shown by the graph in\n",
      "Figure 8.19. As we shall see, this has a more subtle behaviour than the two previous\n",
      "graphs.\n",
      "The joint distribution can again be written down using our general result (8.5) to\n",
      "give\n",
      "p(a, b, c)=p(a)p(b)p(c|a, b). (8.28)\n",
      "Consider ﬁrst the case where none of the variables are observed. Marginalizing both\n",
      "sides of (8.28) over cwe obtain\n",
      "p(a, b)=p(a)p(b)\n",
      "Figure 8.19 The last of our three examples of 3-node graphs used to\n",
      "explore conditional independence properties in graphi-\n",
      "cal models. This graph has rather different properties\n",
      "from the two previous examples.\n",
      "cab376 8. GRAPHICAL MODELS\n",
      "Figure 8.20 As in Figure 8.19 but conditioning on the value of node\n",
      "c. In this graph, the act of conditioning induces a depen-\n",
      "dence between aandb.\n",
      "cab\n",
      "and so aandbare independent with no variables observed, in contrast to the two\n",
      "previous examples. We can write this result as\n",
      "a⊥⊥b|∅. (8.29)\n",
      "Now suppose we condition on c, as indicated in Figure 8.20. The conditional distri-\n",
      "bution of aandbis then given by\n",
      "p(a, b|c)=p(a, b, c)\n",
      "p(c)\n",
      "=p(a)p(b)p(c|a, b)\n",
      "p(c)\n",
      "which in general does not factorize into the product p(a)p(b), and so\n",
      "a̸⊥⊥b|c.\n",
      "Thus our third example has the opposite behaviour from the ﬁrst two. Graphically,\n",
      "we say that node cishead-to-head with respect to the path from atobbecause it\n",
      "connects to the heads of the two arrows. When node cis unobserved, it ‘blocks’\n",
      "the path, and the variables aandbare independent. However, conditioning on c\n",
      "‘unblocks’ the path and renders aandbdependent.\n",
      "There is one more subtlety associated with this third example that we need to\n",
      "consider. First we introduce some more terminology. We say that node yis a de-\n",
      "scendant of node xif there is a path from xtoyin which each step of the path\n",
      "follows the directions of the arrows. Then it can be shown that a head-to-head path\n",
      "will become unblocked if either the node, or any of its descendants , is observed. Exercise 8.10\n",
      "In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked\n",
      "unless it is observed in which case it blocks the path. By contrast, a head-to-head\n",
      "node blocks a path if it is unobserved, but once the node, and/or at least one of its\n",
      "descendants, is observed the path becomes unblocked.\n",
      "It is worth spending a moment to understand further the unusual behaviour of the\n",
      "graph of Figure 8.20. Consider a particular instance of such a graph corresponding\n",
      "to a problem with three binary random variables relating to the fuel system on a car,\n",
      "as shown in Figure 8.21. The variables are called B, representing the state of a\n",
      "battery that is either charged ( B=1)o rﬂ a t( B=0),Frepresenting the state of\n",
      "the fuel tank that is either full of fuel ( F=1) or empty ( F=0), and G, which is\n",
      "the state of an electric fuel gauge and which indicates either full ( G=1) or empty8.2. Conditional Independence 377\n",
      "GBF\n",
      "GBF\n",
      "GBF\n",
      "Figure 8.21 An example of a 3-node graph used to illustrate the phenomenon of ‘explaining away’. The three\n",
      "nodes represent the state of the battery ( B), the state of the fuel tank ( F) and the reading on the electric fuel\n",
      "gauge ( G). See the text for details.\n",
      "(G=0). The battery is either charged or ﬂat, and independently the fuel tank is\n",
      "either full or empty, with prior probabilities\n",
      "p(B=1 ) = 0 .9\n",
      "p(F=1 ) = 0 .9.\n",
      "Given the state of the fuel tank and the battery, the fuel gauge reads full with proba-\n",
      "bilities given by\n",
      "p(G=1|B=1,F=1 ) = 0 .8\n",
      "p(G=1|B=1,F=0 ) = 0 .2\n",
      "p(G=1|B=0,F=1 ) = 0 .2\n",
      "p(G=1|B=0,F=0 ) = 0 .1\n",
      "so this is a rather unreliable fuel gauge! All remaining probabilities are determined\n",
      "by the requirement that probabilities sum to one, and so we have a complete speciﬁ-\n",
      "cation of the probabilistic model.\n",
      "Before we observe any data, the prior probability of the fuel tank being empty\n",
      "isp(F=0 )=0 .1. Now suppose that we observe the fuel gauge and discover that\n",
      "it reads empty, i.e., G=0, corresponding to the middle graph in Figure 8.21. We\n",
      "can use Bayes’ theorem to evaluate the posterior probability of the fuel tank being\n",
      "empty. First we evaluate the denominator for Bayes’ theorem given by\n",
      "p(G=0 )=∑\n",
      "B∈{0,1}∑\n",
      "F∈{0,1}p(G=0|B,F)p(B)p(F)=0.315 (8.30)\n",
      "and similarly we evaluate\n",
      "p(G=0|F=0 )=∑\n",
      "B∈{0,1}p(G=0|B,F=0 )p(B)=0.81 (8.31)\n",
      "and using these results we have\n",
      "p(F=0|G=0 )=p(G=0|F=0 )p(F=0 )\n",
      "p(G=0 )≃0.257 (8.32)378 8. GRAPHICAL MODELS\n",
      "and so p(F=0|G=0 )>p(F=0 ) . Thus observing that the gauge reads empty\n",
      "makes it more likely that the tank is indeed empty, as we would intuitively expect.Next suppose that we also check the state of the battery and ﬁnd that it is ﬂat, i.e.,\n",
      "B=0. We have now observed the states of both the fuel gauge and the battery, as\n",
      "shown by the right-hand graph in Figure 8.21. The posterior probability that the fueltank is empty given the observations of both the fuel gauge and the battery state is\n",
      "then given by\n",
      "p(F=0|G=0,B=0 )=p(G=0|B=0,F=0 )p(F=0 )\n",
      "∑\n",
      "F∈{0,1}p(G=0|B=0,F)p(F)≃0.111 (8.33)\n",
      "where the prior probability p(B=0 ) has cancelled between numerator and denom-\n",
      "inator. Thus the probability that the tank is empty has decreased (from 0.257to\n",
      "0.111) as a result of the observation of the state of the battery. This accords with our\n",
      "intuition that ﬁnding out that the battery is ﬂat explains away the observation that the\n",
      "fuel gauge reads empty. We see that the state of the fuel tank and that of the battery\n",
      "have indeed become dependent on each other as a result of observing the reading\n",
      "on the fuel gauge. In fact, this would also be the case if, instead of observing the\n",
      "fuel gauge directly, we observed the state of some descendant of G. Note that the\n",
      "probability p(F=0|G=0,B=0 )≃0.111is greater than the prior probability\n",
      "p(F=0 )=0 .1because the observation that the fuel gauge reads zero still provides\n",
      "some evidence in favour of an empty fuel tank.\n",
      "8.2.2 D-separation\n",
      "We now give a general statement of the d-separation property (Pearl, 1988) for\n",
      "directed graphs. Consider a general directed graph in which A,B, andCare arbi-\n",
      "trary nonintersecting sets of nodes (whose union may be smaller than the complete\n",
      "set of nodes in the graph). We wish to ascertain whether a particular conditional\n",
      "independence statement A⊥⊥B|Cis implied by a given directed acyclic graph. To\n",
      "do so, we consider all possible paths from any node in Ato any node in B. Any such\n",
      "path is said to be blocked if it includes a node such that either\n",
      "(a)the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the\n",
      "node is in the set C,o r\n",
      "(b)the arrows meet head-to-head at the node, and neither the node, nor any of its\n",
      "descendants, is in the set C.\n",
      "If all paths are blocked, then Ais said to be d-separated from BbyC, and the joint\n",
      "distribution over all of the variables in the graph will satisfy A⊥⊥B|C.\n",
      "The concept of d-separation is illustrated in Figure 8.22. In graph (a), the path\n",
      "fromatobis not blocked by node fbecause it is a tail-to-tail node for this path\n",
      "and is not observed, nor is it blocked by node ebecause, although the latter is a\n",
      "head-to-head node, it has a descendant cbecause is in the conditioning set. Thus\n",
      "the conditional independence statement a⊥⊥b|cdoes notfollow from this graph.\n",
      "In graph (b), the path from atobis blocked by node fbecause this is a tail-to-tail\n",
      "node that is observed, and so the conditional independence property a⊥⊥b|fwill8.2. Conditional Independence 379\n",
      "Figure 8.22 Illustration of the con-\n",
      "cept of d-separation. See the text for\n",
      "details.f\n",
      "e ba\n",
      "c\n",
      "(a)f\n",
      "e ba\n",
      "c\n",
      "(b)\n",
      "be satisﬁed by any distribution that factorizes according to this graph. Note that this\n",
      "path is also blocked by node ebecause eis a head-to-head node and neither it nor its\n",
      "descendant are in the conditioning set.\n",
      "For the purposes of d-separation, parameters such as αandσ2in Figure 8.5,\n",
      "indicated by small ﬁlled circles, behave in the same was as observed nodes. How-\n",
      "ever, there are no marginal distributions associated with such nodes. Consequently\n",
      "parameter nodes never themselves have parents and so all paths through these nodes\n",
      "will always be tail-to-tail and hence blocked. Consequently they play no role in\n",
      "d-separation.\n",
      "Another example of conditional independence and d-separation is provided by\n",
      "the concept of i.i.d. (independent identically distributed) data introduced in Sec-\n",
      "tion 1.2.4. Consider the problem of ﬁnding the posterior distribution for the mean\n",
      "of a univariate Gaussian distribution. This can be represented by the directed graph Section 2.3\n",
      "shown in Figure 8.23 in which the joint distribution is deﬁned by a prior p(µ)to-\n",
      "gether with a set of conditional distributions p(xn|µ)forn=1,...,N . In practice,\n",
      "we observe D={x1,...,x N}and our goal is to infer µ. Suppose, for a moment,\n",
      "that we condition on µand consider the joint distribution of the observations. Using\n",
      "d-separation, we note that there is a unique path from any xito any other xj̸=iand\n",
      "that this path is tail-to-tail with respect to the observed node µ. Every such path is\n",
      "blocked and so the observations D={x1,...,x N}are independent given µ, so that\n",
      "p(D|µ)=N∏\n",
      "n=1p(xn|µ). (8.34)\n",
      "Figure 8.23 (a) Directed graph corre-\n",
      "sponding to the problem\n",
      "of inferring the mean µof\n",
      "a univariate Gaussian dis-\n",
      "tribution from observations\n",
      "x1,...,x N. (b) The same\n",
      "graph drawn using the plate\n",
      "notation.µ\n",
      "x1 xN\n",
      "(a)xnN\n",
      "Nµ\n",
      "(b)380 8. GRAPHICAL MODELS\n",
      "Figure 8.24 A graphical representation of the ‘naive Bayes’\n",
      "model for classiﬁcation. Conditioned on the\n",
      "class label z, the components of the observed\n",
      "vector x=(x1,...,x D)Tare assumed to be\n",
      "independent.z\n",
      "x1 xD\n",
      "However, if we integrate over µ, the observations are in general no longer indepen-\n",
      "dent\n",
      "p(D)=∫∞\n",
      "0p(D|µ)p(µ)dµ̸=N∏\n",
      "n=1p(xn). (8.35)\n",
      "Hereµis a latent variable, because its value is not observed.\n",
      "Another example of a model representing i.i.d. data is the graph in Figure 8.7\n",
      "corresponding to Bayesian polynomial regression. Here the stochastic nodes corre-\n",
      "spond to {tn},wandˆt. We see that the node for wis tail-to-tail with respect to\n",
      "the path from ˆtto any one of the nodes tnand so we have the following conditional\n",
      "independence property\n",
      "ˆt⊥⊥tn|w. (8.36)\n",
      "Thus, conditioned on the polynomial coefﬁcients w, the predictive distribution for\n",
      "ˆtis independent of the training data {t1,...,t N}. We can therefore ﬁrst use the\n",
      "training data to determine the posterior distribution over the coefﬁcients wand then\n",
      "we can discard the training data and use the posterior distribution for wto make\n",
      "predictions of ˆtfor new input observations ˆx. Section 3.3\n",
      "A related graphical structure arises in an approach to classiﬁcation called the\n",
      "naive Bayes model, in which we use conditional independence assumptions to sim-\n",
      "plify the model structure. Suppose our observed variable consists of a D-dimensional\n",
      "vectorx=(x1,...,x D)T, and we wish to assign observed values of xto one of K\n",
      "classes. Using the 1-of- Kencoding scheme, we can represent these classes by a K-\n",
      "dimensional binary vector z. We can then deﬁne a generative model by introducing\n",
      "a multinomial prior p(z|µ)over the class labels, where the kthcomponent µkofµ\n",
      "is the prior probability of class Ck, together with a conditional distribution p(x|z)\n",
      "for the observed vector x. The key assumption of the naive Bayes model is that,\n",
      "conditioned on the class z, the distributions of the input variables x1,...,x Dare in-\n",
      "dependent. The graphical representation of this model is shown in Figure 8.24. We\n",
      "see that observation of zblocks the path between xiandxjforj̸=i(because such\n",
      "paths are tail-to-tail at the node z) and so xiandxjare conditionally independent\n",
      "givenz. If, however, we marginalize out z(so that zis unobserved) the tail-to-tail\n",
      "path from xitoxjis no longer blocked. This tells us that in general the marginal\n",
      "density p(x)will not factorize with respect to the components of x. We encountered\n",
      "a simple application of the naive Bayes model in the context of fusing data from\n",
      "different sources for medical diagnosis in Section 1.5.\n",
      "If we are given a labelled training set, comprising inputs {x1,...,xN}together\n",
      "with their class labels, then we can ﬁt the naive Bayes model to the training data8.2. Conditional Independence 381\n",
      "using maximum likelihood assuming that the data are drawn independently from\n",
      "the model. The solution is obtained by ﬁtting the model for each class separatelyusing the correspondingly labelled data. As an example, suppose that the probability\n",
      "density within each class is chosen to be Gaussian. In this case, the naive Bayes\n",
      "assumption then implies that the covariance matrix for each Gaussian is diagonal,and the contours of constant density within each class will be axis-aligned ellipsoids.\n",
      "The marginal density, however, is given by a superposition of diagonal Gaussians\n",
      "(with weighting coefﬁcients given by the class priors) and so will no longer factorizewith respect to its components.\n",
      "The naive Bayes assumption is helpful when the dimensionality Dof the input\n",
      "space is high, making density estimation in the full D-dimensional space more chal-\n",
      "lenging. It is also useful if the input vector contains both discrete and continuous\n",
      "variables, since each can be represented separately using appropriate models (e.g.,Bernoulli distributions for binary observations or Gaussians for real-valued vari-\n",
      "ables). The conditional independence assumption of this model is clearly a strong\n",
      "one that may lead to rather poor representations of the class-conditional densities.Nevertheless, even if this assumption is not precisely satisﬁed, the model may still\n",
      "give good classiﬁcation performance in practice because the decision boundaries can\n",
      "be insensitive to some of the details in the class-conditional densities, as illustratedin Figure 1.27.\n",
      "We have seen that a particular directed graph represents a speciﬁc decomposition\n",
      "of a joint probability distribution into a product of conditional probabilities. Thegraph also expresses a set of conditional independence statements obtained through\n",
      "the d-separation criterion, and the d-separation theorem is really an expression of the\n",
      "equivalence of these two properties. In order to make this clear, it is helpful to thinkof a directed graph as a ﬁlter. Suppose we consider a particular joint probability\n",
      "distribution p(x)over the variables xcorresponding to the (nonobserved) nodes of\n",
      "the graph. The ﬁlter will allow this distribution to pass through if, and only if, it can\n",
      "be expressed in terms of the factorization (8.5) implied by the graph. If we present to\n",
      "the ﬁlter the set of all possible distributions p(x)over the set of variables x, then the\n",
      "subset of distributions that are passed by the ﬁlter will be denoted DF,f o r directed\n",
      "factorization . This is illustrated in Figure 8.25. Alternatively, we can use the graph as\n",
      "a different kind of ﬁlter by ﬁrst listing all of the conditional independence propertiesobtained by applying the d-separation criterion to the graph, and then allowing a\n",
      "distribution to pass only if it satisﬁes all of these properties. If we present all possible\n",
      "distributions p(x)to this second kind of ﬁlter, then the d-separation theorem tells us\n",
      "that the set of distributions that will be allowed through is precisely the set DF.\n",
      "It should be emphasized that the conditional independence properties obtained\n",
      "from d-separation apply to any probabilistic model described by that particular di-rected graph. This will be true, for instance, whether the variables are discrete or\n",
      "continuous or a combination of these. Again, we see that a particular graph is de-\n",
      "scribing a whole family of probability distributions.\n",
      "At one extreme we have a fully connected graph that exhibits no conditional in-\n",
      "dependence properties at all, and which can represent any possible joint probability\n",
      "distribution over the given variables. The set DF will contain all possible distribu-382 8. GRAPHICAL MODELS\n",
      "p(x) DF\n",
      "Figure 8.25 We can view a graphical model (in this case a directed graph) as a ﬁlter in which a prob-\n",
      "ability distribution p(x)is allowed through the ﬁlter if, and only if, it satisﬁes the directed\n",
      "factorization property (8.5). The set of all possible probability distributions p(x)that pass\n",
      "through the ﬁlter is denoted DF. We can alternatively use the graph to ﬁlter distributions\n",
      "according to whether they respect all of the conditional independencies implied by the\n",
      "d-separation properties of the graph. The d-separation theorem says that it is the same\n",
      "set of distributions DFthat will be allowed through this second kind of ﬁlter.\n",
      "tionsp(x). At the other extreme, we have the fully disconnected graph, i.e., one\n",
      "having no links at all. This corresponds to joint distributions which factorize into the\n",
      "product of the marginal distributions over the variables comprising the nodes of the\n",
      "graph.\n",
      "Note that for any given graph, the set of distributions DF will include any dis-\n",
      "tributions that have additional independence properties beyond those described by\n",
      "the graph. For instance, a fully factorized distribution will always be passed through\n",
      "the ﬁlter implied by any graph over the corresponding set of variables.\n",
      "We end our discussion of conditional independence properties by exploring the\n",
      "concept of a Markov blanket orMarkov boundary . Consider a joint distribution\n",
      "p(x1,...,xD)represented by a directed graph having Dnodes, and consider the\n",
      "conditional distribution of a particular node with variables xiconditioned on all of\n",
      "the remaining variables xj̸=i. Using the factorization property (8.5), we can express\n",
      "this conditional distribution in the form\n",
      "p(xi|x{j̸=i})=p(x1,...,xD)∫\n",
      "p(x1,...,xD)dxi\n",
      "=∏\n",
      "kp(xk|pak)\n",
      "∫∏\n",
      "kp(xk|pak)dxi\n",
      "in which the integral is replaced by a summation in the case of discrete variables. We\n",
      "now observe that any factor p(xk|pak)that does not have any functional dependence\n",
      "onxican be taken outside the integral over xi, and will therefore cancel between\n",
      "numerator and denominator. The only factors that remain will be the conditional\n",
      "distribution p(xi|pai)for node xiitself, together with the conditional distributions\n",
      "for any nodes xksuch that node xiis in the conditioning set of p(xk|pak), in other\n",
      "words for which xiis a parent of xk. The conditional p(xi|pai)will depend on the\n",
      "parents of node xi, whereas the conditionals p(xk|pak)will depend on the children8.3. Markov Random Fields 383\n",
      "Figure 8.26 The Markov blanket of a node xicomprises the set\n",
      "of parents, children and co-parents of the node. It\n",
      "has the property that the conditional distribution of\n",
      "xi, conditioned on all the remaining variables in the\n",
      "graph, is dependent only on the variables in the\n",
      "Markov blanket.xi\n",
      "ofxias well as on the co-parents , in other words variables corresponding to parents\n",
      "of node xkother than node xi. The set of nodes comprising the parents, the children\n",
      "and the co-parents is called the Markov blanket and is illustrated in Figure 8.26. We\n",
      "can think of the Markov blanket of a node xias being the minimal set of nodes that\n",
      "isolates xifrom the rest of the graph. Note that it is not sufﬁcient to include only the\n",
      "parents and children of node xibecause the phenomenon of explaining away means\n",
      "that observations of the child nodes will not block paths to the co-parents. We must\n",
      "therefore observe the co-parent nodes also.\n",
      "8.3. Markov Random Fields\n",
      "We have seen that directed graphical models specify a factorization of the joint dis-\n",
      "tribution over a set of variables into a product of local conditional distributions. They\n",
      "also deﬁne a set of conditional independence properties that must be satisﬁed by any\n",
      "distribution that factorizes according to the graph. We turn now to the second ma-\n",
      "jor class of graphical models that are described by undirected graphs and that again\n",
      "specify both a factorization and a set of conditional independence relations.\n",
      "AMarkov random ﬁeld , also known as a Markov network or an undirected\n",
      "graphical model (Kindermann and Snell, 1980), has a set of nodes each of which\n",
      "corresponds to a variable or group of variables, as well as a set of links each of\n",
      "which connects a pair of nodes. The links are undirected, that is they do not carry\n",
      "arrows. In the case of undirected graphs, it is convenient to begin with a discussion\n",
      "of conditional independence properties.\n",
      "8.3.1 Conditional independence properties\n",
      "In the case of directed graphs, we saw that it was possible to test whether a par- Section 8.2\n",
      "ticular conditional independence property holds by applying a graphical test called\n",
      "d-separation. This involved testing whether or not the paths connecting two sets of\n",
      "nodes were ‘blocked’. The deﬁnition of blocked, however, was somewhat subtle\n",
      "due to the presence of paths having head-to-head nodes. We might ask whether it\n",
      "is possible to deﬁne an alternative graphical semantics for probability distributions\n",
      "such that conditional independence is determined by simple graph separation. This\n",
      "is indeed the case and corresponds to undirected graphical models. By removing the384 8. GRAPHICAL MODELS\n",
      "Figure 8.27 An example of an undirected graph in\n",
      "which every path from any node in set\n",
      "Ato any node in set Bpasses through\n",
      "at least one node in set C. Conse-\n",
      "quently the conditional independence\n",
      "property A⊥⊥B|Cholds for any\n",
      "probability distribution described by this\n",
      "graph.\n",
      "AC\n",
      "B\n",
      "directionality from the links of the graph, the asymmetry between parent and child\n",
      "nodes is removed, and so the subtleties associated with head-to-head nodes no longer\n",
      "arise.\n",
      "Suppose that in an undirected graph we identify three sets of nodes, denoted A,\n",
      "B, andC, and that we consider the conditional independence property\n",
      "A⊥⊥B|C. (8.37)\n",
      "To test whether this property is satisﬁed by a probability distribution deﬁned by a\n",
      "graph we consider all possible paths that connect nodes in set Ato nodes in set\n",
      "B. If all such paths pass through one or more nodes in set C, then all such paths are\n",
      "‘blocked’ and so the conditional independence property holds. However, if there is at\n",
      "least one such path that is not blocked, then the property does not necessarily hold, or\n",
      "more precisely there will exist at least some distributions corresponding to the graph\n",
      "that do not satisfy this conditional independence relation. This is illustrated with an\n",
      "example in Figure 8.27. Note that this is exactly the same as the d-separation crite-\n",
      "rion except that there is no ‘explaining away’ phenomenon. Testing for conditional\n",
      "independence in undirected graphs is therefore simpler than in directed graphs.\n",
      "An alternative way to view the conditional independence test is to imagine re-\n",
      "moving all nodes in set Cfrom the graph together with any links that connect to\n",
      "those nodes. We then ask if there exists a path that connects any node in Ato any\n",
      "node in B. If there are no such paths, then the conditional independence property\n",
      "must hold.\n",
      "The Markov blanket for an undirected graph takes a particularly simple form,\n",
      "because a node will be conditionally independent of all other nodes conditioned only\n",
      "on the neighbouring nodes, as illustrated in Figure 8.28.\n",
      "8.3.2 Factorization properties\n",
      "We now seek a factorization rule for undirected graphs that will correspond to\n",
      "the above conditional independence test. Again, this will involve expressing the joint\n",
      "distribution p(x)as a product of functions deﬁned over sets of variables that are local\n",
      "to the graph. We therefore need to decide what is the appropriate notion of locality\n",
      "in this case.8.3. Markov Random Fields 385\n",
      "Figure 8.28 For an undirected graph, the Markov blanket of a node\n",
      "xiconsists of the set of neighbouring nodes. It has the\n",
      "property that the conditional distribution of xi, conditioned\n",
      "on all the remaining variables in the graph, is dependent\n",
      "only on the variables in the Markov blanket.\n",
      "If we consider two nodes xiandxjthat are not connected by a link, then these\n",
      "variables must be conditionally independent given all other nodes in the graph. This\n",
      "follows from the fact that there is no direct path between the two nodes, and all other\n",
      "paths pass through nodes that are observed, and hence those paths are blocked. This\n",
      "conditional independence property can be expressed as\n",
      "p(xi,xj|x\\{i,j})=p(xi|x\\{i,j})p(xj|x\\{i,j}) (8.38)\n",
      "wherex\\{i,j}denotes the set xof all variables with xiandxjremoved. The factor-\n",
      "ization of the joint distribution must therefore be such that xiandxjdo not appear\n",
      "in the same factor in order for the conditional independence property to hold for all\n",
      "possible distributions belonging to the graph.\n",
      "This leads us to consider a graphical concept called a clique , which is deﬁned\n",
      "as a subset of the nodes in a graph such that there exists a link between all pairs of\n",
      "nodes in the subset. In other words, the set of nodes in a clique is fully connected.\n",
      "Furthermore, a maximal clique is a clique such that it is not possible to include any\n",
      "other nodes from the graph in the set without it ceasing to be a clique. These concepts\n",
      "are illustrated by the undirected graph over four variables shown in Figure 8.29. This\n",
      "graph has ﬁve cliques of two nodes given by {x1,x2},{x2,x3},{x3,x4},{x4,x2},\n",
      "and{x1,x3}, as well as two maximal cliques given by {x1,x2,x3}and{x2,x3,x4}.\n",
      "The set {x1,x2,x3,x4}is not a clique because of the missing link from x1tox4.\n",
      "We can therefore deﬁne the factors in the decomposition of the joint distribution\n",
      "to be functions of the variables in the cliques. In fact, we can consider functions\n",
      "of the maximal cliques, without loss of generality, because other cliques must be\n",
      "subsets of maximal cliques. Thus, if {x1,x2,x3}is a maximal clique and we deﬁne\n",
      "an arbitrary function over this clique, then including another factor deﬁned over a\n",
      "subset of these variables would be redundant.\n",
      "Let us denote a clique by Cand the set of variables in that clique by xC. Then\n",
      "Figure 8.29 A four-node undirected graph showing a clique (outlined in\n",
      "green) and a maximal clique (outlined in blue).x1\n",
      "x2\n",
      "x3\n",
      "x4386 8. GRAPHICAL MODELS\n",
      "the joint distribution is written as a product of potential functions ψC(xC)over the\n",
      "maximal cliques of the graph\n",
      "p(x)=1\n",
      "Z∏\n",
      "CψC(xC). (8.39)\n",
      "Here the quantity Z, sometimes called the partition function , is a normalization con-\n",
      "stant and is given by\n",
      "Z=∑\n",
      "x∏\n",
      "CψC(xC) (8.40)\n",
      "which ensures that the distribution p(x)given by (8.39) is correctly normalized.\n",
      "By considering only potential functions which satisfy ψC(xC)⩾0we ensure that\n",
      "p(x)⩾0. In (8.40) we have assumed that xcomprises discrete variables, but the\n",
      "framework is equally applicable to continuous variables, or a combination of the two,\n",
      "in which the summation is replaced by the appropriate combination of summation\n",
      "and integration.\n",
      "Note that we do not restrict the choice of potential functions to those that have a\n",
      "speciﬁc probabilistic interpretation as marginal or conditional distributions. This is\n",
      "in contrast to directed graphs in which each factor represents the conditional distribu-\n",
      "tion of the corresponding variable, conditioned on the state of its parents. However,\n",
      "in special cases, for instance where the undirected graph is constructed by startingwith a directed graph, the potential functions may indeed have such an interpretation,\n",
      "as we shall see shortly.\n",
      "One consequence of the generality of the potential functions ψ\n",
      "C(xC)is that\n",
      "their product will in general not be correctly normalized. We therefore have to in-\n",
      "troduce an explicit normalization factor given by (8.40). Recall that for directed\n",
      "graphs, the joint distribution was automatically normalized as a consequence of thenormalization of each of the conditional distributions in the factorization.\n",
      "The presence of this normalization constant is one of the major limitations of\n",
      "undirected graphs. If we have a model with Mdiscrete nodes each having Kstates,\n",
      "then the evaluation of the normalization term involves summing over K\n",
      "Mstates and\n",
      "so (in the worst case) is exponential in the size of the model. The partition function\n",
      "is needed for parameter learning because it will be a function of any parameters thatgovern the potential functions ψ\n",
      "C(xC). However, for evaluation of local conditional\n",
      "distributions, the partition function is not needed because a conditional is the ratio\n",
      "of two marginals, and the partition function cancels between numerator and denom-inator when evaluating this ratio. Similarly, for evaluating local marginal probabil-\n",
      "ities we can work with the unnormalized joint distribution and then normalize the\n",
      "marginals explicitly at the end. Provided the marginals only involves a small number\n",
      "of variables, the evaluation of their normalization coefﬁcient will be feasible.\n",
      "So far, we have discussed the notion of conditional independence based on sim-\n",
      "ple graph separation and we have proposed a factorization of the joint distribution\n",
      "that is intended to correspond to this conditional independence structure. However,\n",
      "we have not made any formal connection between conditional independence andfactorization for undirected graphs. To do so we need to restrict attention to poten-\n",
      "tial functions ψ\n",
      "C(xC)that are strictly positive (i.e., never zero or negative for any8.3. Markov Random Fields 387\n",
      "choice of xC). Given this restriction, we can make a precise relationship between\n",
      "factorization and conditional independence.\n",
      "To do this we again return to the concept of a graphical model as a ﬁlter, corre-\n",
      "sponding to Figure 8.25. Consider the set of all possible distributions deﬁned over\n",
      "a ﬁxed set of variables corresponding to the nodes of a particular undirected graph.We can deﬁne UIto be the set of such distributions that are consistent with the set\n",
      "of conditional independence statements that can be read from the graph using graph\n",
      "separation. Similarly, we can deﬁne UFto be the set of such distributions that can\n",
      "be expressed as a factorization of the form (8.39) with respect to the maximal cliques\n",
      "of the graph. The Hammersley-Clifford theorem (Clifford, 1990) states that the sets\n",
      "UIandUFare identical.\n",
      "Because we are restricted to potential functions which are strictly positive it is\n",
      "convenient to express them as exponentials, so that\n",
      "ψ\n",
      "C(xC) = exp {−E(xC)} (8.41)\n",
      "where E(xC)is called an energy function , and the exponential representation is\n",
      "called the Boltzmann distribution . The joint distribution is deﬁned as the product of\n",
      "potentials, and so the total energy is obtained by adding the energies of each of the\n",
      "maximal cliques.\n",
      "In contrast to the factors in the joint distribution for a directed graph, the po-\n",
      "tentials in an undirected graph do not have a speciﬁc probabilistic interpretation.\n",
      "Although this gives greater ﬂexibility in choosing the potential functions, becausethere is no normalization constraint, it does raise the question of how to motivate a\n",
      "choice of potential function for a particular application. This can be done by view-\n",
      "ing the potential function as expressing which conﬁgurations of the local variables\n",
      "are preferred to others. Global conﬁgurations that have a relatively high probability\n",
      "are those that ﬁnd a good balance in satisfying the (possibly conﬂicting) inﬂuencesof the clique potentials. We turn now to a speciﬁc example to illustrate the use of\n",
      "undirected graphs.\n",
      "8.3.3 Illustration: Image de-noising\n",
      "We can illustrate the application of undirected graphs using an example of noise\n",
      "removal from a binary image (Besag, 1974; Geman and Geman, 1984; Besag, 1986).\n",
      "Although a very simple example, this is typical of more sophisticated applications.\n",
      "Let the observed noisy image be described by an array of binary pixel values yi∈\n",
      "{−1,+1}, where the index i=1,...,D runs over all pixels. We shall suppose\n",
      "that the image is obtained by taking an unknown noise-free image, described by\n",
      "binary pixel values xi∈{ −1,+1}and randomly ﬂipping the sign of pixels with\n",
      "some small probability. An example binary image, together with a noise corrupted\n",
      "image obtained by ﬂipping the sign of the pixels with probability 10%, is shown inFigure 8.30. Given the noisy image, our goal is to recover the original noise-free\n",
      "image.\n",
      "Because the noise level is small, we know that there will be a strong correlation\n",
      "between x\n",
      "iandyi. We also know that neighbouring pixels xiandxjin an image\n",
      "are strongly correlated. This prior knowledge can be captured using the Markov388 8. GRAPHICAL MODELS\n",
      "Figure 8.30 Illustration of image de-noising using a Markov random ﬁeld. The top row shows the original\n",
      "binary image on the left and the corrupted image after randomly changing 10% of the pixels on the right. Thebottom row shows the restored images obtained using iterated conditional models (ICM) on the left and usingthe graph-cut algorithm on the right. ICM produces an image where 96% of the pixels agree with the originalimage, whereas the corresponding number for graph-cut is 99%.\n",
      "random ﬁeld model whose undirected graph is shown in Figure 8.31. This graph has\n",
      "two types of cliques, each of which contains two variables. The cliques of the form{x\n",
      "i,yi}have an associated energy function that expresses the correlation between\n",
      "these variables. We choose a very simple energy function for these cliques of the\n",
      "form−ηxiyiwhere ηis a positive constant. This has the desired effect of giving a\n",
      "lower energy (thus encouraging a higher probability) when xiandyihave the same\n",
      "sign and a higher energy when they have the opposite sign.\n",
      "The remaining cliques comprise pairs of variables {xi,xj}where iandjare\n",
      "indices of neighbouring pixels. Again, we want the energy to be lower when the\n",
      "pixels have the same sign than when they have the opposite sign, and so we choose\n",
      "an energy given by −βxixjwhere βis a positive constant.\n",
      "Because a potential function is an arbitrary, nonnegative function over a maximal\n",
      "clique, we can multiply it by any nonnegative functions of subsets of the clique, or8.3. Markov Random Fields 389\n",
      "Figure 8.31 An undirected graphical model representing a\n",
      "Markov random ﬁeld for image de-noising, in\n",
      "which xiis a binary variable denoting the state\n",
      "of pixel iin the unknown noise-free image, and yi\n",
      "denotes the corresponding value of pixel iin the\n",
      "observed noisy image.\n",
      "xiyi\n",
      "equivalently we can add the corresponding energies. In this example, this allows us\n",
      "to add an extra term hxifor each pixel iin the noise-free image. Such a term has\n",
      "the effect of biasing the model towards pixel values that have one particular sign in\n",
      "preference to the other.\n",
      "The complete energy function for the model then takes the form\n",
      "E(x,y)=h∑\n",
      "ixi−β∑\n",
      "{i,j}xixj−η∑\n",
      "ixiyi (8.42)\n",
      "which deﬁnes a joint distribution over xandygiven by\n",
      "p(x,y)=1\n",
      "Zexp{−E(x,y)}. (8.43)\n",
      "We now ﬁx the elements of yto the observed values given by the pixels of the\n",
      "noisy image, which implicitly deﬁnes a conditional distribution p(x|y)over noise-\n",
      "free images. This is an example of the Ising model , which has been widely studied in\n",
      "statistical physics. For the purposes of image restoration, we wish to ﬁnd an image x\n",
      "having a high probability (ideally the maximum probability). To do this we shall use\n",
      "a simple iterative technique called iterated conditional modes ,o r ICM (Kittler and\n",
      "F¨oglein, 1984), which is simply an application of coordinate-wise gradient ascent.\n",
      "The idea is ﬁrst to initialize the variables {xi}, which we do by simply setting xi=\n",
      "yifor all i. Then we take one node xjat a time and we evaluate the total energy\n",
      "for the two possible states xj=+ 1 andxj=−1, keeping all other node variables\n",
      "ﬁxed, and set xjto whichever state has the lower energy. This will either leave\n",
      "the probability unchanged, if xjis unchanged, or will increase it. Because only\n",
      "one variable is changed, this is a simple local computation that can be performed Exercise 8.13\n",
      "efﬁciently. We then repeat the update for another site, and so on, until some suitable\n",
      "stopping criterion is satisﬁed. The nodes may be updated in a systematic way, for\n",
      "instance by repeatedly raster scanning through the image, or by choosing nodes at\n",
      "random.\n",
      "If we have a sequence of updates in which every site is visited at least once,\n",
      "and in which no changes to the variables are made, then by deﬁnition the algorithm390 8. GRAPHICAL MODELS\n",
      "Figure 8.32 (a) Example of a directed\n",
      "graph. (b) The equivalent undirected\n",
      "graph.(a)x1 x2 xN−1 xN\n",
      "(b)x1 x2 xN−1 xN\n",
      "will have converged to a local maximum of the probability. This need not, however,\n",
      "correspond to the global maximum.\n",
      "For the purposes of this simple illustration, we have ﬁxed the parameters to be\n",
      "β=1.0,η=2.1andh=0. Note that leaving h=0 simply means that the prior\n",
      "probabilities of the two states of xiare equal. Starting with the observed noisy image\n",
      "as the initial conﬁguration, we run ICM until convergence, leading to the de-noised\n",
      "image shown in the lower left panel of Figure 8.30. Note that if we set β=0,\n",
      "which effectively removes the links between neighbouring pixels, then the global\n",
      "most probable solution is given by xi=yifor all i, corresponding to the observed\n",
      "noisy image. Exercise 8.14\n",
      "Later we shall discuss a more effective algorithm for ﬁnding high probability so-\n",
      "lutions called the max-product algorithm, which typically leads to better solutions, Section 8.4\n",
      "although this is still not guaranteed to ﬁnd the global maximum of the posterior dis-\n",
      "tribution. However, for certain classes of model, including the one given by (8.42),\n",
      "there exist efﬁcient algorithms based on graph cuts that are guaranteed to ﬁnd the\n",
      "global maximum (Greig et al. , 1989; Boykov et al. , 2001; Kolmogorov and Zabih,\n",
      "2004). The lower right panel of Figure 8.30 shows the result of applying a graph-cut\n",
      "algorithm to the de-noising problem.\n",
      "8.3.4 Relation to directed graphs\n",
      "We have introduced two graphical frameworks for representing probability dis-\n",
      "tributions, corresponding to directed and undirected graphs, and it is instructive to\n",
      "discuss the relation between these. Consider ﬁrst the problem of taking a model that\n",
      "is speciﬁed using a directed graph and trying to convert it to an undirected graph. In\n",
      "some cases this is straightforward, as in the simple example in Figure 8.32. Here the\n",
      "joint distribution for the directed graph is given as a product of conditionals in the\n",
      "form\n",
      "p(x)=p(x1)p(x2|x1)p(x3|x2)···p(xN|xN−1). (8.44)\n",
      "Now let us convert this to an undirected graph representation, as shown in Fig-\n",
      "ure 8.32. In the undirected graph, the maximal cliques are simply the pairs of neigh-\n",
      "bouring nodes, and so from (8.39) we wish to write the joint distribution in the form\n",
      "p(x)=1\n",
      "Zψ1,2(x1,x2)ψ2,3(x2,x3)···ψN−1,N(xN−1,xN). (8.45)8.3. Markov Random Fields 391\n",
      "Figure 8.33 Example of a simple\n",
      "directed graph (a) and the corre-\n",
      "sponding moral graph (b).x1 x3\n",
      "x4x2\n",
      "(a)x1 x3\n",
      "x4x2\n",
      "(b)\n",
      "This is easily done by identifying\n",
      "ψ1,2(x1,x2)= p(x1)p(x2|x1)\n",
      "ψ2,3(x2,x3)= p(x3|x2)\n",
      "...\n",
      "ψN−1,N(xN−1,xN)= p(xN|xN−1)\n",
      "where we have absorbed the marginal p(x1)for the ﬁrst node into the ﬁrst potential\n",
      "function. Note that in this case, the partition function Z=1.\n",
      "Let us consider how to generalize this construction, so that we can convert any\n",
      "distribution speciﬁed by a factorization over a directed graph into one speciﬁed by a\n",
      "factorization over an undirected graph. This can be achieved if the clique potentials\n",
      "of the undirected graph are given by the conditional distributions of the directed\n",
      "graph. In order for this to be valid, we must ensure that the set of variables that\n",
      "appears in each of the conditional distributions is a member of at least one clique of\n",
      "the undirected graph. For nodes on the directed graph having just one parent, this is\n",
      "achieved simply by replacing the directed link with an undirected link. However, for\n",
      "nodes in the directed graph having more than one parent, this is not sufﬁcient. These\n",
      "are nodes that have ‘head-to-head’ paths encountered in our discussion of conditional\n",
      "independence. Consider a simple directed graph over 4 nodes shown in Figure 8.33.\n",
      "The joint distribution for the directed graph takes the form\n",
      "p(x)=p(x1)p(x2)p(x3)p(x4|x1,x2,x3). (8.46)\n",
      "We see that the factor p(x4|x1,x2,x3)involves the four variables x1,x2,x3, and\n",
      "x4, and so these must all belong to a single clique if this conditional distribution is\n",
      "to be absorbed into a clique potential. To ensure this, we add extra links between\n",
      "all pairs of parents of the node x4. Anachronistically, this process of ‘marrying\n",
      "the parents’ has become known as moralization , and the resulting undirected graph,\n",
      "after dropping the arrows, is called the moral graph . It is important to observe that\n",
      "the moral graph in this example is fully connected and so exhibits no conditional\n",
      "independence properties, in contrast to the original directed graph.\n",
      "Thus in general to convert a directed graph into an undirected graph, we ﬁrst add\n",
      "additional undirected links between all pairs of parents for each node in the graph and392 8. GRAPHICAL MODELS\n",
      "then drop the arrows on the original links to give the moral graph. Then we initialize\n",
      "all of the clique potentials of the moral graph to 1. We then take each conditional\n",
      "distribution factor in the original directed graph and multiply it into one of the clique\n",
      "potentials. There will always exist at least one maximal clique that contains all of\n",
      "the variables in the factor as a result of the moralization step. Note that in all cases\n",
      "the partition function is given by Z=1.\n",
      "The process of converting a directed graph into an undirected graph plays an\n",
      "important role in exact inference techniques such as the junction tree algorithm . Section 8.4\n",
      "Converting from an undirected to a directed representation is much less common\n",
      "and in general presents problems due to the normalization constraints.\n",
      "We saw that in going from a directed to an undirected representation we had to\n",
      "discard some conditional independence properties from the graph. Of course, we\n",
      "could always trivially convert any distribution over a directed graph into one over an\n",
      "undirected graph by simply using a fully connected undirected graph. This would,\n",
      "however, discard all conditional independence properties and so would be vacuous.\n",
      "The process of moralization adds the fewest extra links and so retains the maximum\n",
      "number of independence properties.\n",
      "We have seen that the procedure for determining the conditional independence\n",
      "properties is different between directed and undirected graphs. It turns out that the\n",
      "two types of graph can express different conditional independence properties, and\n",
      "it is worth exploring this issue in more detail. To do so, we return to the view of\n",
      "a speciﬁc (directed or undirected) graph as a ﬁlter, so that the set of all possible Section 8.2\n",
      "distributions over the given variables could be reduced to a subset that respects the\n",
      "conditional independencies implied by the graph. A graph is said to be a D map\n",
      "(for ‘dependency map’) of a distribution if every conditional independence statement\n",
      "satisﬁed by the distribution is reﬂected in the graph. Thus a completely disconnected\n",
      "graph (no links) will be a trivial D map for any distribution.\n",
      "Alternatively, we can consider a speciﬁc distribution and ask which graphs have\n",
      "the appropriate conditional independence properties. If every conditional indepen-\n",
      "dence statement implied by a graph is satisﬁed by a speciﬁc distribution, then the\n",
      "graph is said to be an I map (for ‘independence map’) of that distribution. Clearly a\n",
      "fully connected graph will be a trivial I map for any distribution.\n",
      "If it is the case that every conditional independence property of the distribution\n",
      "is reﬂected in the graph, and vice versa, then the graph is said to be a perfect map for\n",
      "Figure 8.34 Venn diagram illustrating the set of all distributions\n",
      "P over a given set of variables, together with the\n",
      "set of distributions D that can be represented as a\n",
      "perfect map using a directed graph, and the set U\n",
      "that can be represented as a perfect map using an\n",
      "undirected graph.\n",
      "PU D8.4. Inference in Graphical Models 393\n",
      "Figure 8.35 A directed graph whose conditional independence\n",
      "properties cannot be expressed using an undirected\n",
      "graph over the same three variables.\n",
      "CAB\n",
      "that distribution. A perfect map is therefore both an I map and a D map.\n",
      "Consider the set of distributions such that for each distribution there exists a\n",
      "directed graph that is a perfect map. This set is distinct from the set of distributions\n",
      "such that for each distribution there exists an undirected graph that is a perfect map.\n",
      "In addition there are distributions for which neither directed nor undirected graphs\n",
      "offer a perfect map. This is illustrated as a Venn diagram in Figure 8.34.\n",
      "Figure 8.35 shows an example of a directed graph that is a perfect map for\n",
      "a distribution satisfying the conditional independence properties A⊥⊥B|∅and\n",
      "A̸⊥⊥B|C. There is no corresponding undirected graph over the same three vari-\n",
      "ables that is a perfect map.\n",
      "Conversely, consider the undirected graph over four variables shown in Fig-\n",
      "ure 8.36. This graph exhibits the properties A̸⊥⊥B|∅,C⊥⊥D|A∪Band\n",
      "A⊥⊥B|C∪D. There is no directed graph over four variables that implies the same\n",
      "set of conditional independence properties.\n",
      "The graphical framework can be extended in a consistent way to graphs that\n",
      "include both directed and undirected links. These are called chain graphs (Lauritzen\n",
      "and Wermuth, 1989; Frydenberg, 1990), and contain the directed and undirected\n",
      "graphs considered so far as special cases. Although such graphs can represent a\n",
      "broader class of distributions than either directed or undirected alone, there remain\n",
      "distributions for which even a chain graph cannot provide a perfect map. Chain\n",
      "graphs are not discussed further in this book.\n",
      "Figure 8.36 An undirected graph whose conditional independence\n",
      "properties cannot be expressed in terms of a directed\n",
      "graph over the same variables.\n",
      "AC\n",
      "B\n",
      "D\n",
      "8.4. Inference in Graphical Models\n",
      "We turn now to the problem of inference in graphical models, in which some of\n",
      "the nodes in a graph are clamped to observed values, and we wish to compute the\n",
      "posterior distributions of one or more subsets of other nodes. As we shall see, we\n",
      "can exploit the graphical structure both to ﬁnd efﬁcient algorithms for inference, and394 8. GRAPHICAL MODELS\n",
      "Figure 8.37 A graphical representation of Bayes’ theorem.\n",
      "See the text for details.x\n",
      "yx\n",
      "yx\n",
      "y\n",
      "(a) (b) (c)\n",
      "to make the structure of those algorithms transparent. Speciﬁcally, we shall see that\n",
      "many algorithms can be expressed in terms of the propagation of local messages\n",
      "around the graph. In this section, we shall focus primarily on techniques for exact\n",
      "inference, and in Chapter 10 we shall consider a number of approximate inference\n",
      "algorithms.\n",
      "To start with, let us consider the graphical interpretation of Bayes’ theorem.\n",
      "Suppose we decompose the joint distribution p(x, y)over two variables xandyinto\n",
      "a product of factors in the form p(x, y)=p(x)p(y|x). This can be represented by\n",
      "the directed graph shown in Figure 8.37(a). Now suppose we observe the value of\n",
      "y, as indicated by the shaded node in Figure 8.37(b). We can view the marginal\n",
      "distribution p(x)as a prior over the latent variable x, and our goal is to infer the\n",
      "corresponding posterior distribution over x. Using the sum and product rules of\n",
      "probability we can evaluate\n",
      "p(y)=∑\n",
      "x′p(y|x′)p(x′) (8.47)\n",
      "which can then be used in Bayes’ theorem to calculate\n",
      "p(x|y)=p(y|x)p(x)\n",
      "p(y). (8.48)\n",
      "Thus the joint distribution is now expressed in terms of p(y)andp(x|y). From a\n",
      "graphical perspective, the joint distribution p(x, y)is now represented by the graph\n",
      "shown in Figure 8.37(c), in which the direction of the arrow is reversed. This is the\n",
      "simplest example of an inference problem for a graphical model.\n",
      "8.4.1 Inference on a chain\n",
      "Now consider a more complex problem involving the chain of nodes of the form\n",
      "shown in Figure 8.32. This example will lay the foundation for a discussion of exact\n",
      "inference in more general graphs later in this section.\n",
      "Speciﬁcally, we shall consider the undirected graph in Figure 8.32(b). We have\n",
      "already seen that the directed chain can be transformed into an equivalent undirected\n",
      "chain. Because the directed graph does not have any nodes with more than one\n",
      "parent, this does not require the addition of any extra links, and the directed and\n",
      "undirected versions of this graph express exactly the same set of conditional inde-\n",
      "pendence statements.8.4. Inference in Graphical Models 395\n",
      "The joint distribution for this graph takes the form\n",
      "p(x)=1\n",
      "Zψ1,2(x1,x2)ψ2,3(x2,x3)···ψN−1,N(xN−1,xN). (8.49)\n",
      "We shall consider the speciﬁc case in which the Nnodes represent discrete vari-\n",
      "ables each having Kstates, in which case each potential function ψn−1,n(xn−1,xn)\n",
      "comprises an K×Ktable, and so the joint distribution has (N−1)K2parameters.\n",
      "Let us consider the inference problem of ﬁnding the marginal distribution p(xn)\n",
      "for a speciﬁc node xnthat is part way along the chain. Note that, for the moment,\n",
      "there are no observed nodes. By deﬁnition, the required marginal is obtained by\n",
      "summing the joint distribution over all variables except xn, so that\n",
      "p(xn)=∑\n",
      "x1···∑\n",
      "xn−1∑\n",
      "xn+1···∑\n",
      "xNp(x). (8.50)\n",
      "In a naive implementation, we would ﬁrst evaluate the joint distribution and\n",
      "then perform the summations explicitly. The joint distribution can be represented as\n",
      "a set of numbers, one for each possible value for x. Because there are Nvariables\n",
      "each with Kstates, there are KNvalues for xand so evaluation and storage of the\n",
      "joint distribution, as well as marginalization to obtain p(xn), all involve storage and\n",
      "computation that scale exponentially with the length Nof the chain.\n",
      "We can, however, obtain a much more efﬁcient algorithm by exploiting the con-\n",
      "ditional independence properties of the graphical model. If we substitute the factor-\n",
      "ized expression (8.49) for the joint distribution into (8.50), then we can rearrange theorder of the summations and the multiplications to allow the required marginal to be\n",
      "evaluated much more efﬁciently. Consider for instance the summation over x\n",
      "N. The\n",
      "potential ψN−1,N(xN−1,xN)is the only one that depends on xN, and so we can\n",
      "perform the summation∑\n",
      "xNψN−1,N(xN−1,xN) (8.51)\n",
      "ﬁrst to give a function of xN−1. We can then use this to perform the summation\n",
      "overxN−1, which will involve only this new function together with the potential\n",
      "ψN−2,N−1(xN−2,xN−1), because this is the only other place that xN−1appears.\n",
      "Similarly, the summation over x1involves only the potential ψ1,2(x1,x2)and so\n",
      "can be performed separately to give a function of x2, and so on. Because each\n",
      "summation effectively removes a variable from the distribution, this can be viewed\n",
      "as the removal of a node from the graph.\n",
      "If we group the potentials and summations together in this way, we can express396 8. GRAPHICAL MODELS\n",
      "the desired marginal in the form\n",
      "p(xn)=1\n",
      "Z⎡\n",
      "⎣∑\n",
      "xn−1ψn−1,n(xn−1,xn)···[∑\n",
      "x2ψ2,3(x2,x3)[∑\n",
      "x1ψ1,2(x1,x2)]]\n",
      "···⎤\n",
      "⎦\n",
      "  \n",
      "µα(xn)⎡\n",
      "⎣∑\n",
      "xn+1ψn,n+1(xn,xn+1)···[∑\n",
      "xNψN−1,N(xN−1,xN)]\n",
      "···⎤\n",
      "⎦\n",
      "  \n",
      "µβ(xn). (8.52)\n",
      "The reader is encouraged to study this re-ordering carefully as the underlying idea\n",
      "forms the basis for the later discussion of the general sum-product algorithm. Here\n",
      "the key concept that we are exploiting is that multiplication is distributive over addi-\n",
      "tion, so that\n",
      "ab+ac=a(b+c) (8.53)\n",
      "in which the left-hand side involves three arithmetic operations whereas the right-\n",
      "hand side reduces this to two operations.\n",
      "Let us work out the computational cost of evaluating the required marginal using\n",
      "this re-ordered expression. We have to perform N−1summations each of which is\n",
      "overKstates and each of which involves a function of two variables. For instance,\n",
      "the summation over x1involves only the function ψ1,2(x1,x2), which is a table of\n",
      "K×Knumbers. We have to sum this table over x1for each value of x2and so this\n",
      "hasO(K2)cost. The resulting vector of Knumbers is multiplied by the matrix of\n",
      "numbers ψ2,3(x2,x3)and so is again O(K2). Because there are N−1summations\n",
      "and multiplications of this kind, the total cost of evaluating the marginal p(xn)is\n",
      "O(NK2). This is linear in the length of the chain, in contrast to the exponential cost\n",
      "of a naive approach. We have therefore been able to exploit the many conditional\n",
      "independence properties of this simple graph in order to obtain an efﬁcient calcula-\n",
      "tion. If the graph had been fully connected, there would have been no conditional\n",
      "independence properties, and we would have been forced to work directly with the\n",
      "full joint distribution.\n",
      "We now give a powerful interpretation of this calculation in terms of the passing\n",
      "of local messages around on the graph. From (8.52) we see that the expression for the\n",
      "marginal p(xn)decomposes into the product of two factors times the normalization\n",
      "constant\n",
      "p(xn)=1\n",
      "Zµα(xn)µβ(xn). (8.54)\n",
      "We shall interpret µα(xn)as a message passed forwards along the chain from node\n",
      "xn−1to node xn. Similarly, µβ(xn)can be viewed as a message passed backwards8.4. Inference in Graphical Models 397\n",
      "Figure 8.38 The marginal distribution\n",
      "p(xn)for a node xnalong the chain is ob-\n",
      "tained by multiplying the two messages\n",
      "µα(xn)andµβ(xn), and then normaliz-\n",
      "ing. These messages can themselves\n",
      "be evaluated recursively by passing mes-\n",
      "sages from both ends of the chain to-\n",
      "wards node xn.x1 xn−1 xn xn+1 xNµα(xn−1)µα(xn)µβ(xn)µβ(xn+1)\n",
      "along the chain to node xnfrom node xn+1. Note that each of the messages com-\n",
      "prises a set of Kvalues, one for each choice of xn, and so the product of two mes-\n",
      "sages should be interpreted as the point-wise multiplication of the elements of the\n",
      "two messages to give another set of Kvalues.\n",
      "The message µα(xn)can be evaluated recursively because\n",
      "µα(xn)=∑\n",
      "xn−1ψn−1,n(xn−1,xn)⎡\n",
      "⎣∑\n",
      "xn−2···⎤\n",
      "⎦\n",
      "=∑\n",
      "xn−1ψn−1,n(xn−1,xn)µα(xn−1). (8.55)\n",
      "We therefore ﬁrst evaluate\n",
      "µα(x2)=∑\n",
      "x1ψ1,2(x1,x2) (8.56)\n",
      "and then apply (8.55) repeatedly until we reach the desired node. Note carefully the\n",
      "structure of the message passing equation. The outgoing message µα(xn)in (8.55)\n",
      "is obtained by multiplying the incoming message µα(xn−1)by the local potential\n",
      "involving the node variable and the outgoing variable and then summing over the\n",
      "node variable.\n",
      "Similarly, the message µβ(xn)can be evaluated recursively by starting with\n",
      "nodexNand using\n",
      "µβ(xn)=∑\n",
      "xn+1ψn+1,n(xn+1,xn)⎡\n",
      "⎣∑\n",
      "xn+2···⎤\n",
      "⎦\n",
      "=∑\n",
      "xn+1ψn+1,n(xn+1,xn)µβ(xn+1). (8.57)\n",
      "This recursive message passing is illustrated in Figure 8.38. The normalization con-\n",
      "stantZis easily evaluated by summing the right-hand side of (8.54) over all states\n",
      "ofxn, an operation that requires only O(K)computation.\n",
      "Graphs of the form shown in Figure 8.38 are called Markov chains , and the\n",
      "corresponding message passing equations represent an example of the Chapman-\n",
      "Kolmogorov equations for Markov processes (Papoulis, 1984).398 8. GRAPHICAL MODELS\n",
      "Now suppose we wish to evaluate the marginals p(xn)for every node n∈\n",
      "{1,...,N }in the chain. Simply applying the above procedure separately for each\n",
      "node will have computational cost that is O(N2M2). However, such an approach\n",
      "would be very wasteful of computation. For instance, to ﬁnd p(x1)we need to prop-\n",
      "agate a message µβ(·)from node xNback to node x2. Similarly, to evaluate p(x2)\n",
      "we need to propagate a messages µβ(·)from node xNback to node x3. This will\n",
      "involve much duplicated computation because most of the messages will be identical\n",
      "in the two cases.\n",
      "Suppose instead we ﬁrst launch a message µβ(xN−1)starting from node xN\n",
      "and propagate corresponding messages all the way back to node x1, and suppose we\n",
      "similarly launch a message µα(x2)starting from node x1and propagate the corre-\n",
      "sponding messages all the way forward to node xN. Provided we store all of the\n",
      "intermediate messages along the way, then any node can evaluate its marginal sim-ply by applying (8.54). The computational cost is only twice that for ﬁnding the\n",
      "marginal of a single node, rather than Ntimes as much. Observe that a message\n",
      "has passed once in each direction across each link in the graph. Note also that thenormalization constant Zneed be evaluated only once, using any convenient node.\n",
      "If some of the nodes in the graph are observed, then the corresponding variables\n",
      "are simply clamped to their observed values and there is no summation. To seethis, note that the effect of clamping a variable x\n",
      "nto an observed value ˆxncan\n",
      "be expressed by multiplying the joint distribution by (one or more copies of) an\n",
      "additional function I(xn,ˆxn), which takes the value 1whenxn=ˆxnand the value\n",
      "0otherwise. One such function can then be absorbed into each of the potentials that\n",
      "contain xn. Summations over xnthen contain only one term in which xn=ˆxn.\n",
      "Now suppose we wish to calculate the joint distribution p(xn−1,xn)for two\n",
      "neighbouring nodes on the chain. This is similar to the evaluation of the marginal\n",
      "for a single node, except that there are now two variables that are not summed out.\n",
      "A few moments thought will show that the required joint distribution can be written Exercise 8.15\n",
      "in the form\n",
      "p(xn−1,xn)=1\n",
      "Zµα(xn−1)ψn−1,n(xn−1,xn)µβ(xn). (8.58)\n",
      "Thus we can obtain the joint distributions over all of the sets of variables in each\n",
      "of the potentials directly once we have completed the message passing required to\n",
      "obtain the marginals.\n",
      "This is a useful result because in practice we may wish to use parametric forms\n",
      "for the clique potentials, or equivalently for the conditional distributions if we started\n",
      "from a directed graph. In order to learn the parameters of these potentials in situa-\n",
      "tions where not all of the variables are observed, we can employ the EM algorithm , Chapter 9\n",
      "and it turns out that the local joint distributions of the cliques, conditioned on anyobserved data, is precisely what is needed in the E step. We shall consider some\n",
      "examples of this in detail in Chapter 13.\n",
      "8.4.2 Trees\n",
      "We have seen that exact inference on a graph comprising a chain of nodes can be\n",
      "performed efﬁciently in time that is linear in the number of nodes, using an algorithm8.4. Inference in Graphical Models 399\n",
      "Figure 8.39 Examples of tree-\n",
      "structured graphs, showing (a) an\n",
      "undirected tree, (b) a directed tree,\n",
      "and (c) a directed polytree.\n",
      "(a) (b) (c)\n",
      "that can be interpreted in terms of messages passed along the chain. More generally,\n",
      "inference can be performed efﬁciently using local message passing on a broaderclass of graphs called trees . In particular, we shall shortly generalize the message\n",
      "passing formalism derived above for chains to give the sum-product algorithm, which\n",
      "provides an efﬁcient framework for exact inference in tree-structured graphs.\n",
      "In the case of an undirected graph, a tree is deﬁned as a graph in which there\n",
      "is one, and only one, path between any pair of nodes. Such graphs therefore do nothave loops. In the case of directed graphs, a tree is deﬁned such that there is a single\n",
      "node, called the root, which has no parents, and all other nodes have one parent. If\n",
      "we convert a directed tree into an undirected graph, we see that the moralization stepwill not add any links as all nodes have at most one parent, and as a consequence the\n",
      "corresponding moralized graph will be an undirected tree. Examples of undirected\n",
      "and directed trees are shown in Figure 8.39(a) and 8.39(b). Note that a distributionrepresented as a directed tree can easily be converted into one represented by an\n",
      "undirected tree, and vice versa. Exercise 8.18\n",
      "If there are nodes in a directed graph that have more than one parent, but there is\n",
      "still only one path (ignoring the direction of the arrows) between any two nodes, then\n",
      "the graph is a called a polytree , as illustrated in Figure 8.39(c). Such a graph will\n",
      "have more than one node with the property of having no parents, and furthermore,the corresponding moralized undirected graph will have loops.\n",
      "8.4.3 Factor graphs\n",
      "The sum-product algorithm that we derive in the next section is applicable to\n",
      "undirected and directed trees and to polytrees. It can be cast in a particularly simple\n",
      "and general form if we ﬁrst introduce a new graphical construction called a factor\n",
      "graph (Frey, 1998; Kschischnang et al. , 2001).\n",
      "Both directed and undirected graphs allow a global function of several vari-\n",
      "ables to be expressed as a product of factors over subsets of those variables. Factorgraphs make this decomposition explicit by introducing additional nodes for the fac-\n",
      "tors themselves in addition to the nodes representing the variables. They also allow\n",
      "us to be more explicit about the details of the factorization, as we shall see.\n",
      "Let us write the joint distribution over a set of variables in the form of a product\n",
      "of factors\n",
      "p(x)=∏\n",
      "sfs(xs) (8.59)\n",
      "wherexsdenotes a subset of the variables. For convenience, we shall denote the400 8. GRAPHICAL MODELS\n",
      "Figure 8.40 Example of a factor graph, which corresponds\n",
      "to the factorization (8.60).x1 x2 x3\n",
      "fa fb fc fd\n",
      "individual variables by xi, however, as in earlier discussions, these can comprise\n",
      "groups of variables (such as vectors or matrices). Each factor fsis a function of a\n",
      "corresponding set of variables xs.\n",
      "Directed graphs, whose factorization is deﬁned by (8.5), represent special cases\n",
      "of (8.59) in which the factors fs(xs)are local conditional distributions. Similarly,\n",
      "undirected graphs, given by (8.39), are a special case in which the factors are po-\n",
      "tential functions over the maximal cliques (the normalizing coefﬁcient 1/Zcan be\n",
      "viewed as a factor deﬁned over the empty set of variables).\n",
      "In a factor graph, there is a node (depicted as usual by a circle) for every variable\n",
      "in the distribution, as was the case for directed and undirected graphs. There are also\n",
      "additional nodes (depicted by small squares) for each factor fs(xs)in the joint dis-\n",
      "tribution. Finally, there are undirected links connecting each factor node to all of the\n",
      "variables nodes on which that factor depends. Consider, for example, a distribution\n",
      "that is expressed in terms of the factorization\n",
      "p(x)=fa(x1,x2)fb(x1,x2)fc(x2,x3)fd(x3). (8.60)\n",
      "This can be expressed by the factor graph shown in Figure 8.40. Note that there are\n",
      "two factors fa(x1,x2)andfb(x1,x2)that are deﬁned over the same set of variables.\n",
      "In an undirected graph, the product of two such factors would simply be lumped\n",
      "together into the same clique potential. Similarly, fc(x2,x3)andfd(x3)could be\n",
      "combined into a single potential over x2andx3. The factor graph, however, keeps\n",
      "such factors explicit and so is able to convey more detailed information about the\n",
      "underlying factorization.\n",
      "x1 x2\n",
      "x3\n",
      "(a)x1 x2\n",
      "x3f\n",
      "(b)x1 x2\n",
      "x3fa\n",
      "fb\n",
      "(c)\n",
      "Figure 8.41 (a) An undirected graph with a single clique potential ψ(x1,x2,x3). (b) A factor graph with factor\n",
      "f(x1,x2,x3)=ψ(x1,x2,x3)representing the same distribution as the undirected graph. (c) A different factor\n",
      "graph representing the same distribution, whose factors satisfy fa(x1,x2,x3)fb(x1,x2)=ψ(x1,x2,x3).8.4. Inference in Graphical Models 401\n",
      "x1 x2\n",
      "x3\n",
      "(a)x1 x2\n",
      "x3f\n",
      "(b)x1 x2\n",
      "x3fc\n",
      "fa fb\n",
      "(c)\n",
      "Figure 8.42 (a) A directed graph with the factorization p(x1)p(x2)p(x3|x1,x2). (b) A factor graph representing\n",
      "the same distribution as the directed graph, whose factor satisﬁes f(x1,x2,x3)=p(x1)p(x2)p(x3|x1,x2). (c)\n",
      "A different factor graph representing the same distribution with factors fa(x1)=p(x1),fb(x2)=p(x2)and\n",
      "fc(x1,x2,x3)=p(x3|x1,x2).\n",
      "Factor graphs are said to be bipartite because they consist of two distinct kinds\n",
      "of nodes, and all links go between nodes of opposite type. In general, factor graphs\n",
      "can therefore always be drawn as two rows of nodes (variable nodes at the top and\n",
      "factor nodes at the bottom) with links between the rows, as shown in the example in\n",
      "Figure 8.40. In some situations, however, other ways of laying out the graph may\n",
      "be more intuitive, for example when the factor graph is derived from a directed or\n",
      "undirected graph, as we shall see.\n",
      "If we are given a distribution that is expressed in terms of an undirected graph,\n",
      "then we can readily convert it to a factor graph. To do this, we create variable nodes\n",
      "corresponding to the nodes in the original undirected graph, and then create addi-\n",
      "tional factor nodes corresponding to the maximal cliques xs. The factors fs(xs)are\n",
      "then set equal to the clique potentials. Note that there may be several different factor\n",
      "graphs that correspond to the same undirected graph. These concepts are illustrated\n",
      "in Figure 8.41.\n",
      "Similarly, to convert a directed graph to a factor graph, we simply create variable\n",
      "nodes in the factor graph corresponding to the nodes of the directed graph, and then\n",
      "create factor nodes corresponding to the conditional distributions, and then ﬁnally\n",
      "add the appropriate links. Again, there can be multiple factor graphs all of which\n",
      "correspond to the same directed graph. The conversion of a directed graph to a\n",
      "factor graph is illustrated in Figure 8.42.\n",
      "We have already noted the importance of tree-structured graphs for performing\n",
      "efﬁcient inference. If we take a directed or undirected tree and convert it into a factor\n",
      "graph, then the result will again be a tree (in other words, the factor graph will have\n",
      "no loops, and there will be one and only one path connecting any two nodes). In\n",
      "the case of a directed polytree, conversion to an undirected graph results in loops\n",
      "due to the moralization step, whereas conversion to a factor graph again results in a\n",
      "tree, as illustrated in Figure 8.43. In fact, local cycles in a directed graph due to\n",
      "links connecting parents of a node can be removed on conversion to a factor graph\n",
      "by deﬁning the appropriate factor function, as shown in Figure 8.44.\n",
      "We have seen that multiple different factor graphs can represent the same di-\n",
      "rected or undirected graph. This allows factor graphs to be more speciﬁc about the402 8. GRAPHICAL MODELS\n",
      "(a) (b) (c)\n",
      "Figure 8.43 (a) A directed polytree. (b) The result of converting the polytree into an undirected graph showing\n",
      "the creation of loops. (c) The result of converting the polytree into a factor graph, which retains the tree structure.\n",
      "precise form of the factorization. Figure 8.45 shows an example of a fully connected\n",
      "undirected graph along with two different factor graphs. In (b), the joint distri-\n",
      "bution is given by a general form p(x)=f(x1,x2,x3), whereas in (c), it is given\n",
      "by the more speciﬁc factorization p(x)=fa(x1,x2)fb(x1,x3)fc(x2,x3). It should\n",
      "be emphasized that the factorization in (c) does not correspond to any conditional\n",
      "independence properties.\n",
      "8.4.4 The sum-product algorithm\n",
      "We shall now make use of the factor graph framework to derive a powerful class\n",
      "of efﬁcient, exact inference algorithms that are applicable to tree-structured graphs.\n",
      "Here we shall focus on the problem of evaluating local marginals over nodes or\n",
      "subsets of nodes, which will lead us to the sum-product algorithm. Later we shall\n",
      "modify the technique to allow the most probable state to be found, giving rise to the\n",
      "max-sum algorithm.\n",
      "Also we shall suppose that all of the variables in the model are discrete, and\n",
      "so marginalization corresponds to performing sums. The framework, however, is\n",
      "equally applicable to linear-Gaussian models in which case marginalization involves\n",
      "integration, and we shall consider an example of this in detail when we discuss linear\n",
      "dynamical systems. Section 13.3\n",
      "Figure 8.44 (a) A fragment of a di-\n",
      "rected graph having a lo-\n",
      "cal cycle. (b) Conversion\n",
      "to a fragment of a factor\n",
      "graph having a tree struc-\n",
      "ture, in which f(x1,x2,x3)=\n",
      "p(x1)p(x2|x1)p(x3|x1,x2).x1 x2\n",
      "x3\n",
      "(a)x1 x2\n",
      "x3f(x1,x2,x3)\n",
      "(b)8.4. Inference in Graphical Models 403\n",
      "x1 x2\n",
      "x3\n",
      "(a)x1 x2\n",
      "x3f(x1,x2,x3)\n",
      "(b)x1 x2\n",
      "x3fa\n",
      "fc fb\n",
      "(c)\n",
      "Figure 8.45 (a) A fully connected undirected graph. (b) and (c) Two factor graphs each of which corresponds\n",
      "to the undirected graph in (a).\n",
      "There is an algorithm for exact inference on directed graphs without loops known\n",
      "asbelief propagation (Pearl, 1988; Lauritzen and Spiegelhalter, 1988), and is equiv-\n",
      "alent to a special case of the sum-product algorithm. Here we shall consider only the\n",
      "sum-product algorithm because it is simpler to derive and to apply, as well as being\n",
      "more general.\n",
      "We shall assume that the original graph is an undirected tree or a directed tree or\n",
      "polytree, so that the corresponding factor graph has a tree structure. We ﬁrst convert\n",
      "the original graph into a factor graph so that we can deal with both directed and\n",
      "undirected models using the same framework. Our goal is to exploit the structure of\n",
      "the graph to achieve two things: (i) to obtain an efﬁcient, exact inference algorithm\n",
      "for ﬁnding marginals; (ii) in situations where several marginals are required to allow\n",
      "computations to be shared efﬁciently.\n",
      "We begin by considering the problem of ﬁnding the marginal p(x)for partic-\n",
      "ular variable node x. For the moment, we shall suppose that all of the variables\n",
      "are hidden. Later we shall see how to modify the algorithm to incorporate evidence\n",
      "corresponding to observed variables. By deﬁnition, the marginal is obtained by sum-\n",
      "ming the joint distribution over all variables except xso that\n",
      "p(x)=∑\n",
      "x\\xp(x) (8.61)\n",
      "wherex\\xdenotes the set of variables in xwith variable xomitted. The idea is\n",
      "to substitute for p(x)using the factor graph expression (8.59) and then interchange\n",
      "summations and products in order to obtain an efﬁcient algorithm. Consider the\n",
      "fragment of graph shown in Figure 8.46 in which we see that the tree structure of\n",
      "the graph allows us to partition the factors in the joint distribution into groups, with\n",
      "one group associated with each of the factor nodes that is a neighbour of the variable\n",
      "nodex. We see that the joint distribution can be written as a product of the form\n",
      "p(x)=∏\n",
      "s∈ne(x)Fs(x, X s) (8.62)\n",
      "ne(x)denotes the set of factor nodes that are neighbours of x, andXsdenotes the\n",
      "set of all variables in the subtree connected to the variable node xvia the factor node404 8. GRAPHICAL MODELS\n",
      "Figure 8.46 A fragment of a factor graph illustrating the\n",
      "evaluation of the marginal p(x).\n",
      "x fsµfs→x(x)Fs(x,X s)\n",
      "fs, andFs(x, X s)represents the product of all the factors in the group associated\n",
      "with factor fs.\n",
      "Substituting (8.62) into (8.61) and interchanging the sums and products, we ob-\n",
      "tain\n",
      "p(x)=∏\n",
      "s∈ne(x)[∑\n",
      "XsFs(x, X s)]\n",
      "=∏\n",
      "s∈ne(x)µfs→x(x). (8.63)\n",
      "Here we have introduced a set of functions µfs→x(x), deﬁned by\n",
      "µfs→x(x)≡∑\n",
      "XsFs(x, X s) (8.64)\n",
      "which can be viewed as messages from the factor nodes fsto the variable node x.\n",
      "We see that the required marginal p(x)is given by the product of all the incoming\n",
      "messages arriving at node x.\n",
      "In order to evaluate these messages, we again turn to Figure 8.46 and note that\n",
      "each factor Fs(x, X s)is described by a factor (sub-)graph and so can itself be fac-\n",
      "torized. In particular, we can write\n",
      "Fs(x, X s)=fs(x, x1,...,x M)G1(x1,Xs1)...G M(xM,XsM) (8.65)\n",
      "where, for convenience, we have denoted the variables associated with factor fx,i n\n",
      "addition to x,b yx1,...,x M. This factorization is illustrated in Figure 8.47. Note\n",
      "that the set of variables {x, x1,...,x M}is the set of variables on which the factor\n",
      "fsdepends, and so it can also be denoted xs, using the notation of (8.59).\n",
      "Substituting (8.65) into (8.64) we obtain\n",
      "µfs→x(x)=∑\n",
      "x1...∑\n",
      "xMfs(x, x1,...,x M)∏\n",
      "m∈ne(fs)\\x[∑\n",
      "XxmGm(xm,Xsm)]\n",
      "=∑\n",
      "x1...∑\n",
      "xMfs(x, x1,...,x M)∏\n",
      "m∈ne(fs)\\xµxm→fs(xm) (8.66)8.4. Inference in Graphical Models 405\n",
      "Figure 8.47 Illustration of the factorization of the subgraph as-\n",
      "sociated with factor node fs.\n",
      "xmxM\n",
      "xfsµxM→fs(xM)\n",
      "µfs→x(x)\n",
      "Gm(xm,Xsm)\n",
      "where ne(fs)denotes the set of variable nodes that are neighbours of the factor node\n",
      "fs, andne(fs)\\xdenotes the same set but with node xremoved. Here we have\n",
      "deﬁned the following messages from variable nodes to factor nodes\n",
      "µxm→fs(xm)≡∑\n",
      "XsmGm(xm,Xsm). (8.67)\n",
      "We have therefore introduced two distinct kinds of message, those that go from factor\n",
      "nodes to variable nodes denoted µf→x(x), and those that go from variable nodes to\n",
      "factor nodes denoted µx→f(x). In each case, we see that messages passed along a\n",
      "link are always a function of the variable associated with the variable node that link\n",
      "connects to.\n",
      "The result (8.66) says that to evaluate the message sent by a factor node to a vari-\n",
      "able node along the link connecting them, take the product of the incoming messages\n",
      "along all other links coming into the factor node, multiply by the factor associated\n",
      "with that node, and then marginalize over all of the variables associated with the\n",
      "incoming messages. This is illustrated in Figure 8.47. It is important to note that\n",
      "a factor node can send a message to a variable node once it has received incoming\n",
      "messages from all other neighbouring variable nodes.\n",
      "Finally, we derive an expression for evaluating the messages from variable nodes\n",
      "to factor nodes, again by making use of the (sub-)graph factorization. From Fig-\n",
      "ure 8.48, we see that term Gm(xm,Xsm)associated with node xmis given by a\n",
      "product of terms Fl(xm,Xml)each associated with one of the factor nodes flthat is\n",
      "linked to node xm(excluding node fs), so that\n",
      "Gm(xm,Xsm)=∏\n",
      "l∈ne(xm)\\fsFl(xm,Xml) (8.68)\n",
      "where the product is taken over all neighbours of node xmexcept for node fs. Note\n",
      "that each of the factors Fl(xm,Xml)represents a subtree of the original graph of\n",
      "precisely the same kind as introduced in (8.62). Substituting (8.68) into (8.67), we406 8. GRAPHICAL MODELS\n",
      "Figure 8.48 Illustration of the evaluation of the message sent by a\n",
      "variable node to an adjacent factor node.\n",
      "xm\n",
      "flfL\n",
      "fs\n",
      "Fl(xm,Xml)\n",
      "then obtain\n",
      "µxm→fs(xm)=∏\n",
      "l∈ne(xm)\\fs[∑\n",
      "XmlFl(xm,Xml)]\n",
      "=∏\n",
      "l∈ne(xm)\\fsµfl→xm(xm) (8.69)\n",
      "where we have used the deﬁnition (8.64) of the messages passed from factor nodes to\n",
      "variable nodes. Thus to evaluate the message sent by a variable node to an adjacent\n",
      "factor node along the connecting link, we simply take the product of the incoming\n",
      "messages along all of the other links. Note that any variable node that has only\n",
      "two neighbours performs no computation but simply passes messages through un-\n",
      "changed. Also, we note that a variable node can send a message to a factor node\n",
      "once it has received incoming messages from all other neighbouring factor nodes.\n",
      "Recall that our goal is to calculate the marginal for variable node x, and that this\n",
      "marginal is given by the product of incoming messages along all of the links arriving\n",
      "at that node. Each of these messages can be computed recursively in terms of other\n",
      "messages. In order to start this recursion, we can view the node xas the root of the\n",
      "tree and begin at the leaf nodes. From the deﬁnition (8.69), we see that if a leaf node\n",
      "is a variable node, then the message that it sends along its one and only link is given\n",
      "by\n",
      "µx→f(x)=1 (8.70)\n",
      "as illustrated in Figure 8.49(a). Similarly, if the leaf node is a factor node, we see\n",
      "from (8.66) that the message sent should take the form\n",
      "µf→x(x)=f(x) (8.71)\n",
      "Figure 8.49 The sum-product algorithm\n",
      "begins with messages sent\n",
      "by the leaf nodes, which de-\n",
      "pend on whether the leaf\n",
      "node is (a) a variable node,\n",
      "or (b) a factor node.x fµx→f(x)=1\n",
      "(a)x fµf→x(x)=f(x)\n",
      "(b)8.4. Inference in Graphical Models 407\n",
      "as illustrated in Figure 8.49(b).\n",
      "At this point, it is worth pausing to summarize the particular version of the sum-\n",
      "product algorithm obtained so far for evaluating the marginal p(x). We start by\n",
      "viewing the variable node xas the root of the factor graph and initiating messages\n",
      "at the leaves of the graph using (8.70) and (8.71). The message passing steps (8.66)and (8.69) are then applied recursively until messages have been propagated along\n",
      "every link, and the root node has received messages from all of its neighbours. Each\n",
      "node can send a message towards the root once it has received messages from allof its other neighbours. Once the root node has received messages from all of its\n",
      "neighbours, the required marginal can be evaluated using (8.63). We shall illustrate\n",
      "this process shortly.\n",
      "To see that each node will always receive enough messages to be able to send out\n",
      "a message, we can use a simple inductive argument as follows. Clearly, for a graphcomprising a variable root node connected directly to several factor leaf nodes, the\n",
      "algorithm trivially involves sending messages of the form (8.71) directly from the\n",
      "leaves to the root. Now imagine building up a general graph by adding nodes one ata time, and suppose that for some particular graph we have a valid algorithm. When\n",
      "one more (variable or factor) node is added, it can be connected only by a single\n",
      "link because the overall graph must remain a tree, and so the new node will be a leafnode. It therefore sends a message to the node to which it is linked, which in turn\n",
      "will therefore receive all the messages it requires in order to send its own message\n",
      "towards the root, and so again we have a valid algorithm, thereby completing theproof.\n",
      "Now suppose we wish to ﬁnd the marginals for every variable node in the graph.\n",
      "This could be done by simply running the above algorithm afresh for each such node.However, this would be very wasteful as many of the required computations would\n",
      "be repeated. We can obtain a much more efﬁcient procedure by ‘overlaying’ these\n",
      "multiple message passing algorithms to obtain the general sum-product algorithm\n",
      "as follows. Arbitrarily pick any (variable or factor) node and designate it as the\n",
      "root. Propagate messages from the leaves to the root as before. At this point, theroot node will have received messages from all of its neighbours. It can therefore\n",
      "send out messages to all of its neighbours. These in turn will then have received\n",
      "messages from all of their neighbours and so can send out messages along the linksgoing away from the root, and so on. In this way, messages are passed outwards\n",
      "from the root all the way to the leaves. By now, a message will have passed in\n",
      "both directions across every link in the graph, and every node will have receiveda message from all of its neighbours. Again a simple inductive argument can be\n",
      "used to verify the validity of this message passing protocol. Because every variable Exercise 8.20\n",
      "node will have received messages from all of its neighbours, we can readily calculatethe marginal distribution for every variable in the graph. The number of messages\n",
      "that have to be computed is given by twice the number of links in the graph and\n",
      "so involves only twice the computation involved in ﬁnding a single marginal. Bycomparison, if we had run the sum-product algorithm separately for each node, the\n",
      "amount of computation would grow quadratically with the size of the graph. Note\n",
      "that this algorithm is in fact independent of which node was designated as the root,408 8. GRAPHICAL MODELS\n",
      "Figure 8.50 The sum-product algorithm can be viewed\n",
      "purely in terms of messages sent out by factor\n",
      "nodes to other factor nodes. In this example,\n",
      "the outgoing message shown by the blue arrow\n",
      "is obtained by taking the product of all the in-\n",
      "coming messages shown by green arrows, mul-\n",
      "tiplying by the factor fs, and marginalizing over\n",
      "the variables x1andx2. fsx1\n",
      "x2x3\n",
      "and indeed the notion of one node having a special status was introduced only as a\n",
      "convenient way to explain the message passing protocol.\n",
      "Next suppose we wish to ﬁnd the marginal distributions p(xs)associated with\n",
      "the sets of variables belonging to each of the factors. By a similar argument to that\n",
      "used above, it is easy to see that the marginal associated with a factor is given by the Exercise 8.21\n",
      "product of messages arriving at the factor node and the local factor at that node\n",
      "p(xs)=fs(xs)∏\n",
      "i∈ne(fs)µxi→fs(xi) (8.72)\n",
      "in complete analogy with the marginals at the variable nodes. If the factors are\n",
      "parameterized functions and we wish to learn the values of the parameters using\n",
      "the EM algorithm, then these marginals are precisely the quantities we will need to\n",
      "calculate in the E step, as we shall see in detail when we discuss the hidden Markov\n",
      "model in Chapter 13.\n",
      "The message sent by a variable node to a factor node, as we have seen, is simply\n",
      "the product of the incoming messages on other links. We can if we wish view the\n",
      "sum-product algorithm in a slightly different form by eliminating messages from\n",
      "variable nodes to factor nodes and simply considering messages that are sent out by\n",
      "factor nodes. This is most easily seen by considering the example in Figure 8.50.\n",
      "So far, we have rather neglected the issue of normalization. If the factor graph\n",
      "was derived from a directed graph, then the joint distribution is already correctly nor-\n",
      "malized, and so the marginals obtained by the sum-product algorithm will similarly\n",
      "be normalized correctly. However, if we started from an undirected graph, then in\n",
      "general there will be an unknown normalization coefﬁcient 1/Z. As with the simple\n",
      "chain example of Figure 8.38, this is easily handled by working with an unnormal-\n",
      "ized version ˜p(x)of the joint distribution, where p(x)=˜p(x)/Z. We ﬁrst run the\n",
      "sum-product algorithm to ﬁnd the corresponding unnormalized marginals ˜p(xi). The\n",
      "coefﬁcient 1/Zis then easily obtained by normalizing any one of these marginals,\n",
      "and this is computationally efﬁcient because the normalization is done over a single\n",
      "variable rather than over the entire set of variables as would be required to normalize\n",
      "˜p(x)directly.\n",
      "At this point, it may be helpful to consider a simple example to illustrate the\n",
      "operation of the sum-product algorithm. Figure 8.51 shows a simple 4-node factor8.4. Inference in Graphical Models 409\n",
      "Figure 8.51 A simple factor graph used to illustrate the\n",
      "sum-product algorithm.x1 x2 x3\n",
      "x4fa fb\n",
      "fc\n",
      "graph whose unnormalized joint distribution is given by\n",
      "˜p(x)=fa(x1,x2)fb(x2,x3)fc(x2,x4). (8.73)\n",
      "In order to apply the sum-product algorithm to this graph, let us designate node x3\n",
      "as the root, in which case there are two leaf nodes x1andx4. Starting with the leaf\n",
      "nodes, we then have the following sequence of six messages\n",
      "µx1→fa(x1)=1 (8.74)\n",
      "µfa→x2(x2)=∑\n",
      "x1fa(x1,x2) (8.75)\n",
      "µx4→fc(x4)=1 (8.76)\n",
      "µfc→x2(x2)=∑\n",
      "x4fc(x2,x4) (8.77)\n",
      "µx2→fb(x2)= µfa→x2(x2)µfc→x2(x2) (8.78)\n",
      "µfb→x3(x3)=∑\n",
      "x2fb(x2,x3)µx2→fb. (8.79)\n",
      "The direction of ﬂow of these messages is illustrated in Figure 8.52. Once this mes-\n",
      "sage propagation is complete, we can then propagate messages from the root node\n",
      "out to the leaf nodes, and these are given by\n",
      "µx3→fb(x3)=1 (8.80)\n",
      "µfb→x2(x2)=∑\n",
      "x3fb(x2,x3) (8.81)\n",
      "µx2→fa(x2)= µfb→x2(x2)µfc→x2(x2) (8.82)\n",
      "µfa→x1(x1)=∑\n",
      "x2fa(x1,x2)µx2→fa(x2) (8.83)\n",
      "µx2→fc(x2)= µfa→x2(x2)µfb→x2(x2) (8.84)\n",
      "µfc→x4(x4)=∑\n",
      "x2fc(x2,x4)µx2→fc(x2). (8.85)410 8. GRAPHICAL MODELS\n",
      "x1 x2 x3\n",
      "x4\n",
      "(a)x1 x2 x3\n",
      "x4\n",
      "(b)\n",
      "Figure 8.52 Flow of messages for the sum-product algorithm applied to the example graph in Figure 8.51. (a)\n",
      "From the leaf nodes x1andx4towards the root node x3. (b) From the root node towards the leaf nodes.\n",
      "One message has now passed in each direction across each link, and we can now\n",
      "evaluate the marginals. As a simple check, let us verify that the marginal p(x2)is\n",
      "given by the correct expression. Using (8.63) and substituting for the messages using\n",
      "the above results, we have\n",
      "˜p(x2)= µfa→x2(x2)µfb→x2(x2)µfc→x2(x2)\n",
      "=[∑\n",
      "x1fa(x1,x2)][∑\n",
      "x3fb(x2,x3)][∑\n",
      "x4fc(x2,x4)]\n",
      "=∑\n",
      "x1∑\n",
      "x2∑\n",
      "x4fa(x1,x2)fb(x2,x3)fc(x2,x4)\n",
      "=∑\n",
      "x1∑\n",
      "x3∑\n",
      "x4˜p(x) (8.86)\n",
      "as required.\n",
      "So far, we have assumed that all of the variables in the graph are hidden. In most\n",
      "practical applications, a subset of the variables will be observed, and we wish to cal-\n",
      "culate posterior distributions conditioned on these observations. Observed nodes are\n",
      "easily handled within the sum-product algorithm as follows. Suppose we partition x\n",
      "into hidden variables hand observed variables v, and that the observed value of v\n",
      "is denotedˆv. Then we simply multiply the joint distribution p(x)by∏\n",
      "iI(vi,ˆvi),\n",
      "where I(v,ˆv)=1 ifv=ˆvandI(v,ˆv)=0 otherwise. This product corresponds\n",
      "top(h,v=ˆv)and hence is an unnormalized version of p(h|v=ˆv). By run-\n",
      "ning the sum-product algorithm, we can efﬁciently calculate the posterior marginals\n",
      "p(hi|v=ˆv)up to a normalization coefﬁcient whose value can be found efﬁciently\n",
      "using a local computation. Any summations over variables in vthen collapse into a\n",
      "single term.\n",
      "We have assumed throughout this section that we are dealing with discrete vari-\n",
      "ables. However, there is nothing speciﬁc to discrete variables either in the graphical\n",
      "framework or in the probabilistic construction of the sum-product algorithm. For8.4. Inference in Graphical Models 411\n",
      "Table 8.1 Example of a joint distribution over two binary variables for\n",
      "which the maximum of the joint distribution occurs for dif-\n",
      "ferent variable values compared to the maxima of the two\n",
      "marginals.x=0 x=1\n",
      "y=0 0.3 0.4\n",
      "y=1 0.3 0.0\n",
      "continuous variables the summations are simply replaced by integrations. We shall\n",
      "give an example of the sum-product algorithm applied to a graph of linear-Gaussian\n",
      "variables when we consider linear dynamical systems. Section 13.3\n",
      "8.4.5 The max-sum algorithm\n",
      "The sum-product algorithm allows us to take a joint distribution p(x)expressed\n",
      "as a factor graph and efﬁciently ﬁnd marginals over the component variables. Two\n",
      "other common tasks are to ﬁnd a setting of the variables that has the largest prob-\n",
      "ability and to ﬁnd the value of that probability. These can be addressed through aclosely related algorithm called max-sum , which can be viewed as an application of\n",
      "dynamic programming in the context of graphical models (Cormen et al. , 2001).\n",
      "A simple approach to ﬁnding latent variable values having high probability\n",
      "would be to run the sum-product algorithm to obtain the marginals p(x\n",
      "i)for ev-\n",
      "ery variable, and then, for each marginal in turn, to ﬁnd the value x⋆\n",
      "ithat maximizes\n",
      "that marginal. However, this would give the set of values that are individually the\n",
      "most probable. In practice, we typically wish to ﬁnd the set of values that jointly\n",
      "have the largest probability, in other words the vector xmaxthat maximizes the joint\n",
      "distribution, so that\n",
      "xmax=a r gm a x\n",
      "xp(x) (8.87)\n",
      "for which the corresponding value of the joint probability will be given by\n",
      "p(xmax) = max\n",
      "xp(x). (8.88)\n",
      "In general, xmaxis not the same as the set of x⋆\n",
      "ivalues, as we can easily show using\n",
      "a simple example. Consider the joint distribution p(x, y)over two binary variables\n",
      "x, y∈{0,1}given in Table 8.1. The joint distribution is maximized by setting x=\n",
      "1andy=0, corresponding the value 0.4. However, the marginal for p(x), obtained\n",
      "by summing over both values of y,i sg i v e nb y p(x=0 )=0 .6andp(x=1 )=0 .4,\n",
      "and similarly the marginal for yis given by p(y=0 )=0 .7andp(y=1 )=0 .3,\n",
      "and so the marginals are maximized by x=0 andy=0, which corresponds to a\n",
      "value of 0.3for the joint distribution. In fact, it is not difﬁcult to construct examples\n",
      "for which the set of individually most probable values has probability zero under the\n",
      "joint distribution. Exercise 8.27\n",
      "We therefore seek an efﬁcient algorithm for ﬁnding the value of xthat maxi-\n",
      "mizes the joint distribution p(x)and that will allow us to obtain the value of the\n",
      "joint distribution at its maximum. To address the second of these problems, we shall\n",
      "simply write out the max operator in terms of its components\n",
      "max\n",
      "xp(x)=m a x\n",
      "x1...max\n",
      "xMp(x) (8.89)412 8. GRAPHICAL MODELS\n",
      "where Mis the total number of variables, and then substitute for p(x)using its\n",
      "expansion in terms of a product of factors. In deriving the sum-product algorithm,we made use of the distributive law (8.53) for multiplication. Here we make use of\n",
      "the analogous law for the max operator\n",
      "max(ab, ac)=amax(b, c) (8.90)\n",
      "which holds if a⩾0(as will always be the case for the factors in a graphical model).\n",
      "This allows us to exchange products with maximizations.\n",
      "Consider ﬁrst the simple example of a chain of nodes described by (8.49). The\n",
      "evaluation of the probability maximum can be written as\n",
      "max\n",
      "xp(x)=1\n",
      "Zmax\n",
      "x1···max\n",
      "xN[ψ1,2(x1,x2)···ψN−1,N(xN−1,xN)]\n",
      "=1\n",
      "Zmax\n",
      "x1[\n",
      "ψ1,2(x1,x2)[\n",
      "···max\n",
      "xNψN−1,N(xN−1,xN)]]\n",
      ".\n",
      "As with the calculation of marginals, we see that exchanging the max and product\n",
      "operators results in a much more efﬁcient computation, and one that is easily inter-\n",
      "preted in terms of messages passed from node xNbackwards along the chain to node\n",
      "x1.\n",
      "We can readily generalize this result to arbitrary tree-structured factor graphs\n",
      "by substituting the expression (8.59) for the factor graph expansion into (8.89) andagain exchanging maximizations with products. The structure of this calculation is\n",
      "identical to that of the sum-product algorithm, and so we can simply translate those\n",
      "results into the present context. In particular, suppose that we designate a particular\n",
      "variable node as the ‘root’ of the graph. Then we start a set of messages propagating\n",
      "inwards from the leaves of the tree towards the root, with each node sending itsmessage towards the root once it has received all incoming messages from its other\n",
      "neighbours. The ﬁnal maximization is performed over the product of all messages\n",
      "arriving at the root node, and gives the maximum value for p(x). This could be called\n",
      "themax-product algorithm and is identical to the sum-product algorithm except that\n",
      "summations are replaced by maximizations. Note that at this stage, messages have\n",
      "been sent from leaves to the root, but not in the other direction.\n",
      "In practice, products of many small probabilities can lead to numerical under-\n",
      "ﬂow problems, and so it is convenient to work with the logarithm of the joint distri-\n",
      "bution. The logarithm is a monotonic function, so that if a>b thenlna>lnb, and\n",
      "hence the max operator and the logarithm function can be interchanged, so that\n",
      "ln\n",
      "(\n",
      "max\n",
      "xp(x))\n",
      "=m a x\n",
      "xlnp(x). (8.91)\n",
      "The distributive property is preserved because\n",
      "max(a+b, a+c)=a+m a x ( b, c). (8.92)\n",
      "Thus taking the logarithm simply has the effect of replacing the products in the\n",
      "max-product algorithm with sums, and so we obtain the max-sum algorithm. From8.4. Inference in Graphical Models 413\n",
      "the results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can\n",
      "readily write down the max-sum algorithm in terms of message passing simply byreplacing ‘sum’ with ‘max’ and replacing products with sums of logarithms to give\n",
      "µ\n",
      "f→x(x) = max\n",
      "x1,...,x M⎡\n",
      "⎣lnf(x, x1,...,x M)+∑\n",
      "m∈ne(fs)\\xµxm→f(xm)⎤\n",
      "⎦(8.93)\n",
      "µx→f(x)=∑\n",
      "l∈ne(x)\\fµfl→x(x). (8.94)\n",
      "The initial messages sent by the leaf nodes are obtained by analogy with (8.70) and\n",
      "(8.71) and are given by\n",
      "µx→f(x)=0 (8.95)\n",
      "µf→x(x)=l n f(x) (8.96)\n",
      "while at the root node the maximum probability can then be computed, by analogy\n",
      "with (8.63), using\n",
      "pmax=m a x\n",
      "x⎡\n",
      "⎣∑\n",
      "s∈ne(x)µfs→x(x)⎤\n",
      "⎦. (8.97)\n",
      "So far, we have seen how to ﬁnd the maximum of the joint distribution by prop-\n",
      "agating messages from the leaves to an arbitrarily chosen root node. The result will\n",
      "be the same irrespective of which node is chosen as the root. Now we turn to thesecond problem of ﬁnding the conﬁguration of the variables for which the joint dis-\n",
      "tribution attains this maximum value. So far, we have sent messages from the leaves\n",
      "to the root. The process of evaluating (8.97) will also give the value x\n",
      "maxfor the\n",
      "most probable value of the root node variable, deﬁned by\n",
      "xmax=a r gm a x\n",
      "x⎡\n",
      "⎣∑\n",
      "s∈ne(x)µfs→x(x)⎤\n",
      "⎦. (8.98)\n",
      "At this point, we might be tempted simply to continue with the message passing al-\n",
      "gorithm and send messages from the root back out to the leaves, using (8.93) and(8.94), then apply (8.98) to all of the remaining variable nodes. However, because\n",
      "we are now maximizing rather than summing, it is possible that there may be mul-\n",
      "tiple conﬁgurations of xall of which give rise to the maximum value for p(x).I n\n",
      "such cases, this strategy can fail because it is possible for the individual variable\n",
      "values obtained by maximizing the product of messages at each node to belong todifferent maximizing conﬁgurations, giving an overall conﬁguration that no longer\n",
      "corresponds to a maximum.\n",
      "The problem can be resolved by adopting a rather different kind of message\n",
      "passing from the root node to the leaves. To see how this works, let us return once\n",
      "again to the simple chain example of Nvariables x\n",
      "1,...,x Neach having Kstates,414 8. GRAPHICAL MODELS\n",
      "Figure 8.53 A lattice, or trellis, diagram show-\n",
      "ing explicitly the Kpossible states (one per row\n",
      "of the diagram) for each of the variables xnin the\n",
      "chain model. In this illustration K=3. The ar-\n",
      "row shows the direction of message passing in the\n",
      "max-product algorithm. For every state kof each\n",
      "variable xn(corresponding to column nof the dia-\n",
      "gram) the function φ(xn)deﬁnes a unique state at\n",
      "the previous variable, indicated by the black lines.\n",
      "The two paths through the lattice correspond to\n",
      "conﬁgurations that give the global maximum of the\n",
      "joint probability distribution, and either of these\n",
      "can be found by tracing back along the black lines\n",
      "in the opposite direction to the arrow.k=1\n",
      "k=2\n",
      "k=3\n",
      "n−2 n−1 nn +1\n",
      "corresponding to the graph shown in Figure 8.38. Suppose we take node xNto be\n",
      "the root node. Then in the ﬁrst phase, we propagate messages from the leaf node x1\n",
      "to the root node using\n",
      "µxn→fn,n +1(xn)= µfn−1,n→xn(xn)\n",
      "µfn−1,n→xn(xn) = max\n",
      "xn−1[\n",
      "lnfn−1,n(xn−1,xn)+µxn−1→fn−1,n(xn)]\n",
      "which follow from applying (8.94) and (8.93) to this particular graph. The initial\n",
      "message sent from the leaf node is simply\n",
      "µx1→f1,2(x1)=0. (8.99)\n",
      "The most probable value for xNis then given by\n",
      "xmax\n",
      "N=a r gm a x\n",
      "xN[\n",
      "µfN−1,N→xN(xN)]\n",
      ". (8.100)\n",
      "Now we need to determine the states of the previous variables that correspond to the\n",
      "same maximizing conﬁguration. This can be done by keeping track of which values\n",
      "of the variables gave rise to the maximum state of each variable, in other words by\n",
      "storing quantities given by\n",
      "φ(xn) = arg max\n",
      "xn−1[\n",
      "lnfn−1,n(xn−1,xn)+µxn−1→fn−1,n(xn)]\n",
      ". (8.101)\n",
      "To understand better what is happening, it is helpful to represent the chain of vari-\n",
      "ables in terms of a lattice ortrellis diagram as shown in Figure 8.53. Note that this\n",
      "is not a probabilistic graphical model because the nodes represent individual states\n",
      "of variables, while each variable corresponds to a column of such states in the di-\n",
      "agram. For each state of a given variable, there is a unique state of the previous\n",
      "variable that maximizes the probability (ties are broken either systematically or at\n",
      "random), corresponding to the function φ(xn)given by (8.101), and this is indicated8.4. Inference in Graphical Models 415\n",
      "by the lines connecting the nodes. Once we know the most probable value of the ﬁ-\n",
      "nal node xN, we can then simply follow the link back to ﬁnd the most probable state\n",
      "of node xN−1and so on back to the initial node x1. This corresponds to propagating\n",
      "a message back down the chain using\n",
      "xmax\n",
      "n−1=φ(xmax\n",
      "n) (8.102)\n",
      "and is known as back-tracking . Note that there could be several values of xn−1all\n",
      "of which give the maximum value in (8.101). Provided we chose one of these values\n",
      "when we do the back-tracking, we are assured of a globally consistent maximizingconﬁguration.\n",
      "In Figure 8.53, we have indicated two paths, each of which we shall suppose\n",
      "corresponds to a global maximum of the joint probability distribution. If k=2\n",
      "andk=3 each represent possible values of x\n",
      "max\n",
      "N, then starting from either state\n",
      "and tracing back along the black lines, which corresponds to iterating (8.102), we\n",
      "obtain a valid global maximum conﬁguration. Note that if we had run a forward\n",
      "pass of max-sum message passing followed by a backward pass and then applied\n",
      "(8.98) at each node separately, we could end up selecting some states from one pathand some from the other path, giving an overall conﬁguration that is not a global\n",
      "maximizer. We see that it is necessary instead to keep track of the maximizing states\n",
      "during the forward pass using the functions φ(x\n",
      "n)and then use back-tracking to ﬁnd\n",
      "a consistent solution.\n",
      "The extension to a general tree-structured factor graph should now be clear. If\n",
      "a message is sent from a factor node fto a variable node x, a maximization is\n",
      "performed over all other variable nodes x1,...,x Mthat are neighbours of that fac-\n",
      "tor node, using (8.93). When we perform this maximization, we keep a record of\n",
      "which values of the variables x1,...,x Mgave rise to the maximum. Then in the\n",
      "back-tracking step, having found xmax, we can then use these stored values to as-\n",
      "sign consistent maximizing states xmax\n",
      "1,...,xmax\n",
      "M. The max-sum algorithm, with\n",
      "back-tracking, gives an exact maximizing conﬁguration for the variables providedthe factor graph is a tree. An important application of this technique is for ﬁnding\n",
      "the most probable sequence of hidden states in a hidden Markov model, in which\n",
      "case it is known as the Viterbi algorithm. Section 13.2\n",
      "As with the sum-product algorithm, the inclusion of evidence in the form of\n",
      "observed variables is straightforward. The observed variables are clamped to their\n",
      "observed values, and the maximization is performed over the remaining hidden vari-\n",
      "ables. This can be shown formally by including identity functions for the observed\n",
      "variables into the factor functions, as we did for the sum-product algorithm.\n",
      "It is interesting to compare max-sum with the iterated conditional modes (ICM)\n",
      "algorithm described on page 389. Each step in ICM is computationally simpler be-\n",
      "cause the ‘messages’ that are passed from one node to the next comprise a singlevalue consisting of the new state of the node for which the conditional distribution\n",
      "is maximized. The max-sum algorithm is more complex because the messages are\n",
      "functions of node variables xand hence comprise a set of Kvalues for each pos-\n",
      "sible state of x. Unlike max-sum, however, ICM is not guaranteed to ﬁnd a global\n",
      "maximum even for tree-structured graphs.416 8. GRAPHICAL MODELS\n",
      "8.4.6 Exact inference in general graphs\n",
      "The sum-product and max-sum algorithms provide efﬁcient and exact solutions\n",
      "to inference problems in tree-structured graphs. For many practical applications,\n",
      "however, we have to deal with graphs having loops.\n",
      "The message passing framework can be generalized to arbitrary graph topolo-\n",
      "gies, giving an exact inference procedure known as the junction tree algorithm (Lau-\n",
      "ritzen and Spiegelhalter, 1988; Jordan, 2007). Here we give a brief outline of the\n",
      "key steps involved. This is not intended to convey a detailed understanding of the\n",
      "algorithm, but rather to give a ﬂavour of the various stages involved. If the starting\n",
      "point is a directed graph, it is ﬁrst converted to an undirected graph by moraliza-tion, whereas if starting from an undirected graph this step is not required. Next the\n",
      "graph is triangulated , which involves ﬁnding chord-less cycles containing four or\n",
      "more nodes and adding extra links to eliminate such chord-less cycles. For instance,in the graph in Figure 8.36, the cycle A–C–B–D–Ais chord-less a link could be\n",
      "added between AandBor alternatively between CandD. Note that the joint dis-\n",
      "tribution for the resulting triangulated graph is still deﬁned by a product of the samepotential functions, but these are now considered to be functions over expanded sets\n",
      "of variables. Next the triangulated graph is used to construct a new tree-structured\n",
      "undirected graph called a join tree , whose nodes correspond to the maximal cliques\n",
      "of the triangulated graph, and whose links connect pairs of cliques that have vari-\n",
      "ables in common. The selection of which pairs of cliques to connect in this way is\n",
      "important and is done so as to give a maximal spanning tree deﬁned as follows. Of\n",
      "all possible trees that link up the cliques, the one that is chosen is one for which the\n",
      "weight of the tree is largest, where the weight for a link is the number of nodes shared\n",
      "by the two cliques it connects, and the weight for the tree is the sum of the weightsfor the links. If the tree is condensed, so that any clique that is a subset of another\n",
      "clique is absorbed into the larger clique, this gives a junction tree . As a consequence\n",
      "of the triangulation step, the resulting tree satisﬁes the running intersection property ,\n",
      "which means that if a variable is contained in two cliques, then it must also be con-\n",
      "tained in every clique on the path that connects them. This ensures that inferenceabout variables will be consistent across the graph. Finally, a two-stage message\n",
      "passing algorithm, essentially equivalent to the sum-product algorithm, can now be\n",
      "applied to this junction tree in order to ﬁnd marginals and conditionals. Althoughthe junction tree algorithm sounds complicated, at its heart is the simple idea that\n",
      "we have used already of exploiting the factorization properties of the distribution to\n",
      "allow sums and products to be interchanged so that partial summations can be per-formed, thereby avoiding having to work directly with the joint distribution. The\n",
      "role of the junction tree is to provide a precise and efﬁcient way to organize these\n",
      "computations. It is worth emphasizing that this is achieved using purely graphicaloperations!\n",
      "The junction tree is exact for arbitrary graphs and is efﬁcient in the sense that\n",
      "for a given graph there does not in general exist a computationally cheaper approach.Unfortunately, the algorithm must work with the joint distributions within each node\n",
      "(each of which corresponds to a clique of the triangulated graph) and so the compu-\n",
      "tational cost of the algorithm is determined by the number of variables in the largest8.4. Inference in Graphical Models 417\n",
      "clique and will grow exponentially with this number in the case of discrete variables.\n",
      "An important concept is the treewidth of a graph (Bodlaender, 1993), which is de-\n",
      "ﬁned in terms of the number of variables in the largest clique. In fact, it is deﬁned to\n",
      "be as one less than the size of the largest clique, to ensure that a tree has a treewidth\n",
      "of 1. Because there in general there can be multiple different junction trees that canbe constructed from a given starting graph, the treewidth is deﬁned by the junction\n",
      "tree for which the largest clique has the fewest variables. If the treewidth of the\n",
      "original graph is high, the junction tree algorithm becomes impractical.\n",
      "8.4.7 Loopy belief propagation\n",
      "For many problems of practical interest, it will not be feasible to use exact in-\n",
      "ference, and so we need to exploit effective approximation methods. An important\n",
      "class of such approximations, that can broadly be called variational methods, will be\n",
      "discussed in detail in Chapter 10. Complementing these deterministic approaches isa wide range of sampling methods, also called Monte Carlo methods, that are based\n",
      "on stochastic numerical sampling from distributions and that will be discussed at\n",
      "length in Chapter 11.\n",
      "Here we consider one simple approach to approximate inference in graphs with\n",
      "loops, which builds directly on the previous discussion of exact inference in trees.\n",
      "The idea is simply to apply the sum-product algorithm even though there is no guar-antee that it will yield good results. This approach is known as loopy belief propa-\n",
      "gation (Frey and MacKay, 1998) and is possible because the message passing rules\n",
      "(8.66) and (8.69) for the sum-product algorithm are purely local. However, because\n",
      "the graph now has cycles, information can ﬂow many times around the graph. For\n",
      "some models, the algorithm will converge, whereas for others it will not.\n",
      "In order to apply this approach, we need to deﬁne a message passing schedule .\n",
      "Let us assume that one message is passed at a time on any given link and in any\n",
      "given direction. Each message sent from a node replaces any previous message sentin the same direction across the same link and will itself be a function only of the\n",
      "most recent messages received by that node at previous steps of the algorithm.\n",
      "We have seen that a message can only be sent across a link from a node when\n",
      "all other messages have been received by that node across its other links. Because\n",
      "there are loops in the graph, this raises the problem of how to initiate the message\n",
      "passing algorithm. To resolve this, we suppose that an initial message given by theunit function has been passed across every link in each direction. Every node is then\n",
      "in a position to send a message.\n",
      "There are now many possible ways to organize the message passing schedule.\n",
      "For example, the ﬂooding schedule simultaneously passes a message across every\n",
      "link in both directions at each time step, whereas schedules that pass one message at\n",
      "a time are called serial schedules .\n",
      "Following Kschischnang et al. (2001), we will say that a (variable or factor)\n",
      "nodeahas a message pending on its link to a node bif node ahas received any\n",
      "message on any of its other links since the last time it send a message to b. Thus,\n",
      "when a node receives a message on one of its links, this creates pending messages\n",
      "on all of its other links. Only pending messages need to be transmitted because418 8. GRAPHICAL MODELS\n",
      "other messages would simply duplicate the previous message on the same link. For\n",
      "graphs that have a tree structure, any schedule that sends only pending messageswill eventually terminate once a message has passed in each direction across every\n",
      "link. At this point, there are no pending messages, and the product of the received Exercise 8.29\n",
      "messages at every variable give the exact marginal. In graphs having loops, however,the algorithm may never terminate because there might always be pending messages,\n",
      "although in practice it is generally found to converge within a reasonable time for\n",
      "most applications. Once the algorithm has converged, or once it has been stoppedif convergence is not observed, the (approximate) local marginals can be computed\n",
      "using the product of the most recently received incoming messages to each variable\n",
      "node or factor node on every link.\n",
      "In some applications, the loopy belief propagation algorithm can give poor re-\n",
      "sults, whereas in other applications it has proven to be very effective. In particular,state-of-the-art algorithms for decoding certain kinds of error-correcting codes are\n",
      "equivalent to loopy belief propagation (Gallager, 1963; Berrou et al. , 1993; McEliece\n",
      "et al. , 1998; MacKay and Neal, 1999; Frey, 1998).\n",
      "8.4.8 Learning the graph structure\n",
      "In our discussion of inference in graphical models, we have assumed that the\n",
      "structure of the graph is known and ﬁxed. However, there is also interest in go-\n",
      "ing beyond the inference problem and learning the graph structure itself from data(Friedman and Koller, 2003). This requires that we deﬁne a space of possible struc-\n",
      "tures as well as a measure that can be used to score each structure.\n",
      "From a Bayesian viewpoint, we would ideally like to compute a posterior dis-\n",
      "tribution over graph structures and to make predictions by averaging with respect\n",
      "to this distribution. If we have a prior p(m)over graphs indexed by m, then the\n",
      "posterior distribution is given by\n",
      "p(m|D)∝p(m)p(D|m) (8.103)\n",
      "where Dis the observed data set. The model evidence p(D|m)then provides the\n",
      "score for each model. However, evaluation of the evidence involves marginalization\n",
      "over the latent variables and presents a challenging computational problem for many\n",
      "models.\n",
      "Exploring the space of structures can also be problematic. Because the number\n",
      "of different graph structures grows exponentially with the number of nodes, it is\n",
      "often necessary to resort to heuristics to ﬁnd good candidates.\n",
      "Exercises\n",
      "8.1 (⋆)www By marginalizing out the variables in order, show that the representation\n",
      "(8.5) for the joint distribution of a directed graph is correctly normalized, provided\n",
      "each of the conditional distributions is normalized.\n",
      "8.2 (⋆)www Show that the property of there being no directed cycles in a directed\n",
      "graph follows from the statement that there exists an ordered numbering of the nodes\n",
      "such that for each node there are no links going to a lower-numbered node.Exercises 419\n",
      "Table 8.2 The joint distribution over three binary variables. abcp(a, b, c)\n",
      "000 0.192\n",
      "001 0.144\n",
      "010 0.048\n",
      "011 0.216\n",
      "100 0.192\n",
      "101 0.064\n",
      "110 0.048\n",
      "111 0.096\n",
      "8.3 (⋆⋆)Consider three binary variables a, b, c ∈{0,1}having the joint distribution\n",
      "given in Table 8.2. Show by direct evaluation that this distribution has the property\n",
      "thataandbare marginally dependent, so that p(a, b)̸=p(a)p(b), but that they\n",
      "become independent when conditioned on c, so that p(a, b|c)=p(a|c)p(b|c)for\n",
      "bothc=0andc=1.\n",
      "8.4 (⋆⋆)Evaluate the distributions p(a),p(b|c), andp(c|a)corresponding to the joint\n",
      "distribution given in Table 8.2. Hence show by direct evaluation that p(a, b, c)=\n",
      "p(a)p(c|a)p(b|c). Draw the corresponding directed graph.\n",
      "8.5 (⋆)www Draw a directed probabilistic graphical model corresponding to the\n",
      "relevance vector machine described by (7.79) and (7.80).\n",
      "8.6 (⋆)For the model shown in Figure 8.13, we have seen that the number of parameters\n",
      "required to specify the conditional distribution p(y|x1,...,x M), where xi∈{0,1},\n",
      "could be reduced from 2MtoM+1by making use of the logistic sigmoid represen-\n",
      "tation (8.10). An alternative representation (Pearl, 1988) is given by\n",
      "p(y=1|x1,...,x M)=1−(1−µ0)M∏\n",
      "i=1(1−µi)xi(8.104)\n",
      "where the parameters µirepresent the probabilities p(xi=1 ) , andµ0is an additional\n",
      "parameters satisfying 0⩽µ0⩽1. The conditional distribution (8.104) is known as\n",
      "thenoisy-OR . Show that this can be interpreted as a ‘soft’ (probabilistic) form of the\n",
      "logical OR function (i.e., the function that gives y=1whenever at least one of the\n",
      "xi=1). Discuss the interpretation of µ0.\n",
      "8.7 (⋆⋆)Using the recursion relations (8.15) and (8.16), show that the mean and covari-\n",
      "ance of the joint distribution for the graph shown in Figure 8.14 are given by (8.17)\n",
      "and (8.18), respectively.\n",
      "8.8 (⋆)www Show that a⊥⊥b, c|dimplies a⊥⊥b|d.\n",
      "8.9 (⋆)www Using the d-separation criterion, show that the conditional distribution\n",
      "for a node xin a directed graph, conditioned on all of the nodes in the Markov\n",
      "blanket, is independent of the remaining variables in the graph.420 8. GRAPHICAL MODELS\n",
      "Figure 8.54 Example of a graphical model used to explore the con-\n",
      "ditional independence properties of the head-to-head\n",
      "patha–c–bwhen a descendant of c, namely the node\n",
      "d, is observed.\n",
      "cab\n",
      "d\n",
      "8.10 (⋆)Consider the directed graph shown in Figure 8.54 in which none of the variables\n",
      "is observed. Show that a⊥⊥b|∅. Suppose we now observe the variable d. Show\n",
      "that in general a̸⊥⊥b|d.\n",
      "8.11 (⋆⋆)Consider the example of the car fuel system shown in Figure 8.21, and suppose\n",
      "that instead of observing the state of the fuel gauge Gdirectly, the gauge is seen by\n",
      "the driver Dwho reports to us the reading on the gauge. This report is either that the\n",
      "gauge shows full D=1or that it shows empty D=0. Our driver is a bit unreliable,\n",
      "as expressed through the following probabilities\n",
      "p(D=1|G=1 ) = 0 .9 (8.105)\n",
      "p(D=0|G=0 ) = 0 .9. (8.106)\n",
      "Suppose that the driver tells us that the fuel gauge shows empty, in other words\n",
      "that we observe D=0. Evaluate the probability that the tank is empty given only\n",
      "this observation. Similarly, evaluate the corresponding probability given also the\n",
      "observation that the battery is ﬂat, and note that this second probability is lower.\n",
      "Discuss the intuition behind this result, and relate the result to Figure 8.54.\n",
      "8.12 (⋆)www Show that there are 2M(M−1)/2distinct undirected graphs over a set of\n",
      "Mdistinct random variables. Draw the 8possibilities for the case of M=3.\n",
      "8.13 (⋆)Consider the use of iterated conditional modes (ICM) to minimize the energy\n",
      "function given by (8.42). Write down an expression for the difference in the values\n",
      "of the energy associated with the two states of a particular variable xj, with all other\n",
      "variables held ﬁxed, and show that it depends only on quantities that are local to xj\n",
      "in the graph.\n",
      "8.14 (⋆)Consider a particular case of the energy function given by (8.42) in which the\n",
      "coefﬁcients β=h=0. Show that the most probable conﬁguration of the latent\n",
      "variables is given by xi=yifor all i.\n",
      "8.15 (⋆⋆)www Show that the joint distribution p(xn−1,xn)for two neighbouring\n",
      "nodes in the graph shown in Figure 8.38 is given by an expression of the form (8.58).Exercises 421\n",
      "8.16 (⋆⋆)Consider the inference problem of evaluating p(xn|xN)for the graph shown\n",
      "in Figure 8.38, for all nodes n∈{1,...,N −1}. Show that the message passing\n",
      "algorithm discussed in Section 8.4.1 can be used to solve this efﬁciently, and discuss\n",
      "which messages are modiﬁed and in what way.\n",
      "8.17 (⋆⋆)Consider a graph of the form shown in Figure 8.38 having N=5 nodes, in\n",
      "which nodes x3andx5are observed. Use d-separation to show that x2⊥⊥x5|x3.\n",
      "Show that if the message passing algorithm of Section 8.4.1 is applied to the evalu-\n",
      "ation of p(x2|x3,x5), the result will be independent of the value of x5.\n",
      "8.18 (⋆⋆)www Show that a distribution represented by a directed tree can trivially\n",
      "be written as an equivalent distribution over the corresponding undirected tree. Alsoshow that a distribution expressed as an undirected tree can, by suitable normaliza-\n",
      "tion of the clique potentials, be written as a directed tree. Calculate the number of\n",
      "distinct directed trees that can be constructed from a given undirected tree.\n",
      "8.19 (⋆⋆)Apply the sum-product algorithm derived in Section 8.4.4 to the chain-of-\n",
      "nodes model discussed in Section 8.4.1 and show that the results (8.54), (8.55), and\n",
      "(8.57) are recovered as a special case.\n",
      "8.20 (⋆)\n",
      "www Consider the message passing protocol for the sum-product algorithm on\n",
      "a tree-structured factor graph in which messages are ﬁrst propagated from the leavesto an arbitrarily chosen root node and then from the root node out to the leaves. Use\n",
      "proof by induction to show that the messages can be passed in such an order that\n",
      "at every step, each node that must send a message has received all of the incomingmessages necessary to construct its outgoing messages.\n",
      "8.21 (⋆⋆)\n",
      "www Show that the marginal distributions p(xs)over the sets of variables\n",
      "xsassociated with each of the factors fx(xs)in a factor graph can be found by ﬁrst\n",
      "running the sum-product message passing algorithm and then evaluating the requiredmarginals using (8.72).\n",
      "8.22 (⋆)Consider a tree-structured factor graph, in which a given subset of the variable\n",
      "nodes form a connected subgraph (i.e., any variable node of the subset is connected\n",
      "to at least one of the other variable nodes via a single factor node). Show how thesum-product algorithm can be used to compute the marginal distribution over that\n",
      "subset.\n",
      "8.23 (⋆⋆)\n",
      "www In Section 8.4.4, we showed that the marginal distribution p(xi)for a\n",
      "variable node xiin a factor graph is given by the product of the messages arriving at\n",
      "this node from neighbouring factor nodes in the form (8.63). Show that the marginal\n",
      "p(xi)can also be written as the product of the incoming message along any one of\n",
      "the links with the outgoing message along the same link.\n",
      "8.24 (⋆⋆)Show that the marginal distribution for the variables xsin a factor fs(xs)in\n",
      "a tree-structured factor graph, after running the sum-product message passing algo-\n",
      "rithm, can be written as the product of the message arriving at the factor node along\n",
      "all its links, times the local factor f(xs), in the form (8.72).422 8. GRAPHICAL MODELS\n",
      "8.25 (⋆⋆)In (8.86), we veriﬁed that the sum-product algorithm run on the graph in\n",
      "Figure 8.51 with node x3designated as the root node gives the correct marginal for\n",
      "x2. Show that the correct marginals are obtained also for x1andx3. Similarly, show\n",
      "that the use of the result (8.72) after running the sum-product algorithm on this graph\n",
      "gives the correct joint distribution for x1,x2.\n",
      "8.26 (⋆)Consider a tree-structured factor graph over discrete variables, and suppose we\n",
      "wish to evaluate the joint distribution p(xa,xb)associated with two variables xaand\n",
      "xbthat do not belong to a common factor. Deﬁne a procedure for using the sum-\n",
      "product algorithm to evaluate this joint distribution in which one of the variables issuccessively clamped to each of its allowed values.\n",
      "8.27 (⋆⋆)Consider two discrete variables xandyeach having three possible states, for\n",
      "example x, y∈{0,1,2}. Construct a joint distribution p(x, y)over these variables\n",
      "having the property that the value\n",
      "ˆxthat maximizes the marginal p(x), along with\n",
      "the valueˆythat maximizes the marginal p(y), together have probability zero under\n",
      "the joint distribution, so that p(ˆx,ˆy)=0 .\n",
      "8.28 (⋆⋆)www The concept of a pending message in the sum-product algorithm for\n",
      "a factor graph was deﬁned in Section 8.4.7. Show that if the graph has one or morecycles, there will always be at least one pending message irrespective of how long\n",
      "the algorithm runs.\n",
      "8.29 (⋆⋆)\n",
      "www Show that if the sum-product algorithm is run on a factor graph with a\n",
      "tree structure (no loops), then after a ﬁnite number of messages have been sent, therewill be no pending messages.9\n",
      "Mixture Models\n",
      "and EM\n",
      "If we deﬁne a joint distribution over observed and latent variables, the correspond-\n",
      "ing distribution of the observed variables alone is obtained by marginalization. Thisallows relatively complex marginal distributions over observed variables to be ex-\n",
      "pressed in terms of more tractable joint distributions over the expanded space of\n",
      "observed and latent variables. The introduction of latent variables thereby allowscomplicated distributions to be formed from simpler components. In this chapter,\n",
      "we shall see that mixture distributions, such as the Gaussian mixture discussed in\n",
      "Section 2.3.9, can be interpreted in terms of discrete latent variables. Continuouslatent variables will form the subject of Chapter 12.\n",
      "As well as providing a framework for building more complex probability dis-\n",
      "tributions, mixture models can also be used to cluster data. We therefore begin ourdiscussion of mixture distributions by considering the problem of ﬁnding clusters\n",
      "in a set of data points, which we approach ﬁrst using a nonprobabilistic technique\n",
      "called the K-means algorithm (Lloyd, 1982). Then we introduce the latent variable Section 9.1\n",
      "423424 9. MIXTURE MODELS AND EM\n",
      "view of mixture distributions in which the discrete latent variables can be interpreted\n",
      "as deﬁning assignments of data points to speciﬁc components of the mixture. A gen- Section 9.2\n",
      "eral technique for ﬁnding maximum likelihood estimators in latent variable models\n",
      "is the expectation-maximization (EM) algorithm. We ﬁrst of all use the Gaussian\n",
      "mixture distribution to motivate the EM algorithm in a fairly informal way, and thenwe give a more careful treatment based on the latent variable viewpoint. We shall Section 9.3\n",
      "see that the K-means algorithm corresponds to a particular nonprobabilistic limit of\n",
      "EM applied to mixtures of Gaussians. Finally, we discuss EM in some generality. Section 9.4\n",
      "Gaussian mixture models are widely used in data mining, pattern recognition,\n",
      "machine learning, and statistical analysis. In many applications, their parameters are\n",
      "determined by maximum likelihood, typically using the EM algorithm. However, as\n",
      "we shall see there are some signiﬁcant limitations to the maximum likelihood ap-\n",
      "proach, and in Chapter 10 we shall show that an elegant Bayesian treatment can begiven using the framework of variational inference. This requires little additional\n",
      "computation compared with EM, and it resolves the principal difﬁculties of maxi-\n",
      "mum likelihood while also allowing the number of components in the mixture to beinferred automatically from the data.\n",
      "9.1.K-means Clustering\n",
      "We begin by considering the problem of identifying groups, or clusters, of data points\n",
      "in a multidimensional space. Suppose we have a data set {x1,...,xN}consisting\n",
      "ofNobservations of a random D-dimensional Euclidean variable x. Our goal is to\n",
      "partition the data set into some number Kof clusters, where we shall suppose for\n",
      "the moment that the value of Kis given. Intuitively, we might think of a cluster as\n",
      "comprising a group of data points whose inter-point distances are small compared\n",
      "with the distances to points outside of the cluster. We can formalize this notion byﬁrst introducing a set of D-dimensional vectors µ\n",
      "k, where k=1,...,K , in which\n",
      "µkis a prototype associated with the kthcluster. As we shall see shortly, we can\n",
      "think of the µkas representing the centres of the clusters. Our goal is then to ﬁnd\n",
      "an assignment of data points to clusters, as well as a set of vectors {µk}, such that\n",
      "the sum of the squares of the distances of each data point to its closest vector µk,i s\n",
      "a minimum.\n",
      "It is convenient at this point to deﬁne some notation to describe the assignment\n",
      "of data points to clusters. For each data point xn, we introduce a corresponding set\n",
      "of binary indicator variables rnk∈{0,1}, where k=1,...,K describing which of\n",
      "theKclusters the data point xnis assigned to, so that if data point xnis assigned to\n",
      "cluster kthenrnk=1, andrnj=0forj̸=k. This is known as the 1-of-Kcoding\n",
      "scheme. We can then deﬁne an objective function, sometimes called a distortion\n",
      "measure , given by\n",
      "J=N∑\n",
      "n=1K∑\n",
      "k=1rnk∥xn−µk∥2(9.1)\n",
      "which represents the sum of the squares of the distances of each data point to its9.1.K-means Clustering 425\n",
      "assigned vector µk. Our goal is to ﬁnd values for the {rnk}and the {µk}so as to\n",
      "minimize J. We can do this through an iterative procedure in which each iteration\n",
      "involves two successive steps corresponding to successive optimizations with respect\n",
      "to thernkand the µk. First we choose some initial values for the µk. Then in the ﬁrst\n",
      "phase we minimize Jwith respect to the rnk, keeping the µkﬁxed. In the second\n",
      "phase we minimize Jwith respect to the µk, keeping rnkﬁxed. This two-stage\n",
      "optimization is then repeated until convergence. We shall see that these two stages\n",
      "of updating rnkand updating µkcorrespond respectively to the E (expectation) and\n",
      "M (maximization) steps of the EM algorithm, and to emphasize this we shall use the Section 9.4\n",
      "terms E step and M step in the context of the K-means algorithm.\n",
      "Consider ﬁrst the determination of the rnk. Because Jin (9.1) is a linear func-\n",
      "tion of rnk, this optimization can be performed easily to give a closed form solution.\n",
      "The terms involving different nare independent and so we can optimize for each\n",
      "nseparately by choosing rnkto be1for whichever value of kgives the minimum\n",
      "value of ∥xn−µk∥2. In other words, we simply assign the nthdata point to the\n",
      "closest cluster centre. More formally, this can be expressed as\n",
      "rnk={1ifk=a r gm i nj∥xn−µj∥2\n",
      "0otherwise .(9.2)\n",
      "Now consider the optimization of the µkwith the rnkheld ﬁxed. The objective\n",
      "function Jis a quadratic function of µk, and it can be minimized by setting its\n",
      "derivative with respect to µkto zero giving\n",
      "2N∑\n",
      "n=1rnk(xn−µk)=0 (9.3)\n",
      "which we can easily solve for µkto give\n",
      "µk=∑\n",
      "nrnkxn∑\n",
      "nrnk. (9.4)\n",
      "The denominator in this expression is equal to the number of points assigned to\n",
      "cluster k, and so this result has a simple interpretation, namely set µkequal to the\n",
      "mean of all of the data points xnassigned to cluster k. For this reason, the procedure\n",
      "is known as the K-means algorithm.\n",
      "The two phases of re-assigning data points to clusters and re-computing the clus-\n",
      "ter means are repeated in turn until there is no further change in the assignments (or\n",
      "until some maximum number of iterations is exceeded). Because each phase reduces\n",
      "the value of the objective function J, convergence of the algorithm is assured. How- Exercise 9.1\n",
      "ever, it may converge to a local rather than global minimum of J. The convergence\n",
      "properties of the K-means algorithm were studied by MacQueen (1967).\n",
      "TheK-means algorithm is illustrated using the Old Faithful data set in Fig- Appendix A\n",
      "ure 9.1. For the purposes of this example, we have made a linear re-scaling of thedata, known as standardizing , such that each of the variables has zero mean and\n",
      "unit standard deviation. For this example, we have chosen K=2, and so in this426 9. MIXTURE MODELS AND EM\n",
      "(a)\n",
      "−2 0 2−202 (b)\n",
      "−2 0 2−202 (c)\n",
      "−2 0 2−202\n",
      "(d)\n",
      "−2 0 2−202 (e)\n",
      "−2 0 2−202 (f)\n",
      "−2 0 2−202\n",
      "(g)\n",
      "−2 0 2−202 (h)\n",
      "−2 0 2−202 (i)\n",
      "−2 0 2−202\n",
      "Figure 9.1 Illustration of the K-means algorithm using the re-scaled Old Faithful data set. (a) Green points\n",
      "denote the data set in a two-dimensional Euclidean space. The initial choices for centres µ1andµ2are shown\n",
      "by the red and blue crosses, respectively. (b) In the initial E step, each data point is assigned either to the red\n",
      "cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the\n",
      "points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta\n",
      "line, they lie on. (c) In the subsequent M step, each cluster centre is re-computed to be the mean of the points\n",
      "assigned to the corresponding cluster. (d)–(i) show successive E and M steps through to ﬁnal convergence of\n",
      "the algorithm.9.1.K-means Clustering 427\n",
      "Figure 9.2 Plot of the cost function Jgiven by\n",
      "(9.1) after each E step (blue points)\n",
      "and M step (red points) of the K-\n",
      "means algorithm for the example\n",
      "shown in Figure 9.1. The algo-\n",
      "rithm has converged after the third\n",
      "M step, and the ﬁnal EM cycle pro-\n",
      "duces no changes in either the as-\n",
      "signments or the prototype vectors.J\n",
      "1 2 3 405001000\n",
      "case, the assignment of each data point to the nearest cluster centre is equivalent to a\n",
      "classiﬁcation of the data points according to which side they lie of the perpendicular\n",
      "bisector of the two cluster centres. A plot of the cost function Jgiven by (9.1) for\n",
      "the Old Faithful example is shown in Figure 9.2.\n",
      "Note that we have deliberately chosen poor initial values for the cluster centres\n",
      "so that the algorithm takes several steps before convergence. In practice, a better\n",
      "initialization procedure would be to choose the cluster centres µkto be equal to a\n",
      "random subset of Kdata points. It is also worth noting that the K-means algorithm\n",
      "itself is often used to initialize the parameters in a Gaussian mixture model before\n",
      "applying the EM algorithm. Section 9.2.2\n",
      "A direct implementation of the K-means algorithm as discussed here can be\n",
      "relatively slow, because in each E step it is necessary to compute the Euclidean dis-\n",
      "tance between every prototype vector and every data point. Various schemes have\n",
      "been proposed for speeding up the K-means algorithm, some of which are based on\n",
      "precomputing a data structure such as a tree such that nearby points are in the same\n",
      "subtree (Ramasubramanian and Paliwal, 1990; Moore, 2000). Other approaches\n",
      "make use of the triangle inequality for distances, thereby avoiding unnecessary dis-\n",
      "tance calculations (Hodgson, 1998; Elkan, 2003).\n",
      "So far, we have considered a batch version of K-means in which the whole data\n",
      "set is used together to update the prototype vectors. We can also derive an on-line\n",
      "stochastic algorithm (MacQueen, 1967) by applying the Robbins-Monro procedure Section 2.3.5\n",
      "to the problem of ﬁnding the roots of the regression function given by the derivatives\n",
      "ofJin (9.1) with respect to µk. This leads to a sequential update in which, for each Exercise 9.2\n",
      "data point xnin turn, we update the nearest prototype µkusing\n",
      "µnew\n",
      "k=µold\n",
      "k+ηn(xn−µold\n",
      "k) (9.5)\n",
      "where ηnis the learning rate parameter, which is typically made to decrease mono-\n",
      "tonically as more data points are considered.\n",
      "TheK-means algorithm is based on the use of squared Euclidean distance as the\n",
      "measure of dissimilarity between a data point and a prototype vector. Not only does\n",
      "this limit the type of data variables that can be considered (it would be inappropriate\n",
      "for cases where some or all of the variables represent categorical labels for instance),428 9. MIXTURE MODELS AND EM\n",
      "but it can also make the determination of the cluster means nonrobust to outliers. We Section 2.3.7\n",
      "can generalize the K-means algorithm by introducing a more general dissimilarity\n",
      "measure V(x,x′)between two vectors xandx′and then minimizing the following\n",
      "distortion measure\n",
      "˜J=N∑\n",
      "n=1K∑\n",
      "k=1rnkV(xn,µk) (9.6)\n",
      "which gives the K-medoids algorithm. The E step again involves, for given cluster\n",
      "prototypes µk, assigning each data point to the cluster for which the dissimilarity to\n",
      "the corresponding prototype is smallest. The computational cost of this is O(KN),\n",
      "as is the case for the standard K-means algorithm. For a general choice of dissimi-\n",
      "larity measure, the M step is potentially more complex than for K-means, and so it\n",
      "is common to restrict each cluster prototype to be equal to one of the data vectors as-\n",
      "signed to that cluster, as this allows the algorithm to be implemented for any choice\n",
      "of dissimilarity measure V(·,·)so long as it can be readily evaluated. Thus the M\n",
      "step involves, for each cluster k, a discrete search over the Nkpoints assigned to that\n",
      "cluster, which requires O(N2\n",
      "k)evaluations of V(·,·).\n",
      "One notable feature of the K-means algorithm is that at each iteration, every\n",
      "data point is assigned uniquely to one, and only one, of the clusters. Whereas some\n",
      "data points will be much closer to a particular centre µkthan to any other centre,\n",
      "there may be other data points that lie roughly midway between cluster centres. Inthe latter case, it is not clear that the hard assignment to the nearest cluster is the\n",
      "most appropriate. We shall see in the next section that by adopting a probabilistic\n",
      "approach, we obtain ‘soft’ assignments of data points to clusters in a way that reﬂectsthe level of uncertainty over the most appropriate assignment. This probabilistic\n",
      "formulation brings with it numerous beneﬁts.\n",
      "9.1.1 Image segmentation and compression\n",
      "As an illustration of the application of the K-means algorithm, we consider\n",
      "the related problems of image segmentation and image compression. The goal ofsegmentation is to partition an image into regions each of which has a reasonably\n",
      "homogeneous visual appearance or which corresponds to objects or parts of objects\n",
      "(Forsyth and Ponce, 2003). Each pixel in an image is a point in a 3-dimensional spacecomprising the intensities of the red, blue, and green channels, and our segmentation\n",
      "algorithm simply treats each pixel in the image as a separate data point. Note that\n",
      "strictly this space is not Euclidean because the channel intensities are bounded bythe interval [0,1]. Nevertheless, we can apply the K-means algorithm without difﬁ-\n",
      "culty. We illustrate the result of running K-means to convergence, for any particular\n",
      "value of K, by re-drawing the image replacing each pixel vector with the {R, G, B }\n",
      "intensity triplet given by the centre µ\n",
      "kto which that pixel has been assigned. Results\n",
      "for various values of Kare shown in Figure 9.3. We see that for a given value of K,\n",
      "the algorithm is representing the image using a palette of only Kcolours. It should\n",
      "be emphasized that this use of K-means is not a particularly sophisticated approach\n",
      "to image segmentation, not least because it takes no account of the spatial proximityof different pixels. The image segmentation problem is in general extremely difﬁcult9.1.K-means Clustering 429\n",
      "K=2\n",
      " K=3\n",
      " K=1 0\n",
      " Original image\n",
      "Figure 9.3 Two examples of the application of the K-means clustering algorithm to image segmentation show-\n",
      "ing the initial images together with their K-means segmentations obtained using various values of K. This\n",
      "also illustrates of the use of vector quantization for data compression, in which smaller values of Kgive higher\n",
      "compression at the expense of poorer image quality.\n",
      "and remains the subject of active research and is introduced here simply to illustrate\n",
      "the behaviour of the K-means algorithm.\n",
      "We can also use the result of a clustering algorithm to perform data compres-\n",
      "sion. It is important to distinguish between lossless data compression , in which\n",
      "the goal is to be able to reconstruct the original data exactly from the compressed\n",
      "representation, and lossy data compression , in which we accept some errors in the\n",
      "reconstruction in return for higher levels of compression than can be achieved in the\n",
      "lossless case. We can apply the K-means algorithm to the problem of lossy data\n",
      "compression as follows. For each of the Ndata points, we store only the identity\n",
      "kof the cluster to which it is assigned. We also store the values of the Kclus-\n",
      "ter centres µk, which typically requires signiﬁcantly less data, provided we choose\n",
      "K≪N. Each data point is then approximated by its nearest centre µk. New data\n",
      "points can similarly be compressed by ﬁrst ﬁnding the nearest µkand then storing\n",
      "the label kinstead of the original data vector. This framework is often called vector\n",
      "quantization , and the vectors µkare called code-book vectors .430 9. MIXTURE MODELS AND EM\n",
      "The image segmentation problem discussed above also provides an illustration\n",
      "of the use of clustering for data compression. Suppose the original image has N\n",
      "pixels comprising {R, G, B }values each of which is stored with 8 bits of precision.\n",
      "Then to transmit the whole image directly would cost 24Nbits. Now suppose we\n",
      "ﬁrst run K-means on the image data, and then instead of transmitting the original\n",
      "pixel intensity vectors we transmit the identity of the nearest vector µk. Because\n",
      "there are Ksuch vectors, this requires log2Kbits per pixel. We must also transmit\n",
      "theKcode book vectors µk, which requires 24 Kbits, and so the total number of\n",
      "bits required to transmit the image is 24K+Nlog2K(rounding up to the nearest\n",
      "integer). The original image shown in Figure 9.3 has 240×180 = 43 ,200pixels\n",
      "and so requires 24×43,200 = 1 ,036,800bits to transmit directly. By comparison,\n",
      "the compressed images require 43,248bits (K=2),86,472bits (K=3), and\n",
      "173,040bits (K=1 0 ), respectively, to transmit. These represent compression ratios\n",
      "compared to the original image of 4.2%, 8.3%, and 16.7%, respectively. We see that\n",
      "there is a trade-off between degree of compression and image quality. Note that our\n",
      "aim in this example is to illustrate the K-means algorithm. If we had been aiming to\n",
      "produce a good image compressor, then it would be more fruitful to consider small\n",
      "blocks of adjacent pixels, for instance 5×5, and thereby exploit the correlations that\n",
      "exist in natural images between nearby pixels.\n",
      "9.2. Mixtures of Gaussians\n",
      "In Section 2.3.9 we motivated the Gaussian mixture model as a simple linear super-position of Gaussian components, aimed at providing a richer class of density mod-els than the single Gaussian. We now turn to a formulation of Gaussian mixtures in\n",
      "terms of discrete latent variables. This will provide us with a deeper insight into this\n",
      "important distribution, and will also serve to motivate the expectation-maximizationalgorithm.\n",
      "Recall from (2.188) that the Gaussian mixture distribution can be written as a\n",
      "linear superposition of Gaussians in the form\n",
      "p(x)=\n",
      "K∑\n",
      "k=1πkN(x|µk,Σk). (9.7)\n",
      "Let us introduce a K-dimensional binary random variable zhaving a 1-of- Krepre-\n",
      "sentation in which a particular element zkis equal to 1and all other elements are\n",
      "equal to 0. The values of zktherefore satisfy zk∈{0,1}and∑\n",
      "kzk=1, and we\n",
      "see that there are Kpossible states for the vector zaccording to which element is\n",
      "nonzero. We shall deﬁne the joint distribution p(x,z)in terms of a marginal dis-\n",
      "tribution p(z)and a conditional distribution p(x|z), corresponding to the graphical\n",
      "model in Figure 9.4. The marginal distribution over zis speciﬁed in terms of the\n",
      "mixing coefﬁcients πk, such that\n",
      "p(zk=1 )= πk9.2. Mixtures of Gaussians 431\n",
      "Figure 9.4 Graphical representation of a mixture model, in which\n",
      "the joint distribution is expressed in the form p(x,z)=\n",
      "p(z)p(x|z).\n",
      "xz\n",
      "where the parameters {πk}must satisfy\n",
      "0⩽πk⩽1 (9.8)\n",
      "together with\n",
      "K∑\n",
      "k=1πk=1 (9.9)\n",
      "in order to be valid probabilities. Because zuses a 1-of- Krepresentation, we can\n",
      "also write this distribution in the form\n",
      "p(z)=K∏\n",
      "k=1πzk\n",
      "k. (9.10)\n",
      "Similarly, the conditional distribution of xgiven a particular value for zis a Gaussian\n",
      "p(x|zk=1 )= N(x|µk,Σk)\n",
      "which can also be written in the form\n",
      "p(x|z)=K∏\n",
      "k=1N(x|µk,Σk)zk. (9.11)\n",
      "The joint distribution is given by p(z)p(x|z), and the marginal distribution of xis\n",
      "then obtained by summing the joint distribution over all possible states of zto give Exercise 9.3\n",
      "p(x)=∑\n",
      "zp(z)p(x|z)=K∑\n",
      "k=1πkN(x|µk,Σk) (9.12)\n",
      "where we have made use of (9.10) and (9.11). Thus the marginal distribution of xis\n",
      "a Gaussian mixture of the form (9.7). If we have several observations x1,...,xN,\n",
      "then, because we have represented the marginal distribution in the form p(x)=∑\n",
      "zp(x,z), it follows that for every observed data point xnthere is a corresponding\n",
      "latent variable zn.\n",
      "We have therefore found an equivalent formulation of the Gaussian mixture in-\n",
      "volving an explicit latent variable. It might seem that we have not gained much\n",
      "by doing so. However, we are now able to work with the joint distribution p(x,z)432 9. MIXTURE MODELS AND EM\n",
      "instead of the marginal distribution p(x), and this will lead to signiﬁcant simpliﬁca-\n",
      "tions, most notably through the introduction of the expectation-maximization (EM)algorithm.\n",
      "Another quantity that will play an important role is the conditional probability\n",
      "ofzgivenx. We shall use γ(z\n",
      "k)to denote p(zk=1|x), whose value can be found\n",
      "using Bayes’ theorem\n",
      "γ(zk)≡p(zk=1|x)=p(zk=1 )p(x|zk=1 )\n",
      "K∑\n",
      "j=1p(zj=1 )p(x|zj=1 )\n",
      "=πkN(x|µk,Σk)\n",
      "K∑\n",
      "j=1πjN(x|µj,Σj). (9.13)\n",
      "We shall view πkas the prior probability of zk=1, and the quantity γ(zk)as the\n",
      "corresponding posterior probability once we have observed x. As we shall see later,\n",
      "γ(zk)can also be viewed as the responsibility that component ktakes for ‘explain-\n",
      "ing’ the observation x.\n",
      "We can use the technique of ancestral sampling to generate random samples Section 8.1.2\n",
      "distributed according to the Gaussian mixture model. To do this, we ﬁrst generate avalue for z, which we denote\n",
      "ˆz, from the marginal distribution p(z)and then generate\n",
      "a value for xfrom the conditional distribution p(x|ˆz). Techniques for sampling from\n",
      "standard distributions are discussed in Chapter 11. We can depict samples from the\n",
      "joint distribution p(x,z)by plotting points at the corresponding values of xand\n",
      "then colouring them according to the value of z, in other words according to which\n",
      "Gaussian component was responsible for generating them, as shown in Figure 9.5(a).\n",
      "Similarly samples from the marginal distribution p(x)are obtained by taking the\n",
      "samples from the joint distribution and ignoring the values of z. These are illustrated\n",
      "in Figure 9.5(b) by plotting the xvalues without any coloured labels.\n",
      "We can also use this synthetic data set to illustrate the ‘responsibilities’ by eval-\n",
      "uating, for every data point, the posterior probability for each component in themixture distribution from which this data set was generated. In particular, we can\n",
      "represent the value of the responsibilities γ(z\n",
      "nk)associated with data point xnby\n",
      "plotting the corresponding point using proportions of red, blue, and green ink givenbyγ(z\n",
      "nk)fork=1,2,3, respectively, as shown in Figure 9.5(c). So, for instance,\n",
      "a data point for which γ(zn1)=1 will be coloured red, whereas one for which\n",
      "γ(zn2)=γ(zn3)=0.5will be coloured with equal proportions of blue and green\n",
      "ink and so will appear cyan. This should be compared with Figure 9.5(a) in which\n",
      "the data points were labelled using the true identity of the component from which\n",
      "they were generated.\n",
      "9.2.1 Maximum likelihood\n",
      "Suppose we have a data set of observations {x1,...,xN}, and we wish to model\n",
      "this data using a mixture of Gaussians. We can represent this data set as an N×D9.2. Mixtures of Gaussians 433\n",
      "(a)\n",
      "0 0.5 100.51(b)\n",
      "0 0.5 100.51(c)\n",
      "0 0.5 100.51\n",
      "Figure 9.5 Example of 500 points drawn from the mixture of 3 Gaussians shown in Figure 2.23. (a) Samples\n",
      "from the joint distribution p(z)p(x|z)in which the three states of z, corresponding to the three components of the\n",
      "mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution\n",
      "p(x), which is obtained by simply ignoring the values of zand just plotting the xvalues. The data set in (a) is\n",
      "said to be complete , whereas that in (b) is incomplete . (c) The same samples in which the colours represent the\n",
      "value of the responsibilities γ(znk)associated with data point xn, obtained by plotting the corresponding point\n",
      "using proportions of red, blue, and green ink given by γ(znk)fork=1,2,3, respectively\n",
      "matrix Xin which the nthrow is given by xT\n",
      "n. Similarly, the corresponding latent\n",
      "variables will be denoted by an N×Kmatrix Zwith rows zT\n",
      "n. If we assume that\n",
      "the data points are drawn independently from the distribution, then we can express\n",
      "the Gaussian mixture model for this i.i.d. data set using the graphical representation\n",
      "shown in Figure 9.6. From (9.7) the log of the likelihood function is given by\n",
      "lnp(X|π,µ,Σ)=N∑\n",
      "n=1ln{K∑\n",
      "k=1πkN(xn|µk,Σk)}\n",
      ". (9.14)\n",
      "Before discussing how to maximize this function, it is worth emphasizing that\n",
      "there is a signiﬁcant problem associated with the maximum likelihood framework\n",
      "applied to Gaussian mixture models, due to the presence of singularities. For sim-\n",
      "plicity, consider a Gaussian mixture whose components have covariance matrices\n",
      "given by Σk=σ2\n",
      "kI, where Iis the unit matrix, although the conclusions will hold\n",
      "for general covariance matrices. Suppose that one of the components of the mixture\n",
      "model, let us say the jthcomponent, has its mean µjexactly equal to one of the data\n",
      "Figure 9.6 Graphical representation of a Gaussian mixture model\n",
      "for a set of Ni.i.d. data points {xn}, with corresponding\n",
      "latent points {zn}, where n=1,...,N .\n",
      "xnzn\n",
      "Nµ Σπ434 9. MIXTURE MODELS AND EM\n",
      "Figure 9.7 Illustration of how singularities in the\n",
      "likelihood function arise with mixtures\n",
      "of Gaussians. This should be com-\n",
      "pared with the case of a single Gaus-\n",
      "sian shown in Figure 1.14 for which no\n",
      "singularities arise.\n",
      "xp(x)\n",
      "points so that µj=xnfor some value of n. This data point will then contribute a\n",
      "term in the likelihood function of the form\n",
      "N(xn|xn,σ2\n",
      "jI)=1\n",
      "(2π)1/21\n",
      "σj. (9.15)\n",
      "If we consider the limit σj→0, then we see that this term goes to inﬁnity and\n",
      "so the log likelihood function will also go to inﬁnity. Thus the maximization of\n",
      "the log likelihood function is not a well posed problem because such singularities\n",
      "will always be present and will occur whenever one of the Gaussian components\n",
      "‘collapses’ onto a speciﬁc data point. Recall that this problem did not arise in the\n",
      "case of a single Gaussian distribution. To understand the difference, note that if a\n",
      "single Gaussian collapses onto a data point it will contribute multiplicative factors\n",
      "to the likelihood function arising from the other data points and these factors will go\n",
      "to zero exponentially fast, giving an overall likelihood that goes to zero rather than\n",
      "inﬁnity. However, once we have (at least) two components in the mixture, one of\n",
      "the components can have a ﬁnite variance and therefore assign ﬁnite probability to\n",
      "all of the data points while the other component can shrink onto one speciﬁc data\n",
      "point and thereby contribute an ever increasing additive value to the log likelihood.\n",
      "This is illustrated in Figure 9.7. These singularities provide another example of the\n",
      "severe over-ﬁtting that can occur in a maximum likelihood approach. We shall see\n",
      "that this difﬁculty does not occur if we adopt a Bayesian approach. For the moment, Section 10.1\n",
      "however, we simply note that in applying maximum likelihood to Gaussian mixture\n",
      "models we must take steps to avoid ﬁnding such pathological solutions and instead\n",
      "seek local maxima of the likelihood function that are well behaved. We can hope to\n",
      "avoid the singularities by using suitable heuristics, for instance by detecting when a\n",
      "Gaussian component is collapsing and resetting its mean to a randomly chosen value\n",
      "while also resetting its covariance to some large value, and then continuing with the\n",
      "optimization.\n",
      "A further issue in ﬁnding maximum likelihood solutions arises from the fact\n",
      "that for any given maximum likelihood solution, a K-component mixture will have\n",
      "a total of K!equivalent solutions corresponding to the K!ways of assigning K\n",
      "sets of parameters to Kcomponents. In other words, for any given (nondegenerate)\n",
      "point in the space of parameter values there will be a further K!−1additional points\n",
      "all of which give rise to exactly the same distribution. This problem is known as9.2. Mixtures of Gaussians 435\n",
      "identiﬁability (Casella and Berger, 2002) and is an important issue when we wish to\n",
      "interpret the parameter values discovered by a model. Identiﬁability will also arise\n",
      "when we discuss models having continuous latent variables in Chapter 12. However,\n",
      "for the purposes of ﬁnding a good density model, it is irrelevant because any of the\n",
      "equivalent solutions is as good as any other.\n",
      "Maximizing the log likelihood function (9.14) for a Gaussian mixture model\n",
      "turns out to be a more complex problem than for the case of a single Gaussian. The\n",
      "difﬁculty arises from the presence of the summation over kthat appears inside the\n",
      "logarithm in (9.14), so that the logarithm function no longer acts directly on the\n",
      "Gaussian. If we set the derivatives of the log likelihood to zero, we will no longer\n",
      "obtain a closed form solution, as we shall see shortly.\n",
      "One approach is to apply gradient-based optimization techniques (Fletcher, 1987;\n",
      "Nocedal and Wright, 1999; Bishop and Nabney, 2008). Although gradient-based\n",
      "techniques are feasible, and indeed will play an important role when we discuss\n",
      "mixture density networks in Chapter 5, we now consider an alternative approach\n",
      "known as the EM algorithm which has broad applicability and which will lay the\n",
      "foundations for a discussion of variational inference techniques in Chapter 10.\n",
      "9.2.2 EM for Gaussian mixtures\n",
      "An elegant and powerful method for ﬁnding maximum likelihood solutions for\n",
      "models with latent variables is called the expectation-maximization algorithm, or EM\n",
      "algorithm (Dempster et al. , 1977; McLachlan and Krishnan, 1997). Later we shall\n",
      "give a general treatment of EM, and we shall also show how EM can be generalized\n",
      "to obtain the variational inference framework. Initially, we shall motivate the EM Section 10.1\n",
      "algorithm by giving a relatively informal treatment in the context of the Gaussian\n",
      "mixture model. We emphasize, however, that EM has broad applicability, and indeed\n",
      "it will be encountered in the context of a variety of different models in this book.\n",
      "Let us begin by writing down the conditions that must be satisﬁed at a maximum\n",
      "of the likelihood function. Setting the derivatives of lnp(X|π,µ,Σ)in (9.14) with\n",
      "respect to the means µkof the Gaussian components to zero, we obtain\n",
      "0=−N∑\n",
      "n=1πkN(xn|µk,Σk)∑\n",
      "jπjN(xn|µj,Σj)\n",
      "\n",
      "γ(znk)Σk(xn−µk) (9.16)\n",
      "where we have made use of the form (2.43) for the Gaussian distribution. Note that\n",
      "the posterior probabilities, or responsibilities, given by (9.13) appear naturally on\n",
      "the right-hand side. Multiplying by Σ−1\n",
      "k(which we assume to be nonsingular) and\n",
      "rearranging we obtain\n",
      "µk=1\n",
      "NkN∑\n",
      "n=1γ(znk)xn (9.17)\n",
      "where we have deﬁned\n",
      "Nk=N∑\n",
      "n=1γ(znk). (9.18)436 9. MIXTURE MODELS AND EM\n",
      "We can interpret Nkas the effective number of points assigned to cluster k. Note\n",
      "carefully the form of this solution. We see that the mean µkfor the kthGaussian\n",
      "component is obtained by taking a weighted mean of all of the points in the data set,\n",
      "in which the weighting factor for data point xnis given by the posterior probability\n",
      "γ(znk)that component kwas responsible for generating xn.\n",
      "If we set the derivative of lnp(X|π,µ,Σ)with respect to Σkto zero, and follow\n",
      "a similar line of reasoning, making use of the result for the maximum likelihood\n",
      "solution for the covariance matrix of a single Gaussian, we obtain Section 2.3.4\n",
      "Σk=1\n",
      "NkN∑\n",
      "n=1γ(znk)(xn−µk)(xn−µk)T(9.19)\n",
      "which has the same form as the corresponding result for a single Gaussian ﬁtted to\n",
      "the data set, but again with each data point weighted by the corresponding poste-\n",
      "rior probability and with the denominator given by the effective number of pointsassociated with the corresponding component.\n",
      "Finally, we maximize lnp(X|π,µ,Σ)with respect to the mixing coefﬁcients\n",
      "π\n",
      "k. Here we must take account of the constraint (9.9), which requires the mixing\n",
      "coefﬁcients to sum to one. This can be achieved using a Lagrange multiplier and Appendix E\n",
      "maximizing the following quantity\n",
      "lnp(X|π,µ,Σ)+λ(K∑\n",
      "k=1πk−1)\n",
      "(9.20)\n",
      "which gives\n",
      "0=N∑\n",
      "n=1N(xn|µk,Σk)∑\n",
      "jπjN(xn|µj,Σj)+λ (9.21)\n",
      "where again we see the appearance of the responsibilities. If we now multiply both\n",
      "sides by πkand sum over kmaking use of the constraint (9.9), we ﬁnd λ=−N.\n",
      "Using this to eliminate λand rearranging we obtain\n",
      "πk=Nk\n",
      "N(9.22)\n",
      "so that the mixing coefﬁcient for the kthcomponent is given by the average respon-\n",
      "sibility which that component takes for explaining the data points.\n",
      "It is worth emphasizing that the results (9.17), (9.19), and (9.22) do not con-\n",
      "stitute a closed-form solution for the parameters of the mixture model because the\n",
      "responsibilities γ(znk)depend on those parameters in a complex way through (9.13).\n",
      "However, these results do suggest a simple iterative scheme for ﬁnding a solution tothe maximum likelihood problem, which as we shall see turns out to be an instance\n",
      "of the EM algorithm for the particular case of the Gaussian mixture model. We\n",
      "ﬁrst choose some initial values for the means, covariances, and mixing coefﬁcients.\n",
      "Then we alternate between the following two updates that we shall call the E step9.2. Mixtures of Gaussians 437\n",
      "(a) −2 0 2−202\n",
      "(b) −2 0 2−202\n",
      "(c)L=1\n",
      "−2 0 2−202\n",
      "(d)L=2\n",
      "−2 0 2−202\n",
      "(e)L=5\n",
      "−2 0 2−202\n",
      "(f)L=2 0\n",
      "−2 0 2−202\n",
      "Figure 9.8 Illustration of the EM algorithm using the Old Faithful set as used for the illustration of the K-means\n",
      "algorithm in Figure 9.1. See the text for details.\n",
      "and the M step, for reasons that will become apparent shortly. In the expectation\n",
      "step, or E step, we use the current values for the parameters to evaluate the posterior\n",
      "probabilities, or responsibilities, given by (9.13). We then use these probabilities in\n",
      "themaximization step, or M step, to re-estimate the means, covariances, and mix-\n",
      "ing coefﬁcients using the results (9.17), (9.19), and (9.22). Note that in so doing\n",
      "we ﬁrst evaluate the new means using (9.17) and then use these new values to ﬁnd\n",
      "the covariances using (9.19), in keeping with the corresponding result for a single\n",
      "Gaussian distribution. We shall show that each update to the parameters resulting\n",
      "from an E step followed by an M step is guaranteed to increase the log likelihood\n",
      "function. In practice, the algorithm is deemed to have converged when the change Section 9.4\n",
      "in the log likelihood function, or alternatively in the parameters, falls below some\n",
      "threshold. We illustrate the EM algorithm for a mixture of two Gaussians applied to\n",
      "the rescaled Old Faithful data set in Figure 9.8. Here a mixture of two Gaussians\n",
      "is used, with centres initialized using the same values as for the K-means algorithm\n",
      "in Figure 9.1, and with precision matrices initialized to be proportional to the unit\n",
      "matrix. Plot (a) shows the data points in green, together with the initial conﬁgura-\n",
      "tion of the mixture model in which the one standard-deviation contours for the two438 9. MIXTURE MODELS AND EM\n",
      "Gaussian components are shown as blue and red circles. Plot (b) shows the result\n",
      "of the initial E step, in which each data point is depicted using a proportion of blueink equal to the posterior probability of having been generated from the blue com-\n",
      "ponent, and a corresponding proportion of red ink given by the posterior probability\n",
      "of having been generated by the red component. Thus, points that have a signiﬁcantprobability for belonging to either cluster appear purple. The situation after the ﬁrst\n",
      "M step is shown in plot (c), in which the mean of the blue Gaussian has moved to\n",
      "the mean of the data set, weighted by the probabilities of each data point belongingto the blue cluster, in other words it has moved to the centre of mass of the blue ink.\n",
      "Similarly, the covariance of the blue Gaussian is set equal to the covariance of the\n",
      "blue ink. Analogous results hold for the red component. Plots (d), (e), and (f) show\n",
      "the results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the\n",
      "algorithm is close to convergence.\n",
      "Note that the EM algorithm takes many more iterations to reach (approximate)\n",
      "convergence compared with the K-means algorithm, and that each cycle requires\n",
      "signiﬁcantly more computation. It is therefore common to run the K-means algo-\n",
      "rithm in order to ﬁnd a suitable initialization for a Gaussian mixture model that is\n",
      "subsequently adapted using EM. The covariance matrices can conveniently be ini-\n",
      "tialized to the sample covariances of the clusters found by the K-means algorithm,\n",
      "and the mixing coefﬁcients can be set to the fractions of data points assigned to the\n",
      "respective clusters. As with gradient-based approaches for maximizing the log like-\n",
      "lihood, techniques must be employed to avoid singularities of the likelihood functionin which a Gaussian component collapses onto a particular data point. It should be\n",
      "emphasized that there will generally be multiple local maxima of the log likelihood\n",
      "function, and that EM is not guaranteed to ﬁnd the largest of these maxima. Becausethe EM algorithm for Gaussian mixtures plays such an important role, we summarize\n",
      "it below.\n",
      "EM for Gaussian Mixtures\n",
      "Given a Gaussian mixture model, the goal is to maximize the likelihood function\n",
      "with respect to the parameters (comprising the means and covariances of the\n",
      "components and the mixing coefﬁcients).\n",
      "1. Initialize the means µ\n",
      "k, covariances Σkand mixing coefﬁcients πk, and\n",
      "evaluate the initial value of the log likelihood.\n",
      "2.E step . Evaluate the responsibilities using the current parameter values\n",
      "γ(znk)=πkN(xn|µk,Σk)\n",
      "K∑\n",
      "j=1πjN(xn|µj,Σj). (9.23)9.3. An Alternative View of EM 439\n",
      "3.M step . Re-estimate the parameters using the current responsibilities\n",
      "µnew\n",
      "k=1\n",
      "NkN∑\n",
      "n=1γ(znk)xn (9.24)\n",
      "Σnew\n",
      "k=1\n",
      "NkN∑\n",
      "n=1γ(znk)(xn−µnew\n",
      "k)(xn−µnew\n",
      "k)T(9.25)\n",
      "πnew\n",
      "k=Nk\n",
      "N(9.26)\n",
      "where\n",
      "Nk=N∑\n",
      "n=1γ(znk). (9.27)\n",
      "4. Evaluate the log likelihood\n",
      "lnp(X|µ,Σ,π)=N∑\n",
      "n=1ln{K∑\n",
      "k=1πkN(xn|µk,Σk)}\n",
      "(9.28)\n",
      "and check for convergence of either the parameters or the log likelihood. If\n",
      "the convergence criterion is not satisﬁed return to step 2.\n",
      "9.3. An Alternative View of EM\n",
      "In this section, we present a complementary view of the EM algorithm that recog-nizes the key role played by latent variables. We discuss this approach ﬁrst of all\n",
      "in an abstract setting, and then for illustration we consider once again the case of\n",
      "Gaussian mixtures.\n",
      "The goal of the EM algorithm is to ﬁnd maximum likelihood solutions for mod-\n",
      "els having latent variables. We denote the set of all observed data by X, in which the\n",
      "n\n",
      "throw represents xT\n",
      "n, and similarly we denote the set of all latent variables by Z,\n",
      "with a corresponding row zT\n",
      "n. The set of all model parameters is denoted by θ, and\n",
      "so the log likelihood function is given by\n",
      "lnp(X|θ)=l n{∑\n",
      "Zp(X,Z|θ)}\n",
      ". (9.29)\n",
      "Note that our discussion will apply equally well to continuous latent variables simply\n",
      "by replacing the sum over Zwith an integral.\n",
      "A key observation is that the summation over the latent variables appears inside\n",
      "the logarithm. Even if the joint distribution p(X,Z|θ)belongs to the exponential440 9. MIXTURE MODELS AND EM\n",
      "family, the marginal distribution p(X|θ)typically does not as a result of this sum-\n",
      "mation. The presence of the sum prevents the logarithm from acting directly on thejoint distribution, resulting in complicated expressions for the maximum likelihood\n",
      "solution.\n",
      "Now suppose that, for each observation in X, we were told the corresponding\n",
      "value of the latent variable Z. We shall call {X,Z}thecomplete data set, and we\n",
      "shall refer to the actual observed data Xasincomplete , as illustrated in Figure 9.5.\n",
      "The likelihood function for the complete data set simply takes the form lnp(X,Z|θ),\n",
      "and we shall suppose that maximization of this complete-data log likelihood function\n",
      "is straightforward.\n",
      "In practice, however, we are not given the complete data set {X,Z}, but only\n",
      "the incomplete data X. Our state of knowledge of the values of the latent variables\n",
      "inZis given only by the posterior distribution p(Z|X,θ). Because we cannot use\n",
      "the complete-data log likelihood, we consider instead its expected value under the\n",
      "posterior distribution of the latent variable, which corresponds (as we shall see) to the\n",
      "E step of the EM algorithm. In the subsequent M step, we maximize this expectation.If the current estimate for the parameters is denoted θ\n",
      "old, then a pair of successive\n",
      "E and M steps gives rise to a revised estimate θnew. The algorithm is initialized by\n",
      "choosing some starting value for the parameters θ0. The use of the expectation may\n",
      "seem somewhat arbitrary. However, we shall see the motivation for this choice when\n",
      "we give a deeper treatment of EM in Section 9.4.\n",
      "In the E step, we use the current parameter values θoldto ﬁnd the posterior\n",
      "distribution of the latent variables given by p(Z|X,θold). We then use this posterior\n",
      "distribution to ﬁnd the expectation of the complete-data log likelihood evaluated for\n",
      "some general parameter value θ. This expectation, denoted Q(θ,θold),i sg i v e nb y\n",
      "Q(θ,θold)=∑\n",
      "Zp(Z|X,θold)l np(X,Z|θ). (9.30)\n",
      "In the M step, we determine the revised parameter estimate θnewby maximizing this\n",
      "function\n",
      "θnew=a r gm a x\n",
      "θQ(θ,θold). (9.31)\n",
      "Note that in the deﬁnition of Q(θ,θold), the logarithm acts directly on the joint\n",
      "distribution p(X,Z|θ), and so the corresponding M-step maximization will, by sup-\n",
      "position, be tractable.\n",
      "The general EM algorithm is summarized below. It has the property, as we shall\n",
      "show later, that each cycle of EM will increase the incomplete-data log likelihood(unless it is already at a local maximum). Section 9.4\n",
      "The General EM Algorithm\n",
      "Given a joint distribution p(X,Z|θ)over observed variables Xand latent vari-\n",
      "ablesZ, governed by parameters θ, the goal is to maximize the likelihood func-\n",
      "tionp(X|θ)with respect to θ.\n",
      "1. Choose an initial setting for the parameters θ\n",
      "old.9.3. An Alternative View of EM 441\n",
      "2.E step Evaluate p(Z|X,θold).\n",
      "3.M step Evaluate θnewgiven by\n",
      "θnew=a r gm a x\n",
      "θQ(θ,θold) (9.32)\n",
      "where\n",
      "Q(θ,θold)=∑\n",
      "Zp(Z|X,θold)l np(X,Z|θ). (9.33)\n",
      "4. Check for convergence of either the log likelihood or the parameter values.\n",
      "If the convergence criterion is not satisﬁed, then let\n",
      "θold←θnew(9.34)\n",
      "and return to step 2.\n",
      "The EM algorithm can also be used to ﬁnd MAP (maximum posterior) solutions\n",
      "for models in which a prior p(θ)is deﬁned over the parameters. In this case the E Exercise 9.4\n",
      "step remains the same as in the maximum likelihood case, whereas in the M step the\n",
      "quantity to be maximized is given by Q(θ,θold)+l n p(θ). Suitable choices for the\n",
      "prior will remove the singularities of the kind illustrated in Figure 9.7.\n",
      "Here we have considered the use of the EM algorithm to maximize a likelihood\n",
      "function when there are discrete latent variables. However, it can also be applied\n",
      "when the unobserved variables correspond to missing values in the data set. Thedistribution of the observed values is obtained by taking the joint distribution of all\n",
      "the variables and then marginalizing over the missing ones. EM can then be used\n",
      "to maximize the corresponding likelihood function. We shall show an example ofthe application of this technique in the context of principal component analysis in\n",
      "Figure 12.11. This will be a valid procedure if the data values are missing at random ,\n",
      "meaning that the mechanism causing values to be missing does not depend on the\n",
      "unobserved values. In many situations this will not be the case, for instance if a\n",
      "sensor fails to return a value whenever the quantity it is measuring exceeds somethreshold.\n",
      "9.3.1 Gaussian mixtures revisited\n",
      "We now consider the application of this latent variable view of EM to the spe-\n",
      "ciﬁc case of a Gaussian mixture model. Recall that our goal is to maximize the log\n",
      "likelihood function (9.14), which is computed using the observed data set X, and we\n",
      "saw that this was more difﬁcult than for the case of a single Gaussian distribution\n",
      "due to the presence of the summation over kthat occurs inside the logarithm. Sup-\n",
      "pose then that in addition to the observed data set X, we were also given the values\n",
      "of the corresponding discrete variables Z. Recall that Figure 9.5(a) shows a ‘com-\n",
      "plete’ data set (i.e., one that includes labels showing which component generatedeach data point) while Figure 9.5(b) shows the corresponding ‘incomplete’ data set.\n",
      "The graphical model for the complete data is shown in Figure 9.9.442 9. MIXTURE MODELS AND EM\n",
      "Figure 9.9 This shows the same graph as in Figure 9.6 except that\n",
      "we now suppose that the discrete variables znare ob-\n",
      "served, as well as the data variables xn.\n",
      "xnzn\n",
      "Nµ Σπ\n",
      "Now consider the problem of maximizing the likelihood for the complete data\n",
      "set{X,Z}. From (9.10) and (9.11), this likelihood function takes the form\n",
      "p(X,Z|µ,Σ,π)=N∏\n",
      "n=1K∏\n",
      "k=1πznk\n",
      "kN(xn|µk,Σk)znk(9.35)\n",
      "where znkdenotes the kthcomponent of zn. Taking the logarithm, we obtain\n",
      "lnp(X,Z|µ,Σ,π)=N∑\n",
      "n=1K∑\n",
      "k=1znk{lnπk+l nN(xn|µk,Σk)}. (9.36)\n",
      "Comparison with the log likelihood function (9.14) for the incomplete data shows\n",
      "that the summation over kand the logarithm have been interchanged. The loga-\n",
      "rithm now acts directly on the Gaussian distribution, which itself is a member of\n",
      "the exponential family. Not surprisingly, this leads to a much simpler solution to\n",
      "the maximum likelihood problem, as we now show. Consider ﬁrst the maximization\n",
      "with respect to the means and covariances. Because znis aK-dimensional vec-\n",
      "tor with all elements equal to 0except for a single element having the value 1, the\n",
      "complete-data log likelihood function is simply a sum of Kindependent contribu-\n",
      "tions, one for each mixture component. Thus the maximization with respect to a\n",
      "mean or a covariance is exactly as for a single Gaussian, except that it involves only\n",
      "the subset of data points that are ‘assigned’ to that component. For the maximization\n",
      "with respect to the mixing coefﬁcients, we note that these are coupled for different\n",
      "values of kby virtue of the summation constraint (9.9). Again, this can be enforced\n",
      "using a Lagrange multiplier as before, and leads to the result\n",
      "πk=1\n",
      "NN∑\n",
      "n=1znk (9.37)\n",
      "so that the mixing coefﬁcients are equal to the fractions of data points assigned to\n",
      "the corresponding components.\n",
      "Thus we see that the complete-data log likelihood function can be maximized\n",
      "trivially in closed form. In practice, however, we do not have values for the latent\n",
      "variables so, as discussed earlier, we consider the expectation, with respect to the\n",
      "posterior distribution of the latent variables, of the complete-data log likelihood.9.3. An Alternative View of EM 443\n",
      "Using (9.10) and (9.11) together with Bayes’ theorem, we see that this posterior\n",
      "distribution takes the form\n",
      "p(Z|X,µ,Σ,π)∝N∏\n",
      "n=1K∏\n",
      "k=1[πkN(xn|µk,Σk)]znk. (9.38)\n",
      "and hence factorizes over nso that under the posterior distribution the {zn}are\n",
      "independent. This is easily veriﬁed by inspection of the directed graph in Figure 9.6 Exercise 9.5\n",
      "and making use of the d-separation criterion. The expected value of the indicator Section 8.2\n",
      "variable znkunder this posterior distribution is then given by\n",
      "E[znk]=∑\n",
      "znkznk[πkN(xn|µk,Σk)]znk\n",
      "∑\n",
      "znj[\n",
      "πjN(xn|µj,Σj)]znj\n",
      "=πkN(xn|µk,Σk)\n",
      "K∑\n",
      "j=1πjN(xn|µj,Σj)=γ(znk) (9.39)\n",
      "which is just the responsibility of component kfor data point xn. The expected value\n",
      "of the complete-data log likelihood function is therefore given by\n",
      "EZ[lnp(X,Z|µ,Σ,π)] =N∑\n",
      "n=1K∑\n",
      "k=1γ(znk){lnπk+l nN(xn|µk,Σk)}.(9.40)\n",
      "We can now proceed as follows. First we choose some initial values for the param-\n",
      "etersµold,Σoldandπold, and use these to evaluate the responsibilities (the E step).\n",
      "We then keep the responsibilities ﬁxed and maximize (9.40) with respect to µk,Σk\n",
      "andπk(the M step). This leads to closed form solutions for µnew,Σnewandπnew\n",
      "given by (9.17), (9.19), and (9.22) as before. This is precisely the EM algorithm for Exercise 9.8\n",
      "Gaussian mixtures as derived earlier. We shall gain more insight into the role of the\n",
      "expected complete-data log likelihood function when we give a proof of convergence\n",
      "of the EM algorithm in Section 9.4.\n",
      "9.3.2 Relation to K-means\n",
      "Comparison of the K-means algorithm with the EM algorithm for Gaussian\n",
      "mixtures shows that there is a close similarity. Whereas the K-means algorithm\n",
      "performs a hard assignment of data points to clusters, in which each data point is\n",
      "associated uniquely with one cluster, the EM algorithm makes a softassignment\n",
      "based on the posterior probabilities. In fact, we can derive the K-means algorithm\n",
      "as a particular limit of EM for Gaussian mixtures as follows.\n",
      "Consider a Gaussian mixture model in which the covariance matrices of the\n",
      "mixture components are given by ϵI, where ϵis a variance parameter that is shared444 9. MIXTURE MODELS AND EM\n",
      "by all of the components, and Iis the identity matrix, so that\n",
      "p(x|µk,Σk)=1\n",
      "(2πϵ)1/2exp{\n",
      "−1\n",
      "2ϵ∥x−µk∥2}\n",
      ". (9.41)\n",
      "We now consider the EM algorithm for a mixture of KGaussians of this form in\n",
      "which we treat ϵas a ﬁxed constant, instead of a parameter to be re-estimated. From\n",
      "(9.13) the posterior probabilities, or responsibilities, for a particular data point xn,\n",
      "are given by\n",
      "γ(znk)=πkexp{−∥xn−µk∥2/2ϵ}∑\n",
      "jπjexp{\n",
      "−∥xn−µj∥2/2ϵ}. (9.42)\n",
      "If we consider the limit ϵ→0, we see that in the denominator the term for which\n",
      "∥xn−µj∥2is smallest will go to zero most slowly, and hence the responsibilities\n",
      "γ(znk)for the data point xnall go to zero except for term j, for which the responsi-\n",
      "bilityγ(znj)will go to unity. Note that this holds independently of the values of the\n",
      "πkso long as none of the πkis zero. Thus, in this limit, we obtain a hard assignment\n",
      "of data points to clusters, just as in the K-means algorithm, so that γ(znk)→rnk\n",
      "where rnkis deﬁned by (9.2). Each data point is thereby assigned to the cluster\n",
      "having the closest mean.\n",
      "The EM re-estimation equation for the µk, given by (9.17), then reduces to the\n",
      "K-means result (9.4). Note that the re-estimation formula for the mixing coefﬁcients\n",
      "(9.22) simply re-sets the value of πkto be equal to the fraction of data points assigned\n",
      "to cluster k, although these parameters no longer play an active role in the algorithm.\n",
      "Finally, in the limit ϵ→0the expected complete-data log likelihood, given by\n",
      "(9.40), becomes Exercise 9.11\n",
      "EZ[lnp(X,Z|µ,Σ,π)]→−1\n",
      "2N∑\n",
      "n=1K∑\n",
      "k=1rnk∥xn−µk∥2+c o n s t . (9.43)\n",
      "Thus we see that in this limit, maximizing the expected complete-data log likelihood\n",
      "is equivalent to minimizing the distortion measure Jfor the K-means algorithm\n",
      "given by (9.1).\n",
      "Note that the K-means algorithm does not estimate the covariances of the clus-\n",
      "ters but only the cluster means. A hard-assignment version of the Gaussian mixturemodel with general covariance matrices, known as the elliptical K-means algorithm,\n",
      "has been considered by Sung and Poggio (1994).\n",
      "9.3.3 Mixtures of Bernoulli distributions\n",
      "So far in this chapter, we have focussed on distributions over continuous vari-\n",
      "ables described by mixtures of Gaussians. As a further example of mixture mod-\n",
      "elling, and to illustrate the EM algorithm in a different context, we now discuss mix-tures of discrete binary variables described by Bernoulli distributions. This model\n",
      "is also known as latent class analysis (Lazarsfeld and Henry, 1968; McLachlan and\n",
      "Peel, 2000). As well as being of practical importance in its own right, our discus-sion of Bernoulli mixtures will also lay the foundation for a consideration of hidden\n",
      "Markov models over discrete variables. Section 13.29.3. An Alternative View of EM 445\n",
      "Consider a set of Dbinary variables xi, where i=1,...,D , each of which is\n",
      "governed by a Bernoulli distribution with parameter µi, so that\n",
      "p(x|µ)=D∏\n",
      "i=1µxi\n",
      "i(1−µi)(1−xi)(9.44)\n",
      "wherex=(x1,...,x D)Tandµ=(µ1,...,µ D)T. We see that the individual\n",
      "variables xiare independent, given µ. The mean and covariance of this distribution\n",
      "are easily seen to be\n",
      "E[x]= µ (9.45)\n",
      "cov[x]=d i a g {µi(1−µi)}. (9.46)\n",
      "Now let us consider a ﬁnite mixture of these distributions given by\n",
      "p(x|µ,π)=K∑\n",
      "k=1πkp(x|µk) (9.47)\n",
      "where µ={µ1,...,µK},π={π1,...,π K}, and\n",
      "p(x|µk)=D∏\n",
      "i=1µxi\n",
      "ki(1−µki)(1−xi). (9.48)\n",
      "The mean and covariance of this mixture distribution are given by Exercise 9.12\n",
      "E[x]=K∑\n",
      "k=1πkµk (9.49)\n",
      "cov[x]=K∑\n",
      "k=1πk{\n",
      "Σk+µkµT\n",
      "k}\n",
      "−E[x]E[x]T(9.50)\n",
      "whereΣk=d i a g {µki(1−µki)}. Because the covariance matrix cov[x]is no\n",
      "longer diagonal, the mixture distribution can capture correlations between the vari-\n",
      "ables, unlike a single Bernoulli distribution.\n",
      "If we are given a data set X={x1,...,xN}then the log likelihood function\n",
      "for this model is given by\n",
      "lnp(X|µ,π)=N∑\n",
      "n=1ln{K∑\n",
      "k=1πkp(xn|µk)}\n",
      ". (9.51)\n",
      "Again we see the appearance of the summation inside the logarithm, so that the\n",
      "maximum likelihood solution no longer has closed form.\n",
      "We now derive the EM algorithm for maximizing the likelihood function for\n",
      "the mixture of Bernoulli distributions. To do this, we ﬁrst introduce an explicit latent446 9. MIXTURE MODELS AND EM\n",
      "variable zassociated with each instance of x. As in the case of the Gaussian mixture,\n",
      "z=(z1,...,z K)Tis a binary K-dimensional variable having a single component\n",
      "equal to 1, with all other components equal to 0. We can then write the conditional\n",
      "distribution of x, given the latent variable, as\n",
      "p(x|z,µ)=K∏\n",
      "k=1p(x|µk)zk(9.52)\n",
      "while the prior distribution for the latent variables is the same as for the mixture of\n",
      "Gaussians model, so that\n",
      "p(z|π)=K∏\n",
      "k=1πzk\n",
      "k. (9.53)\n",
      "If we form the product of p(x|z,µ)andp(z|π)and then marginalize over z, then we\n",
      "recover (9.47). Exercise 9.14\n",
      "In order to derive the EM algorithm, we ﬁrst write down the complete-data log\n",
      "likelihood function, which is given by\n",
      "lnp(X,Z|µ,π)=N∑\n",
      "n=1K∑\n",
      "k=1znk{\n",
      "lnπk\n",
      "+D∑\n",
      "i=1[xnilnµki+( 1−xni)l n ( 1−µki)]}\n",
      "(9.54)\n",
      "whereX={xn}andZ={zn}. Next we take the expectation of the complete-data\n",
      "log likelihood with respect to the posterior distribution of the latent variables to give\n",
      "EZ[lnp(X,Z|µ,π)] =N∑\n",
      "n=1K∑\n",
      "k=1γ(znk){\n",
      "lnπk\n",
      "+D∑\n",
      "i=1[xnilnµki+( 1−xni)l n ( 1−µki)]}\n",
      "(9.55)\n",
      "where γ(znk)= E[znk]is the posterior probability, or responsibility, of component\n",
      "kgiven data point xn. In the E step, these responsibilities are evaluated using Bayes’\n",
      "theorem, which takes the form\n",
      "γ(znk)= E[znk]=∑\n",
      "znkznk[πkp(xn|µk)]znk\n",
      "∑\n",
      "znj[\n",
      "πjp(xn|µj)]znj\n",
      "=πkp(xn|µk)\n",
      "K∑\n",
      "j=1πjp(xn|µj). (9.56)9.3. An Alternative View of EM 447\n",
      "If we consider the sum over nin (9.55), we see that the responsibilities enter\n",
      "only through two terms, which can be written as\n",
      "Nk=N∑\n",
      "n=1γ(znk) (9.57)\n",
      "xk=1\n",
      "NkN∑\n",
      "n=1γ(znk)xn (9.58)\n",
      "where Nkis the effective number of data points associated with component k. In the\n",
      "M step, we maximize the expected complete-data log likelihood with respect to the\n",
      "parameters µkandπ. If we set the derivative of (9.55) with respect to µkequal to\n",
      "zero and rearrange the terms, we obtain Exercise 9.15\n",
      "µk=xk. (9.59)\n",
      "We see that this sets the mean of component kequal to a weighted mean of the\n",
      "data, with weighting coefﬁcients given by the responsibilities that component ktakes\n",
      "for data points. For the maximization with respect to πk, we need to introduce a\n",
      "Lagrange multiplier to enforce the constraint∑\n",
      "kπk=1. Following analogous\n",
      "steps to those used for the mixture of Gaussians, we then obtain Exercise 9.16\n",
      "πk=Nk\n",
      "N(9.60)\n",
      "which represents the intuitively reasonable result that the mixing coefﬁcient for com-\n",
      "ponent kis given by the effective fraction of points in the data set explained by that\n",
      "component.\n",
      "Note that in contrast to the mixture of Gaussians, there are no singularities in\n",
      "which the likelihood function goes to inﬁnity. This can be seen by noting that the\n",
      "likelihood function is bounded above because 0⩽p(xn|µk)⩽1. There exist Exercise 9.17\n",
      "singularities at which the likelihood function goes to zero, but these will not be\n",
      "found by EM provided it is not initialized to a pathological starting point, becausethe EM algorithm always increases the value of the likelihood function, until a local\n",
      "maximum is found. We illustrate the Bernoulli mixture model in Figure 9.10 by Section 9.4\n",
      "using it to model handwritten digits. Here the digit images have been turned intobinary vectors by setting all elements whose values exceed 0.5to1and setting the\n",
      "remaining elements to 0. We now ﬁt a data set of N= 600 such digits, comprising\n",
      "the digits ‘2’, ‘3’, and ‘4’, with a mixture of K=3 Bernoulli distributions by\n",
      "running 10iterations of the EM algorithm. The mixing coefﬁcients were initialized\n",
      "toπ\n",
      "k=1/K, and the parameters µkjwere set to random values chosen uniformly in\n",
      "the range (0.25,0.75)and then normalized to satisfy the constraint that∑\n",
      "jµkj=1.\n",
      "We see that a mixture of 3Bernoulli distributions is able to ﬁnd the three clusters in\n",
      "the data set corresponding to the different digits.\n",
      "The conjugate prior for the parameters of a Bernoulli distribution is given by\n",
      "the beta distribution, and we have seen that a beta prior is equivalent to introducing448 9. MIXTURE MODELS AND EM\n",
      "Figure 9.10 Illustration of the Bernoulli mixture model in which the top row shows examples from the digits data\n",
      "set after converting the pixel values from grey scale to binary using a threshold of 0.5. On the bottom row the ﬁrst\n",
      "three images show the parameters µkifor each of the three components in the mixture model. As a comparison,\n",
      "we also ﬁt the same data set using a single multivariate Bernoulli distribution, again using maximum likelihood.\n",
      "This amounts to simply averaging the counts in each pixel and is shown by the right-most image on the bottomrow.\n",
      "additional effective observations of x. We can similarly introduce priors into the Section 2.1.1\n",
      "Bernoulli mixture model, and use EM to maximize the posterior probability distri-butions. Exercise 9.18\n",
      "It is straightforward to extend the analysis of Bernoulli mixtures to the case of\n",
      "multinomial binary variables having M>2states by making use of the discrete dis- Exercise 9.19\n",
      "tribution (2.26). Again, we can introduce Dirichlet priors over the model parameters\n",
      "if desired.\n",
      "9.3.4 EM for Bayesian linear regression\n",
      "As a third example of the application of EM, we return to the evidence ap-\n",
      "proximation for Bayesian linear regression. In Section 3.5.2, we obtained the re-estimation equations for the hyperparameters αandβby evaluation of the evidence\n",
      "and then setting the derivatives of the resulting expression to zero. We now turn to\n",
      "an alternative approach for ﬁnding αandβbased on the EM algorithm. Recall that\n",
      "our goal is to maximize the evidence function p(t|α,β)given by (3.77) with respect\n",
      "toαandβ. Because the parameter vector wis marginalized out, we can regard it as\n",
      "a latent variable, and hence we can optimize this marginal likelihood function usingEM. In the E step, we compute the posterior distribution of wgiven the current set-\n",
      "ting of the parameters αandβand then use this to ﬁnd the expected complete-data\n",
      "log likelihood. In the M step, we maximize this quantity with respect to αandβ.W e\n",
      "have already derived the posterior distribution of wbecause this is given by (3.49).\n",
      "The complete-data log likelihood function is then given by\n",
      "lnp(t,w|α,β)=l n p(t|w,β)+l n p(w|α) (9.61)9.3. An Alternative View of EM 449\n",
      "where the likelihood p(t|w,β)and the prior p(w|α)are given by (3.10) and (3.52),\n",
      "respectively, and y(x,w)is given by (3.3). Taking the expectation with respect to\n",
      "the posterior distribution of wthen gives\n",
      "E[lnp(t,w|α,β)] =M\n",
      "2ln(α\n",
      "2π)\n",
      "−α\n",
      "2E[\n",
      "wTw]\n",
      "+N\n",
      "2ln(β\n",
      "2π)\n",
      "−β\n",
      "2N∑\n",
      "n=1E[\n",
      "(tn−wTφn)2]\n",
      ". (9.62)\n",
      "Setting the derivatives with respect to αto zero, we obtain the M step re-estimation\n",
      "equation Exercise 9.20\n",
      "α=M\n",
      "E[wTw]=M\n",
      "mT\n",
      "NmN+Tr(SN). (9.63)\n",
      "An analogous result holds for β. Exercise 9.21\n",
      "Note that this re-estimation equation takes a slightly different form from the\n",
      "corresponding result (3.92) derived by direct evaluation of the evidence function.However, they each involve computation and inversion (or eigen decomposition) of\n",
      "anM×Mmatrix and hence will have comparable computational cost per iteration.\n",
      "These two approaches to determining αshould of course converge to the same\n",
      "result (assuming they ﬁnd the same local maximum of the evidence function). This\n",
      "can be veriﬁed by ﬁrst noting that the quantity γis deﬁned by\n",
      "γ=M−αM∑\n",
      "i=11\n",
      "λi+α=M−αTr(SN). (9.64)\n",
      "At a stationary point of the evidence function, the re-estimation equation (3.92) will\n",
      "be self-consistently satisﬁed, and hence we can substitute for γto give\n",
      "αmT\n",
      "NmN=γ=M−αTr(SN) (9.65)\n",
      "and solving for αwe obtain (9.63), which is precisely the EM re-estimation equation.\n",
      "As a ﬁnal example, we consider a closely related model, namely the relevance\n",
      "vector machine for regression discussed in Section 7.2.1. There we used direct max-\n",
      "imization of the marginal likelihood to derive re-estimation equations for the hyper-parameters αandβ. Here we consider an alternative approach in which we view the\n",
      "weight vector was a latent variable and apply the EM algorithm. The E step involves\n",
      "ﬁnding the posterior distribution over the weights, and this is given by (7.81). In theM step we maximize the expected complete-data log likelihood, which is deﬁned by\n",
      "E\n",
      "w[lnp(t|X,w,β)p(w|α)] (9.66)\n",
      "where the expectation is taken with respect to the posterior distribution computed\n",
      "using the ‘old’ parameter values. To compute the new parameter values we maximize\n",
      "with respect to αandβto give Exercise 9.22450 9. MIXTURE MODELS AND EM\n",
      "αnew\n",
      "i=1\n",
      "m2\n",
      "i+Σii(9.67)\n",
      "(βnew)−1=∥t−Φm N∥2+β−1∑\n",
      "iγi\n",
      "N(9.68)\n",
      "These re-estimation equations are formally equivalent to those obtained by direct\n",
      "maxmization. Exercise 9.23\n",
      "9.4. The EM Algorithm in General\n",
      "The expectation maximization algorithm, or EM algorithm, is a general technique for\n",
      "ﬁnding maximum likelihood solutions for probabilistic models having latent vari-\n",
      "ables (Dempster et al. , 1977; McLachlan and Krishnan, 1997). Here we give a very\n",
      "general treatment of the EM algorithm and in the process provide a proof that the\n",
      "EM algorithm derived heuristically in Sections 9.2 and 9.3 for Gaussian mixtures\n",
      "does indeed maximize the likelihood function (Csisz `ar and Tusn `ady, 1984; Hath-\n",
      "away, 1986; Neal and Hinton, 1999). Our discussion will also form the basis for the\n",
      "derivation of the variational inference framework. Section 10.1\n",
      "Consider a probabilistic model in which we collectively denote all of the ob-\n",
      "served variables by Xand all of the hidden variables by Z. The joint distribution\n",
      "p(X,Z|θ)is governed by a set of parameters denoted θ. Our goal is to maximize\n",
      "the likelihood function that is given by\n",
      "p(X|θ)=∑\n",
      "Zp(X,Z|θ). (9.69)\n",
      "Here we are assuming Zis discrete, although the discussion is identical if Zcom-\n",
      "prises continuous variables or a combination of discrete and continuous variables,\n",
      "with summation replaced by integration as appropriate.\n",
      "We shall suppose that direct optimization of p(X|θ)is difﬁcult, but that opti-\n",
      "mization of the complete-data likelihood function p(X,Z|θ)is signiﬁcantly easier.\n",
      "Next we introduce a distribution q(Z)deﬁned over the latent variables, and we ob-\n",
      "serve that, for any choice of q(Z), the following decomposition holds\n",
      "lnp(X|θ)=L(q,θ)+K L ( q∥p) (9.70)\n",
      "where we have deﬁned\n",
      "L(q,θ)=∑\n",
      "Zq(Z)l n{p(X,Z|θ)\n",
      "q(Z)}\n",
      "(9.71)\n",
      "KL(q∥p)= −∑\n",
      "Zq(Z)l n{p(Z|X,θ)\n",
      "q(Z)}\n",
      ". (9.72)\n",
      "Note that L(q,θ)is a functional (see Appendix D for a discussion of functionals)\n",
      "of the distribution q(Z), and a function of the parameters θ. It is worth studying9.4. The EM Algorithm in General 451\n",
      "Figure 9.11 Illustration of the decomposition given\n",
      "by (9.70), which holds for any choice\n",
      "of distribution q(Z). Because the\n",
      "Kullback-Leibler divergence satisﬁes\n",
      "KL(q∥p)⩾0, we see that the quan-\n",
      "tityL(q,θ)is a lower bound on the log\n",
      "likelihood function lnp(X|θ).\n",
      "lnp(X|θ) L(q,θ)KL(q||p)\n",
      "carefully the forms of the expressions (9.71) and (9.72), and in particular noting that\n",
      "they differ in sign and also that L(q,θ)contains the joint distribution of XandZ\n",
      "while KL(q∥p)contains the conditional distribution of ZgivenX. To verify the\n",
      "decomposition (9.70), we ﬁrst make use of the product rule of probability to give Exercise 9.24\n",
      "lnp(X,Z|θ)=l n p(Z|X,θ)+l n p(X|θ) (9.73)\n",
      "which we then substitute into the expression for L(q,θ). This gives rise to two terms,\n",
      "one of which cancels KL(q∥p)while the other gives the required log likelihood\n",
      "lnp(X|θ)after noting that q(Z)is a normalized distribution that sums to 1.\n",
      "From (9.72), we see that KL(q∥p)is the Kullback-Leibler divergence between\n",
      "q(Z)and the posterior distribution p(Z|X,θ). Recall that the Kullback-Leibler di-\n",
      "vergence satisﬁes KL(q∥p)⩾0, with equality if, and only if, q(Z)=p(Z|X,θ).I t Section 1.6.1\n",
      "therefore follows from (9.70) that L(q,θ)⩽lnp(X|θ), in other words that L(q,θ)\n",
      "is a lower bound on lnp(X|θ). The decomposition (9.70) is illustrated in Fig-\n",
      "ure 9.11.\n",
      "The EM algorithm is a two-stage iterative optimization technique for ﬁnding\n",
      "maximum likelihood solutions. We can use the decomposition (9.70) to deﬁne the\n",
      "EM algorithm and to demonstrate that it does indeed maximize the log likelihood.\n",
      "Suppose that the current value of the parameter vector is θold. In the E step, the\n",
      "lower bound L(q,θold)is maximized with respect to q(Z)while holding θoldﬁxed.\n",
      "The solution to this maximization problem is easily seen by noting that the value\n",
      "oflnp(X|θold)does not depend on q(Z)and so the largest value of L(q,θold)will\n",
      "occur when the Kullback-Leibler divergence vanishes, in other words when q(Z)is\n",
      "equal to the posterior distribution p(Z|X,θold). In this case, the lower bound will\n",
      "equal the log likelihood, as illustrated in Figure 9.12.\n",
      "In the subsequent M step, the distribution q(Z)is held ﬁxed and the lower bound\n",
      "L(q,θ)is maximized with respect to θto give some new value θnew. This will\n",
      "cause the lower bound Lto increase (unless it is already at a maximum), which will\n",
      "necessarily cause the corresponding log likelihood function to increase. Because the\n",
      "distribution qis determined using the old parameter values rather than the new values\n",
      "and is held ﬁxed during the M step, it will not equal the new posterior distribution\n",
      "p(Z|X,θnew), and hence there will be a nonzero KL divergence. The increase in the\n",
      "log likelihood function is therefore greater than the increase in the lower bound, as452 9. MIXTURE MODELS AND EM\n",
      "Figure 9.12 Illustration of the E step of\n",
      "the EM algorithm. The q\n",
      "distribution is set equal to\n",
      "the posterior distribution for\n",
      "the current parameter val-\n",
      "uesθold, causing the lower\n",
      "bound to move up to the\n",
      "same value as the log like-\n",
      "lihood function, with the KL\n",
      "divergence vanishing. lnp(X|θold) L(q,θold)KL(q||p)=0\n",
      "shown in Figure 9.13. If we substitute q(Z)=p(Z|X,θold)into (9.71), we see that,\n",
      "after the E step, the lower bound takes the form\n",
      "L(q,θ)=∑\n",
      "Zp(Z|X,θold)l np(X,Z|θ)−∑\n",
      "Zp(Z|X,θold)l np(Z|X,θold)\n",
      "=Q(θ,θold)+c o n s t (9.74)\n",
      "where the constant is simply the negative entropy of the qdistribution and is there-\n",
      "fore independent of θ. Thus in the M step, the quantity that is being maximized is the\n",
      "expectation of the complete-data log likelihood, as we saw earlier in the case of mix-\n",
      "tures of Gaussians. Note that the variable θover which we are optimizing appears\n",
      "only inside the logarithm. If the joint distribution p(Z,X|θ)comprises a member of\n",
      "the exponential family, or a product of such members, then we see that the logarithm\n",
      "will cancel the exponential and lead to an M step that will be typically much simpler\n",
      "than the maximization of the corresponding incomplete-data log likelihood function\n",
      "p(X|θ).\n",
      "The operation of the EM algorithm can also be viewed in the space of parame-\n",
      "ters, as illustrated schematically in Figure 9.14. Here the red curve depicts the (in-\n",
      "Figure 9.13 Illustration of the M step of the EM\n",
      "algorithm. The distribution q(Z)\n",
      "is held ﬁxed and the lower bound\n",
      "L(q,θ)is maximized with respect\n",
      "to the parameter vector θto give\n",
      "a revised value θnew. Because the\n",
      "KL divergence is nonnegative, this\n",
      "causes the log likelihood lnp(X|θ)\n",
      "to increase by at least as much as\n",
      "the lower bound does.\n",
      "lnp(X|θnew) L(q,θnew)KL(q||p)9.4. The EM Algorithm in General 453\n",
      "Figure 9.14 The EM algorithm involves alter-\n",
      "nately computing a lower bound\n",
      "on the log likelihood for the cur-\n",
      "rent parameter values and then\n",
      "maximizing this bound to obtain\n",
      "the new parameter values. See\n",
      "the text for a full discussion.\n",
      "θoldθnewL(q,θ)lnp(X|θ)\n",
      "complete data) log likelihood function whose value we wish to maximize. We start\n",
      "with some initial parameter value θold, and in the ﬁrst E step we evaluate the poste-\n",
      "rior distribution over latent variables, which gives rise to a lower bound L(θ,θ(old))\n",
      "whose value equals the log likelihood at θ(old), as shown by the blue curve. Note that\n",
      "the bound makes a tangential contact with the log likelihood at θ(old), so that both\n",
      "curves have the same gradient. This bound is a convex function having a unique Exercise 9.25\n",
      "maximum (for mixture components from the exponential family). In the M step, the\n",
      "bound is maximized giving the value θ(new), which gives a larger value of log likeli-\n",
      "hood than θ(old). The subsequent E step then constructs a bound that is tangential at\n",
      "θ(new)as shown by the green curve.\n",
      "For the particular case of an independent, identically distributed data set, X\n",
      "will comprise Ndata points {xn}whileZwill comprise Ncorresponding latent\n",
      "variables {zn}, where n=1,...,N . From the independence assumption, we have\n",
      "p(X,Z)=∏\n",
      "np(xn,zn)and, by marginalizing over the {zn}we have p(X)=∏\n",
      "np(xn). Using the sum and product rules, we see that the posterior probability\n",
      "that is evaluated in the E step takes the form\n",
      "p(Z|X,θ)=p(X,Z|θ)∑\n",
      "Zp(X,Z|θ)=N∏\n",
      "n=1p(xn,zn|θ)\n",
      "∑\n",
      "ZN∏\n",
      "n=1p(xn,zn|θ)=N∏\n",
      "n=1p(zn|xn,θ)(9.75)\n",
      "and so the posterior distribution also factorizes with respect to n. In the case of\n",
      "the Gaussian mixture model this simply says that the responsibility that each of the\n",
      "mixture components takes for a particular data point xndepends only on the value\n",
      "ofxnand on the parameters θof the mixture components, not on the values of the\n",
      "other data points.\n",
      "We have seen that both the E and the M steps of the EM algorithm are increas-\n",
      "ing the value of a well-deﬁned bound on the log likelihood function and that the454 9. MIXTURE MODELS AND EM\n",
      "complete EM cycle will change the model parameters in such a way as to cause\n",
      "the log likelihood to increase (unless it is already at a maximum, in which case theparameters remain unchanged).\n",
      "We can also use the EM algorithm to maximize the posterior distribution p(θ|X)\n",
      "for models in which we have introduced a prior p(θ)over the parameters. To see this,\n",
      "we note that as a function of θ,w eh a v e p(θ|X)=p(θ,X)/p(X)and so\n",
      "lnp(θ|X)=l n p(θ,X)−lnp(X). (9.76)\n",
      "Making use of the decomposition (9.70), we have\n",
      "lnp(θ|X)= L(q,θ)+K L ( q∥p)+l n p(θ)−lnp(X)\n",
      "⩾L(q,θ)+l n p(θ)−lnp(X)\n",
      ". (9.77)\n",
      "where lnp(X)is a constant. We can again optimize the right-hand side alternately\n",
      "with respect to qandθ. The optimization with respect to qgives rise to the same E-\n",
      "step equations as for the standard EM algorithm, because qonly appears in L(q,θ).\n",
      "The M-step equations are modiﬁed through the introduction of the prior term lnp(θ),\n",
      "which typically requires only a small modiﬁcation to the standard maximum likeli-\n",
      "hood M-step equations.\n",
      "The EM algorithm breaks down the potentially difﬁcult problem of maximizing\n",
      "the likelihood function into two stages, the E step and the M step, each of which will\n",
      "often prove simpler to implement. Nevertheless, for complex models it may be thecase that either the E step or the M step, or indeed both, remain intractable. This\n",
      "leads to two possible extensions of the EM algorithm, as follows.\n",
      "The generalized EM ,o rGEM , algorithm addresses the problem of an intractable\n",
      "M step. Instead of aiming to maximize L(q,θ)with respect to θ, it seeks instead\n",
      "to change the parameters in such a way as to increase its value. Again, because\n",
      "L(q,θ)is a lower bound on the log likelihood function, each complete EM cycle of\n",
      "the GEM algorithm is guaranteed to increase the value of the log likelihood (unless\n",
      "the parameters already correspond to a local maximum). One way to exploit the\n",
      "GEM approach would be to use one of the nonlinear optimization strategies, suchas the conjugate gradients algorithm, during the M step. Another form of GEM\n",
      "algorithm, known as the expectation conditional maximization , or ECM, algorithm,\n",
      "involves making several constrained optimizations within each M step (Meng and\n",
      "Rubin, 1993). For instance, the parameters might be partitioned into groups, and the\n",
      "M step is broken down into multiple steps each of which involves optimizing one ofthe subset with the remainder held ﬁxed.\n",
      "We can similarly generalize the E step of the EM algorithm by performing a\n",
      "partial, rather than complete, optimization of L(q,θ)with respect to q(Z)(Neal and\n",
      "Hinton, 1999). As we have seen, for any given value of θthere is a unique maximum\n",
      "ofL(q,θ)with respect to q(Z)that corresponds to the posterior distribution q\n",
      "θ(Z)=\n",
      "p(Z|X,θ)and that for this choice of q(Z)the bound L(q,θ)is equal to the log\n",
      "likelihood function lnp(X|θ). It follows that any algorithm that converges to the\n",
      "global maximum of L(q,θ)will ﬁnd a value of θthat is also a global maximum\n",
      "of the log likelihood lnp(X|θ). Provided p(X,Z|θ)is a continuous function of θExercises 455\n",
      "then, by continuity, any local maximum of L(q,θ)will also be a local maximum of\n",
      "lnp(X|θ).\n",
      "Consider the case of Nindependent data points x1,...,xNwith corresponding\n",
      "latent variables z1,...,zN. The joint distribution p(X,Z|θ)factorizes over the data\n",
      "points, and this structure can be exploited in an incremental form of EM in whichat each EM cycle only one data point is processed at a time. In the E step, instead\n",
      "of recomputing the responsibilities for all of the data points, we just re-evaluate the\n",
      "responsibilities for one data point. It might appear that the subsequent M step wouldrequire computation involving the responsibilities for all of the data points. How-\n",
      "ever, if the mixture components are members of the exponential family, then the\n",
      "responsibilities enter only through simple sufﬁcient statistics, and these can be up-\n",
      "dated efﬁciently. Consider, for instance, the case of a Gaussian mixture, and suppose\n",
      "we perform an update for data point min which the corresponding old and new\n",
      "values of the responsibilities are denoted γ\n",
      "old(zmk)andγnew(zmk). In the M step,\n",
      "the required sufﬁcient statistics can be updated incrementally. For instance, for the\n",
      "means the sufﬁcient statistics are deﬁned by (9.17) and (9.18) from which we obtain Exercise 9.26\n",
      "µnew\n",
      "k=µold\n",
      "k+(γnew(zmk)−γold(zmk)\n",
      "Nnew\n",
      "k)(\n",
      "xm−µold\n",
      "k)\n",
      "(9.78)\n",
      "together with\n",
      "Nnew\n",
      "k=Nold\n",
      "k+γnew(zmk)−γold(zmk). (9.79)\n",
      "The corresponding results for the covariances and the mixing coefﬁcients are analo-\n",
      "gous.\n",
      "Thus both the E step and the M step take ﬁxed time that is independent of the\n",
      "total number of data points. Because the parameters are revised after each data point,\n",
      "rather than waiting until after the whole data set is processed, this incremental ver-sion can converge faster than the batch version. Each E or M step in this incremental\n",
      "algorithm is increasing the value of L(q,θ)and, as we have shown above, if the\n",
      "algorithm converges to a local (or global) maximum of L(q,θ), this will correspond\n",
      "to a local (or global) maximum of the log likelihood function lnp(X|θ).\n",
      "Exercises\n",
      "9.1 (⋆)www Consider the K-means algorithm discussed in Section 9.1. Show that as\n",
      "a consequence of there being a ﬁnite number of possible assignments for the set of\n",
      "discrete indicator variables rnk, and that for each such assignment there is a unique\n",
      "optimum for the {µk}, theK-means algorithm must converge after a ﬁnite number\n",
      "of iterations.\n",
      "9.2 (⋆)Apply the Robbins-Monro sequential estimation procedure described in Sec-\n",
      "tion 2.3.5 to the problem of ﬁnding the roots of the regression function given bythe derivatives of Jin (9.1) with respect to µ\n",
      "k. Show that this leads to a stochastic\n",
      "K-means algorithm in which, for each data point xn, the nearest prototype µkis\n",
      "updated using (9.5).456 9. MIXTURE MODELS AND EM\n",
      "9.3 (⋆)www Consider a Gaussian mixture model in which the marginal distribution\n",
      "p(z)for the latent variable is given by (9.10), and the conditional distribution p(x|z)\n",
      "for the observed variable is given by (9.11). Show that the marginal distribution\n",
      "p(x), obtained by summing p(z)p(x|z)over all possible values of z, is a Gaussian\n",
      "mixture of the form (9.7).\n",
      "9.4 (⋆)Suppose we wish to use the EM algorithm to maximize the posterior distri-\n",
      "bution over parameters p(θ|X)for a model containing latent variables, where Xis\n",
      "the observed data set. Show that the E step remains the same as in the maximum\n",
      "likelihood case, whereas in the M step the quantity to be maximized is given byQ(θ,θ\n",
      "old)+l n p(θ)where Q(θ,θold)is deﬁned by (9.30).\n",
      "9.5 (⋆)Consider the directed graph for a Gaussian mixture model shown in Figure 9.6.\n",
      "By making use of the d-separation criterion discussed in Section 8.2, show that the\n",
      "posterior distribution of the latent variables factorizes with respect to the differentdata points so that\n",
      "p(Z|X,µ,Σ,π)=N∏\n",
      "n=1p(zn|xn,µ,Σ,π). (9.80)\n",
      "9.6 (⋆⋆)Consider a special case of a Gaussian mixture model in which the covari-\n",
      "ance matrices Σkof the components are all constrained to have a common value\n",
      "Σ. Derive the EM equations for maximizing the likelihood function under such a\n",
      "model.\n",
      "9.7 (⋆)www Verify that maximization of the complete-data log likelihood (9.36) for\n",
      "a Gaussian mixture model leads to the result that the means and covariances of eachcomponent are ﬁtted independently to the corresponding group of data points, and\n",
      "the mixing coefﬁcients are given by the fractions of points in each group.\n",
      "9.8 (⋆)\n",
      "www Show that if we maximize (9.40) with respect to µkwhile keeping the\n",
      "responsibilities γ(znk)ﬁxed, we obtain the closed form solution given by (9.17).\n",
      "9.9 (⋆)Show that if we maximize (9.40) with respect to Σkandπkwhile keeping the\n",
      "responsibilities γ(znk)ﬁxed, we obtain the closed form solutions given by (9.19)\n",
      "and (9.22).\n",
      "9.10 (⋆⋆)Consider a density model given by a mixture distribution\n",
      "p(x)=K∑\n",
      "k=1πkp(x|k) (9.81)\n",
      "and suppose that we partition the vector xinto two parts so that x=(xa,xb).\n",
      "Show that the conditional density p(xb|xa)is itself a mixture distribution and ﬁnd\n",
      "expressions for the mixing coefﬁcients and for the component densities.Exercises 457\n",
      "9.11 (⋆)In Section 9.3.2, we obtained a relationship between Kmeans and EM for\n",
      "Gaussian mixtures by considering a mixture model in which all components havecovariance ϵI. Show that in the limit ϵ→0, maximizing the expected complete-\n",
      "data log likelihood for this model, given by (9.40), is equivalent to minimizing the\n",
      "distortion measure Jfor the K-means algorithm given by (9.1).\n",
      "9.12 (⋆)\n",
      "www Consider a mixture distribution of the form\n",
      "p(x)=K∑\n",
      "k=1πkp(x|k) (9.82)\n",
      "where the elements of xcould be discrete or continuous or a combination of these.\n",
      "Denote the mean and covariance of p(x|k)byµkandΣk, respectively. Show that\n",
      "the mean and covariance of the mixture distribution are given by (9.49) and (9.50).\n",
      "9.13 (⋆⋆)Using the re-estimation equations for the EM algorithm, show that a mix-\n",
      "ture of Bernoulli distributions, with its parameters set to values corresponding to amaximum of the likelihood function, has the property that\n",
      "E[x]=1\n",
      "NN∑\n",
      "n=1xn≡x. (9.83)\n",
      "Hence show that if the parameters of this model are initialized such that all compo-\n",
      "nents have the same mean µk=ˆµfork=1,...,K , then the EM algorithm will\n",
      "converge after one iteration, for any choice of the initial mixing coefﬁcients, and that\n",
      "this solution has the property µk=x. Note that this represents a degenerate case of\n",
      "the mixture model in which all of the components are identical, and in practice wetry to avoid such solutions by using an appropriate initialization.\n",
      "9.14 (⋆)Consider the joint distribution of latent and observed variables for the Bernoulli\n",
      "distribution obtained by forming the product of p(x|z,µ)given by (9.52) and p(z|π)\n",
      "given by (9.53). Show that if we marginalize this joint distribution with respect to z,\n",
      "then we obtain (9.47).\n",
      "9.15 (⋆)\n",
      "www Show that if we maximize the expected complete-data log likelihood\n",
      "function (9.55) for a mixture of Bernoulli distributions with respect to µk, we obtain\n",
      "the M step equation (9.59).\n",
      "9.16 (⋆)Show that if we maximize the expected complete-data log likelihood function\n",
      "(9.55) for a mixture of Bernoulli distributions with respect to the mixing coefﬁcients\n",
      "πk, using a Lagrange multiplier to enforce the summation constraint, we obtain the\n",
      "M step equation (9.60).\n",
      "9.17 (⋆)www Show that as a consequence of the constraint 0⩽p(xn|µk)⩽1for\n",
      "the discrete variable xn, the incomplete-data log likelihood function for a mixture\n",
      "of Bernoulli distributions is bounded above, and hence that there are no singularities\n",
      "for which the likelihood goes to inﬁnity.458 9. MIXTURE MODELS AND EM\n",
      "9.18 (⋆⋆)Consider a Bernoulli mixture model as discussed in Section 9.3.3, together\n",
      "with a prior distribution p(µk|ak,bk)over each of the parameter vectors µkgiven\n",
      "by the beta distribution (2.13), and a Dirichlet prior p(π|α)given by (2.38). Derive\n",
      "the EM algorithm for maximizing the posterior probability p(µ,π|X).\n",
      "9.19 (⋆⋆)Consider a D-dimensional variable xeach of whose components iis itself a\n",
      "multinomial variable of degree Mso that xis a binary vector with components xij\n",
      "where i=1,...,D andj=1,...,M , subject to the constraint that∑\n",
      "jxij=1for\n",
      "alli. Suppose that the distribution of these variables is described by a mixture of the\n",
      "discrete multinomial distributions considered in Section 2.2 so that\n",
      "p(x)=K∑\n",
      "k=1πkp(x|µk) (9.84)\n",
      "where\n",
      "p(x|µk)=D∏\n",
      "i=1M∏\n",
      "j=1µxij\n",
      "kij. (9.85)\n",
      "The parameters µkijrepresent the probabilities p(xij=1|µk)and must satisfy\n",
      "0⩽µkij⩽1together with the constraint∑\n",
      "jµkij=1 for all values of kandi.\n",
      "Given an observed data set {xn}, where n=1,...,N , derive the E and M step\n",
      "equations of the EM algorithm for optimizing the mixing coefﬁcients πkand the\n",
      "component parameters µkijof this distribution by maximum likelihood.\n",
      "9.20 (⋆)www Show that maximization of the expected complete-data log likelihood\n",
      "function (9.62) for the Bayesian linear regression model leads to the M step re-\n",
      "estimation result (9.63) for α.\n",
      "9.21 (⋆⋆)Using the evidence framework of Section 3.5, derive the M-step re-estimation\n",
      "equations for the parameter βin the Bayesian linear regression model, analogous to\n",
      "the result (9.63) for α.\n",
      "9.22 (⋆⋆)By maximization of the expected complete-data log likelihood deﬁned by\n",
      "(9.66), derive the M step equations (9.67) and (9.68) for re-estimating the hyperpa-\n",
      "rameters of the relevance vector machine for regression.\n",
      "9.23 (⋆⋆)www In Section 7.2.1 we used direct maximization of the marginal like-\n",
      "lihood to derive the re-estimation equations (7.87) and (7.88) for ﬁnding values of\n",
      "the hyperparameters αandβfor the regression RVM. Similarly, in Section 9.3.4\n",
      "we used the EM algorithm to maximize the same marginal likelihood, giving there-estimation equations (9.67) and (9.68). Show that these two sets of re-estimation\n",
      "equations are formally equivalent.\n",
      "9.24 (⋆)Verify the relation (9.70) in which L(q,θ)andKL(q∥p)are deﬁned by (9.71)\n",
      "and (9.72), respectively.Exercises 459\n",
      "9.25 (⋆)www Show that the lower bound L(q,θ)given by (9.71), with q(Z)=\n",
      "p(Z|X,θ(old)), has the same gradient with respect to θas the log likelihood function\n",
      "lnp(X|θ)at the point θ=θ(old).\n",
      "9.26 (⋆)www Consider the incremental form of the EM algorithm for a mixture of\n",
      "Gaussians, in which the responsibilities are recomputed only for a speciﬁc data pointx\n",
      "m. Starting from the M-step formulae (9.17) and (9.18), derive the results (9.78)\n",
      "and (9.79) for updating the component means.\n",
      "9.27 (⋆⋆)Derive M-step formulae for updating the covariance matrices and mixing\n",
      "coefﬁcients in a Gaussian mixture model when the responsibilities are updated in-crementally, analogous to the result (9.78) for updating the means.10\n",
      "Approximate\n",
      "Inference\n",
      "A central task in the application of probabilistic models is the evaluation of the pos-\n",
      "terior distribution p(Z|X)of the latent variables Zgiven the observed (visible) data\n",
      "variables X, and the evaluation of expectations computed with respect to this dis-\n",
      "tribution. The model might also contain some deterministic parameters, which we\n",
      "will leave implicit for the moment, or it may be a fully Bayesian model in which anyunknown parameters are given prior distributions and are absorbed into the set of\n",
      "latent variables denoted by the vector Z. For instance, in the EM algorithm we need\n",
      "to evaluate the expectation of the complete-data log likelihood with respect to theposterior distribution of the latent variables. For many models of practical interest, it\n",
      "will be infeasible to evaluate the posterior distribution or indeed to compute expec-\n",
      "tations with respect to this distribution. This could be because the dimensionality ofthe latent space is too high to work with directly or because the posterior distribution\n",
      "has a highly complex form for which expectations are not analytically tractable. In\n",
      "the case of continuous variables, the required integrations may not have closed-form\n",
      "461462 10. APPROXIMATE INFERENCE\n",
      "analytical solutions, while the dimensionality of the space and the complexity of the\n",
      "integrand may prohibit numerical integration. For discrete variables, the marginal-izations involve summing over all possible conﬁgurations of the hidden variables,\n",
      "and though this is always possible in principle, we often ﬁnd in practice that there\n",
      "may be exponentially many hidden states so that exact calculation is prohibitivelyexpensive.\n",
      "In such situations, we need to resort to approximation schemes, and these fall\n",
      "broadly into two classes, according to whether they rely on stochastic or determin-istic approximations. Stochastic techniques such as Markov chain Monte Carlo, de-\n",
      "scribed in Chapter 11, have enabled the widespread use of Bayesian methods across\n",
      "many domains. They generally have the property that given inﬁnite computational\n",
      "resource, they can generate exact results, and the approximation arises from the use\n",
      "of a ﬁnite amount of processor time. In practice, sampling methods can be compu-tationally demanding, often limiting their use to small-scale problems. Also, it can\n",
      "be difﬁcult to know whether a sampling scheme is generating independent samples\n",
      "from the required distribution.\n",
      "In this chapter, we introduce a range of deterministic approximation schemes,\n",
      "some of which scale well to large applications. These are based on analytical ap-\n",
      "proximations to the posterior distribution, for example by assuming that it factorizesin a particular way or that it has a speciﬁc parametric form such as a Gaussian. As\n",
      "such, they can never generate exact results, and so their strengths and weaknesses\n",
      "are complementary to those of sampling methods.\n",
      "In Section 4.4, we discussed the Laplace approximation, which is based on a\n",
      "local Gaussian approximation to a mode (i.e., a maximum) of the distribution. Here\n",
      "we turn to a family of approximation techniques called variational inference orvari-\n",
      "ational Bayes , which use more global criteria and which have been widely applied.\n",
      "We conclude with a brief introduction to an alternative variational framework known\n",
      "asexpectation propagation .\n",
      "10.1. Variational Inference\n",
      "Variational methods have their origins in the 18thcentury with the work of Euler,\n",
      "Lagrange, and others on the calculus of variations . Standard calculus is concerned\n",
      "with ﬁnding derivatives of functions. We can think of a function as a mapping that\n",
      "takes the value of a variable as the input and returns the value of the function as theoutput. The derivative of the function then describes how the output value varies\n",
      "as we make inﬁnitesimal changes to the input value. Similarly, we can deﬁne a\n",
      "functional as a mapping that takes a function as the input and that returns the value\n",
      "of the functional as the output. An example would be the entropy H[p], which takes\n",
      "a probability distribution p(x)as the input and returns the quantity\n",
      "H[p]=∫\n",
      "p(x)l np(x)dx (10.1)10.1. Variational Inference 463\n",
      "as the output. We can the introduce the concept of a functional derivative , which ex-\n",
      "presses how the value of the functional changes in response to inﬁnitesimal changesto the input function (Feynman et al. , 1964). The rules for the calculus of variations\n",
      "mirror those of standard calculus and are discussed in Appendix D. Many problems\n",
      "can be expressed in terms of an optimization problem in which the quantity beingoptimized is a functional. The solution is obtained by exploring all possible input\n",
      "functions to ﬁnd the one that maximizes, or minimizes, the functional. Variational\n",
      "methods have broad applicability and include such areas as ﬁnite element methods(Kapur, 1989) and maximum entropy (Schwarz, 1988).\n",
      "Although there is nothing intrinsically approximate about variational methods,\n",
      "they do naturally lend themselves to ﬁnding approximate solutions. This is done\n",
      "by restricting the range of functions over which the optimization is performed, for\n",
      "instance by considering only quadratic functions or by considering functions com-posed of a linear combination of ﬁxed basis functions in which only the coefﬁcients\n",
      "of the linear combination can vary. In the case of applications to probabilistic in-\n",
      "ference, the restriction may for example take the form of factorization assumptions(Jordan et al. , 1999; Jaakkola, 2001).\n",
      "Now let us consider in more detail how the concept of variational optimization\n",
      "can be applied to the inference problem. Suppose we have a fully Bayesian model inwhich all parameters are given prior distributions. The model may also have latent\n",
      "variables as well as parameters, and we shall denote the set of all latent variables\n",
      "and parameters by Z. Similarly, we denote the set of all observed variables by X.\n",
      "For example, we might have a set of Nindependent, identically distributed data,\n",
      "for which X={x\n",
      "1,...,xN}andZ={z1,...,zN}. Our probabilistic model\n",
      "speciﬁes the joint distribution p(X,Z), and our goal is to ﬁnd an approximation for\n",
      "the posterior distribution p(Z|X)as well as for the model evidence p(X).A si no u r\n",
      "discussion of EM, we can decompose the log marginal probability using\n",
      "lnp(X)=L(q) + KL( q∥p) (10.2)\n",
      "where we have deﬁned\n",
      "L(q)=∫\n",
      "q(Z)l n{p(X,Z)\n",
      "q(Z)}\n",
      "dZ (10.3)\n",
      "KL(q∥p)= −∫\n",
      "q(Z)l n{p(Z|X)\n",
      "q(Z)}\n",
      "dZ. (10.4)\n",
      "This differs from our discussion of EM only in that the parameter vector θno longer\n",
      "appears, because the parameters are now stochastic variables and are absorbed into\n",
      "Z. Since in this chapter we will mainly be interested in continuous variables we have\n",
      "used integrations rather than summations in formulating this decomposition. How-\n",
      "ever, the analysis goes through unchanged if some or all of the variables are discretesimply by replacing the integrations with summations as required. As before, we\n",
      "can maximize the lower bound L(q)by optimization with respect to the distribution\n",
      "q(Z), which is equivalent to minimizing the KL divergence. If we allow any possible\n",
      "choice for q(Z), then the maximum of the lower bound occurs when the KL diver-\n",
      "gence vanishes, which occurs when q(Z)equals the posterior distribution p(Z|X).464 10. APPROXIMATE INFERENCE\n",
      "−2 −1 0 1 2 3 400.20.40.60.81\n",
      "−2 −1 0 1 2 3 4010203040\n",
      "Figure 10.1 Illustration of the variational approximation for the example considered earlier in Figure 4.14. The\n",
      "left-hand plot shows the original distribution (yellow) along with the Laplace (red) and variational (green) approx-\n",
      "imations, and the right-hand plot shows the negative logarithms of the corresponding curves.\n",
      "However, we shall suppose the model is such that working with the true posterior\n",
      "distribution is intractable.\n",
      "We therefore consider instead a restricted family of distributions q(Z)and then\n",
      "seek the member of this family for which the KL divergence is minimized. Our goal\n",
      "is to restrict the family sufﬁciently that they comprise only tractable distributions,\n",
      "while at the same time allowing the family to be sufﬁciently rich and ﬂexible that it\n",
      "can provide a good approximation to the true posterior distribution. It is important to\n",
      "emphasize that the restriction is imposed purely to achieve tractability, and that sub-\n",
      "ject to this requirement we should use as rich a family of approximating distributions\n",
      "as possible. In particular, there is no ‘over-ﬁtting’ associated with highly ﬂexible dis-\n",
      "tributions. Using more ﬂexible approximations simply allows us to approach the true\n",
      "posterior distribution more closely.\n",
      "One way to restrict the family of approximating distributions is to use a paramet-\n",
      "ric distribution q(Z|ω)governed by a set of parameters ω. The lower bound L(q)\n",
      "then becomes a function of ω, and we can exploit standard nonlinear optimization\n",
      "techniques to determine the optimal values for the parameters. An example of this\n",
      "approach, in which the variational distribution is a Gaussian and we have optimized\n",
      "with respect to its mean and variance, is shown in Figure 10.1.\n",
      "10.1.1 Factorized distributions\n",
      "Here we consider an alternative way in which to restrict the family of distri-\n",
      "butions q(Z). Suppose we partition the elements of Zinto disjoint groups that we\n",
      "denote by Ziwhere i=1,...,M . We then assume that the qdistribution factorizes\n",
      "with respect to these groups, so that\n",
      "q(Z)=M∏\n",
      "i=1qi(Zi). (10.5)10.1. Variational Inference 465\n",
      "It should be emphasized that we are making no further assumptions about the distri-\n",
      "bution. In particular, we place no restriction on the functional forms of the individualfactors q\n",
      "i(Zi). This factorized form of variational inference corresponds to an ap-\n",
      "proximation framework developed in physics called mean ﬁeld theory (Parisi, 1988).\n",
      "Amongst all distributions q(Z)having the form (10.5), we now seek that distri-\n",
      "bution for which the lower bound L(q)is largest. We therefore wish to make a free\n",
      "form (variational) optimization of L(q)with respect to all of the distributions qi(Zi),\n",
      "which we do by optimizing with respect to each of the factors in turn. To achievethis, we ﬁrst substitute (10.5) into (10.3) and then dissect out the dependence on one\n",
      "of the factors q\n",
      "j(Zj). Denoting qj(Zj)by simply qjto keep the notation uncluttered,\n",
      "we then obtain\n",
      "L(q)=∫∏\n",
      "iqi{\n",
      "lnp(X,Z)−∑\n",
      "ilnqi}\n",
      "dZ\n",
      "=∫\n",
      "qj{∫\n",
      "lnp(X,Z)∏\n",
      "i̸=jqidZi}\n",
      "dZj−∫\n",
      "qjlnqjdZj+c o n s t\n",
      "=∫\n",
      "qjln˜p(X,Zj)dZj−∫\n",
      "qjlnqjdZj+c o n s t (10.6)\n",
      "where we have deﬁned a new distribution ˜p(X,Zj)by the relation\n",
      "ln˜p(X,Zj)= Ei̸=j[lnp(X,Z)] + const . (10.7)\n",
      "Here the notation Ei̸=j[···]denotes an expectation with respect to the qdistributions\n",
      "over all variables zifori̸=j, so that\n",
      "Ei̸=j[lnp(X,Z)] =∫\n",
      "lnp(X,Z)∏\n",
      "i̸=jqidZi. (10.8)\n",
      "Now suppose we keep the {qi̸=j}ﬁxed and maximize L(q)in (10.6) with re-\n",
      "spect to all possible forms for the distribution qj(Zj). This is easily done by rec-\n",
      "ognizing that (10.6) is a negative Kullback-Leibler divergence between qj(Zj)and\n",
      "˜p(X,Zj). Thus maximizing (10.6) is equivalent to minimizing the Kullback-Leibler\n",
      "Leonhard Euler\n",
      "1707–1783\n",
      "Euler was a Swiss mathematician\n",
      "and physicist who worked in St.Petersburg and Berlin and who iswidely considered to be one of thegreatest mathematicians of all time.He is certainly the most proliﬁc, and\n",
      "his collected works ﬁll 75 volumes. Amongst his manycontributions, he formulated the modern theory of the\n",
      "function, he developed (together with Lagrange) thecalculus of variations, and he discovered the formulae\n",
      "iπ=−1, which relates four of the most important\n",
      "numbers in mathematics. During the last 17 years ofhis life, he was almost totally blind, and yet he pro-duced nearly half of his results during this period.466 10. APPROXIMATE INFERENCE\n",
      "divergence, and the minimum occurs when qj(Zj)=˜p(X,Zj). Thus we obtain a\n",
      "general expression for the optimal solution q⋆\n",
      "j(Zj)given by\n",
      "lnq⋆\n",
      "j(Zj)= Ei̸=j[lnp(X,Z)] + const . (10.9)\n",
      "It is worth taking a few moments to study the form of this solution as it provides the\n",
      "basis for applications of variational methods. It says that the log of the optimal so-\n",
      "lution for factor qjis obtained simply by considering the log of the joint distribution\n",
      "over all hidden and visible variables and then taking the expectation with respect to\n",
      "all of the other factors {qi}fori̸=j.\n",
      "The additive constant in (10.9) is set by normalizing the distribution q⋆\n",
      "j(Zj).\n",
      "Thus if we take the exponential of both sides and normalize, we have\n",
      "q⋆\n",
      "j(Zj)=exp ( Ei̸=j[lnp(X,Z)])∫\n",
      "exp ( Ei̸=j[lnp(X,Z)]) dZj.\n",
      "In practice, we shall ﬁnd it more convenient to work with the form (10.9) and then re-\n",
      "instate the normalization constant (where required) by inspection. This will become\n",
      "clear from subsequent examples.\n",
      "The set of equations given by (10.9) for j=1,...,M represent a set of con-\n",
      "sistency conditions for the maximum of the lower bound subject to the factorizationconstraint. However, they do not represent an explicit solution because the expres-\n",
      "sion on the right-hand side of (10.9) for the optimum q\n",
      "⋆\n",
      "j(Zj)depends on expectations\n",
      "computed with respect to the other factors qi(Zi)fori̸=j. We will therefore seek\n",
      "a consistent solution by ﬁrst initializing all of the factors qi(Zi)appropriately and\n",
      "then cycling through the factors and replacing each in turn with a revised estimate\n",
      "given by the right-hand side of (10.9) evaluated using the current estimates for all of\n",
      "the other factors. Convergence is guaranteed because bound is convex with respectto each of the factors q\n",
      "i(Zi)(Boyd and Vandenberghe, 2004).\n",
      "10.1.2 Properties of factorized approximations\n",
      "Our approach to variational inference is based on a factorized approximation to\n",
      "the true posterior distribution. Let us consider for a moment the problem of approx-\n",
      "imating a general distribution by a factorized distribution. To begin with, we discussthe problem of approximating a Gaussian distribution using a factorized Gaussian,\n",
      "which will provide useful insight into the types of inaccuracy introduced in using\n",
      "factorized approximations. Consider a Gaussian distribution p(z)=N(z|µ,Λ\n",
      "−1)\n",
      "over two correlated variables z=(z1,z2)in which the mean and precision have\n",
      "elements\n",
      "µ=(\n",
      "µ1\n",
      "µ2)\n",
      ", Λ=(\n",
      "Λ11Λ12\n",
      "Λ21Λ22)\n",
      "(10.10)\n",
      "andΛ21=Λ 12due to the symmetry of the precision matrix. Now suppose we\n",
      "wish to approximate this distribution using a factorized Gaussian of the form q(z)=\n",
      "q1(z1)q2(z2). We ﬁrst apply the general result (10.9) to ﬁnd an expression for the10.1. Variational Inference 467\n",
      "optimal factor q⋆\n",
      "1(z1). In doing so it is useful to note that on the right-hand side we\n",
      "only need to retain those terms that have some functional dependence on z1because\n",
      "all other terms can be absorbed into the normalization constant. Thus we have\n",
      "lnq⋆\n",
      "1(z1)= Ez2[lnp(z) ]+c o n s t\n",
      "= Ez2[\n",
      "−1\n",
      "2(z1−µ1)2Λ11−(z1−µ1)Λ12(z2−µ2)]\n",
      "+c o n s t\n",
      "=−1\n",
      "2z2\n",
      "1Λ11+z1µ1Λ11−z1Λ12(E[z2]−µ2) + const .(10.11)\n",
      "Next we observe that the right-hand side of this expression is a quadratic function of\n",
      "z1, and so we can identify q⋆(z1)as a Gaussian distribution. It is worth emphasizing\n",
      "that we did not assume that q(zi)is Gaussian, but rather we derived this result by\n",
      "variational optimization of the KL divergence over all possible distributions q(zi).\n",
      "Note also that we do not need to consider the additive constant in (10.9) explicitly\n",
      "because it represents the normalization constant that can be found at the end by\n",
      "inspection if required. Using the technique of completing the square, we can identify Section 2.3.1\n",
      "the mean and precision of this Gaussian, giving\n",
      "q⋆(z1)=N(z1|m1,Λ−1\n",
      "11) (10.12)\n",
      "where\n",
      "m1=µ1−Λ−1\n",
      "11Λ12(E[z2]−µ2). (10.13)\n",
      "By symmetry, q⋆\n",
      "2(z2)is also Gaussian and can be written as\n",
      "q⋆\n",
      "2(z2)=N(z2|m2,Λ−1\n",
      "22) (10.14)\n",
      "in which\n",
      "m2=µ2−Λ−1\n",
      "22Λ21(E[z1]−µ1). (10.15)\n",
      "Note that these solutions are coupled, so that q⋆(z1)depends on expectations com-\n",
      "puted with respect to q⋆(z2)and vice versa. In general, we address this by treating\n",
      "the variational solutions as re-estimation equations and cycling through the variablesin turn updating them until some convergence criterion is satisﬁed. We shall see\n",
      "an example of this shortly. Here, however, we note that the problem is sufﬁciently\n",
      "simple that a closed form solution can be found. In particular, because E[z\n",
      "1]=m1\n",
      "and E[z2]=m2, we see that the two equations are satisﬁed if we take E[z1]=µ1\n",
      "and E[z2]=µ2, and it is easily shown that this is the only solution provided the dis-\n",
      "tribution is nonsingular. This result is illustrated in Figure 10.2(a). We see that the Exercise 10.2\n",
      "mean is correctly captured but that the variance of q(z)is controlled by the direction\n",
      "of smallest variance of p(z), and that the variance along the orthogonal direction is\n",
      "signiﬁcantly under-estimated. It is a general result that a factorized variational ap-\n",
      "proximation tends to give approximations to the posterior distribution that are too\n",
      "compact.\n",
      "By way of comparison, suppose instead that we had been minimizing the reverse\n",
      "Kullback-Leibler divergence KL(p∥q). As we shall see, this form of KL divergence468 10. APPROXIMATE INFERENCE\n",
      "Figure 10.2 Comparison of\n",
      "the two alternative forms for the\n",
      "Kullback-Leibler divergence. The\n",
      "green contours corresponding to\n",
      "1, 2, and 3 standard deviations for\n",
      "a correlated Gaussian distribution\n",
      "p(z)over two variables z1andz2,\n",
      "and the red contours represent\n",
      "the corresponding levels for an\n",
      "approximating distribution q(z)\n",
      "over the same variables given by\n",
      "the product of two independent\n",
      "univariate Gaussian distributions\n",
      "whose parameters are obtained by\n",
      "minimization of (a) the Kullback-\n",
      "Leibler divergence KL(q∥p), and\n",
      "(b) the reverse Kullback-Leibler\n",
      "divergence KL(p∥q).z1z2\n",
      "(a)0 0.5 100.51\n",
      "z1z2\n",
      "(b)0 0.5 100.51\n",
      "is used in an alternative approximate inference framework called expectation prop-\n",
      "agation . We therefore consider the general problem of minimizing KL(p∥q)when Section 10.7\n",
      "q(Z)is a factorized approximation of the form (10.5). The KL divergence can then\n",
      "be written in the form\n",
      "KL(p∥q)=−∫\n",
      "p(Z)[M∑\n",
      "i=1lnqi(Zi)]\n",
      "dZ+c o n s t (10.16)\n",
      "where the constant term is simply the entropy of p(Z)and so does not depend on\n",
      "q(Z). We can now optimize with respect to each of the factors qj(Zj), which is\n",
      "easily done using a Lagrange multiplier to give Exercise 10.3\n",
      "q⋆\n",
      "j(Zj)=∫\n",
      "p(Z)∏\n",
      "i̸=jdZi=p(Zj). (10.17)\n",
      "In this case, we ﬁnd that the optimal solution for qj(Zj)is just given by the corre-\n",
      "sponding marginal distribution of p(Z). Note that this is a closed-form solution and\n",
      "so does not require iteration.\n",
      "To apply this result to the illustrative example of a Gaussian distribution p(z)\n",
      "over a vector zwe can use (2.98), which gives the result shown in Figure 10.2(b).\n",
      "We see that once again the mean of the approximation is correct, but that it places\n",
      "signiﬁcant probability mass in regions of variable space that have very low probabil-\n",
      "ity.\n",
      "The difference between these two results can be understood by noting that there\n",
      "is a large positive contribution to the Kullback-Leibler divergence\n",
      "KL(q∥p)=−∫\n",
      "q(Z)l n{p(Z)\n",
      "q(Z)}\n",
      "dZ (10.18)10.1. Variational Inference 469\n",
      "(a) (b) (c)\n",
      "Figure 10.3 Another comparison of the two alternative forms for the Kullback-Leibler divergence. (a) The blue\n",
      "contours show a bimodal distribution p(Z)given by a mixture of two Gaussians, and the red contours correspond\n",
      "to the single Gaussian distribution q(Z)that best approximates p(Z)in the sense of minimizing the Kullback-\n",
      "Leibler divergence KL(p∥q). (b) As in (a) but now the red contours correspond to a Gaussian distribution q(Z)\n",
      "found by numerical minimization of the Kullback-Leibler divergence KL(q∥p). (c) As in (b) but showing a different\n",
      "local minimum of the Kullback-Leibler divergence.\n",
      "from regions of Zspace in which p(Z)is near zero unless q(Z)is also close to\n",
      "zero. Thus minimizing this form of KL divergence leads to distributions q(Z)that\n",
      "avoid regions in which p(Z)is small. Conversely, the Kullback-Leibler divergence\n",
      "KL(p∥q)is minimized by distributions q(Z)that are nonzero in regions where p(Z)\n",
      "is nonzero.\n",
      "We can gain further insight into the different behaviour of the two KL diver-\n",
      "gences if we consider approximating a multimodal distribution by a unimodal one,\n",
      "as illustrated in Figure 10.3. In practical applications, the true posterior distri-\n",
      "bution will often be multimodal, with most of the posterior mass concentrated insome number of relatively small regions of parameter space. These multiple modes\n",
      "may arise through nonidentiﬁability in the latent space or through complex nonlin-\n",
      "ear dependence on the parameters. Both types of multimodality were encountered in\n",
      "Chapter 9 in the context of Gaussian mixtures, where they manifested themselves as\n",
      "multiple maxima in the likelihood function, and a variational treatment based on theminimization of KL(q∥p)will tend to ﬁnd one of these modes. By contrast, if we\n",
      "were to minimize KL(p∥q), the resulting approximations would average across all\n",
      "of the modes and, in the context of the mixture model, would lead to poor predictivedistributions (because the average of two good parameter values is typically itself\n",
      "not a good parameter value). It is possible to make use of KL(p∥q)to deﬁne a useful\n",
      "inference procedure, but this requires a rather different approach to the one discussedhere, and will be considered in detail when we discuss expectation propagation. Section 10.7\n",
      "The two forms of Kullback-Leibler divergence are members of the alpha family470 10. APPROXIMATE INFERENCE\n",
      "of divergences (Ali and Silvey, 1966; Amari, 1985; Minka, 2005) deﬁned by\n",
      "Dα(p∥q)=4\n",
      "1−α2(\n",
      "1−∫\n",
      "p(x)(1+α)/2q(x)(1−α)/2dx)\n",
      "(10.19)\n",
      "where −∞<α< ∞is a continuous parameter. The Kullback-Leibler divergence\n",
      "KL(p∥q)corresponds to the limit α→1, whereas KL(q∥p)corresponds to the limit\n",
      "α→−1. For all values of αwe have Dα(p∥q)⩾0, with equality if, and only if, Exercise 10.6\n",
      "p(x)=q(x). Suppose p(x)is a ﬁxed distribution, and we minimize Dα(p∥q)with\n",
      "respect to some set of distributions q(x). Then for α⩽−1the divergence is zero\n",
      "forcing , so that any values of xfor which p(x)=0 will have q(x)=0 , and typically\n",
      "q(x)will under-estimate the support of p(x)and will tend to seek the mode with the\n",
      "largest mass. Conversely for α⩾1the divergence is zero-avoiding , so that values\n",
      "ofxfor which p(x)>0will have q(x)>0, and typically q(x)will stretch to cover\n",
      "all ofp(x), and will over-estimate the support of p(x). When α=0 we obtain a\n",
      "symmetric divergence that is linearly related to the Hellinger distance given by\n",
      "DH(p∥q)=∫(\n",
      "p(x)1/2−q(x)1/2)\n",
      "dx. (10.20)\n",
      "The square root of the Hellinger distance is a valid distance metric.\n",
      "10.1.3 Example: The univariate Gaussian\n",
      "We now illustrate the factorized variational approximation using a Gaussian dis-\n",
      "tribution over a single variable x(MacKay, 2003). Our goal is to infer the posterior\n",
      "distribution for the mean µand precision τ, given a data set D={x1,...,x N}of\n",
      "observed values of xwhich are assumed to be drawn independently from the Gaus-\n",
      "sian. The likelihood function is given by\n",
      "p(D|µ, τ)=(τ\n",
      "2π)N/2\n",
      "exp{\n",
      "−τ\n",
      "2N∑\n",
      "n=1(xn−µ)2}\n",
      ". (10.21)\n",
      "We now introduce conjugate prior distributions for µandτgiven by\n",
      "p(µ|τ)= N(\n",
      "µ|µ0,(λ0τ)−1)\n",
      "(10.22)\n",
      "p(τ)=G a m ( τ|a0,b0) (10.23)\n",
      "where Gam( τ|a0,b0)is the gamma distribution deﬁned by (2.146). Together these\n",
      "distributions constitute a Gaussian-Gamma conjugate prior distribution. Section 2.3.6\n",
      "For this simple problem the posterior distribution can be found exactly, and again\n",
      "takes the form of a Gaussian-gamma distribution. However, for tutorial purposes Exercise 2.44\n",
      "we will consider a factorized variational approximation to the posterior distributiongiven by\n",
      "q(µ, τ)=q\n",
      "µ(µ)qτ(τ). (10.24)10.1. Variational Inference 471\n",
      "Note that the true posterior distribution does not factorize in this way. The optimum\n",
      "factors qµ(µ)andqτ(τ)can be obtained from the general result (10.9) as follows.\n",
      "Forqµ(µ)we have\n",
      "lnq⋆\n",
      "µ(µ)= Eτ[lnp(D|µ, τ)+l n p(µ|τ) ]+c o n s t\n",
      "=−E[τ]\n",
      "2{\n",
      "λ0(µ−µ0)2+N∑\n",
      "n=1(xn−µ)2}\n",
      "+c o n s t .(10.25)\n",
      "Completing the square over µwe see that qµ(µ)is a Gaussian N(\n",
      "µ|µN,λ−1\n",
      "N)\n",
      "with\n",
      "mean and precision given by Exercise 10.7\n",
      "µN=λ0µ0+Nx\n",
      "λ0+N(10.26)\n",
      "λN=(λ0+N)E[τ]. (10.27)\n",
      "Note that for N→∞ this gives the maximum likelihood result in which µN=x\n",
      "and the precision is inﬁnite.\n",
      "Similarly, the optimal solution for the factor qτ(τ)is given by\n",
      "lnq⋆\n",
      "τ(τ)= Eµ[lnp(D|µ, τ)+l n p(µ|τ) ]+l n p(τ)+c o n s t\n",
      "=(a0−1) lnτ−b0τ+N\n",
      "2lnτ\n",
      "−τ\n",
      "2Eµ[N∑\n",
      "n=1(xn−µ)2+λ0(µ−µ0)2]\n",
      "+c o n s t (10.28)\n",
      "and hence qτ(τ)is a gamma distribution Gam( τ|aN,bN)with parameters\n",
      "aN=a0+N\n",
      "2(10.29)\n",
      "bN=b0+1\n",
      "2Eµ[N∑\n",
      "n=1(xn−µ)2+λ0(µ−µ0)2]\n",
      ". (10.30)\n",
      "Again this exhibits the expected behaviour when N→∞ . Exercise 10.8\n",
      "It should be emphasized that we did not assume these speciﬁc functional forms\n",
      "for the optimal distributions qµ(µ)andqτ(τ). They arose naturally from the structure\n",
      "of the likelihood function and the corresponding conjugate priors. Section 10.4.1\n",
      "Thus we have expressions for the optimal distributions qµ(µ)andqτ(τ)each of\n",
      "which depends on moments evaluated with respect to the other distribution. One ap-\n",
      "proach to ﬁnding a solution is therefore to make an initial guess for, say, the moment\n",
      "E[τ]and use this to re-compute the distribution qµ(µ). Given this revised distri-\n",
      "bution we can then extract the required moments E[µ]and E[µ2], and use these to\n",
      "recompute the distribution qτ(τ), and so on. Since the space of hidden variables for\n",
      "this example is only two dimensional, we can illustrate the variational approxima-tion to the posterior distribution by plotting contours of both the true posterior and\n",
      "the factorized approximation, as illustrated in Figure 10.4.472 10. APPROXIMATE INFERENCE\n",
      "µτ(a)\n",
      "−1 0 1012\n",
      "µτ(b)\n",
      "−1 0 1012\n",
      "µτ(c)\n",
      "−1 0 1012\n",
      "µτ(d)\n",
      "−1 0 1012\n",
      "Figure 10.4 Illustration of variational inference for the mean µand precision τof a univariate Gaussian distribu-\n",
      "tion. Contours of the true posterior distribution p(µ, τ|D)are shown in green. (a) Contours of the initial factorized\n",
      "approximation qµ(µ)qτ(τ)are shown in blue. (b) After re-estimating the factor qµ(µ). (c) After re-estimating the\n",
      "factor qτ(τ). (d) Contours of the optimal factorized approximation, to which the iterative scheme converges, are\n",
      "shown in red.\n",
      "In general, we will need to use an iterative approach such as this in order to\n",
      "solve for the optimal factorized posterior distribution. For the very simple example\n",
      "we are considering here, however, we can ﬁnd an explicit solution by solving the\n",
      "simultaneous equations for the optimal factors qµ(µ)andqτ(τ). Before doing this,\n",
      "we can simplify these expressions by considering broad, noninformative priors in\n",
      "which µ0=a0=b0=λ0=0. Although these parameter settings correspond to\n",
      "improper priors, we see that the posterior distribution is still well deﬁned. Using the\n",
      "standard result E[τ]=aN/bNfor the mean of a gamma distribution, together with Appendix B\n",
      "(10.29) and (10.30), we have\n",
      "1\n",
      "E[τ]=E[\n",
      "1\n",
      "NN∑\n",
      "n=1(xn−µ)2]\n",
      "=x2−2xE[µ]+ E[µ2]. (10.31)\n",
      "Then, using (10.26) and (10.27), we obtain the ﬁrst and second order moments of10.1. Variational Inference 473\n",
      "qµ(µ)in the form\n",
      "E[µ]=x, E[µ2]=x2+1\n",
      "NE[τ]. (10.32)\n",
      "We can now substitute these moments into (10.31) and then solve for E[τ]to give Exercise 10.9\n",
      "1\n",
      "E[τ]=1\n",
      "N−1(x2−x2)\n",
      "=1\n",
      "N−1N∑\n",
      "n=1(xn−x)2. (10.33)\n",
      "We recognize the right-hand side as the familiar unbiased estimator for the variance\n",
      "of a univariate Gaussian distribution, and so we see that the use of a Bayesian ap-proach has avoided the bias of the maximum likelihood solution. Section 1.2.4\n",
      "10.1.4 Model comparison\n",
      "As well as performing inference over the hidden variables Z, we may also\n",
      "wish to compare a set of candidate models, labelled by the index m, and having\n",
      "prior probabilities p(m). Our goal is then to approximate the posterior probabilities\n",
      "p(m|X), where Xis the observed data. This is a slightly more complex situation\n",
      "than that considered so far because different models may have different structure\n",
      "and indeed different dimensionality for the hidden variables Z. We cannot there-\n",
      "fore simply consider a factorized approximation q(Z)q(m), but must instead recog-\n",
      "nize that the posterior over Zmust be conditioned on m, and so we must consider\n",
      "q(Z,m)=q(Z|m)q(m). We can readily verify the following decomposition based\n",
      "on this variational distribution Exercise 10.10\n",
      "lnp(X)=Lm−∑\n",
      "m∑\n",
      "Zq(Z|m)q(m)l n{p(Z,m|X)\n",
      "q(Z|m)q(m)}\n",
      "(10.34)\n",
      "where the Lmis a lower bound on lnp(X)and is given by\n",
      "Lm=∑\n",
      "m∑\n",
      "Zq(Z|m)q(m)l n{p(Z,X,m)\n",
      "q(Z|m)q(m)}\n",
      ". (10.35)\n",
      "Here we are assuming discrete Z, but the same analysis applies to continuous latent\n",
      "variables provided the summations are replaced with integrations. We can maximize\n",
      "Lmwith respect to the distribution q(m)using a Lagrange multiplier, with the result Exercise 10.11\n",
      "q(m)∝p(m)e x p{Lm}. (10.36)\n",
      "However, if we maximize Lmwith respect to the q(Z|m), we ﬁnd that the solutions\n",
      "for different mare coupled, as we expect because they are conditioned on m.W e\n",
      "proceed instead by ﬁrst optimizing each of the q(Z|m)individually by optimization474 10. APPROXIMATE INFERENCE\n",
      "of (10.35), and then subsequently determining the q(m)using (10.36). After nor-\n",
      "malization the resulting values for q(m)can be used for model selection or model\n",
      "averaging in the usual way.\n",
      "10.2. Illustration: Variational Mixture of Gaussians\n",
      "We now return to our discussion of the Gaussian mixture model and apply the vari-\n",
      "ational inference machinery developed in the previous section. This will provide a\n",
      "good illustration of the application of variational methods and will also demonstrate\n",
      "how a Bayesian treatment elegantly resolves many of the difﬁculties associated with\n",
      "the maximum likelihood approach (Attias, 1999b). The reader is encouraged to workthrough this example in detail as it provides many insights into the practical appli-\n",
      "cation of variational methods. Many Bayesian models, corresponding to much more\n",
      "sophisticated distributions, can be solved by straightforward extensions and general-izations of this analysis.\n",
      "Our starting point is the likelihood function for the Gaussian mixture model, il-\n",
      "lustrated by the graphical model in Figure 9.6. For each observation x\n",
      "nwe have\n",
      "a corresponding latent variable zncomprising a 1-of- Kbinary vector with ele-\n",
      "ments znkfork=1,...,K . As before we denote the observed data set by X=\n",
      "{x1,...,xN}, and similarly we denote the latent variables by Z={z1,...,zN}.\n",
      "From (9.10) we can write down the conditional distribution of Z, given the mixing\n",
      "coefﬁcients π, in the form\n",
      "p(Z|π)=N∏\n",
      "n=1K∏\n",
      "k=1πznk\n",
      "k. (10.37)\n",
      "Similarly, from (9.11), we can write down the conditional distribution of the ob-\n",
      "served data vectors, given the latent variables and the component parameters\n",
      "p(X|Z,µ,Λ)=N∏\n",
      "n=1K∏\n",
      "k=1N(\n",
      "xn|µk,Λ−1\n",
      "k)znk(10.38)\n",
      "where µ={µk}andΛ={Λk}. Note that we are working in terms of precision\n",
      "matrices rather than covariance matrices as this somewhat simpliﬁes the mathemat-\n",
      "ics.\n",
      "Next we introduce priors over the parameters µ,Λandπ. The analysis is con-\n",
      "siderably simpliﬁed if we use conjugate prior distributions. We therefore choose a Section 10.4.1\n",
      "Dirichlet distribution over the mixing coefﬁcients π\n",
      "p(π)=D i r ( π|α0)=C(α0)K∏\n",
      "k=1πα0−1\n",
      "k(10.39)\n",
      "where by symmetry we have chosen the same parameter α0for each of the compo-\n",
      "nents, and C(α0)is the normalization constant for the Dirichlet distribution deﬁned10.2. Illustration: Variational Mixture of Gaussians 475\n",
      "Figure 10.5 Directed acyclic graph representing the Bayesian mix-\n",
      "ture of Gaussians model, in which the box (plate) de-\n",
      "notes a set of Ni.i.d. observations. Here µdenotes\n",
      "{µk}andΛdenotes {Λk}.\n",
      "xnzn\n",
      "Nπ\n",
      "µΛ\n",
      "by (B.23). As we have seen, the parameter α0can be interpreted as the effective Section 2.2.1\n",
      "prior number of observations associated with each component of the mixture. If the\n",
      "value of α0is small, then the posterior distribution will be inﬂuenced primarily by\n",
      "the data rather than by the prior.\n",
      "Similarly, we introduce an independent Gaussian-Wishart prior governing the\n",
      "mean and precision of each Gaussian component, given by\n",
      "p(µ,Λ)= p(µ|Λ)p(Λ)\n",
      "=K∏\n",
      "k=1N(\n",
      "µk|m0,(β0Λk)−1)\n",
      "W(Λk|W0,ν0) (10.40)\n",
      "because this represents the conjugate prior distribution when both the mean and pre-\n",
      "cision are unknown. Typically we would choose m0=0by symmetry. Section 2.3.6\n",
      "The resulting model can be represented as a directed graph as shown in Fig-\n",
      "ure 10.5. Note that there is a link from Λtoµsince the variance of the distribution\n",
      "overµin (10.40) is a function of Λ.\n",
      "This example provides a nice illustration of the distinction between latent vari-\n",
      "ables and parameters. Variables such as znthat appear inside the plate are regarded\n",
      "as latent variables because the number of such variables grows with the size of the\n",
      "data set. By contrast, variables such as µthat are outside the plate are ﬁxed in\n",
      "number independently of the size of the data set, and so are regarded as parameters.\n",
      "From the perspective of graphical models, however, there is really no fundamental\n",
      "difference between them.\n",
      "10.2.1 Variational distribution\n",
      "In order to formulate a variational treatment of this model, we next write down\n",
      "the joint distribution of all of the random variables, which is given by\n",
      "p(X,Z,π,µ,Λ)=p(X|Z,µ,Λ)p(Z|π)p(π)p(µ|Λ)p(Λ) (10.41)\n",
      "in which the various factors are deﬁned above. The reader should take a moment to\n",
      "verify that this decomposition does indeed correspond to the probabilistic graphical\n",
      "model shown in Figure 10.5. Note that only the variables X={x1,...,xN}are\n",
      "observed.476 10. APPROXIMATE INFERENCE\n",
      "We now consider a variational distribution which factorizes between the latent\n",
      "variables and the parameters so that\n",
      "q(Z,π,µ,Λ)=q(Z)q(π,µ,Λ). (10.42)\n",
      "It is remarkable that this is the only assumption that we need to make in order to\n",
      "obtain a tractable practical solution to our Bayesian mixture model. In particular, thefunctional form of the factors q(Z)andq(π,µ,Λ)will be determined automatically\n",
      "by optimization of the variational distribution. Note that we are omitting the sub-\n",
      "scripts on the qdistributions, much as we do with the pdistributions in (10.41), and\n",
      "are relying on the arguments to distinguish the different distributions.\n",
      "The corresponding sequential update equations for these factors can be easily\n",
      "derived by making use of the general result (10.9). Let us consider the derivation of\n",
      "the update equation for the factor q(Z). The log of the optimized factor is given by\n",
      "lnq\n",
      "⋆(Z)= Eπ,µ,Λ[lnp(X,Z,π,µ,Λ) ]+c o n s t . (10.43)\n",
      "We now make use of the decomposition (10.41). Note that we are only interested in\n",
      "the functional dependence of the right-hand side on the variable Z. Thus any terms\n",
      "that do not depend on Zcan be absorbed into the additive normalization constant,\n",
      "giving\n",
      "lnq⋆(Z)= Eπ[lnp(Z|π)] + Eµ,Λ[lnp(X|Z,µ,Λ) ]+c o n s t . (10.44)\n",
      "Substituting for the two conditional distributions on the right-hand side, and again\n",
      "absorbing any terms that are independent of Zinto the additive constant, we have\n",
      "lnq⋆(Z)=N∑\n",
      "n=1K∑\n",
      "k=1znklnρnk+c o n s t (10.45)\n",
      "where we have deﬁned\n",
      "lnρnk= E[lnπk]+1\n",
      "2E[ln|Λk|]−D\n",
      "2ln(2π)\n",
      "−1\n",
      "2Eµk,Λk[\n",
      "(xn−µk)TΛk(xn−µk)]\n",
      "(10.46)\n",
      "where Dis the dimensionality of the data variable x. Taking the exponential of both\n",
      "sides of (10.45) we obtain\n",
      "q⋆(Z)∝N∏\n",
      "n=1K∏\n",
      "k=1ρznk\n",
      "nk. (10.47)\n",
      "Requiring that this distribution be normalized, and noting that for each value of n\n",
      "the quantities znkare binary and sum to 1 over all values of k, we obtain Exercise 10.12\n",
      "q⋆(Z)=N∏\n",
      "n=1K∏\n",
      "k=1rznk\n",
      "nk(10.48)10.2. Illustration: Variational Mixture of Gaussians 477\n",
      "where\n",
      "rnk=ρnk\n",
      "K∑\n",
      "j=1ρnj. (10.49)\n",
      "We see that the optimal solution for the factor q(Z)takes the same functional form\n",
      "as the prior p(Z|π). Note that because ρnkis given by the exponential of a real\n",
      "quantity, the quantities rnkwill be nonnegative and will sum to one, as required.\n",
      "For the discrete distribution q⋆(Z)we have the standard result\n",
      "E[znk]=rnk (10.50)\n",
      "from which we see that the quantities rnkare playing the role of responsibilities.\n",
      "Note that the optimal solution for q⋆(Z)depends on moments evaluated with respect\n",
      "to the distributions of other variables, and so again the variational update equations\n",
      "are coupled and must be solved iteratively.\n",
      "At this point, we shall ﬁnd it convenient to deﬁne three statistics of the observed\n",
      "data set evaluated with respect to the responsibilities, given by\n",
      "Nk=N∑\n",
      "n=1rnk (10.51)\n",
      "xk=1\n",
      "NkN∑\n",
      "n=1rnkxn (10.52)\n",
      "Sk=1\n",
      "NkN∑\n",
      "n=1rnk(xn−xk)(xn−xk)T. (10.53)\n",
      "Note that these are analogous to quantities evaluated in the maximum likelihood EM\n",
      "algorithm for the Gaussian mixture model.\n",
      "Now let us consider the factor q(π,µ,Λ)in the variational posterior distribu-\n",
      "tion. Again using the general result (10.9) we have\n",
      "lnq⋆(π,µ,Λ)=l n p(π)+K∑\n",
      "k=1lnp(µk,Λk)+ EZ[lnp(Z|π)]\n",
      "+K∑\n",
      "k=1N∑\n",
      "n=1E[znk]l nN(\n",
      "xn|µk,Λ−1\n",
      "k)\n",
      "+c o n s t . (10.54)\n",
      "We observe that the right-hand side of this expression decomposes into a sum of\n",
      "terms involving only πtogether with terms only involving µandΛ, which implies\n",
      "that the variational posterior q(π,µ,Λ)factorizes to give q(π)q(µ,Λ). Further-\n",
      "more, the terms involving µandΛthemselves comprise a sum over kof terms\n",
      "involving µkandΛkleading to the further factorization\n",
      "q(π,µ,Λ)=q(π)K∏\n",
      "k=1q(µk,Λk). (10.55)478 10. APPROXIMATE INFERENCE\n",
      "Identifying the terms on the right-hand side of (10.54) that depend on π,w eh a v e\n",
      "lnq⋆(π)=(α0−1)K∑\n",
      "k=1lnπk+K∑\n",
      "k=1N∑\n",
      "n=1rnklnπk+c o n s t (10.56)\n",
      "where we have used (10.50). Taking the exponential of both sides, we recognize\n",
      "q⋆(π)as a Dirichlet distribution\n",
      "q⋆(π)=D i r ( π|α) (10.57)\n",
      "where αhas components αkgiven by\n",
      "αk=α0+Nk. (10.58)\n",
      "Finally, the variational posterior distribution q⋆(µk,Λk)does not factorize into\n",
      "the product of the marginals, but we can always use the product rule to write it in the\n",
      "formq⋆(µk,Λk)=q⋆(µk|Λk)q⋆(Λk). The two factors can be found by inspecting\n",
      "(10.54) and reading off those terms that involve µkandΛk. The result, as expected,\n",
      "is a Gaussian-Wishart distribution and is given by Exercise 10.13\n",
      "q⋆(µk,Λk)=N(\n",
      "µk|mk,(βkΛk)−1)\n",
      "W(Λk|Wk,νk) (10.59)\n",
      "where we have deﬁned\n",
      "βk=β0+Nk (10.60)\n",
      "mk=1\n",
      "βk(β0m0+Nkxk) (10.61)\n",
      "W−1\n",
      "k=W−1\n",
      "0+NkSk+β0Nk\n",
      "β0+Nk(xk−m0)(xk−m0)T(10.62)\n",
      "νk=ν0+Nk. (10.63)\n",
      "These update equations are analogous to the M-step equations of the EM algorithm\n",
      "for the maximum likelihood solution of the mixture of Gaussians. We see that the\n",
      "computations that must be performed in order to update the variational posterior\n",
      "distribution over the model parameters involve evaluation of the same sums over thedata set, as arose in the maximum likelihood treatment.\n",
      "In order to perform this variational M step, we need the expectations E[z\n",
      "nk]=\n",
      "rnkrepresenting the responsibilities. These are obtained by normalizing the ρnkthat\n",
      "are given by (10.46). We see that this expression involves expectations with respect\n",
      "to the variational distributions of the parameters, and these are easily evaluated to\n",
      "give Exercise 10.14\n",
      "Eµk,Λk[\n",
      "(xn−µk)TΛk(xn−µk)]\n",
      "=Dβ−1\n",
      "k+νk(xn−mk)TWk(xn−mk)(10.64)\n",
      "ln˜Λk≡E[ln|Λk|]=D∑\n",
      "i=1ψ(νk+1−i\n",
      "2)\n",
      "+Dln2 + ln |Wk|(10.65)\n",
      "ln˜πk≡E[lnπk]= ψ(αk)−ψ(ˆα) (10.66)10.2. Illustration: Variational Mixture of Gaussians 479\n",
      "where we have introduced deﬁnitions of ˜Λkand˜πk, andψ(·)is the digamma function\n",
      "deﬁned by (B.25), with ˆα=∑\n",
      "kαk. The results (10.65) and (10.66) follow from\n",
      "the standard properties of the Wishart and Dirichlet distributions. Appendix B\n",
      "If we substitute (10.64), (10.65), and (10.66) into (10.46) and make use of\n",
      "(10.49), we obtain the following result for the responsibilities\n",
      "rnk∝˜πk˜Λ1/2\n",
      "kexp{\n",
      "−D\n",
      "2βk−νk\n",
      "2(xn−mk)TWk(xn−mk)}\n",
      ". (10.67)\n",
      "Notice the similarity to the corresponding result for the responsibilities in maximum\n",
      "likelihood EM, which from (9.13) can be written in the form\n",
      "rnk∝πk|Λk|1/2exp{\n",
      "−1\n",
      "2(xn−µk)TΛk(xn−µk)}\n",
      "(10.68)\n",
      "where we have used the precision in place of the covariance to highlight the similarity\n",
      "to (10.67).\n",
      "Thus the optimization of the variational posterior distribution involves cycling\n",
      "between two stages analogous to the E and M steps of the maximum likelihood EM\n",
      "algorithm. In the variational equivalent of the E step, we use the current distributions\n",
      "over the model parameters to evaluate the moments in (10.64), (10.65), and (10.66)and hence evaluate E[z\n",
      "nk]=rnk. Then in the subsequent variational equivalent\n",
      "of the M step, we keep these responsibilities ﬁxed and use them to re-compute the\n",
      "variational distribution over the parameters using (10.57) and (10.59). In each case,we see that the variational posterior distribution has the same functional form as the\n",
      "corresponding factor in the joint distribution (10.41). This is a general result and is\n",
      "a consequence of the choice of conjugate distributions. Section 10.4.1\n",
      "Figure 10.6 shows the results of applying this approach to the rescaled Old Faith-\n",
      "ful data set for a Gaussian mixture model having K=6 components. We see that\n",
      "after convergence, there are only two components for which the expected valuesof the mixing coefﬁcients are numerically distinguishable from their prior values.\n",
      "This effect can be understood qualitatively in terms of the automatic trade-off in a\n",
      "Bayesian model between ﬁtting the data and the complexity of the model, in which Section 3.4\n",
      "the complexity penalty arises from components whose parameters are pushed away\n",
      "from their prior values. Components that take essentially no responsibility for ex-plaining the data points have r\n",
      "nk≃0and hence Nk≃0. From (10.58), we see\n",
      "thatαk≃α0and from (10.60)–(10.63) we see that the other parameters revert to\n",
      "their prior values. In principle such components are ﬁtted slightly to the data points,but for broad priors this effect is too small to be seen numerically. For the varia-\n",
      "tional Gaussian mixture model the expected values of the mixing coefﬁcients in the\n",
      "posterior distribution are given by Exercise 10.15\n",
      "E[π\n",
      "k]=αk+Nk\n",
      "Kα0+N. (10.69)\n",
      "Consider a component for which Nk≃0andαk≃α0. If the prior is broad so that\n",
      "α0→0, then E[πk]→0and the component plays no role in the model, whereas if480 10. APPROXIMATE INFERENCE\n",
      "Figure 10.6 Variational Bayesian\n",
      "mixture of K=6 Gaussians ap-\n",
      "plied to the Old Faithful data set, in\n",
      "which the ellipses denote the one\n",
      "standard-deviation density contours\n",
      "for each of the components, and the\n",
      "density of red ink inside each ellipse\n",
      "corresponds to the mean value of\n",
      "the mixing coefﬁcient for each com-\n",
      "ponent. The number in the top left\n",
      "of each diagram shows the num-\n",
      "ber of iterations of variational infer-\n",
      "ence. Components whose expected\n",
      "mixing coefﬁcient are numerically in-\n",
      "distinguishable from zero are not\n",
      "plotted.0 15\n",
      "60 120\n",
      "the prior tightly constrains the mixing coefﬁcients so that α0→∞ , then E[πk]→\n",
      "1/K.\n",
      "In Figure 10.6, the prior over the mixing coefﬁcients is a Dirichlet of the form\n",
      "(10.39). Recall from Figure 2.5 that for α0<1the prior favours solutions in which\n",
      "some of the mixing coefﬁcients are zero. Figure 10.6 was obtained using α0=1 0−3,\n",
      "and resulted in two components having nonzero mixing coefﬁcients. If instead we\n",
      "choose α0=1 we obtain three components with nonzero mixing coefﬁcients, and\n",
      "forα=1 0 all six components have nonzero mixing coefﬁcients.\n",
      "As we have seen there is a close similarity between the variational solution for\n",
      "the Bayesian mixture of Gaussians and the EM algorithm for maximum likelihood.\n",
      "In fact if we consider the limit N→∞ then the Bayesian treatment converges to the\n",
      "maximum likelihood EM algorithm. For anything other than very small data sets,\n",
      "the dominant computational cost of the variational algorithm for Gaussian mixtures\n",
      "arises from the evaluation of the responsibilities, together with the evaluation and\n",
      "inversion of the weighted data covariance matrices. These computations mirror pre-\n",
      "cisely those that arise in the maximum likelihood EM algorithm, and so there is little\n",
      "computational overhead in using this Bayesian approach as compared to the tradi-\n",
      "tional maximum likelihood one. There are, however, some substantial advantages.\n",
      "First of all, the singularities that arise in maximum likelihood when a Gaussian com-\n",
      "ponent ‘collapses’ onto a speciﬁc data point are absent in the Bayesian treatment.10.2. Illustration: Variational Mixture of Gaussians 481\n",
      "Indeed, these singularities are removed if we simply introduce a prior and then use a\n",
      "MAP estimate instead of maximum likelihood. Furthermore, there is no over-ﬁttingif we choose a large number Kof components in the mixture, as we saw in Fig-\n",
      "ure 10.6. Finally, the variational treatment opens up the possibility of determining\n",
      "the optimal number of components in the mixture without resorting to techniquessuch as cross validation. Section 10.2.4\n",
      "10.2.2 Variational lower bound\n",
      "We can also straightforwardly evaluate the lower bound (10.3) for this model.\n",
      "In practice, it is useful to be able to monitor the bound during the re-estimation in\n",
      "order to test for convergence. It can also provide a valuable check on both the math-ematical expressions for the solutions and their software implementation, because at\n",
      "each step of the iterative re-estimation procedure the value of this bound should not\n",
      "decrease. We can take this a stage further to provide a deeper test of the correctnessof both the mathematical derivation of the update equations and of their software im-\n",
      "plementation by using ﬁnite differences to check that each update does indeed give\n",
      "a (constrained) maximum of the bound (Svens ´en and Bishop, 2004).\n",
      "For the variational mixture of Gaussians, the lower bound (10.3) is given by\n",
      "L=\n",
      "∑\n",
      "Z∫∫∫\n",
      "q(Z,π,µ,Λ)l n{p(X,Z,π,µ,Λ)\n",
      "q(Z,π,µ,Λ)}\n",
      "dπdµdΛ\n",
      "= E[lnp(X,Z,π,µ,Λ)]−E[lnq(Z,π,µ,Λ)]\n",
      "= E[lnp(X|Z,µ,Λ)] + E[lnp(Z|π)] + E[lnp(π)] + E[lnp(µ,Λ)]\n",
      "−E[lnq(Z)]−E[lnq(π)]−E[lnq(µ,Λ)] (10.70)\n",
      "where, to keep the notation uncluttered, we have omitted the ⋆superscript on the\n",
      "qdistributions, along with the subscripts on the expectation operators because each\n",
      "expectation is taken with respect to all of the random variables in its argument. The\n",
      "various terms in the bound are easily evaluated to give the following results Exercise 10.16\n",
      "E[lnp(X|Z,µ,Λ)] =1\n",
      "2K∑\n",
      "k=1Nk{\n",
      "ln˜Λk−Dβ−1\n",
      "k−νkTr(SkWk)\n",
      "−νk(xk−mk)TWk(xk−mk)−Dln(2π)}\n",
      "(10.71)\n",
      "E[lnp(Z|π)] =N∑\n",
      "n=1K∑\n",
      "k=1rnkln˜πk (10.72)\n",
      "E[lnp(π)] = ln C(α0)+(α0−1)K∑\n",
      "k=1ln˜πk (10.73)482 10. APPROXIMATE INFERENCE\n",
      "E[lnp(µ,Λ)] =1\n",
      "2K∑\n",
      "k=1{\n",
      "Dln(β0/2π)+l n˜Λk−Dβ0\n",
      "βk\n",
      "−β0νk(mk−m0)TWk(mk−m0)}\n",
      "+KlnB(W0,ν0)\n",
      "+(ν0−D−1)\n",
      "2K∑\n",
      "k=1ln˜Λk−1\n",
      "2K∑\n",
      "k=1νkTr(W−1\n",
      "0Wk) (10.74)\n",
      "E[lnq(Z)] =N∑\n",
      "n=1K∑\n",
      "k=1rnklnrnk (10.75)\n",
      "E[lnq(π)] =K∑\n",
      "k=1(αk−1) ln˜πk+l nC(α) (10.76)\n",
      "E[lnq(µ,Λ)] =K∑\n",
      "k=1{1\n",
      "2ln˜Λk+D\n",
      "2ln(βk\n",
      "2π)\n",
      "−D\n",
      "2−H[q(Λk)]}\n",
      "(10.77)\n",
      "where Dis the dimensionality of x,H[q(Λk)]is the entropy of the Wishart distribu-\n",
      "tion given by (B.82), and the coefﬁcients C(α)andB(W,ν)are deﬁned by (B.23)\n",
      "and (B.79), respectively. Note that the terms involving expectations of the logs of theqdistributions simply represent the negative entropies of those distributions. Some\n",
      "simpliﬁcations and combination of terms can be performed when these expressions\n",
      "are summed to give the lower bound. However, we have kept the expressions sepa-rate for ease of understanding.\n",
      "Finally, it is worth noting that the lower bound provides an alternative approach\n",
      "for deriving the variational re-estimation equations obtained in Section 10.2.1. To dothis we use the fact that, since the model has conjugate priors, the functional form of\n",
      "the factors in the variational posterior distribution is known, namely discrete for Z,\n",
      "Dirichlet for π, and Gaussian-Wishart for (µ\n",
      "k,Λk). By taking general parametric\n",
      "forms for these distributions we can derive the form of the lower bound as a function\n",
      "of the parameters of the distributions. Maximizing the bound with respect to these\n",
      "parameters then gives the required re-estimation equations. Exercise 10.18\n",
      "10.2.3 Predictive density\n",
      "In applications of the Bayesian mixture of Gaussians model we will often be\n",
      "interested in the predictive density for a new value ˆxof the observed variable. As-\n",
      "sociated with this observation will be a corresponding latent variable ˆz, and the pre-\n",
      "dictive density is then given by\n",
      "p(ˆx|X)=∑\n",
      "bz∫∫∫\n",
      "p(ˆx|ˆz,µ,Λ)p(ˆz|π)p(π,µ,Λ|X)dπdµdΛ (10.78)10.2. Illustration: Variational Mixture of Gaussians 483\n",
      "where p(π,µ,Λ|X)is the (unknown) true posterior distribution of the parameters.\n",
      "Using (10.37) and (10.38) we can ﬁrst perform the summation over ˆzto give\n",
      "p(ˆx|X)=K∑\n",
      "k=1∫∫∫\n",
      "πkN(\n",
      "ˆx|µk,Λ−1\n",
      "k)\n",
      "p(π,µ,Λ|X)dπdµdΛ. (10.79)\n",
      "Because the remaining integrations are intractable, we approximate the predictive\n",
      "density by replacing the true posterior distribution p(π,µ,Λ|X)with its variational\n",
      "approximation q(π)q(µ,Λ)to give\n",
      "p(ˆx|X)=K∑\n",
      "k=1∫∫∫\n",
      "πkN(ˆx|µk,Λ−1\n",
      "k)\n",
      "q(π)q(µk,Λk)dπdµkdΛk(10.80)\n",
      "where we have made use of the factorization (10.55) and in each term we have im-\n",
      "plicitly integrated out all variables {µj,Λj}forj̸=kThe remaining integrations\n",
      "can now be evaluated analytically giving a mixture of Student’s t-distributions Exercise 10.19\n",
      "p(ˆx|X)=1\n",
      "ˆαK∑\n",
      "k=1αkSt(ˆx|mk,Lk,νk+1−D) (10.81)\n",
      "in which the kthcomponent has mean mk, and the precision is given by\n",
      "Lk=(νk+1−D)βk\n",
      "(1 +βk)Wk (10.82)\n",
      "in which νkis given by (10.63). When the size Nof the data set is large the predictive\n",
      "distribution (10.81) reduces to a mixture of Gaussians. Exercise 10.20\n",
      "10.2.4 Determining the number of components\n",
      "We have seen that the variational lower bound can be used to determine a pos-\n",
      "terior distribution over the number Kof components in the mixture model. There Section 10.1.4\n",
      "is, however, one subtlety that needs to be addressed. For any given setting of the\n",
      "parameters in a Gaussian mixture model (except for speciﬁc degenerate settings),there will exist other parameter settings for which the density over the observed vari-\n",
      "ables will be identical. These parameter values differ only through a re-labelling of\n",
      "the components. For instance, consider a mixture of two Gaussians and a single ob-served variable x, in which the parameters have the values π\n",
      "1=a,π2=b,µ1=c,\n",
      "µ2=d,σ1=e,σ2=f. Then the parameter values π1=b,π2=a,µ1=d,\n",
      "µ2=c,σ1=f,σ2=e, in which the two components have been exchanged, will\n",
      "by symmetry give rise to the same value of p(x). If we have a mixture model com-\n",
      "prising Kcomponents, then each parameter setting will be a member of a family of\n",
      "K!equivalent settings. Exercise 10.21\n",
      "In the context of maximum likelihood, this redundancy is irrelevant because the\n",
      "parameter optimization algorithm (for example EM) will, depending on the initial-ization of the parameters, ﬁnd one speciﬁc solution, and the other equivalent solu-\n",
      "tions play no role. In a Bayesian setting, however, we marginalize over all possible484 10. APPROXIMATE INFERENCE\n",
      "Figure 10.7 Plot of the variational lower bound\n",
      "Lversus the number Kof com-\n",
      "ponents in the Gaussian mixture\n",
      "model, for the Old Faithful data,\n",
      "showing a distinct peak at K=\n",
      "2components. For each value\n",
      "ofK, the model is trained from\n",
      "100different random starts, and\n",
      "the results shown as ‘ +’ symbols\n",
      "plotted with small random hori-\n",
      "zontal perturbations so that they\n",
      "can be distinguished. Note that\n",
      "some solutions ﬁnd suboptimal\n",
      "local maxima, but that this hap-\n",
      "pens infrequently.\n",
      "Kp(D|K)\n",
      "1 2 3 4 5 6\n",
      "parameter values. We have seen in Figure 10.2 that if the true posterior distribution\n",
      "is multimodal, variational inference based on the minimization of KL(q∥p)will tend\n",
      "to approximate the distribution in the neighbourhood of one of the modes and ignore\n",
      "the others. Again, because equivalent modes have equivalent predictive densities,\n",
      "this is of no concern provided we are considering a model having a speciﬁc number\n",
      "Kof components. If, however, we wish to compare different values of K, then we\n",
      "need to take account of this multimodality. A simple approximate solution is to add\n",
      "a term lnK!onto the lower bound when used for model comparison and averaging. Exercise 10.22\n",
      "Figure 10.7 shows a plot of the lower bound, including the multimodality fac-\n",
      "tor, versus the number Kof components for the Old Faithful data set. It is worth\n",
      "emphasizing once again that maximum likelihood would lead to values of the likeli-\n",
      "hood function that increase monotonically with K(assuming the singular solutions\n",
      "have been avoided, and discounting the effects of local maxima) and so cannot be\n",
      "used to determine an appropriate model complexity. By contrast, Bayesian inference\n",
      "automatically makes the trade-off between model complexity and ﬁtting the data. Section 3.4\n",
      "This approach to the determination of Krequires that a range of models having\n",
      "different Kvalues be trained and compared. An alternative approach to determining\n",
      "a suitable value for Kis to treat the mixing coefﬁcients πas parameters and make\n",
      "point estimates of their values by maximizing the lower bound (Corduneanu and\n",
      "Bishop, 2001) with respect to πinstead of maintaining a probability distribution\n",
      "over them as in the fully Bayesian approach. This leads to the re-estimation equation Exercise 10.23\n",
      "πk=1\n",
      "NN∑\n",
      "n=1rnk (10.83)\n",
      "and this maximization is interleaved with the variational updates for the qdistribution\n",
      "over the remaining parameters. Components that provide insufﬁcient contribution10.2. Illustration: Variational Mixture of Gaussians 485\n",
      "to explaining the data will have their mixing coefﬁcients driven to zero during the\n",
      "optimization, and so they are effectively removed from the model through automatic\n",
      "relevance determination . This allows us to make a single training run in which we\n",
      "start with a relatively large initial value of K, and allow surplus components to be\n",
      "pruned out of the model. The origins of the sparsity when optimizing with respect tohyperparameters is discussed in detail in the context of the relevance vector machine. Section 7.2.2\n",
      "10.2.5 Induced factorizations\n",
      "In deriving these variational update equations for the Gaussian mixture model,\n",
      "we assumed a particular factorization of the variational posterior distribution given\n",
      "by (10.42). However, the optimal solutions for the various factors exhibit additional\n",
      "factorizations. In particular, the solution for q⋆(µ,Λ)is given by the product of an\n",
      "independent distribution q⋆(µk,Λk)over each of the components kof the mixture,\n",
      "whereas the variational posterior distribution q⋆(Z)over the latent variables, given\n",
      "by (10.48), factorizes into an independent distribution q⋆(zn)for each observation n\n",
      "(note that it does not further factorize with respect to kbecause, for each value of n,\n",
      "theznkare constrained to sum to one over k). These additional factorizations are a\n",
      "consequence of the interaction between the assumed factorization and the conditionalindependence properties of the true distribution, as characterized by the directed\n",
      "graph in Figure 10.5.\n",
      "We shall refer to these additional factorizations as induced factorizations be-\n",
      "cause they arise from an interaction between the factorization assumed in the varia-\n",
      "tional posterior distribution and the conditional independence properties of the true\n",
      "joint distribution. In a numerical implementation of the variational approach it is\n",
      "important to take account of such additional factorizations. For instance, it would\n",
      "be very inefﬁcient to maintain a full precision matrix for the Gaussian distributionover a set of variables if the optimal form for that distribution always had a diago-\n",
      "nal precision matrix (corresponding to a factorization with respect to the individual\n",
      "variables described by that Gaussian).\n",
      "Such induced factorizations can easily be detected using a simple graphical test\n",
      "based on d-separation as follows. We partition the latent variables into three disjoint\n",
      "groups A,B,Cand then let us suppose that we are assuming a factorization between\n",
      "Cand the remaining latent variables, so that\n",
      "q(A,B,C)=q(A,B)q(C). (10.84)\n",
      "Using the general result (10.9), together with the product rule for probabilities, we\n",
      "see that the optimal solution for q(A,B)is given by\n",
      "lnq\n",
      "⋆(A,B)= EC[lnp(X,A,B,C)] + const\n",
      "= EC[lnp(A,B|X,C)] + const . (10.85)\n",
      "We now ask whether this resulting solution will factorize between AandB,i n\n",
      "other words whether q⋆(A,B)=q⋆(A)q⋆(B). This will happen if, and only if,\n",
      "lnp(A,B|X,C)=l n p(A|X,C)+l n p(B|X,C), that is, if the conditional inde-\n",
      "pendence relation\n",
      "A⊥⊥B|X,C (10.86)486 10. APPROXIMATE INFERENCE\n",
      "is satisﬁed. We can test to see if this relation does hold, for any choice of AandB\n",
      "by making use of the d-separation criterion.\n",
      "To illustrate this, consider again the Bayesian mixture of Gaussians represented\n",
      "by the directed graph in Figure 10.5, in which we are assuming a variational fac-\n",
      "torization given by (10.42). We can see immediately that the variational posteriordistribution over the parameters must factorize between πand the remaining param-\n",
      "etersµandΛbecause all paths connecting πto either µorΛmust pass through\n",
      "one of the nodes z\n",
      "nall of which are in the conditioning set for our conditional inde-\n",
      "pendence test and all of which are head-to-tail with respect to such paths.\n",
      "10.3. Variational Linear Regression\n",
      "As a second illustration of variational inference, we return to the Bayesian linear\n",
      "regression model of Section 3.3. In the evidence framework, we approximated the\n",
      "integration over αandβby making point estimates obtained by maximizing the log\n",
      "marginal likelihood. A fully Bayesian approach would integrate over the hyperpa-\n",
      "rameters as well as over the parameters. Although exact integration is intractable,\n",
      "we can use variational methods to ﬁnd a tractable approximation. In order to sim-\n",
      "plify the discussion, we shall suppose that the noise precision parameter βis known,\n",
      "and is ﬁxed to its true value, although the framework is easily extended to includethe distribution over β. For the linear regression model, the variational treatment Exercise 10.26\n",
      "will turn out to be equivalent to the evidence framework. Nevertheless, it provides a\n",
      "good exercise in the use of variational methods and will also lay the foundation forvariational treatment of Bayesian logistic regression in Section 10.6.\n",
      "Recall that the likelihood function for w, and the prior over w, are given by\n",
      "p(t|w)=\n",
      "N∏\n",
      "n=1N(tn|wTφn,β−1) (10.87)\n",
      "p(w|α)= N(w|0,α−1I) (10.88)\n",
      "where φn=φ(xn). We now introduce a prior distribution over α. From our dis-\n",
      "cussion in Section 2.3.6, we know that the conjugate prior for the precision of a\n",
      "Gaussian is given by a gamma distribution, and so we choose\n",
      "p(α)=G a m ( α|a0,b0) (10.89)\n",
      "where Gam(·|·,·)is deﬁned by (B.26). Thus the joint distribution of all the variables\n",
      "is given by\n",
      "p(t,w,α)=p(t|w)p(w|α)p(α). (10.90)\n",
      "This can be represented as a directed graphical model as shown in Figure 10.8.\n",
      "10.3.1 Variational distribution\n",
      "Our ﬁrst goal is to ﬁnd an approximation to the posterior distribution p(w,α|t).\n",
      "To do this, we employ the variational framework of Section 10.1, with a variational10.3. Variational Linear Regression 487\n",
      "Figure 10.8 Probabilistic graphical model representing the joint dis-\n",
      "tribution (10.90) for the Bayesian linear regression\n",
      "model.\n",
      "tnφn\n",
      "Nwα\n",
      "β\n",
      "posterior distribution given by the factorized expression\n",
      "q(w,α)=q(w)q(α). (10.91)\n",
      "We can ﬁnd re-estimation equations for the factors in this distribution by making use\n",
      "of the general result (10.9). Recall that for each factor, we take the log of the joint\n",
      "distribution over all variables and then average with respect to those variables not in\n",
      "that factor. Consider ﬁrst the distribution over α. Keeping only terms that have a\n",
      "functional dependence on α,w eh a v e\n",
      "lnq⋆(α)=l n p(α)+ Ew[lnp(w|α) ]+c o n s t\n",
      "=(a0−1) lnα−b0α+M\n",
      "2lnα−α\n",
      "2E[wTw]+c o n s t . (10.92)\n",
      "We recognize this as the log of a gamma distribution, and so identifying the coefﬁ-\n",
      "cients of αandlnαwe obtain\n",
      "q⋆(α)=G a m ( α|aN,bN) (10.93)\n",
      "where\n",
      "aN=a0+M\n",
      "2(10.94)\n",
      "bN=b0+1\n",
      "2E[wTw]. (10.95)\n",
      "Similarly, we can ﬁnd the variational re-estimation equation for the posterior\n",
      "distribution over w. Again, using the general result (10.9), and keeping only those\n",
      "terms that have a functional dependence on w,w eh a v e\n",
      "lnq⋆(w)=l n p(t|w)+ Eα[lnp(w|α)] + const (10.96)\n",
      "=−β\n",
      "2N∑\n",
      "n=1{wTφn−tn}2−1\n",
      "2E[α]wTw+c o n s t (10.97)\n",
      "=−1\n",
      "2wT(\n",
      "E[α]I+βΦTΦ)\n",
      "w+βwTΦTt+c o n s t . (10.98)\n",
      "Because this is a quadratic form, the distribution q⋆(w)is Gaussian, and so we can\n",
      "complete the square in the usual way to identify the mean and covariance, giving\n",
      "q⋆(w)=N(w|mN,SN) (10.99)488 10. APPROXIMATE INFERENCE\n",
      "where\n",
      "mN=βSNΦTt (10.100)\n",
      "SN=(\n",
      "E[α]I+βΦTΦ)−1. (10.101)\n",
      "Note the close similarity to the posterior distribution (3.52) obtained when αwas\n",
      "treated as a ﬁxed parameter. The difference is that here αis replaced by its expecta-\n",
      "tion E[α]under the variational distribution. Indeed, we have chosen to use the same\n",
      "notation for the covariance matrix SNin both cases.\n",
      "Using the standard results (B.27), (B.38), and (B.39), we can obtain the required\n",
      "moments as follows\n",
      "E[α]= aN/bN (10.102)\n",
      "E[wwT]= mNmT\n",
      "N+SN. (10.103)\n",
      "The evaluation of the variational posterior distribution begins by initializing the pa-\n",
      "rameters of one of the distributions q(w)orq(α), and then alternately re-estimates\n",
      "these factors in turn until a suitable convergence criterion is satisﬁed (usually speci-\n",
      "ﬁed in terms of the lower bound to be discussed shortly).\n",
      "It is instructive to relate the variational solution to that found using the evidence\n",
      "framework in Section 3.5. To do this consider the case a0=b0=0, corresponding\n",
      "to the limit of an inﬁnitely broad prior over α. The mean of the variational posterior\n",
      "distribution q(α)is then given by\n",
      "E[α]=aN\n",
      "bN=M/2\n",
      "E[wTw]/2=M\n",
      "mT\n",
      "NmN+Tr(SN). (10.104)\n",
      "Comparison with (9.63) shows that in the case of this particularly simple model,\n",
      "the variational approach gives precisely the same expression as that obtained by\n",
      "maximizing the evidence function using EM except that the point estimate for α\n",
      "is replaced by its expected value. Because the distribution q(w)depends on q(α)\n",
      "only through the expectation E[α], we see that the two approaches will give identical\n",
      "results for the case of an inﬁnitely broad prior.\n",
      "10.3.2 Predictive distribution\n",
      "The predictive distribution over t, given a new input x, is easily evaluated for\n",
      "this model using the Gaussian variational posterior for the parameters\n",
      "p(t|x,t)=∫\n",
      "p(t|x,w)p(w|t)dw\n",
      "≃∫\n",
      "p(t|x,w)q(w)dw\n",
      "=∫\n",
      "N(t|wTφ(x),β−1)N(w|mN,SN)dw\n",
      "=N(t|mT\n",
      "Nφ(x),σ2(x)) (10.105)10.3. Variational Linear Regression 489\n",
      "where we have evaluated the integral by making use of the result (2.115) for the\n",
      "linear-Gaussian model. Here the input-dependent variance is given by\n",
      "σ2(x)=1\n",
      "β+φ(x)TSNφ(x). (10.106)\n",
      "Note that this takes the same form as the result (3.59) obtained with ﬁxed αexcept\n",
      "that now the expected value E[α]appears in the deﬁnition of SN.\n",
      "10.3.3 Lower bound\n",
      "Another quantity of importance is the lower bound Ldeﬁned by\n",
      "L(q)= E[lnp(w,α,t)]−E[lnq(w,α)]\n",
      "= Ew[lnp(t|w)] + Ew,α[lnp(w|α)] + Eα[lnp(α)]\n",
      "−Eα[lnq(w)]w−E[lnq(α)]. (10.107)\n",
      "Evaluation of the various terms is straightforward, making use of results obtained in Exercise 10.27\n",
      "previous chapters, and gives\n",
      "E[lnp(t|w)]w=N\n",
      "2ln(β\n",
      "2π)\n",
      "−β\n",
      "2tTt+βmT\n",
      "NΦTt\n",
      "−β\n",
      "2Tr[\n",
      "ΦTΦ(mNmT\n",
      "N+SN)]\n",
      "(10.108)\n",
      "E[lnp(w|α)]w,α=−M\n",
      "2ln(2π)+M\n",
      "2(ψ(aN)−lnbN)\n",
      "−aN\n",
      "2bN[\n",
      "mT\n",
      "NmN+Tr(SN)]\n",
      "(10.109)\n",
      "E[lnp(α)]α=a0lnb0+(a0−1) [ψ(aN)−lnbN]\n",
      "−b0aN\n",
      "bN−lnΓ(aN) (10.110)\n",
      "−E[lnq(w)]w=1\n",
      "2ln|SN|+M\n",
      "2[1 + ln(2 π)] (10.111)\n",
      "−E[lnq(α)]α=l n Γ ( aN)−(aN−1)ψ(aN)−lnbN+aN.(10.112)\n",
      "Figure 10.9 shows a plot of the lower bound L(q)versus the degree of a polynomial\n",
      "model for a synthetic data set generated from a degree three polynomial. Here theprior parameters have been set to a\n",
      "0=b0=0, corresponding to the noninformative\n",
      "priorp(α)∝1/α, which is uniform over lnαas discussed in Section 2.3.6. As\n",
      "we saw in Section 10.1, the quantity Lrepresents lower bound on the log marginal\n",
      "likelihood p(t|M)for the model. If we assign equal prior probabilities p(M)to the\n",
      "different values of M, then we can interpret Las an approximation to the poste-\n",
      "rior model probability p(M|t). Thus the variational framework assigns the highest\n",
      "probability to the model with M=3. This should be contrasted with the maximum\n",
      "likelihood result, which assigns ever smaller residual error to models of increasingcomplexity until the residual error is driven to zero, causing maximum likelihood to\n",
      "favour severely over-ﬁtted models.490 10. APPROXIMATE INFERENCE\n",
      "Figure 10.9 Plot of the lower bound Lver-\n",
      "sus the order Mof the polyno-\n",
      "mial, for a polynomial model, in\n",
      "which a set of 10data points is\n",
      "generated from a polynomial with\n",
      "M=3 sampled over the inter-\n",
      "val(−5,5)with additive Gaussian\n",
      "noise of variance 0.09. The value\n",
      "of the bound gives the log prob-\n",
      "ability of the model, and we see\n",
      "that the value of the bound peaks\n",
      "atM=3, corresponding to the\n",
      "true model from which the data\n",
      "set was generated.\n",
      "1 3 5 7 9\n",
      "10.4. Exponential Family Distributions\n",
      "In Chapter 2, we discussed the important role played by the exponential family of\n",
      "distributions and their conjugate priors. For many of the models discussed in this\n",
      "book, the complete-data likelihood is drawn from the exponential family. However,\n",
      "in general this will not be the case for the marginal likelihood function for the ob-\n",
      "served data. For example, in a mixture of Gaussians, the joint distribution of obser-\n",
      "vations xnand corresponding hidden variables znis a member of the exponential\n",
      "family, whereas the marginal distribution of xnis a mixture of Gaussians and hence\n",
      "is not.\n",
      "Up to now we have grouped the variables in the model into observed variables\n",
      "and hidden variables. We now make a further distinction between latent variables,\n",
      "denoted Z, and parameters, denoted θ, where parameters are intensive (ﬁxed in num-\n",
      "ber independent of the size of the data set), whereas latent variables are extensive\n",
      "(scale in number with the size of the data set). For example, in a Gaussian mixture\n",
      "model, the indicator variables zkn(which specify which component kis responsible\n",
      "for generating data point xn) represent the latent variables, whereas the means µk,\n",
      "precisions Λkand mixing proportions πkrepresent the parameters.\n",
      "Consider the case of independent identically distributed data. We denote the\n",
      "data values by X={xn}, where n=1,...N , with corresponding latent variables\n",
      "Z={zn}. Now suppose that the joint distribution of observed and latent variables\n",
      "is a member of the exponential family, parameterized by natural parameters ηso that\n",
      "p(X,Z|η)=N∏\n",
      "n=1h(xn,zn)g(η)e x p{\n",
      "ηTu(xn,zn)}\n",
      ". (10.113)\n",
      "We shall also use a conjugate prior for η, which can be written as\n",
      "p(η|ν0,v0)=f(ν0,χ0)g(η)ν0exp{\n",
      "νoηTχ0}\n",
      ". (10.114)\n",
      "Recall that the conjugate prior distribution can be interpreted as a prior number ν0\n",
      "of observations all having the value χ0for the uvector. Now consider a variational10.4. Exponential Family Distributions 491\n",
      "distribution that factorizes between the latent variables and the parameters, so that\n",
      "q(Z,η)=q(Z)q(η). Using the general result (10.9), we can solve for the two\n",
      "factors as follows\n",
      "lnq⋆(Z)= Eη[lnp(X,Z|η) ]+c o n s t\n",
      "=N∑\n",
      "n=1{\n",
      "lnh(xn,zn)+ E[ηT]u(xn,zn)}\n",
      "+c o n s t .(10.115)\n",
      "Thus we see that this decomposes into a sum of independent terms, one for each\n",
      "value of n, and hence the solution for q⋆(Z)will factorize over nso that q⋆(Z)=∏\n",
      "nq⋆(zn). This is an example of an induced factorization. Taking the exponential Section 10.2.5\n",
      "of both sides, we have\n",
      "q⋆(zn)=h(xn,zn)g(E[η])exp{\n",
      "E[ηT]u(xn,zn)}\n",
      "(10.116)\n",
      "where the normalization coefﬁcient has been re-instated by comparison with the\n",
      "standard form for the exponential family.\n",
      "Similarly, for the variational distribution over the parameters, we have\n",
      "lnq⋆(η)=l n p(η|ν0,χ0)+ EZ[lnp(X,Z|η) ]+c o n s t (10.117)\n",
      "=ν0lng(η)+ηTχ0+N∑\n",
      "n=1{\n",
      "lng(η)+ηTEzn[u(xn,zn)]}\n",
      "+c o n s t .(10.118)\n",
      "Again, taking the exponential of both sides, and re-instating the normalization coef-\n",
      "ﬁcient by inspection, we have\n",
      "q⋆(η)=f(νN,χN)g(η)νNexp{\n",
      "ηTχN}\n",
      "(10.119)\n",
      "where we have deﬁned\n",
      "νN=ν0+N (10.120)\n",
      "χN=χ0+N∑\n",
      "n=1Ezn[u(xn,zn)]. (10.121)\n",
      "Note that the solutions for q⋆(zn)andq⋆(η)are coupled, and so we solve them iter-\n",
      "atively in a two-stage procedure. In the variational E step, we evaluate the expected\n",
      "sufﬁcient statistics E[u(xn,zn)]using the current posterior distribution q(zn)over\n",
      "the latent variables and use this to compute a revised posterior distribution q(η)over\n",
      "the parameters. Then in the subsequent variational M step, we use this revised pa-\n",
      "rameter posterior distribution to ﬁnd the expected natural parameters E[ηT], which\n",
      "gives rise to a revised variational distribution over the latent variables.\n",
      "10.4.1 Variational message passing\n",
      "We have illustrated the application of variational methods by considering a spe-\n",
      "ciﬁc model, the Bayesian mixture of Gaussians, in some detail. This model can be492 10. APPROXIMATE INFERENCE\n",
      "described by the directed graph shown in Figure 10.5. Here we consider more gen-\n",
      "erally the use of variational methods for models described by directed graphs andderive a number of widely applicable results.\n",
      "The joint distribution corresponding to a directed graph can be written using the\n",
      "decomposition\n",
      "p(x)=∏\n",
      "ip(xi|pai) (10.122)\n",
      "wherexidenotes the variable(s) associated with node i, andpaidenotes the parent\n",
      "set corresponding to node i. Note that ximay be a latent variable or it may belong\n",
      "to the set of observed variables. Now consider a variational approximation in which\n",
      "the distribution q(x)is assumed to factorize with respect to the xiso that\n",
      "q(x)=∏\n",
      "iqi(xi). (10.123)\n",
      "Note that for observed nodes, there is no factor q(xi)in the variational distribution.\n",
      "We now substitute (10.122) into our general result (10.9) to give\n",
      "lnq⋆\n",
      "j(xj)= Ei̸=j[∑\n",
      "ilnp(xi|pai)]\n",
      "+c o n s t . (10.124)\n",
      "Any terms on the right-hand side that do not depend on xjcan be absorbed into\n",
      "the additive constant. In fact, the only terms that do depend on xjare the con-\n",
      "ditional distribution for xjgiven by p(xj|paj)together with any other conditional\n",
      "distributions that have xjin the conditioning set. By deﬁnition, these conditional\n",
      "distributions correspond to the children of node j, and they therefore also depend on\n",
      "theco-parents of the child nodes, i.e., the other parents of the child nodes besides\n",
      "nodexjitself. We see that the set of all nodes on which q⋆(xj)depends corresponds\n",
      "to the Markov blanket of node xj, as illustrated in Figure 8.26. Thus the update\n",
      "of the factors in the variational posterior distribution represents a local calculationon the graph. This makes possible the construction of general purpose software for\n",
      "variational inference in which the form of the model does not need to be speciﬁed in\n",
      "advance (Bishop et al. , 2003).\n",
      "If we now specialize to the case of a model in which all of the conditional dis-\n",
      "tributions have a conjugate-exponential structure, then the variational update proce-\n",
      "dure can be cast in terms of a local message passing algorithm (Winn and Bishop,2005). In particular, the distribution associated with a particular node can be updated\n",
      "once that node has received messages from all of its parents and all of its children.\n",
      "This in turn requires that the children have already received messages from their co-parents. The evaluation of the lower bound can also be simpliﬁed because many of\n",
      "the required quantities are already evaluated as part of the message passing scheme.\n",
      "This distributed message passing formulation has good scaling properties and is well\n",
      "suited to large networks.10.5. Local Variational Methods 493\n",
      "10.5. Local Variational Methods\n",
      "The variational framework discussed in Sections 10.1 and 10.2 can be considered a\n",
      "‘global’ method in the sense that it directly seeks an approximation to the full poste-\n",
      "rior distribution over all random variables. An alternative ‘local’ approach involves\n",
      "ﬁnding bounds on functions over individual variables or groups of variables within\n",
      "a model. For instance, we might seek a bound on a conditional distribution p(y|x),\n",
      "which is itself just one factor in a much larger probabilistic model speciﬁed by a\n",
      "directed graph. The purpose of introducing the bound of course is to simplify the\n",
      "resulting distribution. This local approximation can be applied to multiple variables\n",
      "in turn until a tractable approximation is obtained, and in Section 10.6.1 we shall\n",
      "give a practical example of this approach in the context of logistic regression. Here\n",
      "we focus on developing the bounds themselves.\n",
      "We have already seen in our discussion of the Kullback-Leibler divergence that\n",
      "the convexity of the logarithm function played a key role in developing the lower\n",
      "bound in the global variational approach. We have deﬁned a (strictly) convex func-\n",
      "tion as one for which every chord lies above the function. Convexity also plays a Section 1.6.1\n",
      "central role in the local variational framework. Note that our discussion will ap-\n",
      "ply equally to concave functions with ‘min’ and ‘max’ interchanged and with lower\n",
      "bounds replaced by upper bounds.\n",
      "Let us begin by considering a simple example, namely the function f(x)=\n",
      "exp(−x), which is a convex function of x, and which is shown in the left-hand plot\n",
      "of Figure 10.10. Our goal is to approximate f(x)by a simpler function, in particular\n",
      "a linear function of x. From Figure 10.10, we see that this linear function will be a\n",
      "lower bound on f(x)if it corresponds to a tangent. We can obtain the tangent line\n",
      "y(x)at a speciﬁc value of x, sayx=ξ, by making a ﬁrst order Taylor expansion\n",
      "y(x)=f(ξ)+f′(ξ)(x−ξ) (10.125)\n",
      "so that y(x)⩽f(x)with equality when x=ξ. For our example function f(x)=\n",
      "Figure 10.10 In the left-hand ﬁg-\n",
      "ure the red curve shows the function\n",
      "exp(−x), and the blue line shows\n",
      "the tangent at x=ξdeﬁned by\n",
      "(10.125) with ξ=1. This line has\n",
      "slope λ=f′(ξ)=−exp(−ξ). Note\n",
      "that any other tangent line, for ex-\n",
      "ample the ones shown in green, will\n",
      "have a smaller value of yatx=\n",
      "ξ. The right-hand ﬁgure shows the\n",
      "corresponding plot of the function\n",
      "λξ−g(λ), where g(λ)is given by\n",
      "(10.131), versus λforξ=1,i n\n",
      "which the maximum corresponds to\n",
      "λ=−exp(−ξ)=−1/e.x ξ 0 1.5 300.51\n",
      "λλξ−g(λ)\n",
      "−1 −0.5 000.20.4494 10. APPROXIMATE INFERENCE\n",
      "xy\n",
      "f(x)\n",
      "λxxy\n",
      "f(x)\n",
      "λx−g(λ)−g(λ)\n",
      "Figure 10.11 In the left-hand plot the red curve shows a convex function f(x), and the blue line represents the\n",
      "linear function λx, which is a lower bound on f(x)because f(x)>λ x for all x. For the given value of slope λthe\n",
      "contact point of the tangent line having the same slope is found by minimizing with respect to xthe discrepancy\n",
      "(shown by the green dashed lines) given by f(x)−λx. This deﬁnes the dual function g(λ), which corresponds\n",
      "to the (negative of the) intercept of the tangent line having slope λ.\n",
      "exp(−x), we therefore obtain the tangent line in the form\n",
      "y(x)=e x p ( −ξ)−exp(−ξ)(x−ξ) (10.126)\n",
      "which is a linear function parameterized by ξ. For consistency with subsequent\n",
      "discussion, let us deﬁne λ=−exp(−ξ)so that\n",
      "y(x, λ)=λx−λ+λln(−λ). (10.127)\n",
      "Different values of λcorrespond to different tangent lines, and because all such lines\n",
      "are lower bounds on the function, we have f(x)⩾y(x, λ). Thus we can write the\n",
      "function in the form\n",
      "f(x)=m a x\n",
      "λ{λx−λ+λln(−λ)}. (10.128)\n",
      "We have succeeded in approximating the convex function f(x)by a simpler, lin-\n",
      "ear function y(x, λ). The price we have paid is that we have introduced a variational\n",
      "parameter λ, and to obtain the tightest bound we must optimize with respect to λ.\n",
      "We can formulate this approach more generally using the framework of convex\n",
      "duality (Rockafellar, 1972; Jordan et al. , 1999). Consider the illustration of a convex\n",
      "function f(x)shown in the left-hand plot in Figure 10.11. In this example, the\n",
      "function λxis a lower bound on f(x)but it is not the best lower bound that can\n",
      "be achieved by a linear function having slope λ, because the tightest bound is given\n",
      "by the tangent line. Let us write the equation of the tangent line, having slope λas\n",
      "λx−g(λ)where the (negative) intercept g(λ)clearly depends on the slope λof the\n",
      "tangent. To determine the intercept, we note that the line must be moved vertically by\n",
      "an amount equal to the smallest vertical distance between the line and the function,\n",
      "as shown in Figure 10.11. Thus\n",
      "g(λ)= −min\n",
      "x{f(x)−λx}\n",
      "=m a x\n",
      "x{λx−f(x)}. (10.129)10.5. Local Variational Methods 495\n",
      "Now, instead of ﬁxing λand varying x, we can consider a particular xand then\n",
      "adjust λuntil the tangent plane is tangent at that particular x. Because the yvalue\n",
      "of the tangent line at a particular xis maximized when that value coincides with its\n",
      "contact point, we have\n",
      "f(x) = max\n",
      "λ{λx−g(λ)}. (10.130)\n",
      "We see that the functions f(x)andg(λ)play a dual role, and are related through\n",
      "(10.129) and (10.130).\n",
      "Let us apply these duality relations to our simple example f(x)=e x p ( −x).\n",
      "From (10.129) we see that the maximizing value of xis given by ξ=−ln(−λ), and\n",
      "back-substituting we obtain the conjugate function g(λ)in the form\n",
      "g(λ)=λ−λln(−λ) (10.131)\n",
      "as obtained previously. The function λξ−g(λ)is shown, for ξ=1in the right-hand\n",
      "plot in Figure 10.10. As a check, we can substitute (10.131) into (10.130), which\n",
      "gives the maximizing value of λ=−exp(−x), and back-substituting then recovers\n",
      "the original function f(x)=e x p ( −x).\n",
      "For concave functions, we can follow a similar argument to obtain upper bounds,\n",
      "in which max’ is replaced with ‘ min’, so that\n",
      "f(x) = min\n",
      "λ{λx−g(λ)} (10.132)\n",
      "g(λ) = min\n",
      "x{λx−f(x)}. (10.133)\n",
      "If the function of interest is not convex (or concave), then we cannot directly\n",
      "apply the method above to obtain a bound. However, we can ﬁrst seek invertibletransformations either of the function or of its argument which change it into a con-\n",
      "vex form. We then calculate the conjugate function and then transform back to the\n",
      "original variables.\n",
      "An important example, which arises frequently in pattern recognition, is the\n",
      "logistic sigmoid function deﬁned by\n",
      "σ(x)=1\n",
      "1+e−x. (10.134)\n",
      "As it stands this function is neither convex nor concave. However, if we take the\n",
      "logarithm we obtain a function which is concave, as is easily veriﬁed by ﬁnding the\n",
      "second derivative. From (10.133) the corresponding conjugate function then takes Exercise 10.30\n",
      "the form\n",
      "g(λ)=m i n\n",
      "x{λx−f(x)}=−λlnλ−(1−λ)l n ( 1−λ) (10.135)\n",
      "which we recognize as the binary entropy function for a variable whose probability\n",
      "of having the value 1isλ. Using (10.132), we then obtain an upper bound on the log Appendix B\n",
      "sigmoid\n",
      "lnσ(x)⩽λx−g(λ) (10.136)496 10. APPROXIMATE INFERENCE\n",
      "λ=0.2\n",
      "λ=0.7\n",
      "−6 0 600.51\n",
      "ξ=2.5\n",
      "−ξξ −6 0 600.51\n",
      "Figure 10.12 The left-hand plot shows the logistic sigmoid function σ(x)deﬁned by (10.134) in red, together\n",
      "with two examples of the exponential upper bound (10.137) shown in blue. The right-hand plot shows the logistic\n",
      "sigmoid again in red together with the Gaussian lower bound (10.144) shown in blue. Here the parameter\n",
      "ξ=2.5, and the bound is exact at x=ξandx=−ξ, denoted by the dashed green lines.\n",
      "and taking the exponential, we obtain an upper bound on the logistic sigmoid itself\n",
      "of the form\n",
      "σ(x)⩽exp(λx−g(λ)) (10.137)\n",
      "which is plotted for two values of λon the left-hand plot in Figure 10.12.\n",
      "We can also obtain a lower bound on the sigmoid having the functional form of\n",
      "a Gaussian. To do this, we follow Jaakkola and Jordan (2000) and make transforma-\n",
      "tions both of the input variable and of the function itself. First we take the log of the\n",
      "logistic function and then decompose it so that\n",
      "lnσ(x)= −ln(1 + e−x)=−ln{\n",
      "e−x/2(ex/2+e−x/2)}\n",
      "=x/2−ln(ex/2+e−x/2). (10.138)\n",
      "We now note that the function f(x)=−ln(ex/2+e−x/2)is a convex function of\n",
      "the variable x2, as can again be veriﬁed by ﬁnding the second derivative. This leads Exercise 10.31\n",
      "to a lower bound on f(x), which is a linear function of x2whose conjugate function\n",
      "is given by\n",
      "g(λ)=m a x\n",
      "x2{\n",
      "λx2−f(√\n",
      "x2)}\n",
      ". (10.139)\n",
      "The stationarity condition leads to\n",
      "0=λ−dx\n",
      "dx2d\n",
      "dxf(x)=λ+1\n",
      "4xtanh(x\n",
      "2)\n",
      ". (10.140)\n",
      "If we denote this value of x, corresponding to the contact point of the tangent line\n",
      "for this particular value of λ,b yξ, then we have\n",
      "λ(ξ)=−1\n",
      "4ξtanh(ξ\n",
      "2)\n",
      "=−1\n",
      "2ξ[\n",
      "σ(ξ)−1\n",
      "2]\n",
      ". (10.141)10.5. Local Variational Methods 497\n",
      "Instead of thinking of λas the variational parameter, we can let ξplay this role as\n",
      "this leads to simpler expressions for the conjugate function, which is then given by\n",
      "g(λ)=λ(ξ)ξ2−f(ξ)=λ(ξ)ξ2+l n (eξ/2+e−ξ/2). (10.142)\n",
      "Hence the bound on f(x)can be written as\n",
      "f(x)⩾λx2−g(λ)=λx2−λξ2−ln(eξ/2+e−ξ/2). (10.143)\n",
      "The bound on the sigmoid then becomes\n",
      "σ(x)⩾σ(ξ)e x p{\n",
      "(x−ξ)/2−λ(ξ)(x2−ξ2)}\n",
      "(10.144)\n",
      "where λ(ξ)is deﬁned by (10.141). This bound is illustrated in the right-hand plot of\n",
      "Figure 10.12. We see that the bound has the form of the exponential of a quadratic\n",
      "function of x, which will prove useful when we seek Gaussian representations of\n",
      "posterior distributions deﬁned through logistic sigmoid functions. Section 4.5\n",
      "The logistic sigmoid arises frequently in probabilistic models over binary vari-\n",
      "ables because it is the function that transforms a log odds ratio into a posterior prob-\n",
      "ability. The corresponding transformation for a multiclass distribution is given bythe softmax function. Unfortunately, the lower bound derived here for the logistic Section 4.3\n",
      "sigmoid does not directly extend to the softmax. Gibbs (1997) proposes a method\n",
      "for constructing a Gaussian distribution that is conjectured to be a bound (although\n",
      "no rigorous proof is given), which may be used to apply local variational methods to\n",
      "multiclass problems.\n",
      "We shall see an example of the use of local variational bounds in Sections 10.6.1.\n",
      "For the moment, however, it is instructive to consider in general terms how these\n",
      "bounds can be used. Suppose we wish to evaluate an integral of the form\n",
      "I=∫\n",
      "σ(a)p(a)da (10.145)\n",
      "where σ(a)is the logistic sigmoid, and p(a)is a Gaussian probability density. Such\n",
      "integrals arise in Bayesian models when, for instance, we wish to evaluate the pre-\n",
      "dictive distribution, in which case p(a)represents a posterior parameter distribution.\n",
      "Because the integral is intractable, we employ the variational bound (10.144), which\n",
      "we write in the form σ(a)⩾f(a, ξ)where ξis a variational parameter. The inte-\n",
      "gral now becomes the product of two exponential-quadratic functions and so can beintegrated analytically to give a bound on I\n",
      "I⩾∫\n",
      "f(a, ξ)p(a)da=F(ξ). (10.146)\n",
      "We now have the freedom to choose the variational parameter ξ, which we do by\n",
      "ﬁnding the value ξ⋆that maximizes the function F(ξ). The resulting value F(ξ⋆)\n",
      "represents the tightest bound within this family of bounds and can be used as an\n",
      "approximation to I. This optimized bound, however, will in general not be exact.498 10. APPROXIMATE INFERENCE\n",
      "Although the bound σ(a)⩾f(a, ξ)on the logistic sigmoid can be optimized exactly,\n",
      "the required choice for ξdepends on the value of a, so that the bound is exact for one\n",
      "value of aonly. Because the quantity F(ξ)is obtained by integrating over all values\n",
      "ofa, the value of ξ⋆represents a compromise, weighted by the distribution p(a).\n",
      "10.6. Variational Logistic Regression\n",
      "We now illustrate the use of local variational methods by returning to the Bayesian\n",
      "logistic regression model studied in Section 4.5. There we focussed on the use of\n",
      "the Laplace approximation, while here we consider a variational treatment based on\n",
      "the approach of Jaakkola and Jordan (2000). Like the Laplace method, this alsoleads to a Gaussian approximation to the posterior distribution. However, the greater\n",
      "ﬂexibility of the variational approximation leads to improved accuracy compared\n",
      "to the Laplace method. Furthermore (unlike the Laplace method), the variational\n",
      "approach is optimizing a well deﬁned objective function given by a rigourous bound\n",
      "on the model evidence. Logistic regression has also been treated by Dybowski andRoberts (2005) from a Bayesian perspective using Monte Carlo sampling techniques.\n",
      "10.6.1 Variational posterior distribution\n",
      "Here we shall make use of a variational approximation based on the local bounds\n",
      "introduced in Section 10.5. This allows the likelihood function for logistic regres-\n",
      "sion, which is governed by the logistic sigmoid, to be approximated by the expo-\n",
      "nential of a quadratic form. It is therefore again convenient to choose a conjugateGaussian prior of the form (4.140). For the moment, we shall treat the hyperparam-\n",
      "etersm\n",
      "0andS0as ﬁxed constants. In Section 10.6.3, we shall demonstrate how the\n",
      "variational formalism can be extended to the case where there are unknown hyper-parameters whose values are to be inferred from the data.\n",
      "In the variational framework, we seek to maximize a lower bound on the marginal\n",
      "likelihood. For the Bayesian logistic regression model, the marginal likelihood takesthe form\n",
      "p(t)=∫\n",
      "p(t|w)p(w)dw=∫[N∏\n",
      "n=1p(tn|w)]\n",
      "p(w)dw. (10.147)\n",
      "We ﬁrst note that the conditional distribution for tcan be written as\n",
      "p(t|w)= σ(a)t{1−σ(a)}1−t\n",
      "=(1\n",
      "1+e−a)t(\n",
      "1−1\n",
      "1+e−a)1−t\n",
      "=eate−a\n",
      "1+e−a=eatσ(−a) (10.148)\n",
      "where a=wTφ. In order to obtain a lower bound on p(t), we make use of the\n",
      "variational lower bound on the logistic sigmoid function given by (10.144), which10.6. Variational Logistic Regression 499\n",
      "we reproduce here for convenience\n",
      "σ(z)⩾σ(ξ)e x p{\n",
      "(z−ξ)/2−λ(ξ)(z2−ξ2)}\n",
      "(10.149)\n",
      "where\n",
      "λ(ξ)=1\n",
      "2ξ[\n",
      "σ(ξ)−1\n",
      "2]\n",
      ". (10.150)\n",
      "We can therefore write\n",
      "p(t|w)=eatσ(−a)⩾eatσ(ξ)e x p{\n",
      "−(a+ξ)/2−λ(ξ)(a2−ξ2)}\n",
      ".(10.151)\n",
      "Note that because this bound is applied to each of the terms in the likelihood function\n",
      "separately, there is a variational parameter ξncorresponding to each training set\n",
      "observation (φn,tn). Using a=wTφ, and multiplying by the prior distribution, we\n",
      "obtain the following bound on the joint distribution of tandw\n",
      "p(t,w)=p(t|w)p(w)⩾h(w,ξ)p(w) (10.152)\n",
      "where ξdenotes the set {ξn}of variational parameters, and\n",
      "h(w,ξ)=N∏\n",
      "n=1σ(ξn)e x p{\n",
      "wTφntn−(wTφn+ξn)/2\n",
      "−λ(ξn)([wTφn]2−ξ2\n",
      "n)}\n",
      ". (10.153)\n",
      "Evaluation of the exact posterior distribution would require normalization of the left-\n",
      "hand side of this inequality. Because this is intractable, we work instead with the\n",
      "right-hand side. Note that the function on the right-hand side cannot be interpreted\n",
      "as a probability density because it is not normalized. Once it is normalized to give avariational posterior distribution q(w), however, it no longer represents a bound.\n",
      "Because the logarithm function is monotonically increasing, the inequality A⩾\n",
      "Bimplies lnA⩾lnB. This gives a lower bound on the log of the joint distribution\n",
      "oftandwof the form\n",
      "ln{p(t|w)p(w)}⩾lnp(w)+N∑\n",
      "n=1{\n",
      "lnσ(ξn)+wTφntn\n",
      "−(wTφn+ξn)/2−λ(ξn)([wTφn]2−ξ2\n",
      "n)}\n",
      ". (10.154)\n",
      "Substituting for the prior p(w), the right-hand side of this inequality becomes, as a\n",
      "function of w\n",
      "−1\n",
      "2(w−m0)TS−1\n",
      "0(w−m0)\n",
      "+N∑\n",
      "n=1{\n",
      "wTφn(tn−1/2)−λ(ξn)wT(φnφT\n",
      "n)w}\n",
      "+c o n s t . (10.155)500 10. APPROXIMATE INFERENCE\n",
      "This is a quadratic function of w, and so we can obtain the corresponding variational\n",
      "approximation to the posterior distribution by identifying the linear and quadraticterms in w, giving a Gaussian variational posterior of the form\n",
      "q(w)=N(w|m\n",
      "N,SN) (10.156)\n",
      "where\n",
      "mN=SN(\n",
      "S−1\n",
      "0m0+N∑\n",
      "n=1(tn−1/2)φn)\n",
      "(10.157)\n",
      "S−1\n",
      "N=S−1\n",
      "0+2N∑\n",
      "n=1λ(ξn)φnφT\n",
      "n. (10.158)\n",
      "As with the Laplace framework, we have again obtained a Gaussian approximation\n",
      "to the posterior distribution. However, the additional ﬂexibility provided by the vari-ational parameters {ξ\n",
      "n}leads to improved accuracy in the approximation (Jaakkola\n",
      "and Jordan, 2000).\n",
      "Here we have considered a batch learning context in which all of the training\n",
      "data is available at once. However, Bayesian methods are intrinsically well suited\n",
      "to sequential learning in which the data points are processed one at a time and then\n",
      "discarded. The formulation of this variational approach for the sequential case is\n",
      "straightforward. Exercise 10.32\n",
      "Note that the bound given by (10.149) applies only to the two-class problem and\n",
      "so this approach does not directly generalize to classiﬁcation problems with K>2\n",
      "classes. An alternative bound for the multiclass case has been explored by Gibbs\n",
      "(1997).\n",
      "10.6.2 Optimizing the variational parameters\n",
      "We now have a normalized Gaussian approximation to the posterior distribution,\n",
      "which we shall use shortly to evaluate the predictive distribution for new data points.\n",
      "First, however, we need to determine the variational parameters {ξn}by maximizing\n",
      "the lower bound on the marginal likelihood.\n",
      "To do this, we substitute the inequality (10.152) back into the marginal likeli-\n",
      "hood to give\n",
      "lnp(t)=l n∫\n",
      "p(t|w)p(w)dw⩾ln∫\n",
      "h(w,ξ)p(w)dw=L(ξ). (10.159)\n",
      "As with the optimization of the hyperparameter αin the linear regression model of\n",
      "Section 3.5, there are two approaches to determining the ξn. In the ﬁrst approach, we\n",
      "recognize that the function L(ξ)is deﬁned by an integration over wand so we can\n",
      "viewwas a latent variable and invoke the EM algorithm. In the second approach,\n",
      "we integrate over wanalytically and then perform a direct maximization over ξ. Let\n",
      "us begin by considering the EM approach.\n",
      "The EM algorithm starts by choosing some initial values for the parameters\n",
      "{ξn}, which we denote collectively by ξold. In the E step of the EM algorithm,10.6. Variational Logistic Regression 501\n",
      "we then use these parameter values to ﬁnd the posterior distribution over w, which\n",
      "is given by (10.156). In the M step, we then maximize the expected complete-datalog likelihood which is given by\n",
      "Q(ξ,ξ\n",
      "old)= E[lnh(w,ξ)p(w)] (10.160)\n",
      "where the expectation is taken with respect to the posterior distribution q(w)evalu-\n",
      "ated using ξold. Noting that p(w)does not depend on ξ, and substituting for h(w,ξ)\n",
      "we obtain\n",
      "Q(ξ,ξold)=N∑\n",
      "n=1{\n",
      "lnσ(ξn)−ξn/2−λ(ξn)(φT\n",
      "nE[wwT]φn−ξ2\n",
      "n)}\n",
      "+c o n s t\n",
      "(10.161)\n",
      "where ‘const’ denotes terms that are independent of ξ. We now set the derivative with\n",
      "respect to ξnequal to zero. A few lines of algebra, making use of the deﬁnitions of\n",
      "σ(ξ)andλ(ξ), then gives\n",
      "0=λ′(ξn)(φT\n",
      "nE[wwT]φn−ξ2\n",
      "n). (10.162)\n",
      "We now note that λ′(ξ)is a monotonic function of ξforξ⩾0, and that we can\n",
      "restrict attention to nonnegative values of ξwithout loss of generality due to the\n",
      "symmetry of the bound around ξ=0. Thus λ′(ξ)̸=0, and hence we obtain the\n",
      "following re-estimation equations Exercise 10.33\n",
      "(ξnew\n",
      "n)2=φT\n",
      "nE[wwT]φn=φT\n",
      "n(\n",
      "SN+mNmT\n",
      "N)\n",
      "φn (10.163)\n",
      "where we have used (10.156).\n",
      "Let us summarize the EM algorithm for ﬁnding the variational posterior distri-\n",
      "bution. We ﬁrst initialize the variational parameters ξold. In the E step, we evaluate\n",
      "the posterior distribution over wgiven by (10.156), in which the mean and covari-\n",
      "ance are deﬁned by (10.157) and (10.158). In the M step, we then use this variational\n",
      "posterior to compute a new value for ξgiven by (10.163). The E and M steps are\n",
      "repeated until a suitable convergence criterion is satisﬁed, which in practice typicallyrequires only a few iterations.\n",
      "An alternative approach to obtaining re-estimation equations for ξis to note\n",
      "that in the integral over win the deﬁnition (10.159) of the lower bound L(ξ), the\n",
      "integrand has a Gaussian-like form and so the integral can be evaluated analytically.\n",
      "Having evaluated the integral, we can then differentiate with respect to ξ\n",
      "n. It turns\n",
      "out that this gives rise to exactly the same re-estimation equations as does the EMapproach given by (10.163). Exercise 10.34\n",
      "As we have emphasized already, in the application of variational methods it is\n",
      "useful to be able to evaluate the lower bound L(ξ)given by (10.159). The integration\n",
      "overwcan be performed analytically by noting that p(w)is Gaussian and h(w,ξ)\n",
      "is the exponential of a quadratic function of w. Thus, by completing the square\n",
      "and making use of the standard result for the normalization coefﬁcient of a Gaussian\n",
      "distribution, we can obtain a closed form solution which takes the form Exercise 10.35502 10. APPROXIMATE INFERENCE\n",
      "0.010.250.750.99\n",
      "−4 −2 0 2 4−6−4−20246\n",
      "−4 −2 0 2 4−6−4−20246\n",
      "Figure 10.13 Illustration of the Bayesian approach to logistic regression for a simple linearly separable data\n",
      "set. The plot on the left shows the predictive distribution obtained using variational inference. We see that\n",
      "the decision boundary lies roughly mid way between the clusters of data points, and that the contours of the\n",
      "predictive distribution splay out away from the data reﬂecting the greater uncertainty in the classiﬁcation of such\n",
      "regions. The plot on the right shows the decision boundaries corresponding to ﬁve samples of the parameter\n",
      "vector wdrawn from the posterior distribution p(w|t).\n",
      "L(ξ)=1\n",
      "2ln|SN|\n",
      "|S0|−1\n",
      "2mT\n",
      "NS−1\n",
      "NmN+1\n",
      "2mT\n",
      "0S−1\n",
      "0m0\n",
      "+N∑\n",
      "n=1{\n",
      "lnσ(ξn)−1\n",
      "2ξn−λ(ξn)ξ2\n",
      "n}\n",
      ". (10.164)\n",
      "This variational framework can also be applied to situations in which the data\n",
      "is arriving sequentially (Jaakkola and Jordan, 2000). In this case we maintain a\n",
      "Gaussian posterior distribution over w, which is initialized using the prior p(w).A s\n",
      "each data point arrives, the posterior is updated by making use of the bound (10.151)\n",
      "and then normalized to give an updated posterior distribution.\n",
      "The predictive distribution is obtained by marginalizing over the posterior dis-\n",
      "tribution, and takes the same form as for the Laplace approximation discussed in\n",
      "Section 4.5.2. Figure 10.13 shows the variational predictive distributions for a syn-\n",
      "thetic data set. This example provides interesting insights into the concept of ‘large\n",
      "margin’, which was discussed in Section 7.1 and which has qualitatively similar be-\n",
      "haviour to the Bayesian solution.\n",
      "10.6.3 Inference of hyperparameters\n",
      "So far, we have treated the hyperparameter αin the prior distribution as a known\n",
      "constant. We now extend the Bayesian logistic regression model to allow the value of\n",
      "this parameter to be inferred from the data set. This can be achieved by combining\n",
      "the global and local variational approximations into a single framework, so as to\n",
      "maintain a lower bound on the marginal likelihood at each stage. Such a combined\n",
      "approach was adopted by Bishop and Svens ´en (2003) in the context of a Bayesian\n",
      "treatment of the hierarchical mixture of experts model.10.6. Variational Logistic Regression 503\n",
      "Speciﬁcally, we consider once again a simple isotropic Gaussian prior distribu-\n",
      "tion of the form\n",
      "p(w|α)=N(w|0,α−1I). (10.165)\n",
      "Our analysis is readily extended to more general Gaussian priors, for instance if we\n",
      "wish to associate a different hyperparameter with different subsets of the parame-\n",
      "terswj. As usual, we consider a conjugate hyperprior over αgiven by a gamma\n",
      "distribution\n",
      "p(α)=G a m ( α|a0,b0) (10.166)\n",
      "governed by the constants a0andb0.\n",
      "The marginal likelihood for this model now takes the form\n",
      "p(t)=∫∫\n",
      "p(w,α,t)dwdα (10.167)\n",
      "where the joint distribution is given by\n",
      "p(w,α,t)=p(t|w)p(w|α)p(α). (10.168)\n",
      "We are now faced with an analytically intractable integration over wandα, which\n",
      "we shall tackle by using both the local and global variational approaches in the samemodel\n",
      "To begin with, we introduce a variational distribution q(w,α), and then apply\n",
      "the decomposition (10.2), which in this instance takes the form\n",
      "lnp(t)=L(q) + KL( q∥p) (10.169)\n",
      "where the lower bound L(q)and the Kullback-Leibler divergence KL(q∥p)are de-\n",
      "ﬁned by\n",
      "L(q)=∫∫\n",
      "q(w,α)l n{p(w,α,t)\n",
      "q(w,α)}\n",
      "dwdα (10.170)\n",
      "KL(q∥p)= −∫∫\n",
      "q(w,α)l n{p(w,α|t))\n",
      "q(w,α)}\n",
      "dwdα. (10.171)\n",
      "At this point, the lower bound L(q)is still intractable due to the form of the\n",
      "likelihood factor p(t|w). We therefore apply the local variational bound to each of\n",
      "the logistic sigmoid factors as before. This allows us to use the inequality (10.152)and place a lower bound on L(q), which will therefore also be a lower bound on the\n",
      "log marginal likelihood\n",
      "lnp(t)⩾L(q)⩾˜L(q,ξ)\n",
      "=∫∫\n",
      "q(w,α)l n{h(w,ξ)p(w|α)p(α)\n",
      "q(w,α)}\n",
      "dwdα.(10.172)\n",
      "Next we assume that the variational distribution factorizes between parameters and\n",
      "hyperparameters so that\n",
      "q(w,α)=q(w)q(α). (10.173)504 10. APPROXIMATE INFERENCE\n",
      "With this factorization we can appeal to the general result (10.9) to ﬁnd expressions\n",
      "for the optimal factors. Consider ﬁrst the distribution q(w). Discarding terms that\n",
      "are independent of w,w eh a v e\n",
      "lnq(w)= Eα[ln{h(w,ξ)p(w|α)p(α)}]+c o n s t\n",
      "=l n h(w,ξ)+ Eα[lnp(w|α) ]+c o n s t .\n",
      "We now substitute for lnh(w,ξ)using (10.153), and for lnp(w|α)using (10.165),\n",
      "giving\n",
      "lnq(w)=−E[α]\n",
      "2wTw+N∑\n",
      "n=1{\n",
      "(tn−1/2)wTφn−λ(ξn)wTφnφT\n",
      "nw}\n",
      "+c o n s t .\n",
      "We see that this is a quadratic function of wand so the solution for q(w)will be\n",
      "Gaussian. Completing the square in the usual way, we obtain\n",
      "q(w)=N(w|µN,ΣN) (10.174)\n",
      "where we have deﬁned\n",
      "Σ−1\n",
      "NµN=N∑\n",
      "n=1(tn−1/2)φn (10.175)\n",
      "Σ−1\n",
      "N= E[α]I+2N∑\n",
      "n=1λ(ξn)φnφT\n",
      "n. (10.176)\n",
      "Similarly, the optimal solution for the factor q(α)is obtained from\n",
      "lnq(α)= Ew[lnp(w|α)] + ln p(α)+c o n s t .\n",
      "Substituting for lnp(w|α)using (10.165), and for lnp(α)using (10.166), we obtain\n",
      "lnq(α)=M\n",
      "2lnα−α\n",
      "2E[\n",
      "wTw]\n",
      "+(a0−1) lnα−b0α+c o n s t .\n",
      "We recognize this as the log of a gamma distribution, and so we obtain\n",
      "q(α)=G a m ( α|aN,bN)=1\n",
      "Γ(a0)ab0\n",
      "0αa0−1e−b0α(10.177)\n",
      "where\n",
      "aN=a0+M\n",
      "2(10.178)\n",
      "bN=b0+1\n",
      "2Ew[\n",
      "wTw]\n",
      ". (10.179)10.7. Expectation Propagation 505\n",
      "We also need to optimize the variational parameters ξn, and this is also done by\n",
      "maximizing the lower bound ˜L(q,ξ). Omitting terms that are independent of ξ, and\n",
      "integrating over α,w eh a v e\n",
      "˜L(q,ξ)=∫\n",
      "q(w)l nh(w,ξ)dw+c o n s t . (10.180)\n",
      "Note that this has precisely the same form as (10.159), and so we can again appeal\n",
      "to our earlier result (10.163), which can be obtained by direct optimization of the\n",
      "marginal likelihood function, leading to re-estimation equations of the form\n",
      "(ξnew\n",
      "n)2=φT\n",
      "n(\n",
      "ΣN+µNµT\n",
      "N)\n",
      "φn. (10.181)\n",
      "We have obtained re-estimation equations for the three quantities q(w),q(α),\n",
      "andξ, and so after making suitable initializations, we can cycle through these quan-\n",
      "tities, updating each in turn. The required moments are given by Appendix B\n",
      "E[α]=aN\n",
      "bN(10.182)\n",
      "E[\n",
      "wTw]\n",
      "=ΣN+µT\n",
      "NµN. (10.183)\n",
      "10.7. Expectation Propagation\n",
      "We conclude this chapter by discussing an alternative form of deterministic approx-\n",
      "imate inference, known as expectation propagation orEP(Minka, 2001a; Minka,\n",
      "2001b). As with the variational Bayes methods discussed so far, this too is based\n",
      "on the minimization of a Kullback-Leibler divergence but now of the reverse form,\n",
      "which gives the approximation rather different properties.\n",
      "Consider for a moment the problem of minimizing KL(p∥q)with respect to q(z)\n",
      "whenp(z)is a ﬁxed distribution and q(z)is a member of the exponential family and\n",
      "so, from (2.194), can be written in the form\n",
      "q(z)=h(z)g(η)e x p{\n",
      "ηTu(z)}\n",
      ". (10.184)\n",
      "As a function of η, the Kullback-Leibler divergence then becomes\n",
      "KL(p∥q)=−lng(η)−ηTEp(z)[u(z) ]+c o n s t (10.185)\n",
      "where the constant terms are independent of the natural parameters η. We can mini-\n",
      "mizeKL(p∥q)within this family of distributions by setting the gradient with respect\n",
      "toηto zero, giving\n",
      "−∇lng(η)= Ep(z)[u(z)]. (10.186)\n",
      "However, we have already seen in (2.226) that the negative gradient of lng(η)is\n",
      "given by the expectation of u(z)under the distribution q(z). Equating these two\n",
      "results, we obtain\n",
      "Eq(z)[u(z)] = Ep(z)[u(z)]. (10.187)506 10. APPROXIMATE INFERENCE\n",
      "We see that the optimum solution simply corresponds to matching the expected suf-\n",
      "ﬁcient statistics. So, for instance, if q(z)is a Gaussian N(z|µ,Σ)then we minimize\n",
      "the Kullback-Leibler divergence by setting the mean µofq(z)equal to the mean of\n",
      "the distribution p(z)and the covariance Σequal to the covariance of p(z). This is\n",
      "sometimes called moment matching . An example of this was seen in Figure 10.3(a).\n",
      "Now let us exploit this result to obtain a practical algorithm for approximate\n",
      "inference. For many probabilistic models, the joint distribution of data Dand hidden\n",
      "variables (including parameters) θcomprises a product of factors in the form\n",
      "p(D,θ)=∏\n",
      "ifi(θ). (10.188)\n",
      "This would arise, for example, in a model for independent, identically distributed\n",
      "data in which there is one factor fn(θ)=p(xn|θ)for each data point xn, along\n",
      "with a factor f0(θ)=p(θ)corresponding to the prior. More generally, it would also\n",
      "apply to any model deﬁned by a directed probabilistic graph in which each factor is a\n",
      "conditional distribution corresponding to one of the nodes, or an undirected graph inwhich each factor is a clique potential. We are interested in evaluating the posterior\n",
      "distribution p(θ|D)for the purpose of making predictions, as well as the model\n",
      "evidence p(D)for the purpose of model comparison. From (10.188) the posterior is\n",
      "given by\n",
      "p(θ|D)=1\n",
      "p(D)∏\n",
      "ifi(θ) (10.189)\n",
      "and the model evidence is given by\n",
      "p(D)=∫∏\n",
      "ifi(θ)dθ. (10.190)\n",
      "Here we are considering continuous variables, but the following discussion applies\n",
      "equally to discrete variables with integrals replaced by summations. We shall sup-pose that the marginalization over θ, along with the marginalizations with respect to\n",
      "the posterior distribution required to make predictions, are intractable so that some\n",
      "form of approximation is required.\n",
      "Expectation propagation is based on an approximation to the posterior distribu-\n",
      "tion which is also given by a product of factors\n",
      "q(θ)=1\n",
      "Z∏\n",
      "i˜fi(θ) (10.191)\n",
      "in which each factor ˜fi(θ)in the approximation corresponds to one of the factors\n",
      "fi(θ)in the true posterior (10.189), and the factor 1/Zis the normalizing constant\n",
      "needed to ensure that the left-hand side of (10.191) integrates to unity. In order to\n",
      "obtain a practical algorithm, we need to constrain the factors ˜fi(θ)in some way,\n",
      "and in particular we shall assume that they come from the exponential family. The\n",
      "product of the factors will therefore also be from the exponential family and so can10.7. Expectation Propagation 507\n",
      "be described by a ﬁnite set of sufﬁcient statistics. For example, if each of the ˜fi(θ)\n",
      "is a Gaussian, then the overall approximation q(θ)will also be Gaussian.\n",
      "Ideally we would like to determine the ˜fi(θ)by minimizing the Kullback-Leibler\n",
      "divergence between the true posterior and the approximation given by\n",
      "KL(p∥q)=K L(\n",
      "1\n",
      "p(D)∏\n",
      "ifi(θ)1\n",
      "Z∏\n",
      "i˜fi(θ))\n",
      ". (10.192)\n",
      "Note that this is the reverse form of KL divergence compared with that used in varia-\n",
      "tional inference. In general, this minimization will be intractable because the KL di-vergence involves averaging with respect to the true distribution. As a rough approx-\n",
      "imation, we could instead minimize the KL divergences between the corresponding\n",
      "pairsf\n",
      "i(θ)and˜fi(θ)of factors. This represents a much simpler problem to solve,\n",
      "and has the advantage that the algorithm is noniterative. However, because each fac-tor is individually approximated, the product of the factors could well give a poor\n",
      "approximation.\n",
      "Expectation propagation makes a much better approximation by optimizing each\n",
      "factor in turn in the context of all of the remaining factors. It starts by initializing\n",
      "the factors˜fi(θ), and then cycles through the factors reﬁning them one at a time.\n",
      "This is similar in spirit to the update of factors in the variational Bayes framework\n",
      "considered earlier. Suppose we wish to reﬁne factor ˜fj(θ). We ﬁrst remove this\n",
      "factor from the product to give∏\n",
      "i̸=j˜fi(θ). Conceptually, we will now determine a\n",
      "revised form of the factor ˜fj(θ)by ensuring that the product\n",
      "qnew(θ)∝˜fj(θ)∏\n",
      "i̸=j˜fi(θ) (10.193)\n",
      "is as close as possible to\n",
      "fj(θ)∏\n",
      "i̸=j˜fi(θ) (10.194)\n",
      "in which we keep ﬁxed all of the factors ˜fi(θ)fori̸=j. This ensures that the\n",
      "approximation is most accurate in the regions of high posterior probability as deﬁned\n",
      "by the remaining factors. We shall see an example of this effect when we apply EP\n",
      "to the ‘clutter problem’. To achieve this, we ﬁrst remove the factor ˜fj(θ)from the Section 10.7.1\n",
      "current approximation to the posterior by deﬁning the unnormalized distribution\n",
      "q\\j(θ)=q(θ)\n",
      "˜fj(θ). (10.195)\n",
      "Note that we could instead ﬁnd q\\j(θ)from the product of factors i̸=j, although\n",
      "in practice division is usually easier. This is now combined with the factor fj(θ)to\n",
      "give a distribution\n",
      "1\n",
      "Zjfj(θ)q\\j(θ) (10.196)508 10. APPROXIMATE INFERENCE\n",
      "−2 −1 0 1 2 3 400.20.40.60.81\n",
      "−2 −1 0 1 2 3 4010203040\n",
      "Figure 10.14 Illustration of the expectation propagation approximation using a Gaussian distribution for the\n",
      "example considered earlier in Figures 4.14 and 10.1. The left-hand plot shows the original distribution (yellow)\n",
      "along with the Laplace (red), global variational (green), and EP (blue) approximations, and the right-hand plot\n",
      "shows the corresponding negative logarithms of the distributions. Note that the EP distribution is broader than\n",
      "that variational inference, as a consequence of the different form of KL divergence.\n",
      "where Zjis the normalization constant given by\n",
      "Zj=∫\n",
      "fj(θ)q\\j(θ)dθ. (10.197)\n",
      "We now determine a revised factor ˜fj(θ)by minimizing the Kullback-Leibler diver-\n",
      "gence\n",
      "KL(fj(θ)q\\j(θ)\n",
      "Zjqnew(θ))\n",
      ". (10.198)\n",
      "This is easily solved because the approximating distribution qnew(θ)is from the ex-\n",
      "ponential family, and so we can appeal to the result (10.187), which tells us that the\n",
      "parameters of qnew(θ)are obtained by matching its expected sufﬁcient statistics to\n",
      "the corresponding moments of (10.196). We shall assume that this is a tractable oper-\n",
      "ation. For example, if we choose q(θ)to be a Gaussian distribution N(θ|µ,Σ), then\n",
      "µis set equal to the mean of the (unnormalized) distribution fj(θ)q\\j(θ), andΣis\n",
      "set to its covariance. More generally, it is straightforward to obtain the required ex-\n",
      "pectations for any member of the exponential family, provided it can be normalized,\n",
      "because the expected statistics can be related to the derivatives of the normalization\n",
      "coefﬁcient, as given by (2.226). The EP approximation is illustrated in Figure 10.14.\n",
      "From (10.193), we see that the revised factor ˜fj(θ)can be found by taking\n",
      "qnew(θ)and dividing out the remaining factors so that\n",
      "˜fj(θ)=Kqnew(θ)\n",
      "q\\j(θ)(10.199)\n",
      "where we have used (10.195). The coefﬁcient Kis determined by multiplying both10.7. Expectation Propagation 509\n",
      "sides of (10.199) by q\\i(θ)and integrating to give\n",
      "K=∫\n",
      "˜fj(θ)q\\j(θ)dθ (10.200)\n",
      "where we have used the fact that qnew(θ)is normalized. The value of Kcan therefore\n",
      "be found by matching zeroth-order moments\n",
      "∫\n",
      "˜fj(θ)q\\j(θ)dθ=∫\n",
      "fj(θ)q\\j(θ)dθ. (10.201)\n",
      "Combining this with (10.197), we then see that K=Zjand so can be found by\n",
      "evaluating the integral in (10.197).\n",
      "In practice, several passes are made through the set of factors, revising each\n",
      "factor in turn. The posterior distribution p(θ|D)is then approximated using (10.191),\n",
      "and the model evidence p(D)can be approximated by using (10.190) with the factors\n",
      "fi(θ)replaced by their approximations ˜fi(θ).\n",
      "Expectation Propagation\n",
      "We are given a joint distribution over observed data Dand stochastic variables\n",
      "θin the form of a product of factors\n",
      "p(D,θ)=∏\n",
      "ifi(θ) (10.202)\n",
      "and we wish to approximate the posterior distribution p(θ|D)by a distribution\n",
      "of the form\n",
      "q(θ)=1\n",
      "Z∏\n",
      "i˜fi(θ). (10.203)\n",
      "We also wish to approximate the model evidence p(D).\n",
      "1. Initialize all of the approximating factors ˜fi(θ).\n",
      "2. Initialize the posterior approximation by setting\n",
      "q(θ)∝∏\n",
      "i˜fi(θ). (10.204)\n",
      "3. Until convergence:\n",
      "(a) Choose a factor ˜fj(θ)to reﬁne.\n",
      "(b) Remove ˜fj(θ)from the posterior by division\n",
      "q\\j(θ)=q(θ)\n",
      "˜fj(θ). (10.205)510 10. APPROXIMATE INFERENCE\n",
      "(c) Evaluate the new posterior by setting the sufﬁcient statistics (moments)\n",
      "ofqnew(θ)equal to those of q\\j(θ)fj(θ), including evaluation of the\n",
      "normalization constant\n",
      "Zj=∫\n",
      "q\\j(θ)fj(θ)dθ. (10.206)\n",
      "(d) Evaluate and store the new factor\n",
      "˜fj(θ)=Zjqnew(θ)\n",
      "q\\j(θ). (10.207)\n",
      "4. Evaluate the approximation to the model evidence\n",
      "p(D)≃∫∏\n",
      "i˜fi(θ)dθ. (10.208)\n",
      "A special case of EP, known as assumed density ﬁltering (ADF) or moment\n",
      "matching (Maybeck, 1982; Lauritzen, 1992; Boyen and Koller, 1998; Opper and\n",
      "Winther, 1999), is obtained by initializing all of the approximating factors except\n",
      "the ﬁrst to unity and then making one pass through the factors updating each of them\n",
      "once. Assumed density ﬁltering can be appropriate for on-line learning in which datapoints are arriving in a sequence and we need to learn from each data point and then\n",
      "discard it before considering the next point. However, in a batch setting we have the\n",
      "opportunity to re-use the data points many times in order to achieve improved ac-curacy, and it is this idea that is exploited in expectation propagation. Furthermore,\n",
      "if we apply ADF to batch data, the results will have an undesirable dependence on\n",
      "the (arbitrary) order in which the data points are considered, which again EP canovercome.\n",
      "One disadvantage of expectation propagation is that there is no guarantee that\n",
      "the iterations will converge. However, for approximations q(θ)in the exponential\n",
      "family, if the iterations do converge, the resulting solution will be a stationary point\n",
      "of a particular energy function (Minka, 2001a), although each iteration of EP does\n",
      "not necessarily decrease the value of this energy function. This is in contrast tovariational Bayes, which iteratively maximizes a lower bound on the log marginal\n",
      "likelihood, in which each iteration is guaranteed not to decrease the bound. It is\n",
      "possible to optimize the EP cost function directly, in which case it is guaranteedto converge, although the resulting algorithms can be slower and more complex to\n",
      "implement.\n",
      "Another difference between variational Bayes and EP arises from the form of\n",
      "KL divergence that is minimized by the two algorithms, because the former mini-\n",
      "mizes KL(q∥p)whereas the latter minimizes KL(p∥q). As we saw in Figure 10.3,\n",
      "for distributions p(θ)which are multimodal, minimizing KL(p∥q)can lead to poor\n",
      "approximations. In particular, if EP is applied to mixtures the results are not sen-\n",
      "sible because the approximation tries to capture all of the modes of the posteriordistribution. Conversely, in logistic-type models, EP often out-performs both local\n",
      "variational methods and the Laplace approximation (Kuss and Rasmussen, 2006).10.7. Expectation Propagation 511\n",
      "Figure 10.15 Illustration of the clutter problem\n",
      "for a data space dimensionality of\n",
      "D=1. Training data points, de-\n",
      "noted by the crosses, are drawn\n",
      "from a mixture of two Gaussians\n",
      "with components shown in red\n",
      "and green. The goal is to infer the\n",
      "mean of the green Gaussian from\n",
      "the observed data.\n",
      "θ x −5 0 5 10\n",
      "10.7.1 Example: The clutter problem\n",
      "Following Minka (2001b), we illustrate the EP algorithm using a simple exam-\n",
      "ple in which the goal is to infer the mean θof a multivariate Gaussian distribution\n",
      "over a variable xgiven a set of observations drawn from that distribution. To make\n",
      "the problem more interesting, the observations are embedded in background clutter,\n",
      "which itself is also Gaussian distributed, as illustrated in Figure 10.15. The distribu-\n",
      "tion of observed values xis therefore a mixture of Gaussians, which we take to be\n",
      "of the form\n",
      "p(x|θ)=( 1 −w)N(x|θ,I)+wN(x|0,aI) (10.209)\n",
      "where wis the proportion of background clutter and is assumed to be known. The\n",
      "prior over θis taken to be Gaussian\n",
      "p(θ)=N(θ|0,bI) (10.210)\n",
      "and Minka (2001a) chooses the parameter values a=1 0 ,b= 100 andw=0.5.\n",
      "The joint distribution of Nobservations D={x1,...,xN}andθis given by\n",
      "p(D,θ)=p(θ)N∏\n",
      "n=1p(xn|θ) (10.211)\n",
      "and so the posterior distribution comprises a mixture of 2NGaussians. Thus the\n",
      "computational cost of solving this problem exactly would grow exponentially with\n",
      "the size of the data set, and so an exact solution is intractable for moderately large\n",
      "N.\n",
      "To apply EP to the clutter problem, we ﬁrst identify the factors f0(θ)=p(θ)\n",
      "andfn(θ)=p(xn|θ). Next we select an approximating distribution from the expo-\n",
      "nential family, and for this example it is convenient to choose a spherical Gaussian\n",
      "q(θ)=N(θ|m,vI). (10.212)512 10. APPROXIMATE INFERENCE\n",
      "The factor approximations will therefore take the form of exponential-quadratic\n",
      "functions of the form\n",
      "˜fn(θ)=snN(θ|mn,vnI) (10.213)\n",
      "where n=1,...,N , and we set ˜f0(θ)equal to the prior p(θ). Note that the use of\n",
      "N(θ|·,·)does not imply that the right-hand side is a well-deﬁned Gaussian density\n",
      "(in fact, as we shall see, the variance parameter vncan be negative) but is simply a\n",
      "convenient shorthand notation. The approximations ˜fn(θ), forn=1,...,N , can\n",
      "be initialized to unity, corresponding to sn=( 2πvn)D/2,vn→∞ andmn=0,\n",
      "where Dis the dimensionality of xand hence of θ. The initial q(θ), deﬁned by\n",
      "(10.191), is therefore equal to the prior.\n",
      "We then iteratively reﬁne the factors by taking one factor fn(θ)at a time and\n",
      "applying (10.205), (10.206), and (10.207). Note that we do not need to revise the\n",
      "termf0(θ)because an EP update will leave this term unchanged. Here we state the Exercise 10.37\n",
      "results and leave the reader to ﬁll in the details.\n",
      "First we remove the current estimate ˜fn(θ)fromq(θ)by division using (10.205)\n",
      "to give q\\n(θ), which has mean and inverse variance given by Exercise 10.38\n",
      "m\\n=m+v\\nv−1\n",
      "n(m−mn) (10.214)\n",
      "(v\\n)−1=v−1−v−1\n",
      "n. (10.215)\n",
      "Next we evaluate the normalization constant Znusing (10.206) to give\n",
      "Zn=( 1−w)N(xn|m\\n,(v\\n+1 )I)+wN(xn|0,aI). (10.216)\n",
      "Similarly, we compute the mean and variance of qnew(θ)by ﬁnding the mean and\n",
      "variance of q\\n(θ)fn(θ)to give Exercise 10.39\n",
      "m=m\\n+ρnv\\n\n",
      "v\\n+1(xn−m\\n) (10.217)\n",
      "v=v\\n−ρn(v\\n)2\n",
      "v\\n+1+ρn(1−ρn)(v\\n)2∥xn−m\\n∥2\n",
      "D(v\\n+1 )2(10.218)\n",
      "where the quantity\n",
      "ρn=1−w\n",
      "ZnN(xn|0,aI) (10.219)\n",
      "has a simple interpretation as the probability of the point xnnot being clutter. Then\n",
      "we use (10.207) to compute the reﬁned factor ˜fn(θ)whose parameters are given by\n",
      "v−1\n",
      "n=(vnew)−1−(v\\n)−1(10.220)\n",
      "mn=m\\n+(vn+v\\n)(v\\n)−1(mnew−m\\n) (10.221)\n",
      "sn=Zn\n",
      "(2πvn)D/2N(mn|m\\n,(vn+v\\n)I). (10.222)\n",
      "This reﬁnement process is repeated until a suitable termination criterion is satisﬁed,\n",
      "for instance that the maximum change in parameter values resulting from a complete10.7. Expectation Propagation 513\n",
      "θ −5 0 5 10 θ −5 0 5 10\n",
      "Figure 10.16 Examples of the approximation of speciﬁc factors for a one-dimensional version of the clutter\n",
      "problem, showing fn(θ)in blue, efn(θ)in red, and q\\n(θ)in green. Notice that the current form for q\\n(θ)controls\n",
      "the range of θover which efn(θ)will be a good approximation to fn(θ).\n",
      "pass through all factors is less than some threshold. Finally, we use (10.208) to\n",
      "evaluate the approximation to the model evidence, given by\n",
      "p(D)≃(2πvnew)D/2exp(B/2)N∏\n",
      "n=1{\n",
      "sn(2πvn)−D/2}\n",
      "(10.223)\n",
      "where\n",
      "B=(mnew)Tmnew\n",
      "v−N∑\n",
      "n=1mT\n",
      "nmn\n",
      "vn. (10.224)\n",
      "Examples factor approximations for the clutter problem with a one-dimensional pa-\n",
      "rameter space θare shown in Figure 10.16. Note that the factor approximations can\n",
      "have inﬁnite or even negative values for the ‘variance’ parameter vn. This simply\n",
      "corresponds to approximations that curve upwards instead of downwards and are not\n",
      "necessarily problematic provided the overall approximate posterior q(θ)has posi-\n",
      "tive variance. Figure 10.17 compares the performance of EP with variational Bayes\n",
      "(mean ﬁeld theory) and the Laplace approximation on the clutter problem.\n",
      "10.7.2 Expectation propagation on graphs\n",
      "So far in our general discussion of EP, we have allowed the factors fi(θ)in the\n",
      "distribution p(θ)to be functions of all of the components of θ, and similarly for the\n",
      "approximating factors ˜f(θ)in the approximating distribution q(θ). We now consider\n",
      "situations in which the factors depend only on subsets of the variables. Such restric-\n",
      "tions can be conveniently expressed using the framework of probabilistic graphical\n",
      "models, as discussed in Chapter 8. Here we use a factor graph representation because\n",
      "this encompasses both directed and undirected graphs.514 10. APPROXIMATE INFERENCE\n",
      "epvb laplacePosterior mean\n",
      "FLOPSError\n",
      "10410610−5100\n",
      "epvb\n",
      "laplaceEvidence\n",
      "FLOPSError\n",
      "10410610−20410−20210−200\n",
      "Figure 10.17 Comparison of expectation propagation, variational inference, and the Laplace approximation on\n",
      "the clutter problem. The left-hand plot shows the error in the predicted posterior mean versus the number of\n",
      "ﬂoating point operations, and the right-hand plot shows the corresponding results for the model evidence.\n",
      "We shall focus on the case in which the approximating distribution is fully fac-\n",
      "torized, and we shall show that in this case expectation propagation reduces to loopy\n",
      "belief propagation (Minka, 2001a). To start with, we show this in the context of a\n",
      "simple example, and then we shall explore the general case.\n",
      "First of all, recall from (10.17) that if we minimize the Kullback-Leibler diver-\n",
      "gence KL(p∥q)with respect to a factorized distribution q, then the optimal solution\n",
      "for each factor is simply the corresponding marginal of p.\n",
      "Now consider the factor graph shown on the left in Figure 10.18, which was\n",
      "introduced earlier in the context of the sum-product algorithm. The joint distribution Section 8.4.4\n",
      "is given by\n",
      "p(x)=fa(x1,x2)fb(x2,x3)fc(x2,x4). (10.225)\n",
      "We seek an approximation q(x)that has the same factorization, so that\n",
      "q(x)∝˜fa(x1,x2)˜fb(x2,x3)˜fc(x2,x4). (10.226)\n",
      "Note that normalization constants have been omitted, and these can be re-instated at\n",
      "the end by local normalization, as is generally done in belief propagation. Now sup-\n",
      "pose we restrict attention to approximations in which the factors themselves factorize\n",
      "with respect to the individual variables so that\n",
      "q(x)∝˜fa1(x1)˜fa2(x2)˜fb2(x2)˜fb3(x3)˜fc2(x2)˜fc4(x4) (10.227)\n",
      "which corresponds to the factor graph shown on the right in Figure 10.18. Because\n",
      "the individual factors are factorized, the overall distribution q(x)is itself fully fac-\n",
      "torized.\n",
      "Now we apply the EP algorithm using the fully factorized approximation. Sup-\n",
      "pose that we have initialized all of the factors and that we choose to reﬁne factor10.7. Expectation Propagation 515\n",
      "x1 x2 x3\n",
      "x4fa fb\n",
      "fcx1 x2 x3\n",
      "x4˜fa1˜fa2˜fb2˜fb3\n",
      "˜fc2\n",
      "˜fc4\n",
      "Figure 10.18 On the left is a simple factor graph from Figure 8.51 and reproduced here for convenience. On\n",
      "the right is the corresponding factorized approximation.\n",
      "˜fb(x2,x3)=˜fb2(x2)˜fb3(x3). We ﬁrst remove this factor from the approximating\n",
      "distribution to give\n",
      "q\\b(x)=˜fa1(x1)˜fa2(x2)˜fc2(x2)˜fc4(x4) (10.228)\n",
      "and we then multiply this by the exact factor fb(x2,x3)to give\n",
      "ˆp(x)=q\\b(x)fb(x2,x3)=˜fa1(x1)˜fa2(x2)˜fc2(x2)˜fc4(x4)fb(x2,x3).(10.229)\n",
      "We now ﬁnd qnew(x)by minimizing the Kullback-Leibler divergence KL(ˆp∥qnew).\n",
      "The result, as noted above, is that qnew(z)comprises the product of factors, one for\n",
      "each variable xi, in which each factor is given by the corresponding marginal of\n",
      "ˆp(x). These four marginals are given by\n",
      "ˆp(x1)∝˜fa1(x1) (10.230)\n",
      "ˆp(x2)∝˜fa2(x2)˜fc2(x2)∑\n",
      "x3fb(x2,x3) (10.231)\n",
      "ˆp(x3)∝∑\n",
      "x2{\n",
      "fb(x2,x3)˜fa2(x2)˜fc2(x2)}\n",
      "(10.232)\n",
      "ˆp(x4)∝˜fc4(x4) (10.233)\n",
      "andqnew(x)is obtained by multiplying these marginals together. We see that the\n",
      "only factors in q(x)that change when we update ˜fb(x2,x3)are those that involve\n",
      "the variables in fbnamely x2andx3. To obtain the reﬁned factor ˜fb(x2,x3)=\n",
      "˜fb2(x2)˜fb3(x3)we simply divide qnew(x)byq\\b(x), which gives\n",
      "˜fb2(x2)∝∑\n",
      "x3fb(x2,x3) (10.234)\n",
      "˜fb3(x3)∝∑\n",
      "x2{\n",
      "fb(x2,x3)˜fa2(x2)˜fc2(x2)}\n",
      ". (10.235)516 10. APPROXIMATE INFERENCE\n",
      "These are precisely the messages obtained using belief propagation in which mes- Section 8.4.4\n",
      "sages from variable nodes to factor nodes have been folded into the messages from\n",
      "factor nodes to variable nodes. In particular, ˜fb2(x2)corresponds to the message\n",
      "µfb→x2(x2)sent by factor node fbto variable node x2and is given by (8.81). Simi-\n",
      "larly, if we substitute (8.78) into (8.79), we obtain (10.235) in which ˜fa2(x2)corre-\n",
      "sponds to µfa→x2(x2)and˜fc2(x2)corresponds to µfc→x2(x2), giving the message\n",
      "˜fb3(x3)which corresponds to µfb→x3(x3).\n",
      "This result differs slightly from standard belief propagation in that messages are\n",
      "passed in both directions at the same time. We can easily modify the EP procedureto give the standard form of the sum-product algorithm by updating just one of the\n",
      "factors at a time, for instance if we reﬁne only˜fb3(x3), then˜fb2(x2)is unchanged\n",
      "by deﬁnition, while the reﬁned version of ˜fb3(x3)is again given by (10.235). If\n",
      "we are reﬁning only one term at a time, then we can choose the order in which the\n",
      "reﬁnements are done as we wish. In particular, for a tree-structured graph we canfollow a two-pass update scheme, corresponding to the standard belief propagation\n",
      "schedule, which will result in exact inference of the variable and factor marginals.\n",
      "The initialization of the approximation factors in this case is unimportant.\n",
      "Now let us consider a general factor graph corresponding to the distribution\n",
      "p(θ)=∏\n",
      "ifi(θi) (10.236)\n",
      "where θirepresents the subset of variables associated with factor fi. We approximate\n",
      "this using a fully factorized distribution of the form\n",
      "q(θ)∝∏\n",
      "i∏\n",
      "k˜fik(θk) (10.237)\n",
      "where θkcorresponds to an individual variable node. Suppose that we wish to reﬁne\n",
      "the particular term ˜fjl(θl)keeping all other terms ﬁxed. We ﬁrst remove the term\n",
      "˜fj(θj)fromq(θ)to give\n",
      "q\\j(θ)∝∏\n",
      "i̸=j∏\n",
      "k˜fik(θk) (10.238)\n",
      "and then multiply by the exact factor fj(θj). To determine the reﬁned term ˜fjl(θl),\n",
      "we need only consider the functional dependence on θl, and so we simply ﬁnd the\n",
      "corresponding marginal of\n",
      "q\\j(θ)fj(θj). (10.239)\n",
      "Up to a multiplicative constant, this involves taking the marginal of fj(θj)multiplied\n",
      "by any terms from q\\j(θ)that are functions of any of the variables in θj. Terms that\n",
      "correspond to other factors ˜fi(θi)fori̸=jwill cancel between numerator and\n",
      "denominator when we subsequently divide by q\\j(θ). We therefore obtain\n",
      "˜fjl(θl)∝∑\n",
      "θm̸=l∈θjfj(θj)∏\n",
      "k∏\n",
      "m̸=l˜fkm(θm). (10.240)Exercises 517\n",
      "We recognize this as the sum-product rule in the form in which messages from vari-\n",
      "able nodes to factor nodes have been eliminated, as illustrated by the example shown\n",
      "in Figure 8.50. The quantity ˜fjm(θm)corresponds to the message µfj→θm(θm),\n",
      "which factor node jsends to variable node m, and the product over kin (10.240)\n",
      "is over all factors that depend on the variables θmthat have variables (other than\n",
      "variable θl) in common with factor fj(θj). In other words, to compute the outgoing\n",
      "message from a factor node, we take the product of all the incoming messages from\n",
      "other factor nodes, multiply by the local factor, and then marginalize.\n",
      "Thus, the sum-product algorithm arises as a special case of expectation propa-\n",
      "gation if we use an approximating distribution that is fully factorized. This suggests\n",
      "that more ﬂexible approximating distributions, corresponding to partially discon-\n",
      "nected graphs, could be used to achieve higher accuracy. Another generalization is\n",
      "to group factors fi(θi)together into sets and to reﬁne all the factors in a set together\n",
      "at each iteration. Both of these approaches can lead to improvements in accuracy(Minka, 2001b). In general, the problem of choosing the best combination of group-\n",
      "ing and disconnection is an open research issue.\n",
      "We have seen that variational message passing and expectation propagation op-\n",
      "timize two different forms of the Kullback-Leibler divergence. Minka (2005) has\n",
      "shown that a broad range of message passing algorithms can be derived from a com-\n",
      "mon framework involving minimization of members of the alpha family of diver-gences, given by (10.19). These include variational message passing, loopy belief\n",
      "propagation, and expectation propagation, as well as a range of other algorithms,\n",
      "which we do not have space to discuss here, such as tree-reweighted message pass-\n",
      "ing(Wainwright et al. , 2005), fractional belief propagation (Wiegerinck and Heskes,\n",
      "2003), and power EP (Minka, 2004).\n",
      "Exercises\n",
      "10.1 (⋆)www Verify that the log marginal distribution of the observed data lnp(X)\n",
      "can be decomposed into two terms in the form (10.2) where L(q)is given by (10.3)\n",
      "andKL(q∥p)is given by (10.4).\n",
      "10.2 (⋆)Use the properties E[z1]=m1and E[z2]=m2to solve the simultaneous equa-\n",
      "tions (10.13) and (10.15), and hence show that, provided the original distributionp(z)is nonsingular, the unique solution for the means of the factors in the approxi-\n",
      "mation distribution is given by E[z\n",
      "1]=µ1and E[z2]=µ2.\n",
      "10.3 (⋆⋆)www Consider a factorized variational distribution q(Z)of the form (10.5).\n",
      "By using the technique of Lagrange multipliers, verify that minimization of theKullback-Leibler divergence KL(p∥q)with respect to one of the factors q\n",
      "i(Zi),\n",
      "keeping all other factors ﬁxed, leads to the solution (10.17).\n",
      "10.4 (⋆⋆)Suppose that p(x)is some ﬁxed distribution and that we wish to approximate\n",
      "it using a Gaussian distribution q(x)=N(x|µ,Σ). By writing down the form of\n",
      "the KL divergence KL(p∥q)for a Gaussian q(x)and then differentiating, show that518 10. APPROXIMATE INFERENCE\n",
      "minimization of KL(p∥q)with respect to µandΣleads to the result that µis given\n",
      "by the expectation of xunder p(x)and that Σis given by the covariance.\n",
      "10.5 (⋆⋆)www Consider a model in which the set of all hidden stochastic variables, de-\n",
      "noted collectively by Z, comprises some latent variables ztogether with some model\n",
      "parameters θ. Suppose we use a variational distribution that factorizes between la-\n",
      "tent variables and parameters so that q(z,θ)=qz(z)qθ(θ), in which the distribution\n",
      "qθ(θ)is approximated by a point estimate of the form qθ(θ)=δ(θ−θ0)where θ0\n",
      "is a vector of free parameters. Show that variational optimization of this factorized\n",
      "distribution is equivalent to an EM algorithm, in which the E step optimizes qz(z),\n",
      "and the M step maximizes the expected complete-data log posterior distribution of θ\n",
      "with respect to θ0.\n",
      "10.6 (⋆⋆)The alpha family of divergences is deﬁned by (10.19). Show that the Kullback-\n",
      "Leibler divergence KL(p∥q)corresponds to α→1. This can be done by writing\n",
      "pϵ= exp( ϵlnp)=1+ ϵlnp+O(ϵ2)and then taking ϵ→0. Similarly show that\n",
      "KL(q∥p)corresponds to α→−1.\n",
      "10.7 (⋆⋆)Consider the problem of inferring the mean and precision of a univariate Gaus-\n",
      "sian using a factorized variational approximation, as considered in Section 10.1.3.Show that the factor q\n",
      "µ(µ)is a Gaussian of the form N(µ|µN,λ−1\n",
      "N)with mean and\n",
      "precision given by (10.26) and (10.27), respectively. Similarly show that the factor\n",
      "qτ(τ)is a gamma distribution of the form Gam( τ|aN,bN)with parameters given by\n",
      "(10.29) and (10.30).\n",
      "10.8 (⋆)Consider the variational posterior distribution for the precision of a univariate\n",
      "Gaussian whose parameters are given by (10.29) and (10.30). By using the standard\n",
      "results for the mean and variance of the gamma distribution given by (B.27) and(B.28), show that if we let N→∞ , this variational posterior distribution has a\n",
      "mean given by the inverse of the maximum likelihood estimator for the variance of\n",
      "the data, and a variance that goes to zero.\n",
      "10.9 (⋆⋆)By making use of the standard result E[τ]=a\n",
      "N/bNfor the mean of a gamma\n",
      "distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result\n",
      "(10.33) for the reciprocal of the expected precision in the factorized variational treat-\n",
      "ment of a univariate Gaussian.\n",
      "10.10 (⋆)www Derive the decomposition given by (10.34) that is used to ﬁnd approxi-\n",
      "mate posterior distributions over models using variational inference.\n",
      "10.11 (⋆⋆)www By using a Lagrange multiplier to enforce the normalization constraint\n",
      "on the distribution q(m), show that the maximum of the lower bound (10.35) is given\n",
      "by (10.36).\n",
      "10.12 (⋆⋆)Starting from the joint distribution (10.41), and applying the general result\n",
      "(10.9), show that the optimal variational distribution q⋆(Z)over the latent variables\n",
      "for the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps\n",
      "given in the text.Exercises 519\n",
      "10.13 (⋆⋆)www Starting from (10.54), derive the result (10.59) for the optimum vari-\n",
      "ational posterior distribution over µkandΛkin the Bayesian mixture of Gaussians,\n",
      "and hence verify the expressions for the parameters of this distribution given by\n",
      "(10.60)–(10.63).\n",
      "10.14 (⋆⋆)Using the distribution (10.59), verify the result (10.64).\n",
      "10.15 (⋆)Using the result (B.17), show that the expected value of the mixing coefﬁcients\n",
      "in the variational mixture of Gaussians is given by (10.69).\n",
      "10.16 (⋆⋆)www Verify the results (10.71) and (10.72) for the ﬁrst two terms in the\n",
      "lower bound for the variational Gaussian mixture model given by (10.70).\n",
      "10.17 (⋆⋆⋆ )Verify the results (10.73)–(10.77) for the remaining terms in the lower bound\n",
      "for the variational Gaussian mixture model given by (10.70).\n",
      "10.18 (⋆⋆⋆ )In this exercise, we shall derive the variational re-estimation equations for\n",
      "the Gaussian mixture model by direct differentiation of the lower bound. To do this\n",
      "we assume that the variational distribution has the factorization deﬁned by (10.42)and (10.55) with factors given by (10.48), (10.57), and (10.59). Substitute these into\n",
      "(10.70) and hence obtain the lower bound as a function of the parameters of the varia-\n",
      "tional distribution. Then, by maximizing the bound with respect to these parameters,derive the re-estimation equations for the factors in the variational distribution, and\n",
      "show that these are the same as those obtained in Section 10.2.1.\n",
      "10.19 (⋆⋆)Derive the result (10.81) for the predictive distribution in the variational treat-\n",
      "ment of the Bayesian mixture of Gaussians model.\n",
      "10.20 (⋆⋆)\n",
      "www This exercise explores the variational Bayes solution for the mixture of\n",
      "Gaussians model when the size Nof the data set is large and shows that it reduces (as\n",
      "we would expect) to the maximum likelihood solution based on EM derived in Chap-\n",
      "ter 9. Note that results from Appendix B may be used to help answer this exercise.First show that the posterior distribution q\n",
      "⋆(Λk)of the precisions becomes sharply\n",
      "peaked around the maximum likelihood solution. Do the same for the posterior dis-\n",
      "tribution of the means q⋆(µk|Λk). Next consider the posterior distribution q⋆(π)\n",
      "for the mixing coefﬁcients and show that this too becomes sharply peaked around\n",
      "the maximum likelihood solution. Similarly, show that the responsibilities become\n",
      "equal to the corresponding maximum likelihood values for large N, by making use\n",
      "of the following asymptotic result for the digamma function for large x\n",
      "ψ(x)=l n x+O(1/x). (10.241)\n",
      "Finally, by making use of (10.80), show that for large N, the predictive distribution\n",
      "becomes a mixture of Gaussians.\n",
      "10.21 (⋆)Show that the number of equivalent parameter settings due to interchange sym-\n",
      "metries in a mixture model with Kcomponents is K!.520 10. APPROXIMATE INFERENCE\n",
      "10.22 (⋆⋆)We have seen that each mode of the posterior distribution in a Gaussian mix-\n",
      "ture model is a member of a family of K!equivalent modes. Suppose that the result\n",
      "of running the variational inference algorithm is an approximate posterior distribu-\n",
      "tionqthat is localized in the neighbourhood of one of the modes. We can then\n",
      "approximate the full posterior distribution as a mixture of K!suchqdistributions,\n",
      "once centred on each mode and having equal mixing coefﬁcients. Show that if we\n",
      "assume negligible overlap between the components of the qmixture, the resulting\n",
      "lower bound differs from that for a single component qdistribution through the ad-\n",
      "dition of an extra term lnK!.\n",
      "10.23 (⋆⋆)www Consider a variational Gaussian mixture model in which there is no\n",
      "prior distribution over mixing coefﬁcients {πk}. Instead, the mixing coefﬁcients are\n",
      "treated as parameters, whose values are to be found by maximizing the variationallower bound on the log marginal likelihood. Show that maximizing this lower bound\n",
      "with respect to the mixing coefﬁcients, using a Lagrange multiplier to enforce the\n",
      "constraint that the mixing coefﬁcients sum to one, leads to the re-estimation result\n",
      "(10.83). Note that there is no need to consider all of the terms in the lower bound but\n",
      "only the dependence of the bound on the {π\n",
      "k}.\n",
      "10.24 (⋆⋆)www We have seen in Section 10.2 that the singularities arising in the max-\n",
      "imum likelihood treatment of Gaussian mixture models do not arise in a Bayesiantreatment. Discuss whether such singularities would arise if the Bayesian model\n",
      "were solved using maximum posterior (MAP) estimation.\n",
      "10.25 (⋆⋆)The variational treatment of the Bayesian mixture of Gaussians, discussed in\n",
      "Section 10.2, made use of a factorized approximation (10.5) to the posterior distribu-tion. As we saw in Figure 10.2, the factorized assumption causes the variance of the\n",
      "posterior distribution to be under-estimated for certain directions in parameter space.\n",
      "Discuss qualitatively the effect this will have on the variational approximation to the\n",
      "model evidence, and how this effect will vary with the number of components in\n",
      "the mixture. Hence explain whether the variational Gaussian mixture will tend tounder-estimate or over-estimate the optimal number of components.\n",
      "10.26 (⋆⋆⋆ )Extend the variational treatment of Bayesian linear regression to include\n",
      "a gamma hyperprior Gam( β|c\n",
      "0,d0)overβand solve variationally, by assuming a\n",
      "factorized variational distribution of the form q(w)q(α)q(β). Derive the variational\n",
      "update equations for the three factors in the variational distribution and also obtain\n",
      "an expression for the lower bound and for the predictive distribution.\n",
      "10.27 (⋆⋆)By making use of the formulae given in Appendix B show that the variational\n",
      "lower bound for the linear basis function regression model, deﬁned by (10.107), can\n",
      "be written in the form (10.107) with the various terms deﬁned by (10.108)–(10.112).\n",
      "10.28 (⋆⋆⋆ )Rewrite the model for the Bayesian mixture of Gaussians, introduced in\n",
      "Section 10.2, as a conjugate model from the exponential family, as discussed in\n",
      "Section 10.4. Hence use the general results (10.115) and (10.119) to derive the\n",
      "speciﬁc results (10.48), (10.57), and (10.59).Exercises 521\n",
      "10.29 (⋆)www Show that the function f(x)=l n ( x)is concave for 0<x< ∞\n",
      "by computing its second derivative. Determine the form of the dual function g(λ)\n",
      "deﬁned by (10.133), and verify that minimization of λx−g(λ)with respect to λ\n",
      "according to (10.132) indeed recovers the function ln(x).\n",
      "10.30 (⋆)By evaluating the second derivative, show that the log logistic function f(x)=\n",
      "−ln(1 + e−x)is concave. Derive the variational upper bound (10.137) directly by\n",
      "making a second order Taylor expansion of the log logistic function around a point\n",
      "x=ξ.\n",
      "10.31 (⋆⋆)By ﬁnding the second derivative with respect to x, show that the function\n",
      "f(x)=−ln(ex/2+e−x/2)is a concave function of x. Now consider the second\n",
      "derivatives with respect to the variable x2and hence show that it is a convex function\n",
      "ofx2. Plot graphs of f(x)against xand against x2. Derive the lower bound (10.144)\n",
      "on the logistic sigmoid function directly by making a ﬁrst order Taylor series expan-sion of the function f(x)in the variable x\n",
      "2centred on the value ξ2.\n",
      "10.32 (⋆⋆)www Consider the variational treatment of logistic regression with sequen-\n",
      "tial learning in which data points are arriving one at a time and each must be pro-\n",
      "cessed and discarded before the next data point arrives. Show that a Gaussian ap-proximation to the posterior distribution can be maintained through the use of the\n",
      "lower bound (10.151), in which the distribution is initialized using the prior, and as\n",
      "each data point is absorbed its corresponding variational parameter ξ\n",
      "nis optimized.\n",
      "10.33 (⋆)By differentiating the quantity Q(ξ,ξold)deﬁned by (10.161) with respect to\n",
      "the variational parameter ξnshow that the update equation for ξnfor the Bayesian\n",
      "logistic regression model is given by (10.163).\n",
      "10.34 (⋆⋆)In this exercise we derive re-estimation equations for the variational parame-\n",
      "tersξin the Bayesian logistic regression model of Section 4.5 by direct maximization\n",
      "of the lower bound given by (10.164). To do this set the derivative of L(ξ)with re-\n",
      "spect to ξnequal to zero, making use of the result (3.117) for the derivative of the log\n",
      "of a determinant, together with the expressions (10.157) and (10.158) which deﬁnethe mean and covariance of the variational posterior distribution q(w).\n",
      "10.35 (⋆⋆)Derive the result (10.164) for the lower bound L(ξ)in the variational logistic\n",
      "regression model. This is most easily done by substituting the expressions for the\n",
      "Gaussian prior q(w)=N(w|m\n",
      "0,S0), together with the lower bound h(w,ξ)on\n",
      "the likelihood function, into the integral (10.159) which deﬁnes L(ξ). Next gather\n",
      "together the terms which depend on win the exponential and complete the square\n",
      "to give a Gaussian integral, which can then be evaluated by invoking the standardresult for the normalization coefﬁcient of a multivariate Gaussian. Finally take the\n",
      "logarithm to obtain (10.164).\n",
      "10.36 (⋆⋆)Consider the ADF approximation scheme discussed in Section 10.7, and show\n",
      "that inclusion of the factor f\n",
      "j(θ)leads to an update of the model evidence of the\n",
      "form\n",
      "pj(D)≃pj−1(D)Zj (10.242)522 10. APPROXIMATE INFERENCE\n",
      "where Zjis the normalization constant deﬁned by (10.197). By applying this result\n",
      "recursively, and initializing with p0(D)=1 , derive the result\n",
      "p(D)≃∏\n",
      "jZj. (10.243)\n",
      "10.37 (⋆)www Consider the expectation propagation algorithm from Section 10.7, and\n",
      "suppose that one of the factors f0(θ)in the deﬁnition (10.188) has the same expo-\n",
      "nential family functional form as the approximating distribution q(θ). Show that if\n",
      "the factor˜f0(θ)is initialized to be f0(θ), then an EP update to reﬁne ˜f0(θ)leaves\n",
      "˜f0(θ)unchanged. This situation typically arises when one of the factors is the prior\n",
      "p(θ), and so we see that the prior factor can be incorporated once exactly and does\n",
      "not need to be reﬁned.\n",
      "10.38 (⋆⋆⋆ )In this exercise and the next, we shall verify the results (10.214)–(10.224)\n",
      "for the expectation propagation algorithm applied to the clutter problem. Begin byusing the division formula (10.205) to derive the expressions (10.214) and (10.215)\n",
      "by completing the square inside the exponential to identify the mean and variance.\n",
      "Also, show that the normalization constant Z\n",
      "n, deﬁned by (10.206), is given for the\n",
      "clutter problem by (10.216). This can be done by making use of the general result\n",
      "(2.115).\n",
      "10.39 (⋆⋆⋆ )Show that the mean and variance of qnew(θ)for EP applied to the clutter\n",
      "problem are given by (10.217) and (10.218). To do this, ﬁrst prove the followingresults for the expectations of θandθθ\n",
      "Tunder qnew(θ)\n",
      "E[θ]= m\\n+v\\n∇m\\nlnZn (10.244)\n",
      "E[θTθ]=2 ( v\\n)2∇v\\nlnZn+2 E[θ]Tm\\n−∥m\\n∥2(10.245)\n",
      "and then make use of the result (10.216) for Zn. Next, prove the results (10.220)–\n",
      "(10.222) by using (10.207) and completing the square in the exponential. Finally,\n",
      "use (10.208) to derive the result (10.223).11\n",
      "Sampling\n",
      "Methods\n",
      "For most probabilistic models of practical interest, exact inference is intractable, and\n",
      "so we have to resort to some form of approximation. In Chapter 10, we discussedinference algorithms based on deterministic approximations, which include methods\n",
      "such as variational Bayes and expectation propagation. Here we consider approxi-\n",
      "mate inference methods based on numerical sampling, also known as Monte Carlo\n",
      "techniques.\n",
      "Although for some applications the posterior distribution over unobserved vari-\n",
      "ables will be of direct interest in itself, for most situations the posterior distributionis required primarily for the purpose of evaluating expectations, for example in order\n",
      "to make predictions. The fundamental problem that we therefore wish to address in\n",
      "this chapter involves ﬁnding the expectation of some function f(z)with respect to a\n",
      "probability distribution p(z). Here, the components of zmight comprise discrete or\n",
      "continuous variables or some combination of the two. Thus in the case of continuous\n",
      "523524 11. SAMPLING METHODS\n",
      "Figure 11.1 Schematic illustration of a function f(z)\n",
      "whose expectation is to be evaluated with\n",
      "respect to a distribution p(z).p(z)f(z)\n",
      "z\n",
      "variables, we wish to evaluate the expectation\n",
      "E[f]=∫\n",
      "f(z)p(z)dz (11.1)\n",
      "where the integral is replaced by summation in the case of discrete variables. This\n",
      "is illustrated schematically for a single continuous variable in Figure 11.1. We shall\n",
      "suppose that such expectations are too complex to be evaluated exactly using analyt-\n",
      "ical techniques.\n",
      "The general idea behind sampling methods is to obtain a set of samples z(l)\n",
      "(where l=1,...,L ) drawn independently from the distribution p(z). This allows\n",
      "the expectation (11.1) to be approximated by a ﬁnite sum\n",
      "ˆf=1\n",
      "LL∑\n",
      "l=1f(z(l)). (11.2)\n",
      "As long as the samples z(l)are drawn from the distribution p(z), then E[ˆf]= E[f]\n",
      "and so the estimator ˆfhas the correct mean. The variance of the estimator is given\n",
      "by Exercise 11.1\n",
      "var[ˆf]=1\n",
      "LE[\n",
      "(f−E[f])2]\n",
      "(11.3)\n",
      "is the variance of the function f(z)under the distribution p(z). It is worth emphasiz-\n",
      "ing that the accuracy of the estimator therefore does not depend on the dimension-\n",
      "ality of z, and that, in principle, high accuracy may be achievable with a relatively\n",
      "small number of samples z(l). In practice, ten or twenty independent samples may\n",
      "sufﬁce to estimate an expectation to sufﬁcient accuracy.\n",
      "The problem, however, is that the samples {z(l)}might not be independent, and\n",
      "so the effective sample size might be much smaller than the apparent sample size.\n",
      "Also, referring back to Figure 11.1, we note that if f(z)is small in regions where\n",
      "p(z)is large, and vice versa, then the expectation may be dominated by regions\n",
      "of small probability, implying that relatively large sample sizes will be required to\n",
      "achieve sufﬁcient accuracy.\n",
      "For many models, the joint distribution p(z)is conveniently speciﬁed in terms\n",
      "of a graphical model. In the case of a directed graph with no observed variables, it is11. SAMPLING METHODS 525\n",
      "straightforward to sample from the joint distribution (assuming that it is possible to\n",
      "sample from the conditional distributions at each node) using the following ances-\n",
      "tral sampling approach, discussed brieﬂy in Section 8.1.2. The joint distribution is\n",
      "speciﬁed by\n",
      "p(z)=M∏\n",
      "i=1p(zi|pai) (11.4)\n",
      "whereziare the set of variables associated with node i, andpaidenotes the set of\n",
      "variables associated with the parents of node i. To obtain a sample from the joint\n",
      "distribution, we make one pass through the set of variables in the order z1,...,zM\n",
      "sampling from the conditional distributions p(zi|pai). This is always possible be-\n",
      "cause at each step all of the parent values will have been instantiated. After one pass\n",
      "through the graph, we will have obtained a sample from the joint distribution.\n",
      "Now consider the case of a directed graph in which some of the nodes are in-\n",
      "stantiated with observed values. We can in principle extend the above procedure, at\n",
      "least in the case of nodes representing discrete variables, to give the following logic\n",
      "sampling approach (Henrion, 1988), which can be seen as a special case of impor-\n",
      "tance sampling discussed in Section 11.1.4. At each step, when a sampled value is\n",
      "obtained for a variable ziwhose value is observed, the sampled value is compared\n",
      "to the observed value, and if they agree then the sample value is retained and the al-\n",
      "gorithm proceeds to the next variable in turn. However, if the sampled value and the\n",
      "observed value disagree, then the whole sample so far is discarded and the algorithmstarts again with the ﬁrst node in the graph. This algorithm samples correctly from\n",
      "the posterior distribution because it corresponds simply to drawing samples from the\n",
      "joint distribution of hidden variables and data variables and then discarding those\n",
      "samples that disagree with the observed data (with the slight saving of not continu-\n",
      "ing with the sampling from the joint distribution as soon as one contradictory value isobserved). However, the overall probability of accepting a sample from the posterior\n",
      "decreases rapidly as the number of observed variables increases and as the number\n",
      "of states that those variables can take increases, and so this approach is rarely usedin practice.\n",
      "In the case of probability distributions deﬁned by an undirected graph, there is\n",
      "no one-pass sampling strategy that will sample even from the prior distribution withno observed variables. Instead, computationally more expensive techniques must be\n",
      "employed, such as Gibbs sampling, which is discussed in Section 11.3.\n",
      "As well as sampling from conditional distributions, we may also require samples\n",
      "from a marginal distribution. If we already have a strategy for sampling from a joint\n",
      "distribution p(u,v), then it is straightforward to obtain samples from the marginal\n",
      "distribution p(u)simply by ignoring the values for vin each sample.\n",
      "There are numerous texts dealing with Monte Carlo methods. Those of partic-\n",
      "ular interest from the statistical inference perspective include Chen et al. (2001),\n",
      "Gamerman (1997), Gilks et al. (1996), Liu (2001), Neal (1996), and Robert and\n",
      "Casella (1999). Also there are review articles by Besag et al. (1995), Brooks (1998),\n",
      "Diaconis and Saloff-Coste (1998), Jerrum and Sinclair (1996), Neal (1993), Tierney(1994), and Andrieu et al. (2003) that provide additional information on sampling526 11. SAMPLING METHODS\n",
      "methods for statistical inference.\n",
      "Diagnostic tests for convergence of Markov chain Monte Carlo algorithms are\n",
      "summarized in Robert and Casella (1999), and some practical guidance on the use of\n",
      "sampling methods in the context of machine learning is given in Bishop and Nabney\n",
      "(2008).\n",
      "11.1. Basic Sampling Algorithms\n",
      "In this section, we consider some simple strategies for generating random samplesfrom a given distribution. Because the samples will be generated by a computer\n",
      "algorithm they will in fact be pseudo-random numbers, that is, they will be deter-\n",
      "ministically calculated, but must nevertheless pass appropriate tests for randomness.\n",
      "Generating such numbers raises several subtleties (Press et al. , 1992) that lie outside\n",
      "the scope of this book. Here we shall assume that an algorithm has been providedthat generates pseudo-random numbers distributed uniformly over (0,1), and indeed\n",
      "most software environments have such a facility built in.\n",
      "11.1.1 Standard distributions\n",
      "We ﬁrst consider how to generate random numbers from simple nonuniform dis-\n",
      "tributions, assuming that we already have available a source of uniformly distributed\n",
      "random numbers. Suppose that zis uniformly distributed over the interval (0,1),\n",
      "and that we transform the values of zusing some function f(·)so that y=f(z).\n",
      "The distribution of ywill be governed by\n",
      "p(y)=p(z)⏐⏐⏐⏐dz\n",
      "dy⏐⏐⏐⏐ (11.5)\n",
      "where, in this case, p(z)=1 . Our goal is to choose the function f(z)such that the\n",
      "resulting values of yhave some speciﬁc desired distribution p(y). Integrating (11.5)\n",
      "we obtain\n",
      "z=h(y)≡∫y\n",
      "−∞p(ˆy)dˆy (11.6)\n",
      "which is the indeﬁnite integral of p(y). Thus, y=h−1(z), and so we have to Exercise 11.2\n",
      "transform the uniformly distributed random numbers using a function which is the\n",
      "inverse of the indeﬁnite integral of the desired distribution. This is illustrated in\n",
      "Figure 11.2.\n",
      "Consider for example the exponential distribution\n",
      "p(y)=λexp(−λy) (11.7)\n",
      "where 0⩽y<∞. In this case the lower limit of the integral in (11.6) is 0, and so\n",
      "h(y)=1−exp(−λy). Thus, if we transform our uniformly distributed variable z\n",
      "usingy=−λ−1ln(1−z), thenywill have an exponential distribution.11.1. Basic Sampling Algorithms 527\n",
      "Figure 11.2 Geometrical interpretation of the trans-\n",
      "formation method for generating nonuni-\n",
      "formly distributed random numbers. h(y)\n",
      "is the indeﬁnite integral of the desired dis-\n",
      "tribution p(y). If a uniformly distributed\n",
      "random variable zis transformed using\n",
      "y=h−1(z), then ywill be distributed ac-\n",
      "cording to p(y).p(y)h(y)\n",
      "y01\n",
      "Another example of a distribution to which the transformation method can be\n",
      "applied is given by the Cauchy distribution\n",
      "p(y)=1\n",
      "π1\n",
      "1+y2. (11.8)\n",
      "In this case, the inverse of the indeﬁnite integral can be expressed in terms of the\n",
      "‘tan’ function. Exercise 11.3\n",
      "The generalization to multiple variables is straightforward and involves the Ja-\n",
      "cobian of the change of variables, so that\n",
      "p(y1,...,y M)=p(z1,...,z M)⏐⏐⏐⏐∂(z1,...,z M)\n",
      "∂(y1,...,y M)⏐⏐⏐⏐. (11.9)\n",
      "As a ﬁnal example of the transformation method we consider the Box-Muller\n",
      "method for generating samples from a Gaussian distribution. First, suppose we gen-\n",
      "erate pairs of uniformly distributed random numbers z1,z2∈(−1,1), which we can\n",
      "do by transforming a variable distributed uniformly over (0,1)usingz→2z−1.\n",
      "Next we discard each pair unless it satisﬁes z2\n",
      "1+z2\n",
      "2⩽1. This leads to a uniform\n",
      "distribution of points inside the unit circle with p(z1,z2)=1 /π, as illustrated in\n",
      "Figure 11.3. Then, for each pair z1,z2we evaluate the quantities\n",
      "Figure 11.3 The Box-Muller method for generating Gaussian dis-\n",
      "tributed random numbers starts by generating samples\n",
      "from a uniform distribution inside the unit circle.\n",
      "−1−11\n",
      "1 z1z2528 11. SAMPLING METHODS\n",
      "y1=z1(−2l nz1\n",
      "r2)1/2\n",
      "(11.10)\n",
      "y2=z2(−2l nz2\n",
      "r2)1/2\n",
      "(11.11)\n",
      "where r2=z2\n",
      "1+z2\n",
      "2. Then the joint distribution of y1andy2is given by Exercise 11.4\n",
      "p(y1,y2)= p(z1,z2)⏐⏐⏐⏐∂(z1,z2)\n",
      "∂(y1,y2)⏐⏐⏐⏐\n",
      "=[1√\n",
      "2πexp(−y2\n",
      "1/2)][1√\n",
      "2πexp(−y2\n",
      "2/2)]\n",
      "(11.12)\n",
      "and so y1andy2are independent and each has a Gaussian distribution with zero\n",
      "mean and unit variance.\n",
      "Ifyhas a Gaussian distribution with zero mean and unit variance, then σy+µ\n",
      "will have a Gaussian distribution with mean µand variance σ2. To generate vector-\n",
      "valued variables having a multivariate Gaussian distribution with mean µand co-\n",
      "variance Σ, we can make use of the Cholesky decomposition , which takes the form\n",
      "Σ=LLT(Press et al. , 1992). Then, if zis a vector valued random variable whose\n",
      "components are independent and Gaussian distributed with zero mean and unit vari-\n",
      "ance, then y=µ+Lzwill have mean µand covariance Σ. Exercise 11.5\n",
      "Obviously, the transformation technique depends for its success on the ability\n",
      "to calculate and then invert the indeﬁnite integral of the required distribution. Such\n",
      "operations will only be feasible for a limited number of simple distributions, and sowe must turn to alternative approaches in search of a more general strategy. Here\n",
      "we consider two techniques called rejection sampling and importance sampling .A l -\n",
      "though mainly limited to univariate distributions and thus not directly applicable to\n",
      "complex problems in many dimensions, they do form important components in more\n",
      "general strategies.\n",
      "11.1.2 Rejection sampling\n",
      "The rejection sampling framework allows us to sample from relatively complex\n",
      "distributions, subject to certain constraints. We begin by considering univariate dis-tributions and discuss the extension to multiple dimensions subsequently.\n",
      "Suppose we wish to sample from a distribution p(z)that is not one of the simple,\n",
      "standard distributions considered so far, and that sampling directly from p(z)is dif-\n",
      "ﬁcult. Furthermore suppose, as is often the case, that we are easily able to evaluate\n",
      "p(z)for any given value of z, up to some normalizing constant Z, so that\n",
      "p(z)=1\n",
      "Zp˜p(z) (11.13)\n",
      "where˜p(z)can readily be evaluated, but Zpis unknown.\n",
      "In order to apply rejection sampling, we need some simpler distribution q(z),\n",
      "sometimes called a proposal distribution , from which we can readily draw samples.11.1. Basic Sampling Algorithms 529\n",
      "Figure 11.4 In the rejection sampling method,\n",
      "samples are drawn from a sim-\n",
      "ple distribution q(z)and rejected\n",
      "if they fall in the grey area be-\n",
      "tween the unnormalized distribu-\n",
      "tion ep(z)and the scaled distribu-\n",
      "tionkq(z). The resulting samples\n",
      "are distributed according to p(z),\n",
      "which is the normalized version of\n",
      "ep(z). z0 zu0kq(z0)kq(z)\n",
      "˜p(z)\n",
      "We next introduce a constant kwhose value is chosen such that kq(z)⩾˜p(z)for\n",
      "all values of z. The function kq(z)is called the comparison function and is illus-\n",
      "trated for a univariate distribution in Figure 11.4. Each step of the rejection sampler\n",
      "involves generating two random numbers. First, we generate a number z0from the\n",
      "distribution q(z). Next, we generate a number u0from the uniform distribution over\n",
      "[0,kq(z0)]. This pair of random numbers has uniform distribution under the curve\n",
      "of the function kq(z). Finally, if u0>˜p(z0)then the sample is rejected, otherwise\n",
      "u0is retained. Thus the pair is rejected if it lies in the grey shaded region in Fig-\n",
      "ure 11.4. The remaining pairs then have uniform distribution under the curve of ˜p(z),\n",
      "and hence the corresponding zvalues are distributed according to p(z), as desired. Exercise 11.6\n",
      "The original values of zare generated from the distribution q(z), and these sam-\n",
      "ples are then accepted with probability ˜p(z)/kq(z), and so the probability that a\n",
      "sample will be accepted is given by\n",
      "p(accept) =∫\n",
      "{˜p(z)/kq(z)}q(z)dz\n",
      "=1\n",
      "k∫\n",
      "˜p(z)dz. (11.14)\n",
      "Thus the fraction of points that are rejected by this method depends on the ratio of\n",
      "the area under the unnormalized distribution ˜p(z)to the area under the curve kq(z).\n",
      "We therefore see that the constant kshould be as small as possible subject to the\n",
      "limitation that kq(z)must be nowhere less than ˜p(z).\n",
      "As an illustration of the use of rejection sampling, consider the task of sampling\n",
      "from the gamma distribution\n",
      "Gam( z|a, b)=baza−1exp(−bz)\n",
      "Γ(a)(11.15)\n",
      "which, for a>1, has a bell-shaped form, as shown in Figure 11.5. A suitable\n",
      "proposal distribution is therefore the Cauchy (11.8) because this too is bell-shaped\n",
      "and because we can use the transformation method, discussed earlier, to sample from\n",
      "it. We need to generalize the Cauchy slightly to ensure that it nowhere has a smaller\n",
      "value than the gamma distribution. This can be achieved by transforming a uniform\n",
      "random variable yusingz=btany+c, which gives random numbers distributed\n",
      "according to. Exercise 11.7530 11. SAMPLING METHODS\n",
      "Figure 11.5 Plot showing the gamma distribu-\n",
      "tion given by (11.15) as the green\n",
      "curve, with a scaled Cauchy pro-\n",
      "posal distribution shown by the red\n",
      "curve. Samples from the gamma\n",
      "distribution can be obtained by\n",
      "sampling from the Cauchy and\n",
      "then applying the rejection sam-\n",
      "pling criterion.\n",
      "zp(z)\n",
      "0 10 20 3000.050.10.15\n",
      "q(z)=k\n",
      "1+(z−c)2/b2. (11.16)\n",
      "The minimum reject rate is obtained by setting c=a−1,b2=2a−1and choos-\n",
      "ing the constant kto be as small as possible while still satisfying the requirement\n",
      "kq(z)⩾˜p(z). The resulting comparison function is also illustrated in Figure 11.5.\n",
      "11.1.3 Adaptive rejection sampling\n",
      "In many instances where we might wish to apply rejection sampling, it proves\n",
      "difﬁcult to determine a suitable analytic form for the envelope distribution q(z).A n\n",
      "alternative approach is to construct the envelope function on the ﬂy based on mea-\n",
      "sured values of the distribution p(z)(Gilks and Wild, 1992). Construction of an\n",
      "envelope function is particularly straightforward for cases in which p(z)is log con-\n",
      "cave, in other words when lnp(z)has derivatives that are nonincreasing functions\n",
      "ofz. The construction of a suitable envelope function is illustrated graphically in\n",
      "Figure 11.6.\n",
      "The function lnp(z)and its gradient are evaluated at some initial set of grid\n",
      "points, and the intersections of the resulting tangent lines are used to construct the\n",
      "envelope function. Next a sample value is drawn from the envelope distribution.\n",
      "This is straightforward because the log of the envelope distribution is a succession Exercise 11.9\n",
      "Figure 11.6 In the case of distributions that are\n",
      "log concave, an envelope function\n",
      "for use in rejection sampling can be\n",
      "constructed using the tangent lines\n",
      "computed at a set of grid points. If a\n",
      "sample point is rejected, it is added\n",
      "to the set of grid points and used to\n",
      "reﬁne the envelope distribution.\n",
      "z1 z2 z3 zlnp(z)11.1. Basic Sampling Algorithms 531\n",
      "Figure 11.7 Illustrative example of rejection\n",
      "sampling involving sampling from a\n",
      "Gaussian distribution p(z)shown by\n",
      "the green curve, by using rejection\n",
      "sampling from a proposal distri-\n",
      "bution q(z)that is also Gaussian\n",
      "and whose scaled version kq(z)is\n",
      "shown by the red curve.\n",
      "zp(z)\n",
      "−5 0 500.250.5\n",
      "of linear functions, and hence the envelope distribution itself comprises a piecewise\n",
      "exponential distribution of the form\n",
      "q(z)=kiλiexp{−λi(z−zi−1)} zi−1<z⩽zi. (11.17)\n",
      "Once a sample has been drawn, the usual rejection criterion can be applied. If the\n",
      "sample is accepted, then it will be a draw from the desired distribution. If, however,\n",
      "the sample is rejected, then it is incorporated into the set of grid points, a new tangent\n",
      "line is computed, and the envelope function is thereby reﬁned. As the number of\n",
      "grid points increases, so the envelope function becomes a better approximation of\n",
      "the desired distribution p(z)and the probability of rejection decreases.\n",
      "A variant of the algorithm exists that avoids the evaluation of derivatives (Gilks,\n",
      "1992). The adaptive rejection sampling framework can also be extended to distri-\n",
      "butions that are not log concave, simply by following each rejection sampling step\n",
      "with a Metropolis-Hastings step (to be discussed in Section 11.2.2), giving rise to\n",
      "adaptive rejection Metropolis sampling (Gilks et al. , 1995).\n",
      "Clearly for rejection sampling to be of practical value, we require that the com-\n",
      "parison function be close to the required distribution so that the rate of rejection is\n",
      "kept to a minimum. Now let us examine what happens when we try to use rejection\n",
      "sampling in spaces of high dimensionality. Consider, for the sake of illustration,\n",
      "a somewhat artiﬁcial problem in which we wish to sample from a zero-mean mul-\n",
      "tivariate Gaussian distribution with covariance σ2\n",
      "pI, where Iis the unit matrix, by\n",
      "rejection sampling from a proposal distribution that is itself a zero-mean Gaussian\n",
      "distribution having covariance σ2\n",
      "qI. Obviously, we must have σ2\n",
      "q⩾σ2\n",
      "pin order that\n",
      "there exists a ksuch that kq(z)⩾p(z).I nD-dimensions the optimum value of k\n",
      "is given by k=(σq/σp)D, as illustrated for D=1in Figure 11.7. The acceptance\n",
      "rate will be the ratio of volumes under p(z)andkq(z), which, because both distribu-\n",
      "tions are normalized, is just 1/k. Thus the acceptance rate diminishes exponentially\n",
      "with dimensionality. Even if σqexceeds σpby just one percent, for D=1,000the\n",
      "acceptance ratio will be approximately 1/20,000. In this illustrative example the\n",
      "comparison function is close to the required distribution. For more practical exam-\n",
      "ples, where the desired distribution may be multimodal and sharply peaked, it will\n",
      "be extremely difﬁcult to ﬁnd a good proposal distribution and comparison function.532 11. SAMPLING METHODS\n",
      "Figure 11.8 Importance sampling addresses the prob-\n",
      "lem of evaluating the expectation of a func-\n",
      "tionf(z)with respect to a distribution p(z)\n",
      "from which it is difﬁcult to draw samples di-\n",
      "rectly. Instead, samples {z(l)}are drawn\n",
      "from a simpler distribution q(z), and the\n",
      "corresponding terms in the summation are\n",
      "weighted by the ratios p(z(l))/q(z(l)).p(z)f(z)\n",
      "zq(z)\n",
      "Furthermore, the exponential decrease of acceptance rate with dimensionality is a\n",
      "generic feature of rejection sampling. Although rejection can be a useful technique\n",
      "in one or two dimensions it is unsuited to problems of high dimensionality. It can,\n",
      "however, play a role as a subroutine in more sophisticated algorithms for sampling\n",
      "in high dimensional spaces.\n",
      "11.1.4 Importance sampling\n",
      "One of the principal reasons for wishing to sample from complicated probability\n",
      "distributions is to be able to evaluate expectations of the form (11.1). The technique\n",
      "ofimportance sampling provides a framework for approximating expectations di-\n",
      "rectly but does not itself provide a mechanism for drawing samples from distribution\n",
      "p(z).\n",
      "The ﬁnite sum approximation to the expectation, given by (11.2), depends on\n",
      "being able to draw samples from the distribution p(z). Suppose, however, that it is\n",
      "impractical to sample directly from p(z)but that we can evaluate p(z)easily for any\n",
      "given value of z. One simplistic strategy for evaluating expectations would be to\n",
      "discretize z-space into a uniform grid and to evaluate the integrand as a sum of the\n",
      "form\n",
      "E[f]≃L∑\n",
      "l=1p(z(l))f(z(l)). (11.18)\n",
      "An obvious problem with this approach is that the number of terms in the summation\n",
      "grows exponentially with the dimensionality of z. Furthermore, as we have already\n",
      "noted, the kinds of probability distributions of interest will often have much of their\n",
      "mass conﬁned to relatively small regions of zspace and so uniform sampling will be\n",
      "very inefﬁcient because in high-dimensional problems, only a very small proportion\n",
      "of the samples will make a signiﬁcant contribution to the sum. We would really like\n",
      "to choose the sample points to fall in regions where p(z)is large, or ideally where\n",
      "the product p(z)f(z)is large.\n",
      "As in the case of rejection sampling, importance sampling is based on the use\n",
      "of a proposal distribution q(z)from which it is easy to draw samples, as illustrated\n",
      "in Figure 11.8. We can then express the expectation in the form of a ﬁnite sum over11.1. Basic Sampling Algorithms 533\n",
      "samples {z(l)}drawn from q(z)\n",
      "E[f]=∫\n",
      "f(z)p(z)dz\n",
      "=∫\n",
      "f(z)p(z)\n",
      "q(z)q(z)dz\n",
      "≃1\n",
      "LL∑\n",
      "l=1p(z(l))\n",
      "q(z(l))f(z(l)). (11.19)\n",
      "The quantities rl=p(z(l))/q(z(l))are known as importance weights , and they cor-\n",
      "rect the bias introduced by sampling from the wrong distribution. Note that, unlike\n",
      "rejection sampling, all of the samples generated are retained.\n",
      "It will often be the case that the distribution p(z)can only be evaluated up to a\n",
      "normalization constant, so that p(z)=˜p(z)/Zpwhere˜p(z)can be evaluated easily,\n",
      "whereas Zpis unknown. Similarly, we may wish to use an importance sampling\n",
      "distribution q(z)=˜q(z)/Zq, which has the same property. We then have\n",
      "E[f]=∫\n",
      "f(z)p(z)dz\n",
      "=Zq\n",
      "Zp∫\n",
      "f(z)˜p(z)\n",
      "˜q(z)q(z)dz\n",
      "≃Zq\n",
      "Zp1\n",
      "LL∑\n",
      "l=1˜rlf(z(l)). (11.20)\n",
      "where˜rl=˜p(z(l))/˜q(z(l)). We can use the same sample set to evaluate the ratio\n",
      "Zp/Zqwith the result\n",
      "Zp\n",
      "Zq=1\n",
      "Zq∫\n",
      "˜p(z)dz=∫˜p(z)\n",
      "˜q(z)q(z)dz\n",
      "≃1\n",
      "LL∑\n",
      "l=1˜rl (11.21)\n",
      "and hence\n",
      "E[f]≃L∑\n",
      "l=1wlf(z(l)) (11.22)\n",
      "where we have deﬁned\n",
      "wl=˜rl∑\n",
      "m˜rm=˜p(z(l))/q(z(l))∑\n",
      "m˜p(z(m))/q(z(m)). (11.23)\n",
      "As with rejection sampling, the success of the importance sampling approach\n",
      "depends crucially on how well the sampling distribution q(z)matches the desired534 11. SAMPLING METHODS\n",
      "distribution p(z). If, as is often the case, p(z)f(z)is strongly varying and has a sig-\n",
      "niﬁcant proportion of its mass concentrated over relatively small regions of zspace,\n",
      "then the set of importance weights {rl}may be dominated by a few weights hav-\n",
      "ing large values, with the remaining weights being relatively insigniﬁcant. Thus the\n",
      "effective sample size can be much smaller than the apparent sample size L. The prob-\n",
      "lem is even more severe if none of the samples falls in the regions where p(z)f(z)\n",
      "is large. In that case, the apparent variances of rlandrlf(z(l))may be small even\n",
      "though the estimate of the expectation may be severely wrong. Hence a major draw-back of the importance sampling method is the potential to produce results that are\n",
      "arbitrarily in error and with no diagnostic indication. This also highlights a key re-\n",
      "quirement for the sampling distribution q(z), namely that it should not be small or\n",
      "zero in regions where p(z)may be signiﬁcant.\n",
      "For distributions deﬁned in terms of a graphical model, we can apply the impor-\n",
      "tance sampling technique in various ways. For discrete variables, a simple approach\n",
      "is called uniform sampling . The joint distribution for a directed graph is deﬁned\n",
      "by (11.4). Each sample from the joint distribution is obtained by ﬁrst setting thosevariables z\n",
      "ithat are in the evidence set equal to their observed values. Each of the\n",
      "remaining variables is then sampled independently from a uniform distribution over\n",
      "the space of possible instantiations. To determine the corresponding weight associ-ated with a sample z\n",
      "(l), we note that the sampling distribution ˜q(z)is uniform over\n",
      "the possible choices for z, and that˜p(z|x)=˜p(z), where xdenotes the subset of\n",
      "variables that are observed, and the equality follows from the fact that every samplezthat is generated is necessarily consistent with the evidence. Thus the weights r\n",
      "l\n",
      "are simply proportional to p(z). Note that the variables can be sampled in any order.\n",
      "This approach can yield poor results if the posterior distribution is far from uniform,as is often the case in practice.\n",
      "An improvement on this approach is called likelihood weighted sampling (Fung\n",
      "and Chang, 1990; Shachter and Peot, 1990) and is based on ancestral sampling of\n",
      "the variables. For each variable in turn, if that variable is in the evidence set, then it\n",
      "is just set to its instantiated value. If it is not in the evidence set, then it is sampledfrom the conditional distribution p(z\n",
      "i|pai)in which the conditioning variables are\n",
      "set to their currently sampled values. The weighting associated with the resulting\n",
      "sample zis then given by\n",
      "r(z)=∏\n",
      "zi̸∈ep(zi|pai)\n",
      "p(zi|pai)∏\n",
      "zi∈ep(zi|pai)\n",
      "1=∏\n",
      "zi∈ep(zi|pai). (11.24)\n",
      "This method can be further extended using self-importance sampling (Shachter and\n",
      "Peot, 1990) in which the importance sampling distribution is continually updated to\n",
      "reﬂect the current estimated posterior distribution.\n",
      "11.1.5 Sampling-importance-resampling\n",
      "The rejection sampling method discussed in Section 11.1.2 depends in part for\n",
      "its success on the determination of a suitable value for the constant k. For many\n",
      "pairs of distributions p(z)andq(z), it will be impractical to determine a suitable11.1. Basic Sampling Algorithms 535\n",
      "value for kin that any value that is sufﬁciently large to guarantee a bound on the\n",
      "desired distribution will lead to impractically small acceptance rates.\n",
      "As in the case of rejection sampling, the sampling-importance-resampling (SIR)\n",
      "approach also makes use of a sampling distribution q(z)but avoids having to de-\n",
      "termine the constant k. There are two stages to the scheme. In the ﬁrst stage,\n",
      "Lsamples z(1),...,z(L)are drawn from q(z). Then in the second stage, weights\n",
      "w1,...,w Lare constructed using (11.23). Finally, a second set of Lsamples is\n",
      "drawn from the discrete distribution (z(1),...,z(L))with probabilities given by the\n",
      "weights (w1,...,w L).\n",
      "The resulting Lsamples are only approximately distributed according to p(z),\n",
      "but the distribution becomes correct in the limit L→∞ . To see this, consider the\n",
      "univariate case, and note that the cumulative distribution of the resampled values is\n",
      "given by\n",
      "p(z⩽a)=∑\n",
      "l:z(l)⩽awl\n",
      "=∑\n",
      "lI(z(l)⩽a)˜p(z(l))/q(z(l))∑\n",
      "l˜p(z(l))/q(z(l))(11.25)\n",
      "where I(.)is the indicator function (which equals 1if its argument is true and 0\n",
      "otherwise). Taking the limit L→∞ , and assuming suitable regularity of the dis-\n",
      "tributions, we can replace the sums by integrals weighted according to the original\n",
      "sampling distribution q(z)\n",
      "p(z⩽a)=∫\n",
      "I(z⩽a){˜p(z)/q(z)}q(z)dz\n",
      "∫\n",
      "{˜p(z)/q(z)}q(z)dz\n",
      "=∫\n",
      "I(z⩽a)˜p(z)dz\n",
      "∫\n",
      "˜p(z)dz\n",
      "=∫\n",
      "I(z⩽a)p(z)dz (11.26)\n",
      "which is the cumulative distribution function of p(z). Again, we see that the normal-\n",
      "ization of p(z)is not required.\n",
      "For a ﬁnite value of L, and a given initial sample set, the resampled values will\n",
      "only approximately be drawn from the desired distribution. As with rejection sam-\n",
      "pling, the approximation improves as the sampling distribution q(z)gets closer to\n",
      "the desired distribution p(z). When q(z)=p(z), the initial samples (z(1),...,z(L))\n",
      "have the desired distribution, and the weights wn=1/Lso that the resampled values\n",
      "also have the desired distribution.\n",
      "If moments with respect to the distribution p(z)are required, then they can be536 11. SAMPLING METHODS\n",
      "evaluated directly using the original samples together with the weights, because\n",
      "E[f(z)] =∫\n",
      "f(z)p(z)dz\n",
      "=∫\n",
      "f(z)[˜p(z)/q(z)]q(z)dz\n",
      "∫\n",
      "[˜p(z)/q(z)]q(z)dz\n",
      "≃L∑\n",
      "l=1wlf(zl). (11.27)\n",
      "11.1.6 Sampling and the EM algorithm\n",
      "In addition to providing a mechanism for direct implementation of the Bayesian\n",
      "framework, Monte Carlo methods can also play a role in the frequentist paradigm,for example to ﬁnd maximum likelihood solutions. In particular, sampling methods\n",
      "can be used to approximate the E step of the EM algorithm for models in which the\n",
      "E step cannot be performed analytically. Consider a model with hidden variablesZ, visible (observed) variables X, and parameters θ. The function that is optimized\n",
      "with respect to θin the M step is the expected complete-data log likelihood, given\n",
      "by\n",
      "Q(θ,θ\n",
      "old)=∫\n",
      "p(Z|X,θold)l np(Z,X|θ)dZ. (11.28)\n",
      "We can use sampling methods to approximate this integral by a ﬁnite sum over sam-\n",
      "ples{Z(l)}, which are drawn from the current estimate for the posterior distribution\n",
      "p(Z|X,θold), so that\n",
      "Q(θ,θold)≃1\n",
      "LL∑\n",
      "l=1lnp(Z(l),X|θ). (11.29)\n",
      "TheQfunction is then optimized in the usual way in the M step. This procedure is\n",
      "called the Monte Carlo EM algorithm .\n",
      "It is straightforward to extend this to the problem of ﬁnding the mode of the\n",
      "posterior distribution over θ(the MAP estimate) when a prior distribution p(θ)has\n",
      "been deﬁned, simply by adding lnp(θ)to the function Q(θ,θold)before performing\n",
      "the M step.\n",
      "A particular instance of the Monte Carlo EM algorithm, called stochastic EM ,\n",
      "arises if we consider a ﬁnite mixture model, and draw just one sample at each E step.\n",
      "Here the latent variable Zcharacterizes which of the Kcomponents of the mixture\n",
      "is responsible for generating each data point. In the E step, a sample of Zis taken\n",
      "from the posterior distribution p(Z|X,θold)whereXis the data set. This effectively\n",
      "makes a hard assignment of each data point to one of the components in the mixture.In the M step, this sampled approximation to the posterior distribution is used to\n",
      "update the model parameters in the usual way.11.2. Markov Chain Monte Carlo 537\n",
      "Now suppose we move from a maximum likelihood approach to a full Bayesian\n",
      "treatment in which we wish to sample from the posterior distribution over the param-eter vector θ. In principle, we would like to draw samples from the joint posterior\n",
      "p(θ,Z|X), but we shall suppose that this is computationally difﬁcult. Suppose fur-\n",
      "ther that it is relatively straightforward to sample from the complete-data parameterposterior p(θ|Z,X). This inspires the data augmentation algorithm, which alter-\n",
      "nates between two steps known as the I-step (imputation step, analogous to an E\n",
      "step) and the P-step (posterior step, analogous to an M step).\n",
      "IP Algorithm\n",
      "I-step. We wish to sample from p(Z|X)but we cannot do this directly. We\n",
      "therefore note the relation\n",
      "p(Z|X)=∫\n",
      "p(Z|θ,X)p(θ|X)dθ (11.30)\n",
      "and hence for l=1,...,L we ﬁrst draw a sample θ(l)from the current esti-\n",
      "mate for p(θ|X), and then use this to draw a sample Z(l)fromp(Z|θ(l),X).\n",
      "P-step. Given the relation\n",
      "p(θ|X)=∫\n",
      "p(θ|Z,X)p(Z|X)dZ (11.31)\n",
      "we use the samples {Z(l)}obtained from the I-step to compute a revised\n",
      "estimate of the posterior distribution over θgiven by\n",
      "p(θ|X)≃1\n",
      "LL∑\n",
      "l=1p(θ|Z(l),X). (11.32)\n",
      "By assumption, it will be feasible to sample from this approximation in the\n",
      "I-step.\n",
      "Note that we are making a (somewhat artiﬁcial) distinction between parameters θ\n",
      "and hidden variables Z. From now on, we blur this distinction and focus simply on\n",
      "the problem of drawing samples from a given posterior distribution.\n",
      "11.2. Markov Chain Monte Carlo\n",
      "In the previous section, we discussed the rejection sampling and importance sam-pling strategies for evaluating expectations of functions, and we saw that they suffer\n",
      "from severe limitations particularly in spaces of high dimensionality. We thereforeturn in this section to a very general and powerful framework called Markov chain\n",
      "Monte Carlo (MCMC), which allows sampling from a large class of distributions,538 11. SAMPLING METHODS\n",
      "and which scales well with the dimensionality of the sample space. Markov chain\n",
      "Monte Carlo methods have their origins in physics (Metropolis and Ulam, 1949),and it was only towards the end of the 1980s that they started to have a signiﬁcant\n",
      "impact in the ﬁeld of statistics.\n",
      "As with rejection and importance sampling, we again sample from a proposal\n",
      "distribution. This time, however, we maintain a record of the current state z\n",
      "(τ), and\n",
      "the proposal distribution q(z|z(τ))depends on this current state, and so the sequence\n",
      "of samples z(1),z(2),...forms a Markov chain. Again, if we write p(z)=˜p(z)/Zp, Section 11.2.1\n",
      "we will assume that ˜p(z)can readily be evaluated for any given value of z, although\n",
      "the value of Zpmay be unknown. The proposal distribution itself is chosen to be\n",
      "sufﬁciently simple that it is straightforward to draw samples from it directly. At\n",
      "each cycle of the algorithm, we generate a candidate sample z⋆from the proposal\n",
      "distribution and then accept the sample according to an appropriate criterion.\n",
      "In the basic Metropolis algorithm (Metropolis et al. , 1953), we assume that the\n",
      "proposal distribution is symmetric, that is q(zA|zB)=q(zB|zA)for all values of\n",
      "zAandzB. The candidate sample is then accepted with probability\n",
      "A(z⋆,z(τ))=m i n(\n",
      "1,˜p(z⋆)\n",
      "˜p(z(τ)))\n",
      ". (11.33)\n",
      "This can be achieved by choosing a random number uwith uniform distribution over\n",
      "the unit interval (0,1)and then accepting the sample if A(z⋆,z(τ))>u. Note that\n",
      "if the step from z(τ)toz⋆causes an increase in the value of p(z), then the candidate\n",
      "point is certain to be kept.\n",
      "If the candidate sample is accepted, then z(τ+1)=z⋆, otherwise the candidate\n",
      "pointz⋆is discarded, z(τ+1)is set to z(τ)and another candidate sample is drawn\n",
      "from the distribution q(z|z(τ+1)). This is in contrast to rejection sampling, where re-\n",
      "jected samples are simply discarded. In the Metropolis algorithm when a candidatepoint is rejected, the previous sample is included instead in the ﬁnal list of samples,\n",
      "leading to multiple copies of samples. Of course, in a practical implementation,\n",
      "only a single copy of each retained sample would be kept, along with an integer\n",
      "weighting factor recording how many times that state appears. As we shall see, as\n",
      "long as q(z\n",
      "A|zB)is positive for any values of zAandzB(this is a sufﬁcient but\n",
      "not necessary condition), the distribution of z(τ)tends to p(z)asτ→∞ . It should\n",
      "be emphasized, however, that the sequence z(1),z(2),... is not a set of independent\n",
      "samples from p(z)because successive samples are highly correlated. If we wish to\n",
      "obtain independent samples, then we can discard most of the sequence and just re-\n",
      "tain every Mthsample. For Msufﬁciently large, the retained samples will for all\n",
      "practical purposes be independent. Figure 11.9 shows a simple illustrative exam-ple of sampling from a two-dimensional Gaussian distribution using the Metropolis\n",
      "algorithm in which the proposal distribution is an isotropic Gaussian.\n",
      "Further insight into the nature of Markov chain Monte Carlo algorithms can be\n",
      "gleaned by looking at the properties of a speciﬁc example, namely a simple random11.2. Markov Chain Monte Carlo 539\n",
      "Figure 11.9 A simple illustration using Metropo-\n",
      "lis algorithm to sample from a\n",
      "Gaussian distribution whose one\n",
      "standard-deviation contour is shown\n",
      "by the ellipse. The proposal distribu-\n",
      "tion is an isotropic Gaussian distri-\n",
      "bution whose standard deviation is\n",
      "0.2. Steps that are accepted are\n",
      "shown as green lines, and rejected\n",
      "steps are shown in red. A total of\n",
      "150 candidate samples are gener-\n",
      "ated, of which 43 are rejected.\n",
      "0 0.5 1 1.5 2 2.5 300.511.522.53\n",
      "walk. Consider a state space zconsisting of the integers, with probabilities\n",
      "p(z(τ+1)=z(τ))=0 .5 (11.34)\n",
      "p(z(τ+1)=z(τ)+1 ) = 0 .25 (11.35)\n",
      "p(z(τ+1)=z(τ)−1) = 0 .25 (11.36)\n",
      "where z(τ)denotes the state at step τ. If the initial state is z(1)=0, then by sym-\n",
      "metry the expected state at time τwill also be zero E[z(τ)]=0 , and similarly it is\n",
      "easily seen that E[(z(τ))2]=τ/2. Thus after τsteps, the random walk has only trav- Exercise 11.10\n",
      "elled a distance that on average is proportional to the square root of τ. This square\n",
      "root dependence is typical of random walk behaviour and shows that random walks\n",
      "are very inefﬁcient in exploring the state space. As we shall see, a central goal in\n",
      "designing Markov chain Monte Carlo methods is to avoid random walk behaviour.\n",
      "11.2.1 Markov chains\n",
      "Before discussing Markov chain Monte Carlo methods in more detail, it is use-\n",
      "ful to study some general properties of Markov chains in more detail. In particular,\n",
      "we ask under what circumstances will a Markov chain converge to the desired dis-\n",
      "tribution. A ﬁrst-order Markov chain is deﬁned to be a series of random variables\n",
      "z(1),...,z(M)such that the following conditional independence property holds for\n",
      "m∈{1,...,M −1}\n",
      "p(z(m+1)|z(1),...,z(m))=p(z(m+1)|z(m)). (11.37)\n",
      "This of course can be represented as a directed graph in the form of a chain, an ex-\n",
      "ample of which is shown in Figure 8.38. We can then specify the Markov chain by\n",
      "giving the probability distribution for the initial variable p(z(0))together with the540 11. SAMPLING METHODS\n",
      "conditional probabilities for subsequent variables in the form of transition probabil-\n",
      "itiesTm(z(m),z(m+1))≡p(z(m+1)|z(m)). A Markov chain is called homogeneous\n",
      "if the transition probabilities are the same for all m.\n",
      "The marginal probability for a particular variable can be expressed in terms of\n",
      "the marginal probability for the previous variable in the chain in the form\n",
      "p(z(m+1))=∑\n",
      "z(m)p(z(m+1)|z(m))p(z(m)). (11.38)\n",
      "A distribution is said to be invariant, or stationary, with respect to a Markov chain\n",
      "if each step in the chain leaves that distribution invariant. Thus, for a homogeneousMarkov chain with transition probabilities T(z\n",
      "′,z), the distribution p⋆(z)is invariant\n",
      "if\n",
      "p⋆(z)=∑\n",
      "z′T(z′,z)p⋆(z′). (11.39)\n",
      "Note that a given Markov chain may have more than one invariant distribution. For\n",
      "instance, if the transition probabilities are given by the identity transformation, then\n",
      "any distribution will be invariant.\n",
      "A sufﬁcient (but not necessary) condition for ensuring that the required distribu-\n",
      "tionp(z)is invariant is to choose the transition probabilities to satisfy the property\n",
      "ofdetailed balance , deﬁned by\n",
      "p⋆(z)T(z,z′)=p⋆(z′)T(z′,z) (11.40)\n",
      "for the particular distribution p⋆(z). It is easily seen that a transition probability\n",
      "that satisﬁes detailed balance with respect to a particular distribution will leave that\n",
      "distribution invariant, because\n",
      "∑\n",
      "z′p⋆(z′)T(z′,z)=∑\n",
      "z′p⋆(z)T(z,z′)=p⋆(z)∑\n",
      "z′p(z′|z)=p⋆(z).(11.41)\n",
      "A Markov chain that respects detailed balance is said to be reversible .\n",
      "Our goal is to use Markov chains to sample from a given distribution. We can\n",
      "achieve this if we set up a Markov chain such that the desired distribution is invariant.\n",
      "However, we must also require that for m→∞ , the distribution p(z(m))converges\n",
      "to the required invariant distribution p⋆(z), irrespective of the choice of initial dis-\n",
      "tribution p(z(0)). This property is called ergodicity , and the invariant distribution\n",
      "is then called the equilibrium distribution. Clearly, an ergodic Markov chain can\n",
      "have only one equilibrium distribution. It can be shown that a homogeneous Markovchain will be ergodic, subject only to weak restrictions on the invariant distribution\n",
      "and the transition probabilities (Neal, 1993).\n",
      "In practice we often construct the transition probabilities from a set of ‘base’\n",
      "transitions B\n",
      "1,...,B K. This can be achieved through a mixture distribution of the\n",
      "form\n",
      "T(z′,z)=K∑\n",
      "k=1αkBk(z′,z) (11.42)11.2. Markov Chain Monte Carlo 541\n",
      "for some set of mixing coefﬁcients α1,...,α Ksatisfying αk⩾0and∑\n",
      "kαk=1.\n",
      "Alternatively, the base transitions may be combined through successive application,so that\n",
      "T(z\n",
      "′,z)=∑\n",
      "z1...∑\n",
      "zn−1B1(z′,z1)...B K−1(zK−2,zK−1)BK(zK−1,z).(11.43)\n",
      "If a distribution is invariant with respect to each of the base transitions, then obvi-\n",
      "ously it will also be invariant with respect to either of the T(z′,z)given by (11.42)\n",
      "or (11.43). For the case of the mixture (11.42), if each of the base transitions sat-\n",
      "isﬁes detailed balance, then the mixture transition Twill also satisfy detailed bal-\n",
      "ance. This does not hold for the transition probability constructed using (11.43), al-though by symmetrizing the order of application of the base transitions, in the form\n",
      "B\n",
      "1,B2,...,B K,BK,...,B 2,B1, detailed balance can be restored. A common ex-\n",
      "ample of the use of composite transition probabilities is where each base transitionchanges only a subset of the variables.\n",
      "11.2.2 The Metropolis-Hastings algorithm\n",
      "Earlier we introduced the basic Metropolis algorithm, without actually demon-\n",
      "strating that it samples from the required distribution. Before giving a proof, we\n",
      "ﬁrst discuss a generalization, known as the Metropolis-Hastings algorithm (Hast-\n",
      "ings, 1970), to the case where the proposal distribution is no longer a symmetric\n",
      "function of its arguments. In particular at step τof the algorithm, in which the cur-\n",
      "rent state is z(τ), we draw a sample z⋆from the distribution qk(z|z(τ))and then\n",
      "accept it with probability Ak(z⋆,zτ)where\n",
      "Ak(z⋆,z(τ))=m i n(\n",
      "1,˜p(z⋆)qk(z(τ)|z⋆)\n",
      "˜p(z(τ))qk(z⋆|z(τ)))\n",
      ". (11.44)\n",
      "Hereklabels the members of the set of possible transitions being considered. Again,\n",
      "the evaluation of the acceptance criterion does not require knowledge of the normal-\n",
      "izing constant Zpin the probability distribution p(z)=˜p(z)/Zp. For a symmetric\n",
      "proposal distribution the Metropolis-Hastings criterion (11.44) reduces to the stan-\n",
      "dard Metropolis criterion given by (11.33).\n",
      "We can show that p(z)is an invariant distribution of the Markov chain deﬁned\n",
      "by the Metropolis-Hastings algorithm by showing that detailed balance, deﬁned by\n",
      "(11.40), is satisﬁed. Using (11.44) we have\n",
      "p(z)qk(z|z′)Ak(z′,z)=m i n ( p(z)qk(z|z′),p(z′)qk(z′|z))\n",
      "=m i n ( p(z′)qk(z′|z),p(z)qk(z|z′))\n",
      "=p(z′)qk(z′|z)Ak(z,z′) (11.45)\n",
      "as required.\n",
      "The speciﬁc choice of proposal distribution can have a marked effect on the\n",
      "performance of the algorithm. For continuous state spaces, a common choice is aGaussian centred on the current state, leading to an important trade-off in determin-\n",
      "ing the variance parameter of this distribution. If the variance is small, then the542 11. SAMPLING METHODS\n",
      "Figure 11.10 Schematic illustration of the use of an isotropic\n",
      "Gaussian proposal distribution (blue circle) to\n",
      "sample from a correlated multivariate Gaussian\n",
      "distribution (red ellipse) having very different stan-\n",
      "dard deviations in different directions, using the\n",
      "Metropolis-Hastings algorithm. In order to keep\n",
      "the rejection rate low, the scale ρof the proposal\n",
      "distribution should be on the order of the smallest\n",
      "standard deviation σmin, which leads to random\n",
      "walk behaviour in which the number of steps sep-\n",
      "arating states that are approximately independent\n",
      "is of order (σmax/σmin)2where σmaxis the largest\n",
      "standard deviation.σmax\n",
      "σminρ\n",
      "proportion of accepted transitions will be high, but progress through the state space\n",
      "takes the form of a slow random walk leading to long correlation times. However,\n",
      "if the variance parameter is large, then the rejection rate will be high because, in the\n",
      "kind of complex problems we are considering, many of the proposed steps will be\n",
      "to states for which the probability p(z)is low. Consider a multivariate distribution\n",
      "p(z)having strong correlations between the components of z, as illustrated in Fig-\n",
      "ure 11.10. The scale ρof the proposal distribution should be as large as possible\n",
      "without incurring high rejection rates. This suggests that ρshould be of the same\n",
      "order as the smallest length scale σmin. The system then explores the distribution\n",
      "along the more extended direction by means of a random walk, and so the number\n",
      "of steps to arrive at a state that is more or less independent of the original state is\n",
      "of order (σmax/σmin)2. In fact in two dimensions, the increase in rejection rate as ρ\n",
      "increases is offset by the larger steps sizes of those transitions that are accepted, and\n",
      "more generally for a multivariate Gaussian the number of steps required to obtain\n",
      "independent samples scales like (σmax/σ2)2where σ2is the second-smallest stan-\n",
      "dard deviation (Neal, 1993). These details aside, it remains the case that if the length\n",
      "scales over which the distributions vary are very different in different directions, then\n",
      "the Metropolis Hastings algorithm can have very slow convergence.\n",
      "11.3. Gibbs Sampling\n",
      "Gibbs sampling (Geman and Geman, 1984) is a simple and widely applicable Markov\n",
      "chain Monte Carlo algorithm and can be seen as a special case of the Metropolis-\n",
      "Hastings algorithm.\n",
      "Consider the distribution p(z)=p(z1,...,z M)from which we wish to sample,\n",
      "and suppose that we have chosen some initial state for the Markov chain. Each step\n",
      "of the Gibbs sampling procedure involves replacing the value of one of the variables\n",
      "by a value drawn from the distribution of that variable conditioned on the values of\n",
      "the remaining variables. Thus we replace ziby a value drawn from the distribution\n",
      "p(zi|z\\i), where zidenotes the ithcomponent of z, andz\\idenotes z1,...,z Mbut\n",
      "withziomitted. This procedure is repeated either by cycling through the variables11.3. Gibbs Sampling 543\n",
      "in some particular order or by choosing the variable to be updated at each step at\n",
      "random from some distribution.\n",
      "For example, suppose we have a distribution p(z1,z2,z3)over three variables,\n",
      "and at step τof the algorithm we have selected values z(τ)\n",
      "1,z(τ)\n",
      "2andz(τ)\n",
      "3. We ﬁrst\n",
      "replace z(τ)\n",
      "1by a new value z(τ+1)\n",
      "1 obtained by sampling from the conditional distri-\n",
      "bution\n",
      "p(z1|z(τ)\n",
      "2,z(τ)\n",
      "3). (11.46)\n",
      "Next we replace z(τ)\n",
      "2by a value z(τ+1)\n",
      "2 obtained by sampling from the conditional\n",
      "distribution\n",
      "p(z2|z(τ+1)\n",
      "1,z(τ)\n",
      "3) (11.47)\n",
      "so that the new value for z1is used straight away in subsequent sampling steps. Then\n",
      "we update z3with a sample z(τ+1)\n",
      "3 drawn from\n",
      "p(z3|z(τ+1)\n",
      "1,z(τ+1)\n",
      "2) (11.48)\n",
      "and so on, cycling through the three variables in turn.\n",
      "Gibbs Sampling\n",
      "1. Initialize {zi:i=1,...,M }\n",
      "2. For τ=1,...,T :\n",
      "–Sample z(τ+1)\n",
      "1∼p(z1|z(τ)\n",
      "2,z(τ)\n",
      "3,...,z(τ)\n",
      "M).\n",
      "–Sample z(τ+1)\n",
      "2∼p(z2|z(τ+1)\n",
      "1,z(τ)\n",
      "3,...,z(τ)\n",
      "M).\n",
      "...\n",
      "–Sample z(τ+1)\n",
      "j∼p(zj|z(τ+1)\n",
      "1,...,z(τ+1)\n",
      "j−1,z(τ)\n",
      "j+1,...,z(τ)\n",
      "M).\n",
      "...\n",
      "–Sample z(τ+1)\n",
      "M∼p(zM|z(τ+1)\n",
      "1,z(τ+1)\n",
      "2,...,z(τ+1)\n",
      "M−1).\n",
      "Josiah Willard Gibbs\n",
      "1839–1903\n",
      "Gibbs spent almost his entire life liv-\n",
      "ing in a house built by his father inNew Haven, Connecticut. In 1863,Gibbs was granted the ﬁrst PhD inengineering in the United States,and in 1871 he was appointed to\n",
      "the ﬁrst chair of mathematical physics in the UnitedStates at Y ale, a post for which he received no salary\n",
      "because at the time he had no publications. He de-veloped the ﬁeld of vector analysis and made contri-butions to crystallography and planetary orbits. His\n",
      "most famous work, entitled\n",
      "On the Equilibrium of Het-\n",
      "erogeneous Substances , laid the foundations for the\n",
      "science of physical chemistry.544 11. SAMPLING METHODS\n",
      "To show that this procedure samples from the required distribution, we ﬁrst of\n",
      "all note that the distribution p(z)is an invariant of each of the Gibbs sampling steps\n",
      "individually and hence of the whole Markov chain. This follows from the fact that\n",
      "when we sample from p(zi|{z\\i), the marginal distribution p(z\\i)is clearly invariant\n",
      "because the value of z\\iis unchanged. Also, each step by deﬁnition samples from the\n",
      "correct conditional distribution p(zi|z\\i). Because these conditional and marginal\n",
      "distributions together specify the joint distribution, we see that the joint distribution\n",
      "is itself invariant.\n",
      "The second requirement to be satisﬁed in order that the Gibbs sampling proce-\n",
      "dure samples from the correct distribution is that it be ergodic. A sufﬁcient condition\n",
      "for ergodicity is that none of the conditional distributions be anywhere zero. If this\n",
      "is the case, then any point in zspace can be reached from any other point in a ﬁnite\n",
      "number of steps involving one update of each of the component variables. If thisrequirement is not satisﬁed, so that some of the conditional distributions have zeros,\n",
      "then ergodicity, if it applies, must be proven explicitly.\n",
      "The distribution of initial states must also be speciﬁed in order to complete the\n",
      "algorithm, although samples drawn after many iterations will effectively become\n",
      "independent of this distribution. Of course, successive samples from the Markov\n",
      "chain will be highly correlated, and so to obtain samples that are nearly independentit will be necessary to subsample the sequence.\n",
      "We can obtain the Gibbs sampling procedure as a particular instance of the\n",
      "Metropolis-Hastings algorithm as follows. Consider a Metropolis-Hastings samplingstep involving the variable z\n",
      "kin which the remaining variables z\\kremain ﬁxed, and\n",
      "for which the transition probability from ztoz⋆is given by qk(z⋆|z)=p(z⋆\n",
      "k|z\\k).\n",
      "We note that z⋆\n",
      "\\k=z\\kbecause these components are unchanged by the sampling\n",
      "step. Also, p(z)=p(zk|z\\k)p(z\\k). Thus the factor that determines the acceptance\n",
      "probability in the Metropolis-Hastings (11.44) is given by\n",
      "A(z⋆,z)=p(z⋆)qk(z|z⋆)\n",
      "p(z)qk(z⋆|z)=p(z⋆\n",
      "k|z⋆\n",
      "\\k)p(z⋆\n",
      "\\k)p(zk|z⋆\n",
      "\\k)\n",
      "p(zk|z\\k)p(z\\k)p(z⋆\n",
      "k|z\\k)=1 (11.49)\n",
      "where we have used z⋆\n",
      "\\k=z\\k. Thus the Metropolis-Hastings steps are always\n",
      "accepted.\n",
      "As with the Metropolis algorithm, we can gain some insight into the behaviour of\n",
      "Gibbs sampling by investigating its application to a Gaussian distribution. Consider\n",
      "a correlated Gaussian in two variables, as illustrated in Figure 11.11, having con-ditional distributions of width land marginal distributions of width L. The typical\n",
      "step size is governed by the conditional distributions and will be of order l. Because\n",
      "the state evolves according to a random walk, the number of steps needed to obtain\n",
      "independent samples from the distribution will be of order (L/l)\n",
      "2. Of course if the\n",
      "Gaussian distribution were uncorrelated, then the Gibbs sampling procedure wouldbe optimally efﬁcient. For this simple problem, we could rotate the coordinate sys-\n",
      "tem in order to decorrelate the variables. However, in practical applications it will\n",
      "generally be infeasible to ﬁnd such transformations.\n",
      "One approach to reducing random walk behaviour in Gibbs sampling is called\n",
      "over-relaxation (Adler, 1981). In its original form, this applies to problems for which11.3. Gibbs Sampling 545\n",
      "Figure 11.11 Illustration of Gibbs sampling by alter-\n",
      "nate updates of two variables whose\n",
      "distribution is a correlated Gaussian.\n",
      "The step size is governed by the stan-\n",
      "dard deviation of the conditional distri-\n",
      "bution (green curve), and is O(l), lead-\n",
      "ing to slow progress in the direction of\n",
      "elongation of the joint distribution (red\n",
      "ellipse). The number of steps needed\n",
      "to obtain an independent sample from\n",
      "the distribution is O((L/l)2).\n",
      "z1z2\n",
      "L\n",
      "l\n",
      "the conditional distributions are Gaussian, which represents a more general class of\n",
      "distributions than the multivariate Gaussian because, for example, the non-Gaussian\n",
      "distribution p(z,y)∝exp(−z2y2)has Gaussian conditional distributions. At each\n",
      "step of the Gibbs sampling algorithm, the conditional distribution for a particular\n",
      "component zihas some mean µiand some variance σ2\n",
      "i. In the over-relaxation frame-\n",
      "work, the value of ziis replaced with\n",
      "z′\n",
      "i=µi+α(zi−µi)+σi(1−α2\n",
      "i)1/2ν (11.50)\n",
      "where νis a Gaussian random variable with zero mean and unit variance, and α\n",
      "is a parameter such that −1<α< 1.F o r α=0, the method is equivalent to\n",
      "standard Gibbs sampling, and for α<0the step is biased to the opposite side of the\n",
      "mean. This step leaves the desired distribution invariant because if zihas mean µi\n",
      "and variance σ2\n",
      "i, then so too does z′\n",
      "i. The effect of over-relaxation is to encourage\n",
      "directed motion through state space when the variables are highly correlated. The\n",
      "framework of ordered over-relaxation (Neal, 1999) generalizes this approach to non-\n",
      "Gaussian distributions.\n",
      "The practical applicability of Gibbs sampling depends on the ease with which\n",
      "samples can be drawn from the conditional distributions p(zk|z\\k). In the case of\n",
      "probability distributions speciﬁed using graphical models, the conditional distribu-\n",
      "tions for individual nodes depend only on the variables in the corresponding Markov\n",
      "blankets, as illustrated in Figure 11.12. For directed graphs, a wide choice of condi-\n",
      "tional distributions for the individual nodes conditioned on their parents will lead to\n",
      "conditional distributions for Gibbs sampling that are log concave. The adaptive re-\n",
      "jection sampling methods discussed in Section 11.1.3 therefore provide a framework\n",
      "for Monte Carlo sampling from directed graphs with broad applicability.\n",
      "If the graph is constructed using distributions from the exponential family, and\n",
      "if the parent-child relationships preserve conjugacy, then the full conditional distri-\n",
      "butions arising in Gibbs sampling will have the same functional form as the orig-546 11. SAMPLING METHODS\n",
      "Figure 11.12 The Gibbs sampling method requires samples\n",
      "to be drawn from the conditional distribution of a variable condi-\n",
      "tioned on the remaining variables. For graphical models, this\n",
      "conditional distribution is a function only of the states of thenodes in the Markov blanket. For an undirected graph this com-prises the set of neighbours, as shown on the left, while for adirected graph the Markov blanket comprises the parents, thechildren, and the co-parents, as shown on the right.\n",
      "inal conditional distributions (conditioned on the parents) deﬁning each node, and\n",
      "so standard sampling techniques can be employed. In general, the full conditional\n",
      "distributions will be of a complex form that does not permit the use of standard sam-\n",
      "pling algorithms. However, if these conditionals are log concave, then sampling canbe done efﬁciently using adaptive rejection sampling (assuming the corresponding\n",
      "variable is a scalar).\n",
      "If, at each stage of the Gibbs sampling algorithm, instead of drawing a sample\n",
      "from the corresponding conditional distribution, we make a point estimate of the\n",
      "variable given by the maximum of the conditional distribution, then we obtain the\n",
      "iterated conditional modes (ICM) algorithm discussed in Section 8.3.3. Thus ICMcan be seen as a greedy approximation to Gibbs sampling.\n",
      "Because the basic Gibbs sampling technique considers one variable at a time,\n",
      "there are strong dependencies between successive samples. At the opposite extreme,if we could draw samples directly from the joint distribution (an operation that we\n",
      "are supposing is intractable), then successive samples would be independent. We can\n",
      "hope to improve on the simple Gibbs sampler by adopting an intermediate strategy in\n",
      "which we sample successively from groups of variables rather than individual vari-\n",
      "ables. This is achieved in the blocking Gibbs sampling algorithm by choosing blocks\n",
      "of variables, not necessarily disjoint, and then sampling jointly from the variables in\n",
      "each block in turn, conditioned on the remaining variables (Jensen et al. , 1995).\n",
      "11.4. Slice Sampling\n",
      "We have seen that one of the difﬁculties with the Metropolis algorithm is the sensi-\n",
      "tivity to step size. If this is too small, the result is slow decorrelation due to random\n",
      "walk behaviour, whereas if it is too large the result is inefﬁciency due to a high rejec-\n",
      "tion rate. The technique of slice sampling (Neal, 2003) provides an adaptive step size\n",
      "that is automatically adjusted to match the characteristics of the distribution. Again\n",
      "it requires that we are able to evaluate the unnormalized distribution ˜p(z).\n",
      "Consider ﬁrst the univariate case. Slice sampling involves augmenting zwith\n",
      "an additional variable uand then drawing samples from the joint (z,u)space. We\n",
      "shall see another example of this approach when we discuss hybrid Monte Carlo in\n",
      "Section 11.5. The goal is to sample uniformly from the area under the distribution11.4. Slice Sampling 547\n",
      "˜p(z)\n",
      "z(τ) zu\n",
      "(a)˜p(z)\n",
      "z(τ) zu zmin zmax\n",
      "(b)\n",
      "Figure 11.13 Illustration of slice sampling. (a) For a given value z(τ), a value of uis chosen uniformly in\n",
      "the region 0⩽u⩽ep(z(τ)), which then deﬁnes a ‘slice’ through the distribution, shown by the solid horizontal\n",
      "lines. (b) Because it is infeasible to sample directly from a slice, a new sample of zis drawn from a region\n",
      "zmin⩽z⩽zmax, which contains the previous value z(τ).\n",
      "given by\n",
      "ˆp(z,u)={1/Zpif0⩽u⩽˜p(z)\n",
      "0 otherwise(11.51)\n",
      "where Zp=∫˜p(z)dz. The marginal distribution over zis given by\n",
      "∫\n",
      "ˆp(z,u)du=∫ep(z)\n",
      "01\n",
      "Zpdu=˜p(z)\n",
      "Zp=p(z) (11.52)\n",
      "and so we can sample from p(z)by sampling from ˆp(z,u)and then ignoring the u\n",
      "values. This can be achieved by alternately sampling zandu. Given the value of z\n",
      "we evaluate ˜p(z)and then sample uuniformly in the range 0⩽u⩽˜p(z), which is\n",
      "straightforward. Then we ﬁx uand sample zuniformly from the ‘slice’ through the\n",
      "distribution deﬁned by {z:˜p(z)>u}. This is illustrated in Figure 11.13(a).\n",
      "In practice, it can be difﬁcult to sample directly from a slice through the distribu-\n",
      "tion and so instead we deﬁne a sampling scheme that leaves the uniform distribution\n",
      "underˆp(z,u)invariant, which can be achieved by ensuring that detailed balance is\n",
      "satisﬁed. Suppose the current value of zis denoted z(τ)and that we have obtained\n",
      "a corresponding sample u. The next value of zis obtained by considering a region\n",
      "zmin⩽z⩽zmaxthat contains z(τ). It is in the choice of this region that the adap-\n",
      "tation to the characteristic length scales of the distribution takes place. We want the\n",
      "region to encompass as much of the slice as possible so as to allow large moves in z\n",
      "space while having as little as possible of this region lying outside the slice, because\n",
      "this makes the sampling less efﬁcient.\n",
      "One approach to the choice of region involves starting with a region containing\n",
      "z(τ)having some width wand then testing each of the end points to see if they lie\n",
      "within the slice. If either end point does not, then the region is extended in that\n",
      "direction by increments of value wuntil the end point lies outside the region. A\n",
      "candidate value z′is then chosen uniformly from this region, and if it lies within the\n",
      "slice, then it forms z(τ+1). If it lies outside the slice, then the region is shrunk such\n",
      "thatz′forms an end point and such that the region still contains z(τ). Then another548 11. SAMPLING METHODS\n",
      "candidate point is drawn uniformly from this reduced region and so on, until a value\n",
      "ofzis found that lies within the slice.\n",
      "Slice sampling can be applied to multivariate distributions by repeatedly sam-\n",
      "pling each variable in turn, in the manner of Gibbs sampling. This requires that\n",
      "we are able to compute, for each component zi, a function that is proportional to\n",
      "p(zi|z\\i).\n",
      "11.5. The Hybrid Monte Carlo Algorithm\n",
      "As we have already noted, one of the major limitations of the Metropolis algorithm\n",
      "is that it can exhibit random walk behaviour whereby the distance traversed through\n",
      "the state space grows only as the square root of the number of steps. The problem\n",
      "cannot be resolved simply by taking bigger steps as this leads to a high rejection rate.\n",
      "In this section, we introduce a more sophisticated class of transitions based on an\n",
      "analogy with physical systems and that has the property of being able to make large\n",
      "changes to the system state while keeping the rejection probability small. It is ap-\n",
      "plicable to distributions over continuous variables for which we can readily evaluate\n",
      "the gradient of the log probability with respect to the state variables. We will discussthe dynamical systems framework in Section 11.5.1, and then in Section 11.5.2 we\n",
      "explain how this may be combined with the Metropolis algorithm to yield the pow-\n",
      "erful hybrid Monte Carlo algorithm. A background in physics is not required as thissection is self-contained and the key results are all derived from ﬁrst principles.\n",
      "11.5.1 Dynamical systems\n",
      "The dynamical approach to stochastic sampling has its origins in algorithms for\n",
      "simulating the behaviour of physical systems evolving under Hamiltonian dynam-\n",
      "ics. In a Markov chain Monte Carlo simulation, the goal is to sample from a given\n",
      "probability distribution p(z). The framework of Hamiltonian dynamics is exploited\n",
      "by casting the probabilistic simulation in the form of a Hamiltonian system. In order\n",
      "to remain in keeping with the literature in this area, we make use of the relevantdynamical systems terminology where appropriate, which will be deﬁned as we go\n",
      "along.\n",
      "The dynamics that we consider corresponds to the evolution of the state variable\n",
      "z={z\n",
      "i}under continuous time, which we denote by τ. Classical dynamics is de-\n",
      "scribed by Newton’s second law of motion in which the acceleration of an object is\n",
      "proportional to the applied force, corresponding to a second-order differential equa-tion over time. We can decompose a second-order equation into two coupled ﬁrst-\n",
      "order equations by introducing intermediate momentum variables r, corresponding\n",
      "to the rate of change of the state variables z, having components\n",
      "r\n",
      "i=dzi\n",
      "dτ(11.53)\n",
      "where the zican be regarded as position variables in this dynamics perspective. Thus11.5. The Hybrid Monte Carlo Algorithm 549\n",
      "for each position variable there is a corresponding momentum variable, and the joint\n",
      "space of position and momentum variables is called phase space .\n",
      "Without loss of generality, we can write the probability distribution p(z)in the\n",
      "form\n",
      "p(z)=1\n",
      "Zpexp (−E(z)) (11.54)\n",
      "where E(z)is interpreted as the potential energy of the system when in state z. The\n",
      "system acceleration is the rate of change of momentum and is given by the appliedforce , which itself is the negative gradient of the potential energy\n",
      "dr\n",
      "i\n",
      "dτ=−∂E(z)\n",
      "∂zi. (11.55)\n",
      "It is convenient to reformulate this dynamical system using the Hamiltonian\n",
      "framework. To do this, we ﬁrst deﬁne the kinetic energy by\n",
      "K(r)=1\n",
      "2∥r∥2=1\n",
      "2∑\n",
      "ir2\n",
      "i. (11.56)\n",
      "The total energy of the system is then the sum of its potential and kinetic energies\n",
      "H(z,r)=E(z)+K(r) (11.57)\n",
      "where His the Hamiltonian function. Using (11.53), (11.55), (11.56), and (11.57),\n",
      "we can now express the dynamics of the system in terms of the Hamiltonian equa-\n",
      "tions given by Exercise 11.15\n",
      "dzi\n",
      "dτ=∂H\n",
      "∂ri(11.58)\n",
      "dri\n",
      "dτ=−∂H\n",
      "∂zi. (11.59)\n",
      "William Hamilton\n",
      "1805–1865\n",
      "William Rowan Hamilton was an\n",
      "Irish mathematician and physicist,and child prodigy, who was ap-pointed Professor of Astronomy atTrinity College, Dublin, in 1827, be-fore he had even graduated. One\n",
      "of Hamilton’s most important contributions was a newformulation of dynamics, which played a signiﬁcantrole in the later development of quantum mechanics.His other great achievement was the development of\n",
      "quaternions , which generalize the concept of complex\n",
      "numbers by introducing three distinct square roots ofminus one, which satisfy i\n",
      "2=j2=k2=ijk=−1.\n",
      "It is said that these equations occurred to him whilewalking along the Royal Canal in Dublin with his wife,on 16 October 1843, and he promptly carved theequations into the side of Broome bridge. Although\n",
      "there is no longer any evidence of the carving, there is\n",
      "now a stone plaque on the bridge commemorating thediscovery and displaying the quaternion equations.550 11. SAMPLING METHODS\n",
      "During the evolution of this dynamical system, the value of the Hamiltonian His\n",
      "constant, as is easily seen by differentiation\n",
      "dH\n",
      "dτ=∑\n",
      "i{∂H\n",
      "∂zidzi\n",
      "dτ+∂H\n",
      "∂ridri\n",
      "dτ}\n",
      "=∑\n",
      "i{∂H\n",
      "∂zi∂H\n",
      "∂ri−∂H\n",
      "∂ri∂H\n",
      "∂zi}\n",
      "=0. (11.60)\n",
      "A second important property of Hamiltonian dynamical systems, known as Li-\n",
      "ouville’s Theorem , is that they preserve volume in phase space. In other words, if\n",
      "we consider a region within the space of variables (z,r), then as this region evolves\n",
      "under the equations of Hamiltonian dynamics, its shape may change but its volume\n",
      "will not. This can be seen by noting that the ﬂow ﬁeld (rate of change of location in\n",
      "phase space) is given by\n",
      "V=(dz\n",
      "dτ,dr\n",
      "dτ)\n",
      "(11.61)\n",
      "and that the divergence of this ﬁeld vanishes\n",
      "divV=∑\n",
      "i{∂\n",
      "∂zidzi\n",
      "dτ+∂\n",
      "∂ridri\n",
      "dτ}\n",
      "=∑\n",
      "i{\n",
      "−∂\n",
      "∂zi∂H\n",
      "∂ri+∂\n",
      "∂ri∂H\n",
      "∂zi}\n",
      "=0. (11.62)\n",
      "Now consider the joint distribution over phase space whose total energy is the\n",
      "Hamiltonian, i.e., the distribution given by\n",
      "p(z,r)=1\n",
      "ZHexp(−H(z,r)). (11.63)\n",
      "Using the two results of conservation of volume and conservation of H, it follows\n",
      "that the Hamiltonian dynamics will leave p(z,r)invariant. This can be seen by\n",
      "considering a small region of phase space over which His approximately constant.\n",
      "If we follow the evolution of the Hamiltonian equations for a ﬁnite time, then the\n",
      "volume of this region will remain unchanged as will the value of Hin this region, and\n",
      "hence the probability density, which is a function only of H, will also be unchanged.\n",
      "Although His invariant, the values of zandrwill vary, and so by integrating\n",
      "the Hamiltonian dynamics over a ﬁnite time duration it becomes possible to make\n",
      "large changes to zin a systematic way that avoids random walk behaviour.\n",
      "Evolution under the Hamiltonian dynamics will not, however, sample ergodi-\n",
      "cally from p(z,r)because the value of His constant. In order to arrive at an ergodic\n",
      "sampling scheme, we can introduce additional moves in phase space that change\n",
      "the value of Hwhile also leaving the distribution p(z,r)invariant. The simplest\n",
      "way to achieve this is to replace the value of rwith one drawn from its distribution\n",
      "conditioned on z. This can be regarded as a Gibbs sampling step, and hence from11.5. The Hybrid Monte Carlo Algorithm 551\n",
      "Section 11.3 we see that this also leaves the desired distribution invariant. Noting\n",
      "thatzandrare independent in the distribution p(z,r), we see that the conditional\n",
      "distribution p(r|z)is a Gaussian from which it is straightforward to sample. Exercise 11.16\n",
      "In a practical application of this approach, we have to address the problem of\n",
      "performing a numerical integration of the Hamiltonian equations. This will neces-sarily introduce numerical errors and so we should devise a scheme that minimizes\n",
      "the impact of such errors. In fact, it turns out that integration schemes can be devised\n",
      "for which Liouville’s theorem still holds exactly. This property will be important inthe hybrid Monte Carlo algorithm, which is discussed in Section 11.5.2. One scheme\n",
      "for achieving this is called the leapfrog discretization and involves alternately updat-\n",
      "ing discrete-time approximations\n",
      "ˆzandˆrto the position and momentum variables\n",
      "using\n",
      "ˆri(τ+ϵ/2) =ˆri(τ)−ϵ\n",
      "2∂E\n",
      "∂zi(ˆz(τ)) (11.64)\n",
      "ˆzi(τ+ϵ)=ˆzi(τ)+ϵˆri(τ+ϵ/2) (11.65)\n",
      "ˆri(τ+ϵ)=ˆri(τ+ϵ/2)−ϵ\n",
      "2∂E\n",
      "∂zi(ˆz(τ+ϵ)). (11.66)\n",
      "We see that this takes the form of a half-step update of the momentum variables with\n",
      "step size ϵ/2, followed by a full-step update of the position variables with step size ϵ,\n",
      "followed by a second half-step update of the momentum variables. If several leapfrog\n",
      "steps are applied in succession, it can be seen that half-step updates to the momentumvariables can be combined into full-step updates with step size ϵ. The successive\n",
      "updates to position and momentum variables then leapfrog over each other. In order\n",
      "to advance the dynamics by a time interval τ, we need to take τ/ϵsteps. The error\n",
      "involved in the discretized approximation to the continuous time dynamics will go to\n",
      "zero, assuming a smooth function E(z), in the limit ϵ→0. However, for a nonzero\n",
      "ϵas used in practice, some residual error will remain. We shall see in Section 11.5.2\n",
      "how the effects of such errors can be eliminated in the hybrid Monte Carlo algorithm.\n",
      "In summary then, the Hamiltonian dynamical approach involves alternating be-\n",
      "tween a series of leapfrog updates and a resampling of the momentum variables fromtheir marginal distribution.\n",
      "Note that the Hamiltonian dynamics method, unlike the basic Metropolis algo-\n",
      "rithm, is able to make use of information about the gradient of the log probability\n",
      "distribution as well as about the distribution itself. An analogous situation is familiar\n",
      "from the domain of function optimization. In most cases where gradient informa-tion is available, it is highly advantageous to make use of it. Informally, this follows\n",
      "from the fact that in a space of dimension D, the additional computational cost of\n",
      "evaluating a gradient compared with evaluating the function itself will typically be aﬁxed factor independent of D, whereas the D-dimensional gradient vector conveys\n",
      "Dpieces of information compared with the one piece of information given by the\n",
      "function itself.552 11. SAMPLING METHODS\n",
      "11.5.2 Hybrid Monte Carlo\n",
      "As we discussed in the previous section, for a nonzero step size ϵ, the discretiza-\n",
      "tion of the leapfrog algorithm will introduce errors into the integration of the Hamil-\n",
      "tonian dynamical equations. Hybrid Monte Carlo (Duane et al. , 1987; Neal, 1996)\n",
      "combines Hamiltonian dynamics with the Metropolis algorithm and thereby removesany bias associated with the discretization.\n",
      "Speciﬁcally, the algorithm uses a Markov chain consisting of alternate stochastic\n",
      "updates of the momentum variable rand Hamiltonian dynamical updates using the\n",
      "leapfrog algorithm. After each application of the leapfrog algorithm, the resulting\n",
      "candidate state is accepted or rejected according to the Metropolis criterion basedon the value of the Hamiltonian H. Thus if (z,r)is the initial state and (z\n",
      "⋆,r⋆)\n",
      "is the state after the leapfrog integration, then this candidate state is accepted with\n",
      "probability\n",
      "min(1 ,exp{H(z,r)−H(z⋆,r⋆)}). (11.67)\n",
      "If the leapfrog integration were to simulate the Hamiltonian dynamics perfectly,\n",
      "then every such candidate step would automatically be accepted because the value\n",
      "ofHwould be unchanged. Due to numerical errors, the value of Hmay sometimes\n",
      "decrease, and we would like the Metropolis criterion to remove any bias due to thiseffect and ensure that the resulting samples are indeed drawn from the required dis-\n",
      "tribution. In order for this to be the case, we need to ensure that the update equations\n",
      "corresponding to the leapfrog integration satisfy detailed balance (11.40). This iseasily achieved by modifying the leapfrog scheme as follows.\n",
      "Before the start of each leapfrog integration sequence, we choose at random,\n",
      "with equal probability, whether to integrate forwards in time (using step size ϵ)o r\n",
      "backwards in time (using step size −ϵ). We ﬁrst note that the leapfrog integration\n",
      "scheme (11.64), (11.65), and (11.66) is time-reversible, so that integration for Lsteps\n",
      "using step size −ϵwill exactly undo the effect of integration for Lsteps using step\n",
      "sizeϵ. Next we show that the leapfrog integration preserves phase-space volume\n",
      "exactly. This follows from the fact that each step in the leapfrog scheme updates\n",
      "either a z\n",
      "ivariable or an rivariable by an amount that is a function only of the other\n",
      "variable. As shown in Figure 11.14, this has the effect of shearing a region of phase\n",
      "space while not altering its volume.\n",
      "Finally, we use these results to show that detailed balance holds. Consider a\n",
      "small region Rof phase space that, under a sequence of Lleapfrog iterations of\n",
      "step size ϵ, maps to a region R′. Using conservation of volume under the leapfrog\n",
      "iteration, we see that if Rhas volume δVthen so too will R′. If we choose an initial\n",
      "point from the distribution (11.63) and then update it using Lleapfrog interactions,\n",
      "the probability of the transition going from RtoR′is given by\n",
      "1\n",
      "ZHexp(−H(R))δV1\n",
      "2min{1,exp(−H(R)+H(R′))}. (11.68)\n",
      "where the factor of 1/2arises from the probability of choosing to integrate with a\n",
      "positive step size rather than a negative one. Similarly, the probability of starting in11.5. The Hybrid Monte Carlo Algorithm 553\n",
      "ri\n",
      "zir′\n",
      "i\n",
      "z′\n",
      "i\n",
      "Figure 11.14 Each step of the leapfrog algorithm (11.64)–(11.66) modiﬁes either a position variable zior a\n",
      "momentum variable ri. Because the change to one variable is a function only of the other, any region in phase\n",
      "space will be sheared without change of volume.\n",
      "region R′and integrating backwards in time to end up in region Ris given by\n",
      "1\n",
      "ZHexp(−H(R′))δV1\n",
      "2min{1,exp(−H(R′)+H(R))}. (11.69)\n",
      "It is easily seen that the two probabilities (11.68) and (11.69) are equal, and hence\n",
      "detailed balance holds. Note that this proof ignores any overlap between the regions Exercise 11.17\n",
      "RandR′but is easily generalized to allow for such overlap.\n",
      "It is not difﬁcult to construct examples for which the leapfrog algorithm returns\n",
      "to its starting position after a ﬁnite number of iterations. In such cases, the random\n",
      "replacement of the momentum values before each leapfrog integration will not be\n",
      "sufﬁcient to ensure ergodicity because the position variables will never be updated.\n",
      "Such phenomena are easily avoided by choosing the magnitude of the step size at\n",
      "random from some small interval, before each leapfrog integration.\n",
      "We can gain some insight into the behaviour of the hybrid Monte Carlo algo-\n",
      "rithm by considering its application to a multivariate Gaussian. For convenience,\n",
      "consider a Gaussian distribution p(z)with independent components, for which the\n",
      "Hamiltonian is given by\n",
      "H(z,r)=1\n",
      "2∑\n",
      "i1\n",
      "σ2\n",
      "iz2\n",
      "i+1\n",
      "2∑\n",
      "ir2\n",
      "i. (11.70)\n",
      "Our conclusions will be equally valid for a Gaussian distribution having correlated\n",
      "components because the hybrid Monte Carlo algorithm exhibits rotational isotropy.\n",
      "During the leapfrog integration, each pair of phase-space variables zi,rievolves in-\n",
      "dependently. However, the acceptance or rejection of the candidate point is based\n",
      "on the value of H, which depends on the values of all of the variables. Thus, a\n",
      "signiﬁcant integration error in any one of the variables could lead to a high prob-\n",
      "ability of rejection. In order that the discrete leapfrog integration be a reasonably554 11. SAMPLING METHODS\n",
      "good approximation to the true continuous-time dynamics, it is necessary for the\n",
      "leapfrog integration scale ϵto be smaller than the shortest length-scale over which\n",
      "the potential is varying signiﬁcantly. This is governed by the smallest value of σi,\n",
      "which we denote by σmin. Recall that the goal of the leapfrog integration in hybrid\n",
      "Monte Carlo is to move a substantial distance through phase space to a new statethat is relatively independent of the initial state and still achieve a high probability of\n",
      "acceptance. In order to achieve this, the leapfrog integration must be continued for a\n",
      "number of iterations of order σ\n",
      "max/σmin.\n",
      "By contrast, consider the behaviour of a simple Metropolis algorithm with an\n",
      "isotropic Gaussian proposal distribution of variance s2, considered earlier. In order\n",
      "to avoid high rejection rates, the value of smust be of order σmin. The exploration of\n",
      "state space then proceeds by a random walk and takes of order (σmax/σmin)2steps\n",
      "to arrive at a roughly independent state.\n",
      "11.6. Estimating the Partition Function\n",
      "As we have seen, most of the sampling algorithms considered in this chapter re-quire only the functional form of the probability distribution up to a multiplicativeconstant. Thus if we write\n",
      "p\n",
      "E(z)=1\n",
      "ZEexp(−E(z)) (11.71)\n",
      "then the value of the normalization constant ZE, also known as the partition func-\n",
      "tion, is not needed in order to draw samples from p(z). However, knowledge of the\n",
      "value of ZEcan be useful for Bayesian model comparison since it represents the\n",
      "model evidence (i.e., the probability of the observed data given the model), and so\n",
      "it is of interest to consider how its value might be obtained. We assume that direct\n",
      "evaluation by summing, or integrating, the function exp(−E(z))over the state space\n",
      "ofzis intractable.\n",
      "For model comparison, it is actually the ratio of the partition functions for two\n",
      "models that is required. Multiplication of this ratio by the ratio of prior probabilitiesgives the ratio of posterior probabilities, which can then be used for model selection\n",
      "or model averaging.\n",
      "One way to estimate a ratio of partition functions is to use importance sampling\n",
      "from a distribution with energy function G(z)\n",
      "Z\n",
      "E\n",
      "ZG=∑\n",
      "zexp(−E(z))∑\n",
      "zexp(−G(z))\n",
      "=∑\n",
      "zexp(−E(z)+G(z)) exp( −G(z))∑\n",
      "zexp(−G(z))\n",
      "= EG(z)[exp(−E+G)]\n",
      "≃∑\n",
      "lexp(−E(z(l))+G(z(l))) (11.72)11.6. Estimating the Partition Function 555\n",
      "where {z(l)}are samples drawn from the distribution deﬁned by pG(z). If the dis-\n",
      "tribution pGis one for which the partition function can be evaluated analytically, for\n",
      "example a Gaussian, then the absolute value of ZEcan be obtained.\n",
      "This approach will only yield accurate results if the importance sampling distri-\n",
      "bution pGis closely matched to the distribution pE, so that the ratio pE/pGdoes not\n",
      "have wide variations. In practice, suitable analytically speciﬁed importance sampling\n",
      "distributions cannot readily be found for the kinds of complex models considered in\n",
      "this book.\n",
      "An alternative approach is therefore to use the samples obtained from a Markov\n",
      "chain to deﬁne the importance-sampling distribution. If the transition probability for\n",
      "the Markov chain is given by T(z,z′), and the sample set is given by z(1),...,z(L),\n",
      "then the sampling distribution can be written as\n",
      "1\n",
      "ZGexp (−G(z)) =L∑\n",
      "l=1T(z(l),z) (11.73)\n",
      "which can be used directly in (11.72).\n",
      "Methods for estimating the ratio of two partition functions require for their suc-\n",
      "cess that the two corresponding distributions be reasonably closely matched. This isespecially problematic if we wish to ﬁnd the absolute value of the partition function\n",
      "for a complex distribution because it is only for relatively simple distributions that\n",
      "the partition function can be evaluated directly, and so attempting to estimate the\n",
      "ratio of partition functions directly is unlikely to be successful. This problem can be\n",
      "tackled using a technique known as chaining (Neal, 1993; Barber and Bishop, 1997),\n",
      "which involves introducing a succession of intermediate distributions p\n",
      "2,...,p M−1\n",
      "that interpolate between a simple distribution p1(z)for which we can evaluate the\n",
      "normalization coefﬁcient Z1and the desired complex distribution pM(z). We then\n",
      "haveZM\n",
      "Z1=Z2\n",
      "Z1Z3\n",
      "Z2···ZM\n",
      "ZM−1(11.74)\n",
      "in which the intermediate ratios can be determined using Monte Carlo methods as\n",
      "discussed above. One way to construct such a sequence of intermediate systems\n",
      "is to use an energy function containing a continuous parameter 0⩽α⩽1that\n",
      "interpolates between the two distributions\n",
      "Eα(z)=( 1 −α)E1(z)+αEM(z). (11.75)\n",
      "If the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be\n",
      "more efﬁcient to use a single Markov chain run than to restart the Markov chain foreach ratio. In this case, the Markov chain is run initially for the system p\n",
      "1and then\n",
      "after some suitable number of steps moves on to the next distribution in the sequence.\n",
      "Note, however, that the system must remain close to the equilibrium distribution ateach stage.556 11. SAMPLING METHODS\n",
      "Exercises\n",
      "11.1 (⋆)www Show that the ﬁnite sample estimator ˆfdeﬁned by (11.2) has mean\n",
      "equal to E[f]and variance given by (11.3).\n",
      "11.2 (⋆)Suppose that zis a random variable with uniform distribution over (0,1)and\n",
      "that we transform zusingy=h−1(z)where h(y)is given by (11.6). Show that y\n",
      "has the distribution p(y).\n",
      "11.3 (⋆)Given a random variable zthat is uniformly distributed over (0,1), ﬁnd a trans-\n",
      "formation y=f(z)such that yhas a Cauchy distribution given by (11.8).\n",
      "11.4 (⋆⋆)Suppose that z1andz2are uniformly distributed over the unit circle, as\n",
      "shown in Figure 11.3, and that we make the change of variables given by (11.10)\n",
      "and (11.11). Show that (y1,y2)will be distributed according to (11.12).\n",
      "11.5 (⋆)www Letzbe aD-dimensional random variable having a Gaussian distribu-\n",
      "tion with zero mean and unit covariance matrix, and suppose that the positive deﬁnite\n",
      "symmetric matrix Σhas the Cholesky decomposition Σ=LLTwhereLis a lower-\n",
      "triangular matrix (i.e., one with zeros above the leading diagonal). Show that thevariable y=µ+Lzhas a Gaussian distribution with mean µand covariance Σ.\n",
      "This provides a technique for generating samples from a general multivariate Gaus-\n",
      "sian using samples from a univariate Gaussian having zero mean and unit variance.\n",
      "11.6 (⋆⋆)\n",
      "www In this exercise, we show more carefully that rejection sampling does\n",
      "indeed draw samples from the desired distribution p(z). Suppose the proposal dis-\n",
      "tribution is q(z)and show that the probability of a sample value zbeing accepted is\n",
      "given by˜p(z)/kq(z)where˜pis any unnormalized distribution that is proportional to\n",
      "p(z), and the constant kis set to the smallest value that ensures kq(z)⩾˜p(z)for all\n",
      "values of z. Note that the probability of drawing a value zis given by the probability\n",
      "of drawing that value from q(z)times the probability of accepting that value given\n",
      "that it has been drawn. Make use of this, along with the sum and product rules of\n",
      "probability, to write down the normalized form for the distribution over z, and show\n",
      "that it equals p(z).\n",
      "11.7 (⋆)Suppose that zhas a uniform distribution over the interval [0,1]. Show that the\n",
      "variable y=btanz+chas a Cauchy distribution given by (11.16).\n",
      "11.8 (⋆⋆)Determine expressions for the coefﬁcients kiin the envelope distribution\n",
      "(11.17) for adaptive rejection sampling using the requirements of continuity and nor-malization.\n",
      "11.9 (⋆⋆)By making use of the technique discussed in Section 11.1.1 for sampling\n",
      "from a single exponential distribution, devise an algorithm for sampling from the\n",
      "piecewise exponential distribution deﬁned by (11.17).\n",
      "11.10 (⋆)Show that the simple random walk over the integers deﬁned by (11.34), (11.35),\n",
      "and (11.36) has the property that E[(z\n",
      "(τ))2]= E[(z(τ−1))2]+1/2and hence by\n",
      "induction that E[(z(τ))2]=τ/2.Exercises 557\n",
      "Figure 11.15 A probability distribution over two variables z1\n",
      "andz2that is uniform over the shaded regions\n",
      "and that is zero everywhere else.\n",
      "z1z2\n",
      "11.11 (⋆⋆)www Show that the Gibbs sampling algorithm, discussed in Section 11.3,\n",
      "satisﬁes detailed balance as deﬁned by (11.40).\n",
      "11.12 (⋆)Consider the distribution shown in Figure 11.15. Discuss whether the standard\n",
      "Gibbs sampling procedure for this distribution is ergodic, and therefore whether it\n",
      "would sample correctly from this distribution\n",
      "11.13 (⋆⋆)Consider the simple 3-node graph shown in Figure 11.16 in which the observed\n",
      "nodexis given by a Gaussian distribution N(x|µ, τ−1)with mean µand precision\n",
      "τ. Suppose that the marginal distributions over the mean and precision are given\n",
      "byN(µ|µ0,s0)andGam( τ|a, b), where Gam(·|·,·)denotes a gamma distribution.\n",
      "Write down expressions for the conditional distributions p(µ|x, τ)andp(τ|x, µ)that\n",
      "would be required in order to apply Gibbs sampling to the posterior distribution\n",
      "p(µ, τ|x).\n",
      "11.14 (⋆)Verify that the over-relaxation update (11.50), in which zihas mean µiand\n",
      "variance σi, and where νhas zero mean and unit variance, gives a value z′\n",
      "iwith\n",
      "meanµiand variance σ2\n",
      "i.\n",
      "11.15 (⋆)www Using (11.56) and (11.57), show that the Hamiltonian equation (11.58)\n",
      "is equivalent to (11.53). Similarly, using (11.57) show that (11.59) is equivalent to\n",
      "(11.55).\n",
      "11.16 (⋆)By making use of (11.56), (11.57), and (11.63), show that the conditional dis-\n",
      "tribution p(r|z)is a Gaussian.\n",
      "Figure 11.16 A graph involving an observed Gaussian variable xwith\n",
      "prior distributions over its mean µand precision τ.µ τ\n",
      "x558 11. SAMPLING METHODS\n",
      "11.17 (⋆)www Verify that the two probabilities (11.68) and (11.69) are equal, and hence\n",
      "that detailed balance holds for the hybrid Monte Carlo algorithm.12\n",
      "Continuous\n",
      "Latent\n",
      "Variables\n",
      "In Chapter 9, we discussed probabilistic models having discrete latent variables, such\n",
      "as the mixture of Gaussians. We now explore models in which some, or all, of thelatent variables are continuous. An important motivation for such models is that\n",
      "many data sets have the property that the data points all lie close to a manifold of\n",
      "much lower dimensionality than that of the original data space. To see why thismight arise, consider an artiﬁcial data set constructed by taking one of the off-line\n",
      "digits, represented by a 64×64pixel grey-level image, and embedding it in a larger Appendix A\n",
      "image of size 100×100by padding with pixels having the value zero (corresponding\n",
      "to white pixels) in which the location and orientation of the digit is varied at random,\n",
      "as illustrated in Figure 12.1. Each of the resulting images is represented by a point in\n",
      "the100×100 = 10 ,000-dimensional data space. However, across a data set of such\n",
      "images, there are only three degrees of freedom of variability, corresponding to the\n",
      "vertical and horizontal translations and the rotations. The data points will therefore\n",
      "live on a subspace of the data space whose intrinsic dimensionality is three. Note\n",
      "559560 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.1 A synthetic data set obtained by taking one of the off-line digit images and creating multi-\n",
      "ple copies in each of which the digit has undergone a random displacement and rotationwithin some larger image ﬁeld. The resulting images each have 100×100 = 10 ,000\n",
      "pixels.\n",
      "that the manifold will be nonlinear because, for instance, if we translate the digit\n",
      "past a particular pixel, that pixel value will go from zero (white) to one (black) and\n",
      "back to zero again, which is clearly a nonlinear function of the digit position. Inthis example, the translation and rotation parameters are latent variables because we\n",
      "observe only the image vectors and are not told which values of the translation or\n",
      "rotation variables were used to create them.\n",
      "For real digit image data, there will be a further degree of freedom arising from\n",
      "scaling. Moreover there will be multiple additional degrees of freedom associated\n",
      "with more complex deformations due to the variability in an individual’s writingas well as the differences in writing styles between individuals. Nevertheless, the\n",
      "number of such degrees of freedom will be small compared to the dimensionality of\n",
      "the data set.\n",
      "Another example is provided by the oil ﬂow data set, in which (for a given ge- Appendix A\n",
      "ometrical conﬁguration of the gas, water, and oil phases) there are only two degrees\n",
      "of freedom of variability corresponding to the fraction of oil in the pipe and the frac-\n",
      "tion of water (the fraction of gas then being determined). Although the data space\n",
      "comprises 12 measurements, a data set of points will lie close to a two-dimensionalmanifold embedded within this space. In this case, the manifold comprises several\n",
      "distinct segments corresponding to different ﬂow regimes, each such segment being\n",
      "a (noisy) continuous two-dimensional manifold. If our goal is data compression, ordensity modelling, then there can be beneﬁts in exploiting this manifold structure.\n",
      "In practice, the data points will not be conﬁned precisely to a smooth low-\n",
      "dimensional manifold, and we can interpret the departures of data points from themanifold as ‘noise’. This leads naturally to a generative view of such models in\n",
      "which we ﬁrst select a point within the manifold according to some latent variable\n",
      "distribution and then generate an observed data point by adding noise, drawn fromsome conditional distribution of the data variables given the latent variables.\n",
      "The simplest continuous latent variable model assumes Gaussian distributions\n",
      "for both the latent and observed variables and makes use of a linear-Gaussian de-pendence of the observed variables on the state of the latent variables. This leads Section 8.1.4\n",
      "to a probabilistic formulation of the well-known technique of principal component\n",
      "analysis (PCA), as well as to a related model called factor analysis.\n",
      "In this chapter w will begin with a standard, nonprobabilistic treatment of PCA, Section 12.1\n",
      "and then we show how PCA arises naturally as the maximum likelihood solution to12.1. Principal Component Analysis 561\n",
      "Figure 12.2 Principal component analysis seeks a space\n",
      "of lower dimensionality, known as the princi-\n",
      "pal subspace and denoted by the magenta\n",
      "line, such that the orthogonal projection of\n",
      "the data points (red dots) onto this subspace\n",
      "maximizes the variance of the projected points\n",
      "(green dots). An alternative deﬁnition of PCA\n",
      "is based on minimizing the sum-of-squares\n",
      "of the projection errors, indicated by the blue\n",
      "lines.x2\n",
      "x1xn\n",
      "˜xnu1\n",
      "a particular form of linear-Gaussian latent variable model. This probabilistic refor- Section 12.2\n",
      "mulation brings many advantages, such as the use of EM for parameter estimation,\n",
      "principled extensions to mixtures of PCA models, and Bayesian formulations that\n",
      "allow the number of principal components to be determined automatically from the\n",
      "data. Finally, we discuss brieﬂy several generalizations of the latent variable concept\n",
      "that go beyond the linear-Gaussian assumption including non-Gaussian latent vari-\n",
      "ables, which leads to the framework of independent component analysis , as well as\n",
      "models having a nonlinear relationship between latent and observed variables. Section 12.4\n",
      "12.1. Principal Component Analysis\n",
      "Principal component analysis, or PCA, is a technique that is widely used for appli-\n",
      "cations such as dimensionality reduction, lossy data compression, feature extraction,\n",
      "and data visualization (Jolliffe, 2002). It is also known as the Karhunen-Lo `evetrans-\n",
      "form.\n",
      "There are two commonly used deﬁnitions of PCA that give rise to the same\n",
      "algorithm. PCA can be deﬁned as the orthogonal projection of the data onto a lower\n",
      "dimensional linear space, known as the principal subspace , such that the variance of\n",
      "the projected data is maximized (Hotelling, 1933). Equivalently, it can be deﬁned as\n",
      "the linear projection that minimizes the average projection cost, deﬁned as the mean\n",
      "squared distance between the data points and their projections (Pearson, 1901). The\n",
      "process of orthogonal projection is illustrated in Figure 12.2. We consider each of\n",
      "these deﬁnitions in turn.\n",
      "12.1.1 Maximum variance formulation\n",
      "Consider a data set of observations {xn}where n=1,...,N , andxnis a\n",
      "Euclidean variable with dimensionality D. Our goal is to project the data onto a\n",
      "space having dimensionality M<D while maximizing the variance of the projected\n",
      "data. For the moment, we shall assume that the value of Mis given. Later in this562 12. CONTINUOUS LATENT V ARIABLES\n",
      "chapter, we shall consider techniques to determine an appropriate value of Mfrom\n",
      "the data.\n",
      "To begin with, consider the projection onto a one-dimensional space ( M=1).\n",
      "We can deﬁne the direction of this space using a D-dimensional vector u1, which\n",
      "for convenience (and without loss of generality) we shall choose to be a unit vectorso that u\n",
      "T\n",
      "1u1=1 (note that we are only interested in the direction deﬁned by u1,\n",
      "not in the magnitude of u1itself). Each data point xnis then projected onto a scalar\n",
      "valueuT\n",
      "1xn. The mean of the projected data is uT\n",
      "1xwherexis the sample set mean\n",
      "given by\n",
      "x=1\n",
      "NN∑\n",
      "n=1xn (12.1)\n",
      "and the variance of the projected data is given by\n",
      "1\n",
      "NN∑\n",
      "n=1{\n",
      "uT\n",
      "1xn−uT\n",
      "1x}2=uT\n",
      "1Su1 (12.2)\n",
      "whereSis the data covariance matrix deﬁned by\n",
      "S=1\n",
      "NN∑\n",
      "n=1(xn−x)(xn−x)T. (12.3)\n",
      "We now maximize the projected variance uT\n",
      "1Su1with respect to u1. Clearly, this has\n",
      "to be a constrained maximization to prevent ∥u1∥→∞ . The appropriate constraint\n",
      "comes from the normalization condition uT\n",
      "1u1=1. To enforce this constraint,\n",
      "we introduce a Lagrange multiplier that we shall denote by λ1, and then make an Appendix E\n",
      "unconstrained maximization of\n",
      "uT\n",
      "1Su1+λ1(\n",
      "1−uT\n",
      "1u1)\n",
      ". (12.4)\n",
      "By setting the derivative with respect to u1equal to zero, we see that this quantity\n",
      "will have a stationary point when\n",
      "Su1=λ1u1 (12.5)\n",
      "which says that u1must be an eigenvector of S. If we left-multiply by uT\n",
      "1and make\n",
      "use ofuT\n",
      "1u1=1, we see that the variance is given by\n",
      "uT\n",
      "1Su1=λ1 (12.6)\n",
      "and so the variance will be a maximum when we set u1equal to the eigenvector\n",
      "having the largest eigenvalue λ1. This eigenvector is known as the ﬁrst principal\n",
      "component.\n",
      "We can deﬁne additional principal components in an incremental fashion by\n",
      "choosing each new direction to be that which maximizes the projected variance12.1. Principal Component Analysis 563\n",
      "amongst all possible directions orthogonal to those already considered. If we con-\n",
      "sider the general case of an M-dimensional projection space, the optimal linear pro-\n",
      "jection for which the variance of the projected data is maximized is now deﬁned by\n",
      "theMeigenvectors u1,...,uMof the data covariance matrix Scorresponding to the\n",
      "Mlargest eigenvalues λ1,...,λ M. This is easily shown using proof by induction. Exercise 12.1\n",
      "To summarize, principal component analysis involves evaluating the mean x\n",
      "and the covariance matrix Sof the data set and then ﬁnding the Meigenvectors of S\n",
      "corresponding to the Mlargest eigenvalues. Algorithms for ﬁnding eigenvectors and\n",
      "eigenvalues, as well as additional theorems related to eigenvector decomposition,\n",
      "can be found in Golub and Van Loan (1996). Note that the computational cost of\n",
      "computing the full eigenvector decomposition for a matrix of size D×DisO(D3).\n",
      "If we plan to project our data onto the ﬁrst Mprincipal components, then we only\n",
      "need to ﬁnd the ﬁrst Meigenvalues and eigenvectors. This can be done with more\n",
      "efﬁcient techniques, such as the power method (Golub and Van Loan, 1996), that\n",
      "scale like O(MD2), or alternatively we can make use of the EM algorithm. Section 12.2.2\n",
      "12.1.2 Minimum-error formulation\n",
      "We now discuss an alternative formulation of PCA based on projection error\n",
      "minimization. To do this, we introduce a complete orthonormal set of D-dimensional Appendix C\n",
      "basis vectors {ui}where i=1,...,D that satisfy\n",
      "uT\n",
      "iuj=δij. (12.7)\n",
      "Because this basis is complete, each data point can be represented exactly by a linear\n",
      "combination of the basis vectors\n",
      "xn=D∑\n",
      "i=1αniui (12.8)\n",
      "where the coefﬁcients αniwill be different for different data points. This simply\n",
      "corresponds to a rotation of the coordinate system to a new system deﬁned by the\n",
      "{ui}, and the original Dcomponents {xn1,...,x nD}are replaced by an equivalent\n",
      "set{αn1,...,α nD}. Taking the inner product with uj, and making use of the or-\n",
      "thonormality property, we obtain αnj=xT\n",
      "nuj, and so without loss of generality we\n",
      "can write\n",
      "xn=D∑\n",
      "i=1(\n",
      "xT\n",
      "nui)\n",
      "ui. (12.9)\n",
      "Our goal, however, is to approximate this data point using a representation in-\n",
      "volving a restricted number M<D of variables corresponding to a projection onto\n",
      "a lower-dimensional subspace. The M-dimensional linear subspace can be repre-\n",
      "sented, without loss of generality, by the ﬁrst Mof the basis vectors, and so we\n",
      "approximate each data point xnby\n",
      "˜xn=M∑\n",
      "i=1zniui+D∑\n",
      "i=M+1biui (12.10)564 12. CONTINUOUS LATENT V ARIABLES\n",
      "where the {zni}depend on the particular data point, whereas the {bi}are constants\n",
      "that are the same for all data points. We are free to choose the {ui}, the{zni}, and\n",
      "the{bi}so as to minimize the distortion introduced by the reduction in dimensional-\n",
      "ity. As our distortion measure, we shall use the squared distance between the original\n",
      "data point xnand its approximation ˜xn, averaged over the data set, so that our goal\n",
      "is to minimize\n",
      "J=1\n",
      "NN∑\n",
      "n=1∥xn−˜xn∥2. (12.11)\n",
      "Consider ﬁrst of all the minimization with respect to the quantities {zni}. Sub-\n",
      "stituting for ˜xn, setting the derivative with respect to znjto zero, and making use of\n",
      "the orthonormality conditions, we obtain\n",
      "znj=xT\n",
      "nuj (12.12)\n",
      "where j=1,...,M . Similarly, setting the derivative of Jwith respect to bito zero,\n",
      "and again making use of the orthonormality relations, gives\n",
      "bj=xTuj (12.13)\n",
      "where j=M+1,...,D . If we substitute for zniandbi, and make use of the general\n",
      "expansion (12.9), we obtain\n",
      "xn−˜xn=D∑\n",
      "i=M+1{\n",
      "(xn−x)Tui}\n",
      "ui (12.14)\n",
      "from which we see that the displacement vector from xnto˜xnlies in the space\n",
      "orthogonal to the principal subspace, because it is a linear combination of {ui}for\n",
      "i=M+1,...,D , as illustrated in Figure 12.2. This is to be expected because the\n",
      "projected points ˜xnmust lie within the principal subspace, but we can move them\n",
      "freely within that subspace, and so the minimum error is given by the orthogonal\n",
      "projection.\n",
      "We therefore obtain an expression for the distortion measure Jas a function\n",
      "purely of the {ui}in the form\n",
      "J=1\n",
      "NN∑\n",
      "n=1D∑\n",
      "i=M+1(\n",
      "xT\n",
      "nui−xTui)2=D∑\n",
      "i=M+1uT\n",
      "iSui. (12.15)\n",
      "There remains the task of minimizing Jwith respect to the {ui}, which must\n",
      "be a constrained minimization otherwise we will obtain the vacuous result ui=0.\n",
      "The constraints arise from the orthonormality conditions and, as we shall see, the\n",
      "solution will be expressed in terms of the eigenvector expansion of the covariance\n",
      "matrix. Before considering a formal solution, let us try to obtain some intuition aboutthe result by considering the case of a two-dimensional data space D=2and a one-\n",
      "dimensional principal subspace M=1. We have to choose a direction u\n",
      "2so as to12.1. Principal Component Analysis 565\n",
      "minimize J=uT\n",
      "2Su2, subject to the normalization constraint uT\n",
      "2u2=1. Using a\n",
      "Lagrange multiplier λ2to enforce the constraint, we consider the minimization of\n",
      "˜J=uT\n",
      "2Su2+λ2(\n",
      "1−uT\n",
      "2u2)\n",
      ". (12.16)\n",
      "Setting the derivative with respect to u2to zero, we obtain Su2=λ2u2so that u2\n",
      "is an eigenvector of Swith eigenvalue λ2. Thus any eigenvector will deﬁne a sta-\n",
      "tionary point of the distortion measure. To ﬁnd the value of Jat the minimum, we\n",
      "back-substitute the solution for u2into the distortion measure to give J=λ2.W e\n",
      "therefore obtain the minimum value of Jby choosing u2to be the eigenvector corre-\n",
      "sponding to the smaller of the two eigenvalues. Thus we should choose the principal\n",
      "subspace to be aligned with the eigenvector having the larger eigenvalue. This result\n",
      "accords with our intuition that, in order to minimize the average squared projection\n",
      "distance, we should choose the principal component subspace to pass through the\n",
      "mean of the data points and to be aligned with the directions of maximum variance.For the case when the eigenvalues are equal, any choice of principal direction will\n",
      "give rise to the same value of J.\n",
      "The general solution to the minimization of Jfor arbitrary Dand arbitrary M< Exercise 12.2\n",
      "Dis obtained by choosing the {u\n",
      "i}to be eigenvectors of the covariance matrix given\n",
      "by\n",
      "Sui=λiui (12.17)\n",
      "where i=1,...,D , and as usual the eigenvectors {ui}are chosen to be orthonor-\n",
      "mal. The corresponding value of the distortion measure is then given by\n",
      "J=D∑\n",
      "i=M+1λi (12.18)\n",
      "which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal\n",
      "to the principal subspace. We therefore obtain the minimum value of Jby selecting\n",
      "these eigenvectors to be those having the D−Msmallest eigenvalues, and hence\n",
      "the eigenvectors deﬁning the principal subspace are those corresponding to the M\n",
      "largest eigenvalues.\n",
      "Although we have considered M<D , the PCA analysis still holds if M=\n",
      "D, in which case there is no dimensionality reduction but simply a rotation of the\n",
      "coordinate axes to align with principal components.\n",
      "Finally, it is worth noting that there exists a closely related linear dimensionality\n",
      "reduction technique called canonical correlation analysis ,o r CCA (Hotelling, 1936;\n",
      "Bach and Jordan, 2002). Whereas PCA works with a single random variable, CCA\n",
      "considers two (or more) variables and tries to ﬁnd a corresponding pair of linearsubspaces that have high cross-correlation, so that each component within one of the\n",
      "subspaces is correlated with a single component from the other subspace. Its solution\n",
      "can be expressed in terms of a generalized eigenvector problem.\n",
      "12.1.3 Applications of PCA\n",
      "We can illustrate the use of PCA for data compression by considering the off-\n",
      "line digits data set. Because each eigenvector of the covariance matrix is a vector Appendix A566 12. CONTINUOUS LATENT V ARIABLES\n",
      "Mean\n",
      " λ1=3.4·105\n",
      "λ2=2.8·105\n",
      "λ3=2.4·105\n",
      "λ4=1.6·105\n",
      "Figure 12.3 The mean vector xalong with the ﬁrst four PCA eigenvectors u1,...,u4for the off-line\n",
      "digits data set, together with the corresponding eigenvalues.\n",
      "in the original D-dimensional space, we can represent the eigenvectors as images of\n",
      "the same size as the data points. The ﬁrst ﬁve eigenvectors, along with the corre-\n",
      "sponding eigenvalues, are shown in Figure 12.3. A plot of the complete spectrum of\n",
      "eigenvalues, sorted into decreasing order, is shown in Figure 12.4(a). The distortion\n",
      "measure Jassociated with choosing a particular value of Mis given by the sum\n",
      "of the eigenvalues from M+1up toDand is plotted for different values of Min\n",
      "Figure 12.4(b).\n",
      "If we substitute (12.12) and (12.13) into (12.10), we can write the PCA approx-\n",
      "imation to a data vector xnin the form\n",
      "˜xn=M∑\n",
      "i=1(xT\n",
      "nui)ui+D∑\n",
      "i=M+1(xTui)ui (12.19)\n",
      "=x+M∑\n",
      "i=1(\n",
      "xT\n",
      "nui−xTui)\n",
      "ui (12.20)\n",
      "iλi\n",
      "(a)0 200 400 6000123x 105\n",
      "MJ\n",
      "(b)0 200 400 6000123x 106\n",
      "Figure 12.4 (a) Plot of the eigenvalue spectrum for the off-line digits data set. (b) Plot of the sum of the\n",
      "discarded eigenvalues, which represents the sum-of-squares distortion Jintroduced by projecting the data onto\n",
      "a principal component subspace of dimensionality M.12.1. Principal Component Analysis 567\n",
      "Original\n",
      " M=1\n",
      " M=1 0\n",
      " M=5 0\n",
      " M= 250\n",
      "Figure 12.5 An original example from the off-line digits data set together with its PCA reconstructions\n",
      "obtained by retaining Mprincipal components for various values of M.A sMincreases\n",
      "the reconstruction becomes more accurate and would become perfect when M=D=\n",
      "28×28 = 784 .\n",
      "where we have made use of the relation\n",
      "x=D∑\n",
      "i=1(\n",
      "xTui)\n",
      "ui (12.21)\n",
      "which follows from the completeness of the {ui}. This represents a compression\n",
      "of the data set, because for each data point we have replaced the D-dimensional\n",
      "vectorxnwith an M-dimensional vector having components(\n",
      "xT\n",
      "nui−xTui)\n",
      ". The\n",
      "smaller the value of M, the greater the degree of compression. Examples of PCA\n",
      "reconstructions of data points for the digits data set are shown in Figure 12.5.\n",
      "Another application of principal component analysis is to data pre-processing.\n",
      "In this case, the goal is not dimensionality reduction but rather the transformation of\n",
      "a data set in order to standardize certain of its properties. This can be important in\n",
      "allowing subsequent pattern recognition algorithms to be applied successfully to the\n",
      "data set. Typically, it is done when the original variables are measured in various dif-\n",
      "ferent units or have signiﬁcantly different variability. For instance in the Old Faithful\n",
      "data set, the time between eruptions is typically an order of magnitude greater than Appendix A\n",
      "the duration of an eruption. When we applied the K-means algorithm to this data\n",
      "set, we ﬁrst made a separate linear re-scaling of the individual variables such that Section 9.1\n",
      "each variable had zero mean and unit variance. This is known as standardizing the\n",
      "data, and the covariance matrix for the standardized data has components\n",
      "ρij=1\n",
      "NN∑\n",
      "n=1(xni−xi)\n",
      "σi(xnj−xj)\n",
      "σj(12.22)\n",
      "where σiis the variance of xi. This is known as the correlation matrix of the original\n",
      "data and has the property that if two components xiandxjof the data are perfectly\n",
      "correlated, then ρij=1, and if they are uncorrelated, then ρij=0.\n",
      "However, using PCA we can make a more substantial normalization of the data\n",
      "to give it zero mean and unit covariance, so that different variables become decorre-\n",
      "lated. To do this, we ﬁrst write the eigenvector equation (12.17) in the form\n",
      "SU=UL (12.23)568 12. CONTINUOUS LATENT V ARIABLES\n",
      "2 4 6405060708090100\n",
      "−2 0 2−202\n",
      "−2 0 2−202\n",
      "Figure 12.6 Illustration of the effects of linear pre-processing applied to the Old Faithful data set. The plot on\n",
      "the left shows the original data. The centre plot shows the result of standardizing the individual variables to zero\n",
      "mean and unit variance. Also shown are the principal axes of this normalized data set, plotted over the range\n",
      "±λ1/2\n",
      "i. The plot on the right shows the result of whitening of the data to give it zero mean and unit covariance.\n",
      "whereLis aD×Ddiagonal matrix with elements λi, andUis aD×Dorthog-\n",
      "onal matrix with columns given by ui. Then we deﬁne, for each data point xn,a\n",
      "transformed value given by\n",
      "yn=L−1/2UT(xn−x) (12.24)\n",
      "wherexis the sample mean deﬁned by (12.1). Clearly, the set {yn}has zero mean,\n",
      "and its covariance is given by the identity matrix because\n",
      "1\n",
      "NN∑\n",
      "n=1ynyT\n",
      "n=1\n",
      "NN∑\n",
      "n=1L−1/2UT(xn−x)(xn−x)TUL−1/2\n",
      "=L−1/2UTSUL−1/2=L−1/2LL−1/2=I. (12.25)\n",
      "This operation is known as whitening orsphereing the data and is illustrated for the\n",
      "Old Faithful data set in Figure 12.6. Appendix A\n",
      "It is interesting to compare PCA with the Fisher linear discriminant which was\n",
      "discussed in Section 4.1.4. Both methods can be viewed as techniques for linear\n",
      "dimensionality reduction. However, PCA is unsupervised and depends only on the\n",
      "valuesxnwhereas Fisher linear discriminant also uses class-label information. This\n",
      "difference is highlighted by the example in Figure 12.7.\n",
      "Another common application of principal component analysis is to data visual-\n",
      "ization. Here each data point is projected onto a two-dimensional (M=2 ) principal\n",
      "subspace, so that a data point xnis plotted at Cartesian coordinates given by xT\n",
      "nu1\n",
      "andxT\n",
      "nu2, where u1andu2are the eigenvectors corresponding to the largest and\n",
      "second largest eigenvalues. An example of such a plot, for the oil ﬂow data set, is Appendix A\n",
      "shown in Figure 12.8.12.1. Principal Component Analysis 569\n",
      "Figure 12.7 A comparison of principal compo-\n",
      "nent analysis with Fisher’s linear\n",
      "discriminant for linear dimension-\n",
      "ality reduction. Here the data in\n",
      "two dimensions, belonging to two\n",
      "classes shown in red and blue, is\n",
      "to be projected onto a single di-\n",
      "mension. PCA chooses the direc-\n",
      "tion of maximum variance, shown\n",
      "by the magenta curve, which leads\n",
      "to strong class overlap, whereas\n",
      "the Fisher linear discriminant takes\n",
      "account of the class labels and\n",
      "leads to a projection onto the green\n",
      "curve giving much better class\n",
      "separation.−5 0 5−2−1.5−1−0.500.511.5\n",
      "Figure 12.8 Visualization of the oil ﬂow data set obtained\n",
      "by projecting the data onto the ﬁrst two prin-\n",
      "cipal components. The red, blue, and green\n",
      "points correspond to the ‘laminar’, ‘homo-\n",
      "geneous’, and ‘annular’ ﬂow conﬁgurations\n",
      "respectively.\n",
      "12.1.4 PCA for high-dimensional data\n",
      "In some applications of principal component analysis, the number of data points\n",
      "is smaller than the dimensionality of the data space. For example, we might want to\n",
      "apply PCA to a data set of a few hundred images, each of which corresponds to a\n",
      "vector in a space of potentially several million dimensions (corresponding to three\n",
      "colour values for each of the pixels in the image). Note that in a D-dimensional space\n",
      "a set of Npoints, where N<D , deﬁnes a linear subspace whose dimensionality\n",
      "is at most N−1, and so there is little point in applying PCA for values of M\n",
      "that are greater than N−1. Indeed, if we perform PCA we will ﬁnd that at least\n",
      "D−N+1of the eigenvalues are zero, corresponding to eigenvectors along whose\n",
      "directions the data set has zero variance. Furthermore, typical algorithms for ﬁnding\n",
      "the eigenvectors of a D×Dmatrix have a computational cost that scales like O(D3),\n",
      "and so for applications such as the image example, a direct application of PCA will\n",
      "be computationally infeasible.\n",
      "We can resolve this problem as follows. First, let us deﬁne Xto be the (N×D)-570 12. CONTINUOUS LATENT V ARIABLES\n",
      "dimensional centred data matrix, whose nthr o wi sg i v e nb y (xn−x)T. The covari-\n",
      "ance matrix (12.3) can then be written as S=N−1XTX, and the corresponding\n",
      "eigenvector equation becomes\n",
      "1\n",
      "NXTXui=λiui. (12.26)\n",
      "Now pre-multiply both sides by Xto give\n",
      "1\n",
      "NXXT(Xui)=λi(Xui). (12.27)\n",
      "If we now deﬁne vi=Xui, we obtain\n",
      "1\n",
      "NXXTvi=λivi (12.28)\n",
      "which is an eigenvector equation for the N×Nmatrix N−1XXT. We see that this\n",
      "has the same N−1eigenvalues as the original covariance matrix (which itself has an\n",
      "additional D−N+1eigenvalues of value zero). Thus we can solve the eigenvector\n",
      "problem in spaces of lower dimensionality with computational cost O(N3)instead\n",
      "ofO(D3). In order to determine the eigenvectors, we multiply both sides of (12.28)\n",
      "byXTto give(1\n",
      "NXTX)\n",
      "(XTvi)=λi(XTvi) (12.29)\n",
      "from which we see that (XTvi)is an eigenvector of Swith eigenvalue λi. Note,\n",
      "however, that these eigenvectors need not be normalized. To determine the appropri-\n",
      "ate normalization, we re-scale ui∝XTviby a constant such that ∥ui∥=1, which,\n",
      "assuming vihas been normalized to unit length, gives\n",
      "ui=1\n",
      "(Nλi)1/2XTvi. (12.30)\n",
      "In summary, to apply this approach we ﬁrst evaluate XXTand then ﬁnd its eigen-\n",
      "vectors and eigenvalues and then compute the eigenvectors in the original data space\n",
      "using (12.30).\n",
      "12.2. Probabilistic PCA\n",
      "The formulation of PCA discussed in the previous section was based on a linear\n",
      "projection of the data onto a subspace of lower dimensionality than the original dataspace. We now show that PCA can also be expressed as the maximum likelihood\n",
      "solution of a probabilistic latent variable model. This reformulation of PCA, known\n",
      "asprobabilistic PCA , brings several advantages compared with conventional PCA:\n",
      "•Probabilistic PCA represents a constrained form of the Gaussian distribution\n",
      "in which the number of free parameters can be restricted while still allowing\n",
      "the model to capture the dominant correlations in a data set.12.2. Probabilistic PCA 571\n",
      "•We can derive an EM algorithm for PCA that is computationally efﬁcient in\n",
      "situations where only a few leading eigenvectors are required and that avoidshaving to evaluate the data covariance matrix as an intermediate step. Section 12.2.2\n",
      "•The combination of a probabilistic model and EM allows us to deal with miss-\n",
      "ing values in the data set.\n",
      "•Mixtures of probabilistic PCA models can be formulated in a principled way\n",
      "and trained using the EM algorithm.\n",
      "•Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which\n",
      "the dimensionality of the principal subspace can be found automatically from\n",
      "the data. Section 12.2.3\n",
      "•The existence of a likelihood function allows direct comparison with other\n",
      "probabilistic density models. By contrast, conventional PCA will assign a low\n",
      "reconstruction cost to data points that are close to the principal subspace even\n",
      "if they lie arbitrarily far from the training data.\n",
      "•Probabilistic PCA can be used to model class-conditional densities and hence\n",
      "be applied to classiﬁcation problems.\n",
      "•The probabilistic PCA model can be run generatively to provide samples from\n",
      "the distribution.\n",
      "This formulation of PCA as a probabilistic model was proposed independently by\n",
      "Tipping and Bishop (1997, 1999b) and by Roweis (1998). As we shall see later, it is\n",
      "closely related to factor analysis (Basilevsky, 1994).\n",
      "Probabilistic PCA is a simple example of the linear-Gaussian framework, in Section 8.1.4\n",
      "which all of the marginal and conditional distributions are Gaussian. We can formu-\n",
      "late probabilistic PCA by ﬁrst introducing an explicit latent variable zcorresponding\n",
      "to the principal-component subspace. Next we deﬁne a Gaussian prior distributionp(z)over the latent variable, together with a Gaussian conditional distribution p(x|z)\n",
      "for the observed variable xconditioned on the value of the latent variable. Speciﬁ-\n",
      "cally, the prior distribution over zis given by a zero-mean unit-covariance Gaussian\n",
      "p(z)=N(z|0,I). (12.31)\n",
      "Similarly, the conditional distribution of the observed variable x, conditioned on the\n",
      "value of the latent variable z, is again Gaussian, of the form\n",
      "p(x|z)=N(x|Wz+µ,σ\n",
      "2I) (12.32)\n",
      "in which the mean of xis a general linear function of zgoverned by the D×M\n",
      "matrixWand the D-dimensional vector µ. Note that this factorizes with respect to\n",
      "the elements of x, in other words this is an example of the naive Bayes model. As Section 8.2.2\n",
      "we shall see shortly, the columns of Wspan a linear subspace within the data space\n",
      "that corresponds to the principal subspace. The other parameter in this model is the\n",
      "scalar σ2governing the variance of the conditional distribution. Note that there is no572 12. CONTINUOUS LATENT V ARIABLES\n",
      "zp(z)\n",
      "ˆzx2\n",
      "x1µp(x|ˆz)\n",
      "}ˆz|w|wx2\n",
      "x1µ\n",
      "p(x)\n",
      "Figure 12.9 An illustration of the generative view of the probabilistic PCA model for a two-dimensional data\n",
      "space and a one-dimensional latent space. An observed data point xis generated by ﬁrst drawing a value bz\n",
      "for the latent variable from its prior distribution p(z)and then drawing a value for xfrom an isotropic Gaussian\n",
      "distribution (illustrated by the red circles) having mean wbz+µand covariance σ2I. The green ellipses show the\n",
      "density contours for the marginal distribution p(x).\n",
      "loss of generality in assuming a zero mean, unit covariance Gaussian for the latent\n",
      "distribution p(z)because a more general Gaussian distribution would give rise to an\n",
      "equivalent probabilistic model. Exercise 12.4\n",
      "We can view the probabilistic PCA model from a generative viewpoint in which\n",
      "a sampled value of the observed variable is obtained by ﬁrst choosing a value for\n",
      "the latent variable and then sampling the observed variable conditioned on this la-\n",
      "tent value. Speciﬁcally, the D-dimensional observed variable xis deﬁned by a lin-\n",
      "ear transformation of the M-dimensional latent variable zplus additive Gaussian\n",
      "‘noise’, so that\n",
      "x=Wz+µ+ϵ (12.33)\n",
      "wherezis anM-dimensional Gaussian latent variable, and ϵis aD-dimensional\n",
      "zero-mean Gaussian-distributed noise variable with covariance σ2I. This generative\n",
      "process is illustrated in Figure 12.9. Note that this framework is based on a mapping\n",
      "from latent space to data space, in contrast to the more conventional view of PCA\n",
      "discussed above. The reverse mapping, from data space to the latent space, will be\n",
      "obtained shortly using Bayes’ theorem.\n",
      "Suppose we wish to determine the values of the parameters W,µandσ2using\n",
      "maximum likelihood. To write down the likelihood function, we need an expression\n",
      "for the marginal distribution p(x)of the observed variable. This is expressed, from\n",
      "the sum and product rules of probability, in the form\n",
      "p(x)=∫\n",
      "p(x|z)p(z)dz. (12.34)\n",
      "Because this corresponds to a linear-Gaussian model, this marginal distribution is\n",
      "again Gaussian, and is given by Exercise 12.7\n",
      "p(x)=N(x|µ,C) (12.35)12.2. Probabilistic PCA 573\n",
      "where the D×Dcovariance matrix Cis deﬁned by\n",
      "C=WWT+σ2I. (12.36)\n",
      "This result can also be derived more directly by noting that the predictive distribution\n",
      "will be Gaussian and then evaluating its mean and covariance using (12.33). This\n",
      "gives\n",
      "E[x]= E[Wz+µ+ϵ]=µ (12.37)\n",
      "cov[x]= E[\n",
      "(Wz+ϵ)(Wz+ϵ)T]\n",
      "= E[\n",
      "WzzTWT]\n",
      "+E[ϵϵT]=WWT+σ2I (12.38)\n",
      "where we have used the fact that zandϵare independent random variables and hence\n",
      "are uncorrelated.\n",
      "Intuitively, we can think of the distribution p(x)as being deﬁned by taking an\n",
      "isotropic Gaussian ‘spray can’ and moving it across the principal subspace sprayingGaussian ink with density determined by σ\n",
      "2and weighted by the prior distribution.\n",
      "The accumulated ink density gives rise to a ‘pancake’ shaped distribution represent-\n",
      "ing the marginal density p(x).\n",
      "The predictive distribution p(x)is governed by the parameters µ,W, andσ2.\n",
      "However, there is redundancy in this parameterization corresponding to rotations of\n",
      "the latent space coordinates. To see this, consider a matrix ˜W=WR whereRis\n",
      "an orthogonal matrix. Using the orthogonality property RRT=I, we see that the\n",
      "quantity˜W˜WTthat appears in the covariance matrix Ctakes the form\n",
      "˜W˜WT=WRRTWT=WWT(12.39)\n",
      "and hence is independent of R. Thus there is a whole family of matrices ˜Wall of\n",
      "which give rise to the same predictive distribution. This invariance can be understoodin terms of rotations within the latent space. We shall return to a discussion of the\n",
      "number of independent parameters in this model later.\n",
      "When we evaluate the predictive distribution, we require C\n",
      "−1, which involves\n",
      "the inversion of a D×Dmatrix. The computation required to do this can be reduced\n",
      "by making use of the matrix inversion identity (C.7) to give\n",
      "C−1=σ−1I−σ−2WM−1WT(12.40)\n",
      "where the M×MmatrixMis deﬁned by\n",
      "M=WTW+σ2I. (12.41)\n",
      "Because we invert Mrather than inverting Cdirectly, the cost of evaluating C−1is\n",
      "reduced from O(D3)toO(M3).\n",
      "As well as the predictive distribution p(x), we will also require the posterior\n",
      "distribution p(z|x), which can again be written down directly using the result (2.116)\n",
      "for linear-Gaussian models to give Exercise 12.8\n",
      "p(z|x)=N(\n",
      "z|M−1WT(x−µ),σ−2M)\n",
      ". (12.42)\n",
      "Note that the posterior mean depends on x, whereas the posterior covariance is in-\n",
      "dependent of x.574 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.10 The probabilistic PCA model for a data set of Nobser-\n",
      "vations of xcan be expressed as a directed graph in\n",
      "which each observation xnis associated with a value\n",
      "znof the latent variable.\n",
      "xnzn\n",
      "Nµσ2\n",
      "W\n",
      "12.2.1 Maximum likelihood PCA\n",
      "We next consider the determination of the model parameters using maximum\n",
      "likelihood. Given a data set X={xn}of observed data points, the probabilistic\n",
      "PCA model can be expressed as a directed graph, as shown in Figure 12.10. The\n",
      "corresponding log likelihood function is given, from (12.35), by\n",
      "lnp(X|µ,W,σ2)=N∑\n",
      "n=1lnp(xn|W,µ,σ2)\n",
      "=−ND\n",
      "2ln(2π)−N\n",
      "2ln|C|−1\n",
      "2N∑\n",
      "n=1(xn−µ)TC−1(xn−µ).(12.43)\n",
      "Setting the derivative of the log likelihood with respect to µequal to zero gives the\n",
      "expected result µ=xwherexis the data mean deﬁned by (12.1). Back-substituting\n",
      "we can then write the log likelihood function in the form\n",
      "lnp(X|W,µ,σ2)=−N\n",
      "2{\n",
      "Dln(2π)+l n|C|+Tr(\n",
      "C−1S)}\n",
      "(12.44)\n",
      "whereSis the data covariance matrix deﬁned by (12.3). Because the log likelihood\n",
      "is a quadratic function of µ, this solution represents the unique maximum, as can be\n",
      "conﬁrmed by computing second derivatives.\n",
      "Maximization with respect to Wandσ2is more complex but nonetheless has\n",
      "an exact closed-form solution. It was shown by Tipping and Bishop (1999b) that all\n",
      "of the stationary points of the log likelihood function can be written as\n",
      "WML=UM(LM−σ2I)1/2R (12.45)\n",
      "whereUMis aD×Mmatrix whose columns are given by any subset (of size M)\n",
      "of the eigenvectors of the data covariance matrix S, theM×Mdiagonal matrix\n",
      "LMhas elements given by the corresponding eigenvalues λi, andRis an arbitrary\n",
      "M×Morthogonal matrix.\n",
      "Furthermore, Tipping and Bishop (1999b) showed that the maximum of the like-\n",
      "lihood function is obtained when the Meigenvectors are chosen to be those whose\n",
      "eigenvalues are the Mlargest (all other solutions being saddle points). A similar re-\n",
      "sult was conjectured independently by Roweis (1998), although no proof was given.12.2. Probabilistic PCA 575\n",
      "Again, we shall assume that the eigenvectors have been arranged in order of decreas-\n",
      "ing values of the corresponding eigenvalues, so that the Mprincipal eigenvectors are\n",
      "u1,...,uM. In this case, the columns of Wdeﬁne the principal subspace of stan-\n",
      "dard PCA. The corresponding maximum likelihood solution for σ2is then given by\n",
      "σ2\n",
      "ML=1\n",
      "D−MD∑\n",
      "i=M+1λi (12.46)\n",
      "so that σ2\n",
      "MLis the average variance associated with the discarded dimensions.\n",
      "Because Ris orthogonal, it can be interpreted as a rotation matrix in the M×M\n",
      "latent space. If we substitute the solution for Winto the expression for C, and make\n",
      "use of the orthogonality property RRT=I, we see that Cis independent of R.\n",
      "This simply says that the predictive density is unchanged by rotations in the latent\n",
      "space as discussed earlier. For the particular case of R=I, we see that the columns\n",
      "ofWare the principal component eigenvectors scaled by the variance parameters\n",
      "λi−σ2. The interpretation of these scaling factors is clear once we recognize that\n",
      "for a convolution of independent Gaussian distributions (in this case the latent spacedistribution and the noise model) the variances are additive. Thus the variance λ\n",
      "i\n",
      "in the direction of an eigenvector uiis composed of the sum of a contribution λi−\n",
      "σ2from the projection of the unit-variance latent space distribution into data space\n",
      "through the corresponding column of W, plus an isotropic contribution of variance\n",
      "σ2which is added in all directions by the noise model.\n",
      "It is worth taking a moment to study the form of the covariance matrix given\n",
      "by (12.36). Consider the variance of the predictive distribution along some direction\n",
      "speciﬁed by the unit vector v, where vTv=1, which is given by vTCv. First\n",
      "suppose that vis orthogonal to the principal subspace, in other words it is given by\n",
      "some linear combination of the discarded eigenvectors. Then vTU=0and hence\n",
      "vTCv=σ2. Thus the model predicts a noise variance orthogonal to the principal\n",
      "subspace, which, from (12.46), is just the average of the discarded eigenvalues. Now\n",
      "suppose that v=uiwhereuiis one of the retained eigenvectors deﬁning the prin-\n",
      "cipal subspace. Then vTCv=(λi−σ2)+σ2=λi. In other words, this model\n",
      "correctly captures the variance of the data along the principal axes, and approximates\n",
      "the variance in all remaining directions with a single average value σ2.\n",
      "One way to construct the maximum likelihood density model would simply be\n",
      "to ﬁnd the eigenvectors and eigenvalues of the data covariance matrix and then to\n",
      "evaluate Wandσ2using the results given above. In this case, we would choose\n",
      "R=Ifor convenience. However, if the maximum likelihood solution is found by\n",
      "numerical optimization of the likelihood function, for instance using an algorithm\n",
      "such as conjugate gradients (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and\n",
      "Nabney, 2008) or through the EM algorithm, then the resulting value of Ris es- Section 12.2.2\n",
      "sentially arbitrary. This implies that the columns of Wneed not be orthogonal. If\n",
      "an orthogonal basis is required, the matrix Wcan be post-processed appropriately\n",
      "(Golub and Van Loan, 1996). Alternatively, the EM algorithm can be modiﬁed in\n",
      "such a way as to yield orthonormal principal directions, sorted in descending order\n",
      "of the corresponding eigenvalues, directly (Ahn and Oh, 2003).576 12. CONTINUOUS LATENT V ARIABLES\n",
      "The rotational invariance in latent space represents a form of statistical noniden-\n",
      "tiﬁability, analogous to that encountered for mixture models in the case of discretelatent variables. Here there is a continuum of parameters all of which lead to the\n",
      "same predictive density, in contrast to the discrete nonidentiﬁability associated with\n",
      "component re-labelling in the mixture setting.\n",
      "If we consider the case of M=D, so that there is no reduction of dimension-\n",
      "ality, then U\n",
      "M=UandLM=L. Making use of the orthogonality properties\n",
      "UUT=IandRRT=I, we see that the covariance Cof the marginal distribution\n",
      "forxbecomes\n",
      "C=U(L−σ2I)1/2RRT(L−σ2I)1/2UT+σ2I=ULUT=S (12.47)\n",
      "and so we obtain the standard maximum likelihood solution for an unconstrained\n",
      "Gaussian distribution in which the covariance matrix is given by the sample covari-\n",
      "ance.\n",
      "Conventional PCA is generally formulated as a projection of points from the D-\n",
      "dimensional data space onto an M-dimensional linear subspace. Probabilistic PCA,\n",
      "however, is most naturally expressed as a mapping from the latent space into the dataspace via (12.33). For applications such as visualization and data compression, we\n",
      "can reverse this mapping using Bayes’ theorem. Any point xin data space can then\n",
      "be summarized by its posterior mean and covariance in latent space. From (12.42)the mean is given by\n",
      "E[z|x]=M\n",
      "−1WT\n",
      "ML(x−x) (12.48)\n",
      "whereMis given by (12.41). This projects to a point in data space given by\n",
      "WE[z|x]+µ. (12.49)\n",
      "Note that this takes the same form as the equations for regularized linear regression Section 3.3.1\n",
      "and is a consequence of maximizing the likelihood function for a linear Gaussianmodel. Similarly, the posterior covariance is given from (12.42) by σ\n",
      "2M−1and is\n",
      "independent of x.\n",
      "If we take the limit σ2→0, then the posterior mean reduces to\n",
      "(WT\n",
      "MLWML)−1WT\n",
      "ML(x−x) (12.50)\n",
      "which represents an orthogonal projection of the data point onto the latent space,\n",
      "and so we recover the standard PCA model. The posterior covariance in this limit is Exercise 12.11\n",
      "zero, however, and the density becomes singular. For σ2>0, the latent projection\n",
      "is shifted towards the origin, relative to the orthogonal projection. Exercise 12.12\n",
      "Finally, we note that an important role for the probabilistic PCA model is in\n",
      "deﬁning a multivariate Gaussian distribution in which the number of degrees of free-\n",
      "dom, in other words the number of independent parameters, can be controlled whilststill allowing the model to capture the dominant correlations in the data. Recall\n",
      "that a general Gaussian distribution has D(D+1 )/2independent parameters in its\n",
      "covariance matrix (plus another Dparameters in its mean). Thus the number of Section 2.3\n",
      "parameters scales quadratically with Dand can become excessive in spaces of high12.2. Probabilistic PCA 577\n",
      "dimensionality. If we restrict the covariance matrix to be diagonal, then it has only D\n",
      "independent parameters, and so the number of parameters now grows linearly withdimensionality. However, it now treats the variables as if they were independent and\n",
      "hence can no longer express any correlations between them. Probabilistic PCA pro-\n",
      "vides an elegant compromise in which the Mmost signiﬁcant correlations can be\n",
      "captured while still ensuring that the total number of parameters grows only linearly\n",
      "withD. We can see this by evaluating the number of degrees of freedom in the\n",
      "PPCA model as follows. The covariance matrix Cdepends on the parameters W,\n",
      "which has size D×M, andσ\n",
      "2, giving a total parameter count of DM+1. However,\n",
      "we have seen that there is some redundancy in this parameterization associated with\n",
      "rotations of the coordinate system in the latent space. The orthogonal matrix Rthat\n",
      "expresses these rotations has size M×M. In the ﬁrst column of this matrix there are\n",
      "M−1independent parameters, because the column vector must be normalized to\n",
      "unit length. In the second column there are M−2independent parameters, because\n",
      "the column must be normalized and also must be orthogonal to the previous column,\n",
      "and so on. Summing this arithmetic series, we see that Rhas a total of M(M−1)/2\n",
      "independent parameters. Thus the number of degrees of freedom in the covariance\n",
      "matrixCis given by\n",
      "DM+1−M(M−1)/2. (12.51)\n",
      "The number of independent parameters in this model therefore only grows linearly\n",
      "withD, for ﬁxed M. I fw et a k e M=D−1, then we recover the standard result\n",
      "for a full covariance Gaussian. In this case, the variance along D−1linearly in- Exercise 12.14\n",
      "dependent directions is controlled by the columns of W, and the variance along the\n",
      "remaining direction is given by σ2.I fM=0, the model is equivalent to the isotropic\n",
      "covariance case.\n",
      "12.2.2 EM algorithm for PCA\n",
      "As we have seen, the probabilistic PCA model can be expressed in terms of a\n",
      "marginalization over a continuous latent space zin which for each data point xn,\n",
      "there is a corresponding latent variable zn. We can therefore make use of the EM\n",
      "algorithm to ﬁnd maximum likelihood estimates of the model parameters. This may\n",
      "seem rather pointless because we have already obtained an exact closed-form so-\n",
      "lution for the maximum likelihood parameter values. However, in spaces of highdimensionality, there may be computational advantages in using an iterative EM\n",
      "procedure rather than working directly with the sample covariance matrix. This EM\n",
      "procedure can also be extended to the factor analysis model, for which there is no Section 12.2.4\n",
      "closed-form solution. Finally, it allows missing data to be handled in a principled\n",
      "way.\n",
      "We can derive the EM algorithm for probabilistic PCA by following the general\n",
      "framework for EM. Thus we write down the complete-data log likelihood and take Section 9.4\n",
      "its expectation with respect to the posterior distribution of the latent distribution\n",
      "evaluated using ‘old’ parameter values. Maximization of this expected complete-data log likelihood then yields the ‘new’ parameter values. Because the data points578 12. CONTINUOUS LATENT V ARIABLES\n",
      "are assumed independent, the complete-data log likelihood function takes the form\n",
      "lnp(\n",
      "X,Z|µ,W,σ2)\n",
      "=N∑\n",
      "n=1{lnp(xn|zn)+l n p(zn)} (12.52)\n",
      "where the nthrow of the matrix Zis given by zn. We already know that the exact\n",
      "maximum likelihood solution for µis given by the sample mean xdeﬁned by (12.1),\n",
      "and it is convenient to substitute for µat this stage. Making use of the expressions\n",
      "(12.31) and (12.32) for the latent and conditional distributions, respectively, and tak-\n",
      "ing the expectation with respect to the posterior distribution over the latent variables,\n",
      "we obtain\n",
      "E[lnp(\n",
      "X,Z|µ,W,σ2)\n",
      "]=−N∑\n",
      "n=1{D\n",
      "2ln(2πσ2)+1\n",
      "2Tr(\n",
      "E[znzT\n",
      "n])\n",
      "+1\n",
      "2σ2∥xn−µ∥2−1\n",
      "σ2E[zn]TWT(xn−µ)\n",
      "+1\n",
      "2σ2Tr(\n",
      "E[znzT\n",
      "n]WTW)}\n",
      ". (12.53)\n",
      "Note that this depends on the posterior distribution only through the sufﬁcient statis-\n",
      "tics of the Gaussian. Thus in the E step, we use the old parameter values to evaluate\n",
      "E[zn]= M−1WT(xn−x) (12.54)\n",
      "E[znzT\n",
      "n]= σ2M−1+E[zn]E[zn]T(12.55)\n",
      "which follow directly from the posterior distribution (12.42) together with the stan-\n",
      "dard result E[znzT\n",
      "n]=c o v [ zn]+ E[zn]E[zn]T. HereMis deﬁned by (12.41).\n",
      "In the M step, we maximize with respect to Wandσ2, keeping the posterior\n",
      "statistics ﬁxed. Maximization with respect to σ2is straightforward. For the maxi-\n",
      "mization with respect to Wwe make use of (C.24), and obtain the M-step equations Exercise 12.15\n",
      "Wnew=[N∑\n",
      "n=1(xn−x)E[zn]T][N∑\n",
      "n=1E[znzT\n",
      "n]]−1\n",
      "(12.56)\n",
      "σ2\n",
      "new=1\n",
      "NDN∑\n",
      "n=1{\n",
      "∥xn−x∥2−2E[zn]TWT\n",
      "new(xn−x)\n",
      "+Tr(\n",
      "E[znzT\n",
      "n]WT\n",
      "newWnew)}\n",
      ". (12.57)\n",
      "The EM algorithm for probabilistic PCA proceeds by initializing the parameters\n",
      "and then alternately computing the sufﬁcient statistics of the latent space posterior\n",
      "distribution using (12.54) and (12.55) in the E step and revising the parameter values\n",
      "using (12.56) and (12.57) in the M step.\n",
      "One of the beneﬁts of the EM algorithm for PCA is computational efﬁciency\n",
      "for large-scale applications (Roweis, 1998). Unlike conventional PCA based on an12.2. Probabilistic PCA 579\n",
      "eigenvector decomposition of the sample covariance matrix, the EM approach is\n",
      "iterative and so might appear to be less attractive. However, each cycle of the EMalgorithm can be computationally much more efﬁcient than conventional PCA in\n",
      "spaces of high dimensionality. To see this, we note that the eigendecomposition of\n",
      "the covariance matrix requires O(D\n",
      "3)computation. Often we are interested only\n",
      "in the ﬁrst Meigenvectors and their corresponding eigenvalues, in which case we\n",
      "can use algorithms that are O(MD2). However, the evaluation of the covariance\n",
      "matrix itself takes O(ND2)computations, where Nis the number of data points.\n",
      "Algorithms such as the snapshot method (Sirovich, 1987), which assume that the\n",
      "eigenvectors are linear combinations of the data vectors, avoid direct evaluation of\n",
      "the covariance matrix but are O(N3)and hence unsuited to large data sets. The EM\n",
      "algorithm described here also does not construct the covariance matrix explicitly.\n",
      "Instead, the most computationally demanding steps are those involving sums overthe data set that are O(NDM ). For large D, andM≪D, this can be a signiﬁcant\n",
      "saving compared to O(ND\n",
      "2)and can offset the iterative nature of the EM algorithm.\n",
      "Note that this EM algorithm can be implemented in an on-line form in which\n",
      "eachD-dimensional data point is read in and processed and then discarded before\n",
      "the next data point is considered. To see this, note that the quantities evaluated in\n",
      "the E step (an M-dimensional vector and an M×Mmatrix) can be computed for\n",
      "each data point separately, and in the M step we need to accumulate sums over data\n",
      "points, which we can do incrementally. This approach can be advantageous if both\n",
      "NandDare large.\n",
      "Because we now have a fully probabilistic model for PCA, we can deal with\n",
      "missing data, provided that it is missing at random, by marginalizing over the dis-\n",
      "tribution of the unobserved variables. Again these missing values can be treatedusing the EM algorithm. We give an example of the use of this approach for data\n",
      "visualization in Figure 12.11.\n",
      "Another elegant feature of the EM approach is that we can take the limit σ\n",
      "2→0,\n",
      "corresponding to standard PCA, and still obtain a valid EM-like algorithm (Roweis,\n",
      "1998). From (12.55), we see that the only quantity we need to compute in the E stepisE[z\n",
      "n]. Furthermore, the M step is simpliﬁed because M=WTW. To emphasize\n",
      "the simplicity of the algorithm, let us deﬁne ˜Xto be a matrix of size N×Dwhose\n",
      "nthrow is given by the vector xn−xand similarly deﬁne Ωto be a matrix of size\n",
      "D×Mwhose nthrow is given by the vector E[zn]. The E step (12.54) of the EM\n",
      "algorithm for PCA then becomes\n",
      "Ω=(WT\n",
      "oldWold)−1WT\n",
      "old˜X (12.58)\n",
      "and the M step (12.56) takes the form\n",
      "Wnew=˜XTΩT(ΩΩT)−1. (12.59)\n",
      "Again these can be implemented in an on-line form. These equations have a simple\n",
      "interpretation as follows. From our earlier discussion, we see that the E step involvesan orthogonal projection of the data points onto the current estimate for the principal\n",
      "subspace. Correspondingly, the M step represents a re-estimation of the principal580 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.11 Probabilistic PCA visualization of a portion of the oil ﬂow data set for the ﬁrst 100 data points. The\n",
      "left-hand plot shows the posterior mean projections of the data points on the principal subspace. The right-handplot is obtained by ﬁrst randomly omitting 30% of the variable values and then using EM to handle the missingvalues. Note that each data point then has at least one missing measurement but that the plot is very similar tothe one obtained without missing values.\n",
      "subspace to minimize the squared reconstruction error in which the projections are\n",
      "ﬁxed. Exercise 12.17\n",
      "We can give a simple physical analogy for this EM algorithm, which is easily\n",
      "visualized for D=2 andM=1. Consider a collection of data points in two\n",
      "dimensions, and let the one-dimensional principal subspace be represented by a solid\n",
      "rod. Now attach each data point to the rod via a spring obeying Hooke’s law (stored\n",
      "energy is proportional to the square of the spring’s length). In the E step, we keepthe rod ﬁxed and allow the attachment points to slide up and down the rod so as to\n",
      "minimize the energy. This causes each attachment point (independently) to position\n",
      "itself at the orthogonal projection of the corresponding data point onto the rod. Inthe M step, we keep the attachment points ﬁxed and then release the rod and allow it\n",
      "to move to the minimum energy position. The E and M steps are then repeated until\n",
      "a suitable convergence criterion is satisﬁed, as is illustrated in Figure 12.12.\n",
      "12.2.3 Bayesian PCA\n",
      "So far in our discussion of PCA, we have assumed that the value Mfor the\n",
      "dimensionality of the principal subspace is given. In practice, we must choose a\n",
      "suitable value according to the application. For visualization, we generally choose\n",
      "M=2, whereas for other applications the appropriate choice for Mmay be less\n",
      "clear. One approach is to plot the eigenvalue spectrum for the data set, analogous\n",
      "to the example in Figure 12.4 for the off-line digits data set, and look to see if the\n",
      "eigenvalues naturally form two groups comprising a set of small values separated bya signiﬁcant gap from a set of relatively large values, indicating a natural choice for\n",
      "M. In practice, such a gap is often not seen.12.2. Probabilistic PCA 581\n",
      "−2 0 2−202(a)\n",
      "−2 0 2−202(b)\n",
      "−2 0 2−202(c)\n",
      "−2 0 2−202(d)\n",
      "−2 0 2−202(e)\n",
      "−2 0 2−202(f)\n",
      "Figure 12.12 Synthetic data illustrating the EM algorithm for PCA deﬁned by (12.58) and (12.59). (a) A data\n",
      "setXwith the data points shown in green, together with the true principal components (shown as eigenvectors\n",
      "scaled by the square roots of the eigenvalues). (b) Initial conﬁguration of the principal subspace deﬁned by W,\n",
      "shown in red, together with the projections of the latent points Zinto the data space, given by ZWT, shown in\n",
      "cyan. (c) After one M step, the latent space has been updated with Zheld ﬁxed. (d) After the successive E step,\n",
      "the values of Zhave been updated, giving orthogonal projections, with Wheld ﬁxed. (e) After the second M\n",
      "step. (f) After the second E step.\n",
      "Because the probabilistic PCA model has a well-deﬁned likelihood function, we\n",
      "could employ cross-validation to determine the value of dimensionality by selecting Section 1.3\n",
      "the largest log likelihood on a validation data set. Such an approach, however, can\n",
      "become computationally costly, particularly if we consider a probabilistic mixture\n",
      "of PCA models (Tipping and Bishop, 1999a) in which we seek to determine the\n",
      "appropriate dimensionality separately for each component in the mixture.\n",
      "Given that we have a probabilistic formulation of PCA, it seems natural to seek\n",
      "a Bayesian approach to model selection. To do this, we need to marginalize out\n",
      "the model parameters µ,W, andσ2with respect to appropriate prior distributions.\n",
      "This can be done by using a variational framework to approximate the analytically\n",
      "intractable marginalizations (Bishop, 1999b). The marginal likelihood values, given\n",
      "by the variational lower bound, can then be compared for a range of different values\n",
      "ofMand the value giving the largest marginal likelihood selected.\n",
      "Here we consider a simpler approach introduced by based on the evidence ap-582 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.13 Probabilistic graphical model for Bayesian PCA in\n",
      "which the distribution over the parameter matrix W\n",
      "is governed by a vector αof hyperparameters.\n",
      "xnzn\n",
      "Nµσ2\n",
      "Wα\n",
      "proximation , which is appropriate when the number of data points is relatively large\n",
      "and the corresponding posterior distribution is tightly peaked (Bishop, 1999a). It\n",
      "involves a speciﬁc choice of prior over Wthat allows surplus dimensions in the\n",
      "principal subspace to be pruned out of the model. This corresponds to an example of\n",
      "automatic relevance determination ,o rARD , discussed in Section 7.2.2. Speciﬁcally,\n",
      "we deﬁne an independent Gaussian prior over each column of W, which represent\n",
      "the vectors deﬁning the principal subspace. Each such Gaussian has an independent\n",
      "variance governed by a precision hyperparameter αiso that\n",
      "p(W|α)=M∏\n",
      "i=1(αi\n",
      "2π)D/2\n",
      "exp{\n",
      "−1\n",
      "2αiwT\n",
      "iwi}\n",
      "(12.60)\n",
      "wherewiis theithcolumn of W. The resulting model can be represented using the\n",
      "directed graph shown in Figure 12.13.\n",
      "The values for αiwill be found iteratively by maximizing the marginal likeli-\n",
      "hood function in which Whas been integrated out. As a result of this optimization,\n",
      "some of the αimay be driven to inﬁnity, with the corresponding parameters vec-\n",
      "torwibeing driven to zero (the posterior distribution becomes a delta function at\n",
      "the origin) giving a sparse solution. The effective dimensionality of the principal\n",
      "subspace is then determined by the number of ﬁnite αivalues, and the correspond-\n",
      "ing vectors wican be thought of as ‘relevant’ for modelling the data distribution.\n",
      "In this way, the Bayesian approach is automatically making the trade-off between\n",
      "improving the ﬁt to the data, by using a larger number of vectors wiwith their cor-\n",
      "responding eigenvalues λieach tuned to the data, and reducing the complexity of\n",
      "the model by suppressing some of the wivectors. The origins of this sparsity were\n",
      "discussed earlier in the context of relevance vector machines. Section 7.2\n",
      "The values of αiare re-estimated during training by maximizing the log marginal\n",
      "likelihood given by\n",
      "p(X|α,µ,σ2)=∫\n",
      "p(X|W,µ,σ2)p(W|α)dW (12.61)\n",
      "where the log of p(X|W,µ,σ2)is given by (12.43). Note that for simplicity we also\n",
      "treatµandσ2as parameters to be estimated, rather than deﬁning priors over these\n",
      "parameters.12.2. Probabilistic PCA 583\n",
      "Because this integration is intractable, we make use of the Laplace approxima-\n",
      "tion. If we assume that the posterior distribution is sharply peaked, as will occur for Section 4.4\n",
      "sufﬁciently large data sets, then the re-estimation equations obtained by maximizing\n",
      "the marginal likelihood with respect to αitake the simple form Section 3.5.3\n",
      "αnew\n",
      "i=D\n",
      "wT\n",
      "iwi(12.62)\n",
      "which follows from (3.98), noting that the dimensionality of wiisD. These re-\n",
      "estimations are interleaved with the EM algorithm updates for determining Wand\n",
      "σ2. The E-step equations are again given by (12.54) and (12.55). Similarly, the M-\n",
      "step equation for σ2is again given by (12.57). The only change is to the M-step\n",
      "equation for W, which is modiﬁed to give\n",
      "Wnew=[N∑\n",
      "n=1(xn−x)E[zn]T][N∑\n",
      "n=1E[znzT\n",
      "n]+σ2A]−1\n",
      "(12.63)\n",
      "whereA= diag( αi). The value of µis given by the sample mean, as before.\n",
      "If we choose M=D−1then, if all αivalues are ﬁnite, the model represents\n",
      "a full-covariance Gaussian, while if all the αigo to inﬁnity the model is equivalent\n",
      "to an isotropic Gaussian, and so the model can encompass all permissible values for\n",
      "the effective dimensionality of the principal subspace. It is also possible to consider\n",
      "smaller values of M, which will save on computational cost but which will limit\n",
      "the maximum dimensionality of the subspace. A comparison of the results of this\n",
      "algorithm with standard probabilistic PCA is shown in Figure 12.14.\n",
      "Bayesian PCA provides an opportunity to illustrate the Gibbs sampling algo-\n",
      "rithm discussed in Section 11.3. Figure 12.15 shows an example of the samples\n",
      "from the hyperparameters lnαifor a data set in D=4dimensions in which the di-\n",
      "mensionality of the latent space is M=3but in which the data set is generated from\n",
      "a probabilistic PCA model having one direction of high variance, with the remaining\n",
      "directions comprising low variance noise. This result shows clearly the presence ofthree distinct modes in the posterior distribution. At each step of the iteration, one of\n",
      "the hyperparameters has a small value and the remaining two have large values, so\n",
      "that two of the three latent variables are suppressed. During the course of the Gibbssampling, the solution makes sharp transitions between the three modes.\n",
      "The model described here involves a prior only over the matrix W. A fully\n",
      "Bayesian treatment of PCA, including priors over µ,σ\n",
      "2, and α, and solved us-\n",
      "ing variational methods, is described in Bishop (1999b). For a discussion of vari-\n",
      "ous Bayesian approaches to determining the appropriate dimensionality for a PCA\n",
      "model, see Minka (2001c).\n",
      "12.2.4 Factor analysis\n",
      "Factor analysis is a linear-Gaussian latent variable model that is closely related\n",
      "to probabilistic PCA. Its deﬁnition differs from that of probabilistic PCA only in that\n",
      "the conditional distribution of the observed variable xgiven the latent variable zis584 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.14 ‘Hinton’ diagrams of the matrix Win which each element of the matrix is depicted as\n",
      "a square (white for positive and black for negative values) whose area is proportionalto the magnitude of that element. The synthetic data set comprises 300data points in\n",
      "D=1 0 dimensions sampled from a Gaussian distribution having standard deviation 1.0\n",
      "in3directions and standard deviation 0.5in the remaining 7directions for a data set in\n",
      "D=1 0 dimensions having M=3directions with larger variance than the remaining 7\n",
      "directions. The left-hand plot shows the result from maximum likelihood probabilistic PCA,and the left-hand plot shows the corresponding result from Bayesian PCA. We see how\n",
      "the Bayesian model is able to discover the appropriate dimensionality by suppressing the\n",
      "6surplus degrees of freedom.\n",
      "taken to have a diagonal rather than an isotropic covariance so that\n",
      "p(x|z)=N(x|Wz+µ,Ψ) (12.64)\n",
      "whereΨis aD×Ddiagonal matrix. Note that the factor analysis model, in common\n",
      "with probabilistic PCA, assumes that the observed variables x1,...,x Dare indepen-\n",
      "dent, given the latent variable z. In essence, the factor analysis model is explaining\n",
      "the observed covariance structure of the data by representing the independent vari-\n",
      "ance associated with each coordinate in the matrix Ψand capturing the covariance\n",
      "between variables in the matrix W. In the factor analysis literature, the columns\n",
      "ofW, which capture the correlations between observed variables, are called factor\n",
      "loadings , and the diagonal elements of Ψ, which represent the independent noise\n",
      "variances for each of the variables, are called uniquenesses .\n",
      "The origins of factor analysis are as old as those of PCA, and discussions of\n",
      "factor analysis can be found in the books by Everitt (1984), Bartholomew (1987),and Basilevsky (1994). Links between factor analysis and PCA were investigated\n",
      "by Lawley (1953) and Anderson (1963) who showed that at stationary points of\n",
      "the likelihood function, for a factor analysis model with Ψ=σ\n",
      "2I, the columns of\n",
      "Ware scaled eigenvectors of the sample covariance matrix, and σ2is the average\n",
      "of the discarded eigenvalues. Later, Tipping and Bishop (1999b) showed that themaximum of the log likelihood function occurs when the eigenvectors comprising\n",
      "Ware chosen to be the principal eigenvectors.\n",
      "Making use of (2.115), we see that the marginal distribution for the observed12.2. Probabilistic PCA 585\n",
      "Figure 12.15 Gibbs sampling for Bayesian\n",
      "PCA showing plots of lnαi\n",
      "versus iteration number for\n",
      "three αvalues, showing\n",
      "transitions between the\n",
      "three modes of the posterior\n",
      "distribution.0510\n",
      "0510\n",
      "0510\n",
      "variable is given by p(x)=N(x|µ,C)where now\n",
      "C=WWT+Ψ. (12.65)\n",
      "As with probabilistic PCA, this model is invariant to rotations in the latent space. Exercise 12.19\n",
      "Historically, factor analysis has been the subject of controversy when attempts\n",
      "have been made to place an interpretation on the individual factors (the coordinates\n",
      "inz-space), which has proven problematic due to the nonidentiﬁability of factor\n",
      "analysis associated with rotations in this space. From our perspective, however, we\n",
      "shall view factor analysis as a form of latent variable density model, in which the\n",
      "form of the latent space is of interest but not the particular choice of coordinates\n",
      "used to describe it. If we wish to remove the degeneracy associated with latent space\n",
      "rotations, we must consider non-Gaussian latent variable distributions, giving rise to\n",
      "independent component analysis (ICA) models. Section 12.4\n",
      "We can determine the parameters µ,W, andΨin the factor analysis model by\n",
      "maximum likelihood. The solution for µis again given by the sample mean. How-\n",
      "ever, unlike probabilistic PCA, there is no longer a closed-form maximum likelihood\n",
      "solution for W, which must therefore be found iteratively. Because factor analysis is\n",
      "a latent variable model, this can be done using an EM algorithm (Rubin and Thayer, Exercise 12.21\n",
      "1982) that is analogous to the one used for probabilistic PCA. Speciﬁcally, the E-step\n",
      "equations are given by\n",
      "E[zn]= GWTΨ−1(xn−x) (12.66)\n",
      "E[znzT\n",
      "n]= G+E[zn]E[zn]T(12.67)\n",
      "where we have deﬁned\n",
      "G=(I+WTΨ−1W)−1. (12.68)\n",
      "Note that this is expressed in a form that involves inversion of matrices of size M×M\n",
      "rather than D×D(except for the D×Ddiagonal matrix Ψwhose inverse is trivial586 12. CONTINUOUS LATENT V ARIABLES\n",
      "to compute in O(D)steps), which is convenient because often M≪D. Similarly,\n",
      "the M-step equations take the form Exercise 12.22\n",
      "Wnew=[N∑\n",
      "n=1(xn−x)E[zn]T][N∑\n",
      "n=1E[znzT\n",
      "n]]−1\n",
      "(12.69)\n",
      "Ψnew=d i a g{\n",
      "S−Wnew1\n",
      "NN∑\n",
      "n=1E[zn](xn−x)T}\n",
      "(12.70)\n",
      "where the ‘ diag’ operator sets all of the nondiagonal elements of a matrix to zero. A\n",
      "Bayesian treatment of the factor analysis model can be obtained by a straightforward\n",
      "application of the techniques discussed in this book.\n",
      "Another difference between probabilistic PCA and factor analysis concerns their\n",
      "different behaviour under transformations of the data set. For PCA and probabilis- Exercise 12.25\n",
      "tic PCA, if we rotate the coordinate system in data space, then we obtain exactly\n",
      "the same ﬁt to the data but with the Wmatrix transformed by the corresponding\n",
      "rotation matrix. However, for factor analysis, the analogous property is that if we\n",
      "make a component-wise re-scaling of the data vectors, then this is absorbed into a\n",
      "corresponding re-scaling of the elements of Ψ.\n",
      "12.3. Kernel PCA\n",
      "In Chapter 6, we saw how the technique of kernel substitution allows us to take an\n",
      "algorithm expressed in terms of scalar products of the form xTx′and generalize\n",
      "that algorithm by replacing the scalar products with a nonlinear kernel. Here we\n",
      "apply this technique of kernel substitution to principal component analysis, thereby\n",
      "obtaining a nonlinear generalization called kernel PCA (Sch ¨olkopf et al. , 1998).\n",
      "Consider a data set {xn}of observations, where n=1,...,N , in a space of\n",
      "dimensionality D. In order to keep the notation uncluttered, we shall assume that\n",
      "we have already subtracted the sample mean from each of the vectors xn, so that∑\n",
      "nxn=0. The ﬁrst step is to express conventional PCA in such a form that the\n",
      "data vectors {xn}appear only in the form of the scalar products xT\n",
      "nxm. Recall that\n",
      "the principal components are deﬁned by the eigenvectors uiof the covariance matrix\n",
      "Sui=λiui (12.71)\n",
      "where i=1,...,D . Here the D×Dsample covariance matrix Sis deﬁned by\n",
      "S=1\n",
      "NN∑\n",
      "n=1xnxT\n",
      "n, (12.72)\n",
      "and the eigenvectors are normalized such that uT\n",
      "iui=1.\n",
      "Now consider a nonlinear transformation φ(x)into an M-dimensional feature\n",
      "space, so that each data point xnis thereby projected onto a point φ(xn). We can12.3. Kernel PCA 587\n",
      "x1x2\n",
      "φ2φ1v1\n",
      "Figure 12.16 Schematic illustration of kernel PCA. A data set in the original data space (left-hand plot) is\n",
      "projected by a nonlinear transformation φ(x)into a feature space (right-hand plot). By performing PCA in the\n",
      "feature space, we obtain the principal components, of which the ﬁrst is shown in blue and is denoted by the\n",
      "vector v1. The green lines in feature space indicate the linear projections onto the ﬁrst principal component,\n",
      "which correspond to nonlinear projections in the original data space. Note that in general it is not possible to\n",
      "represent the nonlinear principal component by a vector in xspace.\n",
      "now perform standard PCA in the feature space, which implicitly deﬁnes a nonlinear\n",
      "principal component model in the original data space, as illustrated in Figure 12.16.\n",
      "For the moment, let us assume that the projected data set also has zero mean,\n",
      "so that∑\n",
      "nφ(xn)=0. We shall return to this point shortly. The M×Msample\n",
      "covariance matrix in feature space is given by\n",
      "C=1\n",
      "NN∑\n",
      "n=1φ(xn)φ(xn)T(12.73)\n",
      "and its eigenvector expansion is deﬁned by\n",
      "Cvi=λivi (12.74)\n",
      "i=1,...,M . Our goal is to solve this eigenvalue problem without having to work\n",
      "explicitly in the feature space. From the deﬁnition of C, the eigenvector equations\n",
      "tells us that visatisﬁes\n",
      "1\n",
      "NN∑\n",
      "n=1φ(xn){\n",
      "φ(xn)Tvi}\n",
      "=λivi (12.75)\n",
      "and so we see that (provided λi>0) the vector viis given by a linear combination\n",
      "of the φ(xn)and so can be written in the form\n",
      "vi=N∑\n",
      "n=1ainφ(xn). (12.76)588 12. CONTINUOUS LATENT V ARIABLES\n",
      "Substituting this expansion back into the eigenvector equation, we obtain\n",
      "1\n",
      "NN∑\n",
      "n=1φ(xn)φ(xn)TN∑\n",
      "m=1aimφ(xm)=λiN∑\n",
      "n=1ainφ(xn). (12.77)\n",
      "The key step is now to express this in terms of the kernel function k(xn,xm)=\n",
      "φ(xn)Tφ(xm), which we do by multiplying both sides by φ(xl)Tto give\n",
      "1\n",
      "NN∑\n",
      "n=1k(xl,xn)m∑\n",
      "m=1aimk(xn,xm)=λiN∑\n",
      "n=1aink(xl,xn). (12.78)\n",
      "This can be written in matrix notation as\n",
      "K2ai=λiNKai (12.79)\n",
      "whereaiis anN-dimensional column vector with elements aniforn=1,...,N .\n",
      "We can ﬁnd solutions for aiby solving the following eigenvalue problem\n",
      "Kai=λiNai (12.80)\n",
      "in which we have removed a factor of Kfrom both sides of (12.79). Note that\n",
      "the solutions of (12.79) and (12.80) differ only by eigenvectors of Khaving zero\n",
      "eigenvalues that do not affect the principal components projection. Exercise 12.26\n",
      "The normalization condition for the coefﬁcients aiis obtained by requiring that\n",
      "the eigenvectors in feature space be normalized. Using (12.76) and (12.80), we have\n",
      "1=vT\n",
      "ivi=N∑\n",
      "n=1N∑\n",
      "m=1ainaimφ(xn)Tφ(xm)=aT\n",
      "iKai=λiNaT\n",
      "iai.(12.81)\n",
      "Having solved the eigenvector problem, the resulting principal component pro-\n",
      "jections can then also be cast in terms of the kernel function so that, using (12.76),\n",
      "the projection of a point xonto eigenvector iis given by\n",
      "yi(x)=φ(x)Tvi=N∑\n",
      "n=1ainφ(x)Tφ(xn)=N∑\n",
      "n=1aink(x,xn) (12.82)\n",
      "and so again is expressed in terms of the kernel function.\n",
      "In the original D-dimensional xspace there are Dorthogonal eigenvectors and\n",
      "hence we can ﬁnd at most Dlinear principal components. The dimensionality M\n",
      "of the feature space, however, can be much larger than D(even inﬁnite), and thus\n",
      "we can ﬁnd a number of nonlinear principal components that can exceed D. Note,\n",
      "however, that the number of nonzero eigenvalues cannot exceed the number Nof\n",
      "data points, because (even if M>N ) the covariance matrix in feature space has\n",
      "rank at most equal to N. This is reﬂected in the fact that kernel PCA involves the\n",
      "eigenvector expansion of the N×NmatrixK.12.3. Kernel PCA 589\n",
      "So far we have assumed that the projected data set given by φ(xn)has zero\n",
      "mean, which in general will not be the case. We cannot simply compute and thensubtract off the mean, since we wish to avoid working directly in feature space, and\n",
      "so again, we formulate the algorithm purely in terms of the kernel function. The\n",
      "projected data points after centralizing, denoted˜φ(xn), are given by\n",
      "˜φ(xn)=φ(xn)−1\n",
      "NN∑\n",
      "l=1φ(xl) (12.83)\n",
      "and the corresponding elements of the Gram matrix are given by\n",
      "˜Knm=˜φ(xn)T˜φ(xm)\n",
      "=φ(xn)Tφ(xm)−1\n",
      "NN∑\n",
      "l=1φ(xn)Tφ(xl)\n",
      "−1\n",
      "NN∑\n",
      "l=1φ(xl)Tφ(xm)+1\n",
      "N2N∑\n",
      "j=1N∑\n",
      "l=1φ(xj)Tφ(xl)\n",
      "=k(xn,xm)−1\n",
      "NN∑\n",
      "l=1k(xl,xm)\n",
      "−1\n",
      "NN∑\n",
      "l=1k(xn,xl)+1\n",
      "N2N∑\n",
      "j=1N∑\n",
      "l=1k(xj,xl). (12.84)\n",
      "This can be expressed in matrix notation as\n",
      "˜K=K−1NK−K1N+1NK1N (12.85)\n",
      "where1Ndenotes the N×Nmatrix in which every element takes the value 1/N.\n",
      "Thus we can evaluate ˜Kusing only the kernel function and then use ˜Kto determine\n",
      "the eigenvalues and eigenvectors. Note that the standard PCA algorithm is recoveredas a special case if we use a linear kernel k(x,x\n",
      "′)=xTx′. Figure 12.17 shows an Exercise 12.27\n",
      "example of kernel PCA applied to a synthetic data set (Sch ¨olkopf et al. , 1998). Here\n",
      "a ‘Gaussian’ kernel of the form\n",
      "k(x,x′)=e x p ( −∥x−x′∥2/0.1) (12.86)\n",
      "is applied to a synthetic data set. The lines correspond to contours along which the\n",
      "projection onto the corresponding principal component, deﬁned by\n",
      "φ(x)Tvi=N∑\n",
      "n=1aink(x,xn) (12.87)\n",
      "is constant.590 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.17 Example of kernel PCA, with a Gaussian kernel applied to a synthetic data set in two dimensions,\n",
      "showing the ﬁrst eight eigenfunctions along with their eigenvalues. The contours are lines along which theprojection onto the corresponding principal component is constant. Note how the ﬁrst two eigenvectors separatethe three clusters, the next three eigenvectors split each of the cluster into halves, and the following threeeigenvectors again split the clusters into halves along directions orthogonal to the previous splits.\n",
      "One obvious disadvantage of kernel PCA is that it involves ﬁnding the eigenvec-\n",
      "tors of the N×Nmatrix˜Krather than the D×Dmatrix Sof conventional linear\n",
      "PCA, and so in practice for large data sets approximations are often used.\n",
      "Finally, we note that in standard linear PCA, we often retain some reduced num-\n",
      "berL<D of eigenvectors and then approximate a data vector xnby its projection\n",
      "ˆxnonto the L-dimensional principal subspace, deﬁned by\n",
      "ˆxn=L∑\n",
      "i=1(\n",
      "xT\n",
      "nui)\n",
      "ui. (12.88)\n",
      "In kernel PCA, this will in general not be possible. To see this, note that the map-\n",
      "pingφ(x)maps the D-dimensional xspace into a D-dimensional manifold in the\n",
      "M-dimensional feature space φ. The vector xis known as the pre-image of the\n",
      "corresponding point φ(x). However, the projection of points in feature space onto\n",
      "the linear PCA subspace in that space will typically not lie on the nonlinear D-\n",
      "dimensional manifold and so will not have a corresponding pre-image in data space.\n",
      "Techniques have therefore been proposed for ﬁnding approximate pre-images (Bakir\n",
      "et al. , 2004).12.4. Nonlinear Latent Variable Models 591\n",
      "12.4. Nonlinear Latent Variable Models\n",
      "In this chapter, we have focussed on the simplest class of models having continuous\n",
      "latent variables, namely those based on linear-Gaussian distributions. As well ashaving great practical importance, these models are relatively easy to analyse and\n",
      "to ﬁt to data and can also be used as components in more complex models. Here\n",
      "we consider brieﬂy some generalizations of this framework to models that are eithernonlinear or non-Gaussian, or both.\n",
      "In fact, the issues of nonlinearity and non-Gaussianity are related because a\n",
      "general probability density can be obtained from a simple ﬁxed reference density,such as a Gaussian, by making a nonlinear change of variables. This idea forms the Exercise 12.28\n",
      "basis of several practical latent variable models as we shall see shortly.\n",
      "12.4.1 Independent component analysis\n",
      "We begin by considering models in which the observed variables are related\n",
      "linearly to the latent variables, but for which the latent distribution is non-Gaussian.An important class of such models, known as independent component analysis ,o r\n",
      "ICA, arises when we consider a distribution over the latent variables that factorizes,\n",
      "so that\n",
      "p(z)=M∏\n",
      "j=1p(zj). (12.89)\n",
      "To understand the role of such models, consider a situation in which two people\n",
      "are talking at the same time, and we record their voices using two microphones.\n",
      "If we ignore effects such as time delay and echoes, then the signals received bythe microphones at any point in time will be given by linear combinations of the\n",
      "amplitudes of the two voices. The coefﬁcients of this linear combination will be\n",
      "constant, and if we can infer their values from sample data, then we can invert themixing process (assuming it is nonsingular) and thereby obtain two clean signals\n",
      "each of which contains the voice of just one person. This is an example of a problem\n",
      "called blind source separation in which ‘blind’ refers to the fact that we are given\n",
      "only the mixed data, and neither the original sources nor the mixing coefﬁcients are\n",
      "observed (Cardoso, 1998).\n",
      "This type of problem is sometimes addressed using the following approach\n",
      "(MacKay, 2003) in which we ignore the temporal nature of the signals and treat the\n",
      "successive samples as i.i.d. We consider a generative model in which there are twolatent variables corresponding to the unobserved speech signal amplitudes, and there\n",
      "are two observed variables given by the signal values at the microphones. The latent\n",
      "variables have a joint distribution that factorizes as above, and the observed variablesare given by a linear combination of the latent variables. There is no need to include\n",
      "a noise distribution because the number of latent variables equals the number of ob-\n",
      "served variables, and therefore the marginal distribution of the observed variableswill not in general be singular, so the observed variables are simply deterministic\n",
      "linear combinations of the latent variables. Given a data set of observations, the592 12. CONTINUOUS LATENT V ARIABLES\n",
      "likelihood function for this model is a function of the coefﬁcients in the linear com-\n",
      "bination. The log likelihood can be maximized using gradient-based optimizationgiving rise to a particular version of independent component analysis.\n",
      "The success of this approach requires that the latent variables have non-Gaussian\n",
      "distributions. To see this, recall that in probabilistic PCA (and in factor analysis) thelatent-space distribution is given by a zero-mean isotropic Gaussian. The model\n",
      "therefore cannot distinguish between two different choices for the latent variables\n",
      "where these differ simply by a rotation in latent space. This can be veriﬁed directlyby noting that the marginal density (12.35), and hence the likelihood function, is\n",
      "unchanged if we make the transformation W→WR whereRis an orthogonal\n",
      "matrix satisfying RR\n",
      "T=I, because the matrix Cgiven by (12.36) is itself invariant.\n",
      "Extending the model to allow more general Gaussian latent distributions does not\n",
      "change this conclusion because, as we have seen, such a model is equivalent to thezero-mean isotropic Gaussian latent variable model.\n",
      "Another way to see why a Gaussian latent variable distribution in a linear model\n",
      "is insufﬁcient to ﬁnd independent components is to note that the principal compo-nents represent a rotation of the coordinate system in data space such as to diagonal-\n",
      "ize the covariance matrix, so that the data distribution in the new coordinates is then\n",
      "uncorrelated. Although zero correlation is a necessary condition for independenceit is not, however, sufﬁcient. In practice, a common choice for the latent-variable Exercise 12.29\n",
      "distribution is given by\n",
      "p(z\n",
      "j)=1\n",
      "πcosh(zj)=1\n",
      "π(ezj+e−zj)(12.90)\n",
      "which has heavy tails compared to a Gaussian, reﬂecting the observation that many\n",
      "real-world distributions also exhibit this property.\n",
      "The original ICA model (Bell and Sejnowski, 1995) was based on the optimiza-\n",
      "tion of an objective function deﬁned by information maximization. One advantage\n",
      "of a probabilistic latent variable formulation is that it helps to motivate and formu-late generalizations of basic ICA. For instance, independent factor analysis (Attias,\n",
      "1999a) considers a model in which the number of latent and observed variables can\n",
      "differ, the observed variables are noisy, and the individual latent variables have ﬂex-ible distributions modelled by mixtures of Gaussians. The log likelihood for this\n",
      "model is maximized using EM, and the reconstruction of the latent variables is ap-\n",
      "proximated using a variational approach. Many other types of model have beenconsidered, and there is now a huge literature on ICA and its applications (Jutten\n",
      "and Herault, 1991; Comon et al. , 1991; Amari et al. , 1996; Pearlmutter and Parra,\n",
      "1997; Hyv ¨arinen and Oja, 1997; Hinton et al. , 2001; Miskin and MacKay, 2001;\n",
      "Hojen-Sorensen et al. , 2002; Choudrey and Roberts, 2003; Chan et al. , 2003; Stone,\n",
      "2004).\n",
      "12.4.2 Autoassociative neural networks\n",
      "In Chapter 5 we considered neural networks in the context of supervised learn-\n",
      "ing, where the role of the network is to predict the output variables given values12.4. Nonlinear Latent Variable Models 593\n",
      "Figure 12.18 An autoassociative multilayer perceptron having\n",
      "two layers of weights. Such a network is trained to\n",
      "map input vectors onto themselves by minimiza-\n",
      "tion of a sum-of-squares error. Even with non-\n",
      "linear units in the hidden layer, such a network\n",
      "is equivalent to linear principal component anal-\n",
      "ysis. Links representing bias parameters have\n",
      "been omitted for clarity.\n",
      "x1xD\n",
      "z1zM\n",
      "x1xD\n",
      "inputs outputs\n",
      "for the input variables. However, neural networks have also been applied to un-\n",
      "supervised learning where they have been used for dimensionality reduction. This\n",
      "is achieved by using a network having the same number of outputs as inputs, and\n",
      "optimizing the weights so as to minimize some measure of the reconstruction error\n",
      "between inputs and outputs with respect to a set of training data.\n",
      "Consider ﬁrst a multilayer perceptron of the form shown in Figure 12.18, hav-\n",
      "ingDinputs, Doutput units and Mhidden units, with M<D . The targets used\n",
      "to train the network are simply the input vectors themselves, so that the network\n",
      "is attempting to map each input vector onto itself. Such a network is said to form\n",
      "anautoassociative mapping. Since the number of hidden units is smaller than the\n",
      "number of inputs, a perfect reconstruction of all input vectors is not in general pos-\n",
      "sible. We therefore determine the network parameters wby minimizing an error\n",
      "function which captures the degree of mismatch between the input vectors and their\n",
      "reconstructions. In particular, we shall choose a sum-of-squares error of the form\n",
      "E(w)=1\n",
      "2N∑\n",
      "n=1∥y(xn,w)−xn∥2. (12.91)\n",
      "If the hidden units have linear activations functions, then it can be shown that the\n",
      "error function has a unique global minimum, and that at this minimum the network\n",
      "performs a projection onto the M-dimensional subspace which is spanned by the ﬁrst\n",
      "Mprincipal components of the data (Bourlard and Kamp, 1988; Baldi and Hornik,\n",
      "1989). Thus, the vectors of weights which lead into the hidden units in Figure 12.18\n",
      "form a basis set which spans the principal subspace. Note, however, that these vec-\n",
      "tors need not be orthogonal or normalized. This result is unsurprising, since both\n",
      "principal component analysis and the neural network are using linear dimensionality\n",
      "reduction and are minimizing the same sum-of-squares error function.\n",
      "It might be thought that the limitations of a linear dimensionality reduction could\n",
      "be overcome by using nonlinear (sigmoidal) activation functions for the hidden units\n",
      "in the network in Figure 12.18. However, even with nonlinear hidden units, the min-\n",
      "imum error solution is again given by the projection onto the principal component\n",
      "subspace (Bourlard and Kamp, 1988). There is therefore no advantage in using two-\n",
      "layer neural networks to perform dimensionality reduction. Standard techniques for\n",
      "principal component analysis (based on singular value decomposition) are guaran-\n",
      "teed to give the correct solution in ﬁnite time, and they also generate an ordered set\n",
      "of eigenvalues with corresponding orthonormal eigenvectors.594 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.19 Addition of extra hidden lay-\n",
      "ers of nonlinear units gives an\n",
      "autoassociative network which\n",
      "can perform a nonlinear dimen-\n",
      "sionality reduction.\n",
      "x1xD\n",
      "x1xD\n",
      "inputs outputsF1 F2\n",
      "non-linear\n",
      "The situation is different, however, if additional hidden layers are permitted in\n",
      "the network. Consider the four-layer autoassociative network shown in Figure 12.19.\n",
      "Again the output units are linear, and the Munits in the second hidden layer can also\n",
      "be linear, however, the ﬁrst and third hidden layers have sigmoidal nonlinear activa-\n",
      "tion functions. The network is again trained by minimization of the error function\n",
      "(12.91). We can view this network as two successive functional mappings F1and\n",
      "F2, as indicated in Figure 12.19. The ﬁrst mapping F1projects the original D-\n",
      "dimensional data onto an M-dimensional subspace Sdeﬁned by the activations of\n",
      "the units in the second hidden layer. Because of the presence of the ﬁrst hidden layer\n",
      "of nonlinear units, this mapping is very general, and in particular is not restricted to\n",
      "being linear. Similarly, the second half of the network deﬁnes an arbitrary functional\n",
      "mapping from the M-dimensional space back into the original D-dimensional input\n",
      "space. This has a simple geometrical interpretation, as indicated for the case D=3\n",
      "andM=2in Figure 12.20.\n",
      "Such a network effectively performs a nonlinear principal component analysis.\n",
      "x1\n",
      "x2x3\n",
      "x1\n",
      "x2x3\n",
      "z1z2F1F2\n",
      "S\n",
      "Figure 12.20 Geometrical interpretation of the mappings performed by the network in Figure 12.19 for the case\n",
      "ofD=3 inputs and M=2 units in the middle hidden layer. The function F2maps from an M-dimensional\n",
      "space Sinto a D-dimensional space and therefore deﬁnes the way in which the space Sis embedded within the\n",
      "original x-space. Since the mapping F2can be nonlinear, the embedding of Scan be nonplanar, as indicated\n",
      "in the ﬁgure. The mapping F1then deﬁnes a projection of points in the original D-dimensional space into the\n",
      "M-dimensional subspace S.12.4. Nonlinear Latent Variable Models 595\n",
      "It has the advantage of not being limited to linear transformations, although it con-\n",
      "tains standard principal component analysis as a special case. However, trainingthe network now involves a nonlinear optimization problem, since the error function\n",
      "(12.91) is no longer a quadratic function of the network parameters. Computation-\n",
      "ally intensive nonlinear optimization techniques must be used, and there is the risk ofﬁnding a suboptimal local minimum of the error function. Also, the dimensionality\n",
      "of the subspace must be speciﬁed before training the network.\n",
      "12.4.3 Modelling nonlinear manifolds\n",
      "As we have already noted, many natural sources of data correspond to low-\n",
      "dimensional, possibly noisy, nonlinear manifolds embedded within the higher di-mensional observed data space. Capturing this property explicitly can lead to im-\n",
      "proved density modelling compared with more general methods. Here we consider\n",
      "brieﬂy a range of techniques that attempt to do this.\n",
      "One way to model the nonlinear structure is through a combination of linear\n",
      "models, so that we make a piece-wise linear approximation to the manifold. This can\n",
      "be obtained, for instance, by using a clustering technique such as K-means based on\n",
      "Euclidean distance to partition the data set into local groups with standard PCA ap-\n",
      "plied to each group. A better approach is to use the reconstruction error for cluster\n",
      "assignment (Kambhatla and Leen, 1997; Hinton et al. , 1997) as then a common cost\n",
      "function is being optimized in each stage. However, these approaches still suffer\n",
      "from limitations due to the absence of an overall density model. By using prob-\n",
      "abilistic PCA it is straightforward to deﬁne a fully probabilistic model simply by\n",
      "considering a mixture distribution in which the components are probabilistic PCA\n",
      "models (Tipping and Bishop, 1999a). Such a model has both discrete latent vari-ables, corresponding to the discrete mixture, as well as continuous latent variables,\n",
      "and the likelihood function can be maximized using the EM algorithm. A fully\n",
      "Bayesian treatment, based on variational inference (Bishop and Winn, 2000), allowsthe number of components in the mixture, as well as the effective dimensionalities\n",
      "of the individual models, to be inferred from the data. There are many variants of\n",
      "this model in which parameters such as the Wmatrix or the noise variances are tied\n",
      "across components in the mixture, or in which the isotropic noise distributions are\n",
      "replaced by diagonal ones, giving rise to a mixture of factor analysers (Ghahramani\n",
      "and Hinton, 1996a; Ghahramani and Beal, 2000). The mixture of probabilistic PCAmodels can also be extended hierarchically to produce an interactive data visualiza-\n",
      "tion algorithm (Bishop and Tipping, 1998).\n",
      "An alternative to considering a mixture of linear models is to consider a single\n",
      "nonlinear model. Recall that conventional PCA ﬁnds a linear subspace that passes\n",
      "close to the data in a least-squares sense. This concept can be extended to one-\n",
      "dimensional nonlinear surfaces in the form of principal curves (Hastie and Stuetzle,\n",
      "1989). We can describe a curve in a D-dimensional data space using a vector-valued\n",
      "function f(λ), which is a vector each of whose elements is a function of the scalar λ.\n",
      "There are many possible ways to parameterize the curve, of which a natural choice\n",
      "is the arc length along the curve. For any given point\n",
      "ˆxin data space, we can ﬁnd\n",
      "the point on the curve that is closest in Euclidean distance. We denote this point by596 12. CONTINUOUS LATENT V ARIABLES\n",
      "λ=gf(x)because it depends on the particular curve f(λ). For a continuous data\n",
      "density p(x), a principal curve is deﬁned as one for which every point on the curve\n",
      "is the mean of all those points in data space that project to it, so that\n",
      "E[x|gf(x)=λ]=f(λ). (12.92)\n",
      "For a given continuous density, there can be many principal curves. In practice, we\n",
      "are interested in ﬁnite data sets, and we also wish to restrict attention to smooth\n",
      "curves. Hastie and Stuetzle (1989) propose a two-stage iterative procedure for ﬁnd-\n",
      "ing such principal curves, somewhat reminiscent of the EM algorithm for PCA. Thecurve is initialized using the ﬁrst principal component, and then the algorithm alter-\n",
      "nates between a data projection step and curve re-estimation step. In the projection\n",
      "step, each data point is assigned to a value of λcorresponding to the closest point\n",
      "on the curve. Then in the re-estimation step, each point on the curve is given by\n",
      "a weighted average of those points that project to nearby points on the curve, with\n",
      "points closest on the curve given the greatest weight. In the case where the subspace\n",
      "is constrained to be linear, the procedure converges to the ﬁrst principal component\n",
      "and is equivalent to the power method for ﬁnding the largest eigenvector of the co-variance matrix. Principal curves can be generalized to multidimensional manifolds\n",
      "called principal surfaces although these have found limited use due to the difﬁculty\n",
      "of data smoothing in higher dimensions even for two-dimensional manifolds.\n",
      "PCA is often used to project a data set onto a lower-dimensional space, for ex-\n",
      "ample two dimensional, for the purposes of visualization. Another linear technique\n",
      "with a similar aim is multidimensional scaling ,o rMDS (Cox and Cox, 2000). It ﬁnds\n",
      "a low-dimensional projection of the data such as to preserve, as closely as possible,\n",
      "the pairwise distances between data points, and involves ﬁnding the eigenvectors of\n",
      "the distance matrix. In the case where the distances are Euclidean, it gives equivalentresults to PCA. The MDS concept can be extended to a wide variety of data types\n",
      "speciﬁed in terms of a similarity matrix, giving nonmetric MDS.\n",
      "Two other nonprobabilistic methods for dimensionality reduction and data vi-\n",
      "sualization are worthy of mention. Locally linear embedding ,o r LLE (Roweis and\n",
      "Saul, 2000) ﬁrst computes the set of coefﬁcients that best reconstructs each data\n",
      "point from its neighbours. These coefﬁcients are arranged to be invariant to rota-tions, translations, and scalings of that data point and its neighbours, and hence they\n",
      "characterize the local geometrical properties of the neighbourhood. LLE then maps\n",
      "the high-dimensional data points down to a lower dimensional space while preserv-\n",
      "ing these neighbourhood coefﬁcients. If the local neighbourhood for a particular\n",
      "data point can be considered linear, then the transformation can be achieved usinga combination of translation, rotation, and scaling, such as to preserve the angles\n",
      "formed between the data points and their neighbours. Because the weights are in-\n",
      "variant to these transformations, we expect the same weight values to reconstruct thedata points in the low-dimensional space as in the high-dimensional data space. In\n",
      "spite of the nonlinearity, the optimization for LLE does not exhibit local minima.\n",
      "Inisometric feature mapping ,o r isomap (Tenenbaum et al. , 2000), the goal is\n",
      "to project the data to a lower-dimensional space using MDS, but where the dissim-\n",
      "ilarities are deﬁned in terms of the geodesic distances measured along the mani-12.4. Nonlinear Latent Variable Models 597\n",
      "fold. For instance, if two points lie on a circle, then the geodesic is the arc-length\n",
      "distance measured around the circumference of the circle not the straight line dis-tance measured along the chord connecting them. The algorithm ﬁrst deﬁnes the\n",
      "neighbourhood for each data point, either by ﬁnding the Knearest neighbours or by\n",
      "ﬁnding all points within a sphere of radius ϵ. A graph is then constructed by link-\n",
      "ing all neighbouring points and labelling them with their Euclidean distance. The\n",
      "geodesic distance between any pair of points is then approximated by the sum of\n",
      "the arc lengths along the shortest path connecting them (which itself is found usingstandard algorithms). Finally, metric MDS is applied to the geodesic distance matrix\n",
      "to ﬁnd the low-dimensional projection.\n",
      "Our focus in this chapter has been on models for which the observed vari-\n",
      "ables are continuous. We can also consider models having continuous latent vari-\n",
      "ables together with discrete observed variables, giving rise to latent trait models\n",
      "(Bartholomew, 1987). In this case, the marginalization over the continuous latent\n",
      "variables, even for a linear relationship between latent and observed variables, can-\n",
      "not be performed analytically, and so more sophisticated techniques are required.Tipping (1999) uses variational inference in a model with a two-dimensional latent\n",
      "space, allowing a binary data set to be visualized analogously to the use of PCA to\n",
      "visualize continuous data. Note that this model is the dual of the Bayesian logisticregression problem discussed in Section 4.5. In the case of logistic regression we\n",
      "haveNobservations of the feature vector φ\n",
      "nwhich are parameterized by a single\n",
      "parameter vector w, whereas in the latent space visualization model there is a single\n",
      "latent space variable x(analogous to φ) andNcopies of the latent variable wn.A\n",
      "generalization of probabilistic latent variable models to general exponential family\n",
      "distributions is described in Collins et al. (2002).\n",
      "We have already noted that an arbitrary distribution can be formed by taking a\n",
      "Gaussian random variable and transforming it through a suitable nonlinearity. This\n",
      "is exploited in a general latent variable model called a density network (MacKay,\n",
      "1995; MacKay and Gibbs, 1999) in which the nonlinear function is governed by a\n",
      "multilayered neural network. If the network has enough hidden units, it can approx-imate a given nonlinear function to any desired accuracy. The downside of having Chapter 5\n",
      "such a ﬂexible model is that the marginalization over the latent variables, required in\n",
      "order to obtain the likelihood function, is no longer analytically tractable. Instead,the likelihood is approximated using Monte Carlo techniques by drawing samples Chapter 11\n",
      "from the Gaussian prior. The marginalization over the latent variables then becomes\n",
      "a simple sum with one term for each sample. However, because a large numberof sample points may be required in order to give an accurate representation of the\n",
      "marginal, this procedure can be computationally costly.\n",
      "If we consider more restricted forms for the nonlinear function, and make an ap-\n",
      "propriate choice of the latent variable distribution, then we can construct a latent vari-\n",
      "able model that is both nonlinear and efﬁcient to train. The generative topographic\n",
      "mapping ,o r GTM (Bishop et al. , 1996; Bishop et al. , 1997a; Bishop et al. , 1998b)\n",
      "uses a latent distribution that is deﬁned by a ﬁnite regular grid of delta functions over\n",
      "the (typically two-dimensional) latent space. Marginalization over the latent space\n",
      "then simply involves summing over the contributions from each of the grid locations.598 12. CONTINUOUS LATENT V ARIABLES\n",
      "Figure 12.21 Plot of the oil ﬂow data set visualized using PCA on the left and GTM on the right. For the GTM\n",
      "model, each data point is plotted at the mean of its posterior distribution in latent space. The nonlinearity of theGTM model allows the separation between the groups of data points to be seen more clearly.\n",
      "The nonlinear mapping is given by a linear regression model that allows for general Chapter 3\n",
      "nonlinearity while being a linear function of the adaptive parameters. Note that the\n",
      "usual limitation of linear regression models arising from the curse of dimensionality Section 1.4\n",
      "does not arise in the context of the GTM since the manifold generally has two dimen-sions irrespective of the dimensionality of the data space. A consequence of these\n",
      "two choices is that the likelihood function can be expressed analytically in closed\n",
      "form and can be optimized efﬁciently using the EM algorithm. The resulting GTMmodel ﬁts a two-dimensional nonlinear manifold to the data set, and by evaluating\n",
      "the posterior distribution over latent space for the data points, they can be projected\n",
      "back to the latent space for visualization purposes. Figure 12.21 shows a comparisonof the oil data set visualized with linear PCA and with the nonlinear GTM.\n",
      "The GTM can be seen as a probabilistic version of an earlier model called the self\n",
      "organizing map ,o r SOM (Kohonen, 1982; Kohonen, 1995), which also represents\n",
      "a two-dimensional nonlinear manifold as a regular array of discrete points. The\n",
      "SOM is somewhat reminiscent of the K-means algorithm in that data points are\n",
      "assigned to nearby prototype vectors that are then subsequently updated. Initially,\n",
      "the prototypes are distributed at random, and during the training process they ‘self\n",
      "organize’ so as to approximate a smooth manifold. Unlike K-means, however, the\n",
      "SOM is not optimizing any well-deﬁned cost function (Erwin et al. , 1992) making it\n",
      "difﬁcult to set the parameters of the model and to assess convergence. There is also\n",
      "no guarantee that the ‘self-organization’ will take place as this is dependent on thechoice of appropriate parameter values for any particular data set.\n",
      "By contrast, GTM optimizes the log likelihood function, and the resulting model\n",
      "deﬁnes a probability density in data space. In fact, it corresponds to a constrainedmixture of Gaussians in which the components share a common variance, and the\n",
      "means are constrained to lie on a smooth two-dimensional manifold. This proba-Exercises 599\n",
      "bilistic foundation also makes it very straightforward to deﬁne generalizations of\n",
      "GTM (Bishop et al. , 1998a) such as a Bayesian treatment, dealing with missing val-\n",
      "ues, a principled extension to discrete variables, the use of Gaussian processes to Section 6.4\n",
      "deﬁne the manifold, or a hierarchical GTM model (Tino and Nabney, 2002).\n",
      "Because the manifold in GTM is deﬁned as a continuous surface, not just at the\n",
      "prototype vectors as in the SOM, it is possible to compute the magniﬁcation factors\n",
      "corresponding to the local expansions and compressions of the manifold needed to\n",
      "ﬁt the data set (Bishop et al. , 1997b) as well as the directional curvatures of the\n",
      "manifold (Tino et al. , 2001). These can be visualized along with the projected data\n",
      "and provide additional insight into the model.\n",
      "Exercises\n",
      "12.1 (⋆⋆)www In this exercise, we use proof by induction to show that the linear\n",
      "projection onto an M-dimensional subspace that maximizes the variance of the pro-\n",
      "jected data is deﬁned by the Meigenvectors of the data covariance matrix S,g i v e n\n",
      "by (12.3), corresponding to the Mlargest eigenvalues. In Section 12.1, this result\n",
      "was proven for the case of M=1. Now suppose the result holds for some general\n",
      "value of Mand show that it consequently holds for dimensionality M+1.T o d o\n",
      "this, ﬁrst set the derivative of the variance of the projected data with respect to avectoru\n",
      "M+1deﬁning the new direction in data space equal to zero. This should\n",
      "be done subject to the constraints that uM+1be orthogonal to the existing vectors\n",
      "u1,...,uM, and also that it be normalized to unit length. Use Lagrange multipli-\n",
      "ers to enforce these constraints. Then make use of the orthonormality properties of Appendix E\n",
      "the vectors u1,...,uMto show that the new vector uM+1is an eigenvector of S.\n",
      "Finally, show that the variance is maximized if the eigenvector is chosen to be theone corresponding to eigenvector λ\n",
      "M+1where the eigenvalues have been ordered in\n",
      "decreasing value.\n",
      "12.2 (⋆⋆)Show that the minimum value of the PCA distortion measure Jgiven by\n",
      "(12.15) with respect to the ui, subject to the orthonormality constraints (12.7), is\n",
      "obtained when the uiare eigenvectors of the data covariance matrix S. To do this,\n",
      "introduce a matrix Hof Lagrange multipliers, one for each constraint, so that the\n",
      "modiﬁed distortion measure, in matrix notation reads\n",
      "˜J=Tr{\n",
      "ˆUTSˆU}\n",
      "+Tr{\n",
      "H(I−ˆUTˆU)}\n",
      "(12.93)\n",
      "whereˆUis a matrix of dimension D×(D−M)whose columns are given by ui.\n",
      "Now minimize ˜Jwith respect to ˆUand show that the solution satisﬁes SˆU=ˆUH.\n",
      "Clearly, one possible solution is that the columns of ˆUare eigenvectors of S,i n\n",
      "which case His a diagonal matrix containing the corresponding eigenvalues. To\n",
      "obtain the general solution, show that Hcan be assumed to be a symmetric matrix,\n",
      "and by using its eigenvector expansion show that the general solution to SˆU=ˆUH\n",
      "gives the same value for ˜Jas the speciﬁc solution in which the columns of ˆUare600 12. CONTINUOUS LATENT V ARIABLES\n",
      "the eigenvectors of S. Because these solutions are all equivalent, it is convenient to\n",
      "choose the eigenvector solution.\n",
      "12.3 (⋆)Verify that the eigenvectors deﬁned by (12.30) are normalized to unit length,\n",
      "assuming that the eigenvectors vihave unit length.\n",
      "12.4 (⋆)www Suppose we replace the zero-mean, unit-covariance latent space distri-\n",
      "bution (12.31) in the probabilistic PCA model by a general Gaussian distribution ofthe form N(z|m,Σ). By redeﬁning the parameters of the model, show that this leads\n",
      "to an identical model for the marginal distribution p(x)over the observed variables\n",
      "for any valid choice of mandΣ.\n",
      "12.5 (⋆⋆)Letxbe aD-dimensional random variable having a Gaussian distribution\n",
      "given by N(x|µ,Σ), and consider the M-dimensional random variable given by\n",
      "y=Ax+bwhereAis anM×Dmatrix. Show that yalso has a Gaussian\n",
      "distribution, and ﬁnd expressions for its mean and covariance. Discuss the form of\n",
      "this Gaussian distribution for M<D , forM=D, and for M>D .\n",
      "12.6 (⋆)\n",
      "www Draw a directed probabilistic graph for the probabilistic PCA model\n",
      "described in Section 12.2 in which the components of the observed variable xare\n",
      "shown explicitly as separate nodes. Hence verify that the probabilistic PCA model\n",
      "has the same independence structure as the naive Bayes model discussed in Sec-tion 8.2.2.\n",
      "12.7 (⋆⋆)By making use of the results (2.270) and (2.271) for the mean and covariance\n",
      "of a general distribution, derive the result (12.35) for the marginal distribution p(x)\n",
      "in the probabilistic PCA model.\n",
      "12.8 (⋆⋆)\n",
      "www By making use of the result (2.116), show that the posterior distribution\n",
      "p(z|x)for the probabilistic PCA model is given by (12.42).\n",
      "12.9 (⋆)Verify that maximizing the log likelihood (12.43) for the probabilistic PCA\n",
      "model with respect to the parameter µgives the result µML=xwherexis the\n",
      "mean of the data vectors.\n",
      "12.10 (⋆⋆)By evaluating the second derivatives of the log likelihood function (12.43) for\n",
      "the probabilistic PCA model with respect to the parameter µ, show that the stationary\n",
      "pointµML=xrepresents the unique maximum.\n",
      "12.11 (⋆⋆)www Show that in the limit σ2→0, the posterior mean for the probabilistic\n",
      "PCA model becomes an orthogonal projection onto the principal subspace, as in\n",
      "conventional PCA.\n",
      "12.12 (⋆⋆)Forσ2>0show that the posterior mean in the probabilistic PCA model is\n",
      "shifted towards the origin relative to the orthogonal projection.\n",
      "12.13 (⋆⋆)Show that the optimal reconstruction of a data point under probabilistic PCA,\n",
      "according to the least squares projection cost of conventional PCA, is given by\n",
      "˜x=WML(WT\n",
      "MLWML)−1ME[z|x]. (12.94)Exercises 601\n",
      "12.14 (⋆)The number of independent parameters in the covariance matrix for the proba-\n",
      "bilistic PCA model with an M-dimensional latent space and a D-dimensional data\n",
      "space is given by (12.51). Verify that in the case of M=D−1, the number of\n",
      "independent parameters is the same as in a general covariance Gaussian, whereas for\n",
      "M=0it is the same as for a Gaussian with an isotropic covariance.\n",
      "12.15 (⋆⋆)www Derive the M-step equations (12.56) and (12.57) for the probabilistic\n",
      "PCA model by maximization of the expected complete-data log likelihood function\n",
      "given by (12.53).\n",
      "12.16 (⋆⋆⋆ )In Figure 12.11, we showed an application of probabilistic PCA to a data set\n",
      "in which some of the data values were missing at random. Derive the EM algorithmfor maximizing the likelihood function for the probabilistic PCA model in this situ-\n",
      "ation. Note that the {z\n",
      "n}, as well as the missing data values that are components of\n",
      "the vectors {xn}, are now latent variables. Show that in the special case in which all\n",
      "of the data values are observed, this reduces to the EM algorithm for probabilistic\n",
      "PCA derived in Section 12.2.2.\n",
      "12.17 (⋆⋆)www LetWbe aD×Mmatrix whose columns deﬁne a linear subspace\n",
      "of dimensionality Membedded within a data space of dimensionality D, and let µ\n",
      "be aD-dimensional vector. Given a data set {xn}where n=1,...,N , we can\n",
      "approximate the data points using a linear mapping from a set of M-dimensional\n",
      "vectors {zn}, so that xnis approximated by Wzn+µ. The associated sum-of-\n",
      "squares reconstruction cost is given by\n",
      "J=N∑\n",
      "n=1∥xn−µ−Wzn∥2. (12.95)\n",
      "First show that minimizing Jwith respect to µleads to an analogous expression with\n",
      "xnandznreplaced by zero-mean variables xn−xandzn−z, respectively, where x\n",
      "andzdenote sample means. Then show that minimizing Jwith respect to zn, where\n",
      "Wis kept ﬁxed, gives rise to the PCA E step (12.58), and that minimizing Jwith\n",
      "respect to W, where {zn}is kept ﬁxed, gives rise to the PCA M step (12.59).\n",
      "12.18 (⋆)Derive an expression for the number of independent parameters in the factor\n",
      "analysis model described in Section 12.2.4.\n",
      "12.19 (⋆⋆)www Show that the factor analysis model described in Section 12.2.4 is\n",
      "invariant under rotations of the latent space coordinates.\n",
      "12.20 (⋆⋆)By considering second derivatives, show that the only stationary point of\n",
      "the log likelihood function for the factor analysis model discussed in Section 12.2.4\n",
      "with respect to the parameter µis given by the sample mean deﬁned by (12.1).\n",
      "Furthermore, show that this stationary point is a maximum.\n",
      "12.21 (⋆⋆)Derive the formulae (12.66) and (12.67) for the E step of the EM algorithm\n",
      "for factor analysis. Note that from the result of Exercise 12.20, the parameter µcan\n",
      "be replaced by the sample mean x.602 12. CONTINUOUS LATENT V ARIABLES\n",
      "12.22 (⋆⋆)Write down an expression for the expected complete-data log likelihood func-\n",
      "tion for the factor analysis model, and hence derive the corresponding M step equa-tions (12.69) and (12.70).\n",
      "12.23 (⋆)\n",
      "www Draw a directed probabilistic graphical model representing a discrete\n",
      "mixture of probabilistic PCA models in which each PCA model has its own valuesofW,µ, andσ\n",
      "2. Now draw a modiﬁed graph in which these parameter values are\n",
      "shared between the components of the mixture.\n",
      "12.24 (⋆⋆⋆ )We saw in Section 2.3.7 that Student’s t-distribution can be viewed as an\n",
      "inﬁnite mixture of Gaussians in which we marginalize with respect to a continu-\n",
      "ous latent variable. By exploiting this representation, formulate an EM algorithm\n",
      "for maximizing the log likelihood function for a multivariate Student’s t-distributiongiven an observed set of data points, and derive the forms of the E and M step equa-\n",
      "tions.\n",
      "12.25 (⋆⋆)\n",
      "www Consider a linear-Gaussian latent-variable model having a latent space\n",
      "distribution p(z)=N(x|0,I)and a conditional distribution for the observed vari-\n",
      "ablep(x|z)=N(x|Wz+µ,Φ)whereΦis an arbitrary symmetric, positive-\n",
      "deﬁnite noise covariance matrix. Now suppose that we make a nonsingular lineartransformation of the data variables x→Ax, where Ais aD×Dmatrix. If\n",
      "µ\n",
      "ML,WMLandΦMLrepresent the maximum likelihood solution corresponding to\n",
      "the original untransformed data, show that AµML,AW ML, andAΦMLATwill rep-\n",
      "resent the corresponding maximum likelihood solution for the transformed data set.\n",
      "Finally, show that the form of the model is preserved in two cases: (i) Ais a diagonal\n",
      "matrix and Φis a diagonal matrix. This corresponds to the case of factor analysis.\n",
      "The transformed Φremains diagonal, and hence factor analysis is covariant under\n",
      "component-wise re-scaling of the data variables; (ii) Ais orthogonal and Φis pro-\n",
      "portional to the unit matrix so that Φ=σ2I. This corresponds to probabilistic PCA.\n",
      "The transformed Φmatrix remains proportional to the unit matrix, and hence proba-\n",
      "bilistic PCA is covariant under a rotation of the axes of data space, as is the case forconventional PCA.\n",
      "12.26 (⋆⋆)Show that any vector a\n",
      "ithat satisﬁes (12.80) will also satisfy (12.79). Also,\n",
      "show that for any solution of (12.80) having eigenvalue λ, we can add any multiple\n",
      "of an eigenvector of Khaving zero eigenvalue, and obtain a solution to (12.79)\n",
      "that also has eigenvalue λ. Finally, show that such modiﬁcations do not affect the\n",
      "principal-component projection given by (12.82).\n",
      "12.27 (⋆⋆)Show that the conventional linear PCA algorithm is recovered as a special case\n",
      "of kernel PCA if we choose the linear kernel function given by k(x,x′)=xTx′.\n",
      "12.28 (⋆⋆)www Use the transformation property (1.27) of a probability density under\n",
      "a change of variable to show that any density p(y)can be obtained from a ﬁxed\n",
      "density q(x)that is everywhere nonzero by making a nonlinear change of variable\n",
      "y=f(x)in which f(x)is a monotonic function so that 0⩽f′(x)<∞. Write\n",
      "down the differential equation satisﬁed by f(x)and draw a diagram illustrating the\n",
      "transformation of the density.Exercises 603\n",
      "12.29 (⋆⋆)www Suppose that two variables z1andz2are independent so that p(z1,z2)=\n",
      "p(z1)p(z2). Show that the covariance matrix between these variables is diagonal.\n",
      "This shows that independence is a sufﬁcient condition for two variables to be un-\n",
      "correlated. Now consider two variables y1andy2in which −1⩽y1⩽1and\n",
      "y2=y2\n",
      "2. Write down the conditional distribution p(y2|y1)and observe that this is\n",
      "dependent on y1, showing that the two variables are not independent. Now show\n",
      "that the covariance matrix between these two variables is again diagonal. To do this,\n",
      "use the relation p(y1,y2)=p(y1)p(y2|y1)to show that the off-diagonal terms are\n",
      "zero. This counter-example shows that zero correlation is not a sufﬁcient condition\n",
      "for independence.13\n",
      "Sequential\n",
      "Data\n",
      "So far in this book, we have focussed primarily on sets of data points that were as-\n",
      "sumed to be independent and identically distributed (i.i.d.). This assumption allowedus to express the likelihood function as the product over all data points of the prob-\n",
      "ability distribution evaluated at each data point. For many applications, however,\n",
      "the i.i.d. assumption will be a poor one. Here we consider a particularly importantclass of such data sets, namely those that describe sequential data. These often arise\n",
      "through measurement of time series, for example the rainfall measurements on suc-\n",
      "cessive days at a particular location, or the daily values of a currency exchange rate,or the acoustic features at successive time frames used for speech recognition. An\n",
      "example involving speech data is shown in Figure 13.1. Sequential data can also\n",
      "arise in contexts other than time series, for example the sequence of nucleotide basepairs along a strand of DNA or the sequence of characters in an English sentence.\n",
      "For convenience, we shall sometimes refer to ‘past’ and ‘future’ observations in a\n",
      "sequence. However, the models explored in this chapter are equally applicable to all\n",
      "605606 13. SEQUENTIAL DATA\n",
      "Figure 13.1 Example of a spectro-\n",
      "gram of the spoken words “Bayes’ theo-\n",
      "rem” showing a plot of the intensity of the\n",
      "spectral coefﬁcients versus time index.\n",
      "forms of sequential data, not just temporal sequences.\n",
      "It is useful to distinguish between stationary and nonstationary sequential dis-\n",
      "tributions. In the stationary case, the data evolves in time, but the distribution from\n",
      "which it is generated remains the same. For the more complex nonstationary situa-\n",
      "tion, the generative distribution itself is evolving with time. Here we shall focus onthe stationary case.\n",
      "For many applications, such as ﬁnancial forecasting, we wish to be able to pre-\n",
      "dict the next value in a time series given observations of the previous values. In-tuitively, we expect that recent observations are likely to be more informative than\n",
      "more historical observations in predicting future values. The example in Figure 13.1\n",
      "shows that successive observations of the speech spectrum are indeed highly cor-related. Furthermore, it would be impractical to consider a general dependence of\n",
      "future observations on all previous observations because the complexity of such a\n",
      "model would grow without limit as the number of observations increases. This leads\n",
      "us to consider Markov models in which we assume that future predictions are inde-13.1. Markov Models 607\n",
      "Figure 13.2 The simplest approach to\n",
      "modelling a sequence of ob-\n",
      "servations is to treat them\n",
      "as independent, correspond-\n",
      "ing to a graph without links.x1 x2 x3 x4\n",
      "pendent of all but the most recent observations.\n",
      "Although such models are tractable, they are also severely limited. We can ob-\n",
      "tain a more general framework, while still retaining tractability, by the introduction\n",
      "of latent variables, leading to state space models . As in Chapters 9 and 12, we shall\n",
      "see that complex models can thereby be constructed from simpler components (in\n",
      "particular, from distributions belonging to the exponential family) and can be read-\n",
      "ily characterized using the framework of probabilistic graphical models. Here we\n",
      "focus on the two most important examples of state space models, namely the hid-\n",
      "den Markov model , in which the latent variables are discrete, and linear dynamical\n",
      "systems , in which the latent variables are Gaussian. Both models are described by di-\n",
      "rected graphs having a tree structure (no loops) for which inference can be performed\n",
      "efﬁciently using the sum-product algorithm.\n",
      "13.1. Markov Models\n",
      "The easiest way to treat sequential data would be simply to ignore the sequential\n",
      "aspects and treat the observations as i.i.d., corresponding to the graph in Figure 13.2.\n",
      "Such an approach, however, would fail to exploit the sequential patterns in the data,\n",
      "such as correlations between observations that are close in the sequence. Suppose,\n",
      "for instance, that we observe a binary variable denoting whether on a particular day\n",
      "it rained or not. Given a time series of recent observations of this variable, we wish\n",
      "to predict whether it will rain on the next day. If we treat the data as i.i.d., then the\n",
      "only information we can glean from the data is the relative frequency of rainy days.\n",
      "However, we know in practice that the weather often exhibits trends that may last for\n",
      "several days. Observing whether or not it rains today is therefore of signiﬁcant help\n",
      "in predicting if it will rain tomorrow.\n",
      "To express such effects in a probabilistic model, we need to relax the i.i.d. as-\n",
      "sumption, and one of the simplest ways to do this is to consider a Markov model .\n",
      "First of all we note that, without loss of generality, we can use the product rule to\n",
      "express the joint distribution for a sequence of observations in the form\n",
      "p(x1,...,xN)=N∏\n",
      "n=1p(xn|x1,...,xn−1). (13.1)\n",
      "If we now assume that each of the conditional distributions on the right-hand side\n",
      "is independent of all previous observations except the most recent, we obtain the\n",
      "ﬁrst-order Markov chain , which is depicted as a graphical model in Figure 13.3. The608 13. SEQUENTIAL DATA\n",
      "Figure 13.3 A ﬁrst-order Markov chain of ob-\n",
      "servations {xn}in which the dis-\n",
      "tribution p(xn|xn−1)of a particu-\n",
      "lar observation xnis conditioned\n",
      "on the value of the previous ob-\n",
      "servation xn−1.x1 x2 x3 x4\n",
      "joint distribution for a sequence of Nobservations under this model is given by\n",
      "p(x1,...,xN)=p(x1)N∏\n",
      "n=2p(xn|xn−1). (13.2)\n",
      "From the d-separation property, we see that the conditional distribution for observa- Section 8.2\n",
      "tionxn, given all of the observations up to time n,i sg i v e nb y\n",
      "p(xn|x1,...,xn−1)=p(xn|xn−1) (13.3)\n",
      "which is easily veriﬁed by direct evaluation starting from (13.2) and using the prod-\n",
      "uct rule of probability. Thus if we use such a model to predict the next observation Exercise 13.1\n",
      "in a sequence, the distribution of predictions will depend only on the value of the im-\n",
      "mediately preceding observation and will be independent of all earlier observations.\n",
      "In most applications of such models, the conditional distributions p(xn|xn−1)\n",
      "that deﬁne the model will be constrained to be equal, corresponding to the assump-\n",
      "tion of a stationary time series. The model is then known as a homogeneous Markov\n",
      "chain. For instance, if the conditional distributions depend on adjustable parameters\n",
      "(whose values might be inferred from a set of training data), then all of the condi-\n",
      "tional distributions in the chain will share the same values of those parameters.\n",
      "Although this is more general than the independence model, it is still very re-\n",
      "strictive. For many sequential observations, we anticipate that the trends in the data\n",
      "over several successive observations will provide important information in predict-\n",
      "ing the next value. One way to allow earlier observations to have an inﬂuence is to\n",
      "move to higher-order Markov chains. If we allow the predictions to depend also on\n",
      "the previous-but-one value, we obtain a second-order Markov chain, represented by\n",
      "the graph in Figure 13.4. The joint distribution is now given by\n",
      "p(x1,...,xN)=p(x1)p(x2|x1)N∏\n",
      "n=3p(xn|xn−1,xn−2). (13.4)\n",
      "Again, using d-separation or by direct evaluation, we see that the conditional distri-\n",
      "bution of xngivenxn−1andxn−2is independent of all observations x1,...xn−3.\n",
      "Figure 13.4 A second-order Markov chain, in\n",
      "which the conditional distribution\n",
      "of a particular observation xn\n",
      "depends on the values of the two\n",
      "previous observations xn−1and\n",
      "xn−2.x1 x2 x3 x413.1. Markov Models 609\n",
      "Figure 13.5 We can represent sequen-\n",
      "tial data using a Markov chain of latent\n",
      "variables, with each observation condi-\n",
      "tioned on the state of the corresponding\n",
      "latent variable. This important graphical\n",
      "structure forms the foundation both for the\n",
      "hidden Markov model and for linear dy-\n",
      "namical systems.zn−1 zn zn+1\n",
      "xn−1 xn xn+1z1 z2\n",
      "x1 x2\n",
      "Each observation is now inﬂuenced by two previous observations. We can similarly\n",
      "consider extensions to an Mthorder Markov chain in which the conditional distri-\n",
      "bution for a particular variable depends on the previous Mvariables. However, we\n",
      "have paid a price for this increased ﬂexibility because the number of parameters in\n",
      "the model is now much larger. Suppose the observations are discrete variables hav-\n",
      "ingKstates. Then the conditional distribution p(xn|xn−1)in a ﬁrst-order Markov\n",
      "chain will be speciﬁed by a set of K−1parameters for each of the Kstates of xn−1\n",
      "giving a total of K(K−1)parameters. Now suppose we extend the model to an\n",
      "Mthorder Markov chain, so that the joint distribution is built up from conditionals\n",
      "p(xn|xn−M,...,xn−1). If the variables are discrete, and if the conditional distri-\n",
      "butions are represented by general conditional probability tables, then the number\n",
      "of parameters in such a model will have KM−1(K−1)parameters. Because this\n",
      "grows exponentially with M, it will often render this approach impractical for larger\n",
      "values of M.\n",
      "For continuous variables, we can use linear-Gaussian conditional distributions\n",
      "in which each node has a Gaussian distribution whose mean is a linear function\n",
      "of its parents. This is known as an autoregressive orARmodel (Box et al. , 1994;\n",
      "Thiesson et al. , 2004). An alternative approach is to use a parametric model for\n",
      "p(xn|xn−M,...,xn−1)such as a neural network. This technique is sometimes\n",
      "called a tapped delay line because it corresponds to storing (delaying) the previous\n",
      "Mvalues of the observed variable in order to predict the next value. The number\n",
      "of parameters can then be much smaller than in a completely general model (for ex-\n",
      "ample it may grow linearly with M), although this is achieved at the expense of a\n",
      "restricted family of conditional distributions.\n",
      "Suppose we wish to build a model for sequences that is not limited by the\n",
      "Markov assumption to any order and yet that can be speciﬁed using a limited number\n",
      "of free parameters. We can achieve this by introducing additional latent variables to\n",
      "permit a rich class of models to be constructed out of simple components, as we did\n",
      "with mixture distributions in Chapter 9 and with continuous latent variable models in\n",
      "Chapter 12. For each observation xn, we introduce a corresponding latent variable\n",
      "zn(which may be of different type or dimensionality to the observed variable). We\n",
      "now assume that it is the latent variables that form a Markov chain, giving rise to the\n",
      "graphical structure known as a state space model , which is shown in Figure 13.5. It\n",
      "satisﬁes the key conditional independence property that zn−1andzn+1are indepen-\n",
      "dent given zn, so that\n",
      "zn+1⊥⊥zn−1|zn. (13.5)610 13. SEQUENTIAL DATA\n",
      "The joint distribution for this model is given by\n",
      "p(x1,...,xN,z1,...,zN)=p(z1)[N∏\n",
      "n=2p(zn|zn−1)]N∏\n",
      "n=1p(xn|zn). (13.6)\n",
      "Using the d-separation criterion, we see that there is always a path connecting any\n",
      "two observed variables xnandxmvia the latent variables, and that this path is never\n",
      "blocked. Thus the predictive distribution p(xn+1|x1,...,xn)for observation xn+1\n",
      "given all previous observations does not exhibit any conditional independence prop-\n",
      "erties, and so our predictions for xn+1depends on all previous observations. The\n",
      "observed variables, however, do not satisfy the Markov property at any order. We\n",
      "shall discuss how to evaluate the predictive distribution in later sections of this chap-ter.\n",
      "There are two important models for sequential data that are described by this\n",
      "graph. If the latent variables are discrete, then we obtain the hidden Markov model ,\n",
      "orHMM (Elliott et al. , 1995). Note that the observed variables in an HMM may Section 13.2\n",
      "be discrete or continuous, and a variety of different conditional distributions can beused to model them. If both the latent and the observed variables are Gaussian (with\n",
      "a linear-Gaussian dependence of the conditional distributions on their parents), then\n",
      "we obtain the linear dynamical system . Section 13.3\n",
      "13.2. Hidden Markov Models\n",
      "The hidden Markov model can be viewed as a speciﬁc instance of the state space\n",
      "model of Figure 13.5 in which the latent variables are discrete. However, if weexamine a single time slice of the model, we see that it corresponds to a mixture\n",
      "distribution, with component densities given by p(x|z). It can therefore also be\n",
      "interpreted as an extension of a mixture model in which the choice of mixture com-ponent for each observation is not selected independently but depends on the choice\n",
      "of component for the previous observation. The HMM is widely used in speech\n",
      "recognition (Jelinek, 1997; Rabiner and Juang, 1993), natural language modelling(Manning and Sch ¨utze, 1999), on-line handwriting recognition (Nag et al. , 1986),\n",
      "and for the analysis of biological sequences such as proteins and DNA (Krogh et al. ,\n",
      "1994; Durbin et al. , 1998; Baldi and Brunak, 2001).\n",
      "As in the case of a standard mixture model, the latent variables are the discrete\n",
      "multinomial variables z\n",
      "ndescribing which component of the mixture is responsible\n",
      "for generating the corresponding observation xn. Again, it is convenient to use a\n",
      "1-of-Kcoding scheme, as used for mixture models in Chapter 9. We now allow the\n",
      "probability distribution of znto depend on the state of the previous latent variable\n",
      "zn−1through a conditional distribution p(zn|zn−1). Because the latent variables are\n",
      "K-dimensional binary variables, this conditional distribution corresponds to a table\n",
      "of numbers that we denote by A, the elements of which are known as transition\n",
      "probabilities . They are given by Ajk≡p(znk=1|zn−1,j=1 ) , and because they\n",
      "are probabilities, they satisfy 0⩽Ajk⩽1with∑\n",
      "kAjk=1, so that the matrix A13.2. Hidden Markov Models 611\n",
      "Figure 13.6 Transition diagram showing a model whose la-\n",
      "tent variables have three possible states corre-\n",
      "sponding to the three boxes. The black lines\n",
      "denote the elements of the transition matrix\n",
      "Ajk.A12\n",
      "A23\n",
      "A31A21\n",
      "A32\n",
      "A13A11A22\n",
      "A33k=1k=2\n",
      "k=3\n",
      "hasK(K−1)independent parameters. We can then write the conditional distribution\n",
      "explicitly in the form\n",
      "p(zn|zn−1,A)=K∏\n",
      "k=1K∏\n",
      "j=1Azn−1,jznk\n",
      "jk. (13.7)\n",
      "The initial latent node z1is special in that it does not have a parent node, and so\n",
      "it has a marginal distribution p(z1)represented by a vector of probabilities πwith\n",
      "elements πk≡p(z1k=1 ) , so that\n",
      "p(z1|π)=K∏\n",
      "k=1πz1k\n",
      "k(13.8)\n",
      "where∑\n",
      "kπk=1.\n",
      "The transition matrix is sometimes illustrated diagrammatically by drawing the\n",
      "states as nodes in a state transition diagram as shown in Figure 13.6 for the case of\n",
      "K=3. Note that this does not represent a probabilistic graphical model, because\n",
      "the nodes are not separate variables but rather states of a single variable, and so we\n",
      "have shown the states as boxes rather than circles.\n",
      "It is sometimes useful to take a state transition diagram, of the kind shown in\n",
      "Figure 13.6, and unfold it over time. This gives an alternative representation of the\n",
      "transitions between latent states, known as a lattice ortrellis diagram, and which is Section 8.4.5\n",
      "shown for the case of the hidden Markov model in Figure 13.7.\n",
      "The speciﬁcation of the probabilistic model is completed by deﬁning the con-\n",
      "ditional distributions of the observed variables p(xn|zn,φ), where φis a set of pa-\n",
      "rameters governing the distribution. These are known as emission probabilities , and\n",
      "might for example be given by Gaussians of the form (9.11) if the elements of xare\n",
      "continuous variables, or by conditional probability tables if xis discrete. Because\n",
      "xnis observed, the distribution p(xn|zn,φ)consists, for a given value of φ,o fa\n",
      "vector of Knumbers corresponding to the Kpossible states of the binary vector zn.612 13. SEQUENTIAL DATA\n",
      "Figure 13.7 If we unfold the state transition dia-\n",
      "gram of Figure 13.6 over time, we obtain a lattice,\n",
      "or trellis, representation of the latent states. Each\n",
      "column of this diagram corresponds to one of the\n",
      "latent variables zn.k=1\n",
      "k=2\n",
      "k=3\n",
      "n−2 n−1 nn +1A11 A11 A11\n",
      "A33 A33 A33\n",
      "We can represent the emission probabilities in the form\n",
      "p(xn|zn,φ)=K∏\n",
      "k=1p(xn|φk)znk. (13.9)\n",
      "We shall focuss attention on homogeneous models for which all of the condi-\n",
      "tional distributions governing the latent variables share the same parameters A, and\n",
      "similarly all of the emission distributions share the same parameters φ(the extension\n",
      "to more general cases is straightforward). Note that a mixture model for an i.i.d. data\n",
      "set corresponds to the special case in which the parameters Ajkare the same for all\n",
      "values of j, so that the conditional distribution p(zn|zn−1)is independent of zn−1.\n",
      "This corresponds to deleting the horizontal links in the graphical model shown in\n",
      "Figure 13.5.\n",
      "The joint probability distribution over both latent and observed variables is then\n",
      "given by\n",
      "p(X,Z|θ)=p(z1|π)[N∏\n",
      "n=2p(zn|zn−1,A)]N∏\n",
      "m=1p(xm|zm,φ) (13.10)\n",
      "whereX={x1,...,xN},Z={z1,...,zN}, andθ={π,A,φ}denotes the set\n",
      "of parameters governing the model. Most of our discussion of the hidden Markov\n",
      "model will be independent of the particular choice of the emission probabilities.\n",
      "Indeed, the model is tractable for a wide range of emission distributions including\n",
      "discrete tables, Gaussians, and mixtures of Gaussians. It is also possible to exploit\n",
      "discriminative models such as neural networks. These can be used to model the Exercise 13.4\n",
      "emission density p(x|z)directly, or to provide a representation for p(z|x)that can\n",
      "be converted into the required emission density p(x|z)using Bayes’ theorem (Bishop\n",
      "et al. , 2004).\n",
      "We can gain a better understanding of the hidden Markov model by considering\n",
      "it from a generative point of view. Recall that to generate samples from a mixture of13.2. Hidden Markov Models 613\n",
      "k=1\n",
      "k=2k=3\n",
      "0 0.5 100.51\n",
      "0 0.5 100.51\n",
      "Figure 13.8 Illustration of sampling from a hidden Markov model having a 3-state latent variable zand a\n",
      "Gaussian emission model p(x|z)where xis 2-dimensional. (a) Contours of constant probability density for the\n",
      "emission distributions corresponding to each of the three states of the latent variable. (b) A sample of 50 points\n",
      "drawn from the hidden Markov model, colour coded according to the component that generated them and with\n",
      "lines connecting the successive observations. Here the transition matrix was ﬁxed so that in any state there is a\n",
      "5% probability of making a transition to each of the other states, and consequently a 90% probability of remaining\n",
      "in the same state.\n",
      "Gaussians, we ﬁrst chose one of the components at random with probability given by\n",
      "the mixing coefﬁcients πkand then generate a sample vector xfrom the correspond-\n",
      "ing Gaussian component. This process is repeated Ntimes to generate a data set of\n",
      "Nindependent samples. In the case of the hidden Markov model, this procedure is\n",
      "modiﬁed as follows. We ﬁrst choose the initial latent variable z1with probabilities\n",
      "governed by the parameters πkand then sample the corresponding observation x1.\n",
      "Now we choose the state of the variable z2according to the transition probabilities\n",
      "p(z2|z1)using the already instantiated value of z1. Thus suppose that the sample for\n",
      "z1corresponds to state j. Then we choose the state kofz2with probabilities Ajk\n",
      "fork=1,...,K . Once we know z2we can draw a sample for x2and also sample\n",
      "the next latent variable z3and so on. This is an example of ancestral sampling for\n",
      "a directed graphical model. If, for instance, we have a model in which the diago- Section 8.1.2\n",
      "nal transition elements Akkare much larger than the off-diagonal elements, then a\n",
      "typical data sequence will have long runs of points generated from a single compo-\n",
      "nent, with infrequent transitions from one component to another. The generation of\n",
      "samples from a hidden Markov model is illustrated in Figure 13.8.\n",
      "There are many variants of the standard HMM model, obtained for instance by\n",
      "imposing constraints on the form of the transition matrix A(Rabiner, 1989). Here we\n",
      "mention one of particular practical importance called the left-to-right HMM, which\n",
      "is obtained by setting the elements AjkofAto zero if k<j , as illustrated in the614 13. SEQUENTIAL DATA\n",
      "Figure 13.9 Example of the state transition diagram for a 3-state\n",
      "left-to-right hidden Markov model. Note that once a\n",
      "state has been vacated, it cannot later be re-entered.\n",
      "k=1 k=2 k=3A11 A22 A33\n",
      "A12 A23\n",
      "A13\n",
      "state transition diagram for a 3-state HMM in Figure 13.9. Typically for such models\n",
      "the initial state probabilities for p(z1)are modiﬁed so that p(z11)=1 andp(z1j)=0\n",
      "forj̸=1, in other words every sequence is constrained to start in state j=1. The\n",
      "transition matrix may be further constrained to ensure that large changes in the state\n",
      "index do not occur, so that Ajk=0ifk>j +∆. This type of model is illustrated\n",
      "using a lattice diagram in Figure 13.10.\n",
      "Many applications of hidden Markov models, for example speech recognition,\n",
      "or on-line character recognition, make use of left-to-right architectures. As an illus-\n",
      "tration of the left-to-right hidden Markov model, we consider an example involving\n",
      "handwritten digits. This uses on-line data, meaning that each digit is represented\n",
      "by the trajectory of the pen as a function of time in the form of a sequence of pen\n",
      "coordinates, in contrast to the off-line digits data, discussed in Appendix A, which\n",
      "comprises static two-dimensional pixellated images of the ink. Examples of the on-\n",
      "line digits are shown in Figure 13.11. Here we train a hidden Markov model on a\n",
      "subset of data comprising 45 examples of the digit ‘2’. There are K=1 6 states,\n",
      "each of which can generate a line segment of ﬁxed length having one of 16 possible\n",
      "angles, and so the emission distribution is simply a 16×16table of probabilities\n",
      "associated with the allowed angle values for each state index value. Transition prob-\n",
      "abilities are all set to zero except for those that keep the state index kthe same or\n",
      "that increment it by 1, and the model parameters are optimized using 25 iterations of\n",
      "EM. We can gain some insight into the resulting model by running it generatively, as\n",
      "shown in Figure 13.11.\n",
      "Figure 13.10 Lattice diagram for a 3-state left-\n",
      "to-right HMM in which the state index kis allowed\n",
      "to increase by at most 1 at each transition. k=1\n",
      "k=2\n",
      "k=3\n",
      "n−2 n−1 nn +1A11 A11 A11\n",
      "A33 A33 A3313.2. Hidden Markov Models 615\n",
      "Figure 13.11 Top row: examples of on-line handwritten\n",
      "digits. Bottom row: synthetic digits sam-\n",
      "pled generatively from a left-to-right hid-\n",
      "den Markov model that has been trainedon a data set of 45 handwritten digits.\n",
      "One of the most powerful properties of hidden Markov models is their ability to\n",
      "exhibit some degree of invariance to local warping (compression and stretching) of\n",
      "the time axis. To understand this, consider the way in which the digit ‘2’ is written\n",
      "in the on-line handwritten digits example. A typical digit comprises two distinct\n",
      "sections joined at a cusp. The ﬁrst part of the digit, which starts at the top left, has asweeping arc down to the cusp or loop at the bottom left, followed by a second more-\n",
      "or-less straight sweep ending at the bottom right. Natural variations in writing style\n",
      "will cause the relative sizes of the two sections to vary, and hence the location of thecusp or loop within the temporal sequence will vary. From a generative perspective\n",
      "such variations can be accommodated by the hidden Markov model through changes\n",
      "in the number of transitions to the same state versus the number of transitions to thesuccessive state. Note, however, that if a digit ‘2’ is written in the reverse order, that\n",
      "is, starting at the bottom right and ending at the top left, then even though the pen tip\n",
      "coordinates may be identical to an example from the training set, the probability ofthe observations under the model will be extremely small. In the speech recognition\n",
      "context, warping of the time axis is associated with natural variations in the speed of\n",
      "speech, and again the hidden Markov model can accommodate such a distortion andnot penalize it too heavily.\n",
      "13.2.1 Maximum likelihood for the HMM\n",
      "If we have observed a data set X={x1,...,xN}, we can determine the param-\n",
      "eters of an HMM using maximum likelihood. The likelihood function is obtained\n",
      "from the joint distribution (13.10) by marginalizing over the latent variables\n",
      "p(X|θ)=∑\n",
      "Zp(X,Z|θ). (13.11)\n",
      "Because the joint distribution p(X,Z|θ)does not factorize over n(in contrast to the\n",
      "mixture distribution considered in Chapter 9), we cannot simply treat each of the\n",
      "summations over znindependently. Nor can we perform the summations explicitly\n",
      "because there are Nvariables to be summed over, each of which has Kstates, re-\n",
      "sulting in a total of KNterms. Thus the number of terms in the summation grows616 13. SEQUENTIAL DATA\n",
      "exponentially with the length of the chain. In fact, the summation in (13.11) cor-\n",
      "responds to summing over exponentially many paths through the lattice diagram inFigure 13.7.\n",
      "We have already encountered a similar difﬁculty when we considered the infer-\n",
      "ence problem for the simple chain of variables in Figure 8.32. There we were ableto make use of the conditional independence properties of the graph to re-order the\n",
      "summations in order to obtain an algorithm whose cost scales linearly, instead of\n",
      "exponentially, with the length of the chain. We shall apply a similar technique to thehidden Markov model.\n",
      "A further difﬁculty with the expression (13.11) for the likelihood function is that,\n",
      "because it corresponds to a generalization of a mixture distribution, it represents a\n",
      "summation over the emission models for different settings of the latent variables.\n",
      "Direct maximization of the likelihood function will therefore lead to complex ex-pressions with no closed-form solutions, as was the case for simple mixture models Section 9.2\n",
      "(recall that a mixture model for i.i.d. data is a special case of the HMM).\n",
      "We therefore turn to the expectation maximization algorithm to ﬁnd an efﬁcient\n",
      "framework for maximizing the likelihood function in hidden Markov models. The\n",
      "EM algorithm starts with some initial selection for the model parameters, which we\n",
      "denote by θ\n",
      "old. In the E step, we take these parameter values and ﬁnd the posterior\n",
      "distribution of the latent variables p(Z|X,θold). We then use this posterior distri-\n",
      "bution to evaluate the expectation of the logarithm of the complete-data likelihood\n",
      "function, as a function of the parameters θ, to give the function Q(θ,θold)deﬁned\n",
      "by\n",
      "Q(θ,θold)=∑\n",
      "Zp(Z|X,θold)l np(X,Z|θ). (13.12)\n",
      "At this point, it is convenient to introduce some notation. We shall use γ(zn)to\n",
      "denote the marginal posterior distribution of a latent variable zn, andξ(zn−1,zn)to\n",
      "denote the joint posterior distribution of two successive latent variables, so that\n",
      "γ(zn)= p(zn|X,θold) (13.13)\n",
      "ξ(zn−1,zn)= p(zn−1,zn|X,θold). (13.14)\n",
      "For each value of n, we can store γ(zn)using a set of Knonnegative numbers\n",
      "that sum to unity, and similarly we can store ξ(zn−1,zn)using a K×Kmatrix of\n",
      "nonnegative numbers that again sum to unity. We shall also use γ(znk)to denote the\n",
      "conditional probability of znk=1, with a similar use of notation for ξ(zn−1,j,znk)\n",
      "and for other probabilistic variables introduced later. Because the expectation of abinary random variable is just the probability that it takes the value 1,w eh a v e\n",
      "γ(z\n",
      "nk)= E[znk]=∑\n",
      "zγ(z)znk (13.15)\n",
      "ξ(zn−1,j,znk)= E[zn−1,jznk]=∑\n",
      "zγ(z)zn−1,jznk. (13.16)\n",
      "If we substitute the joint distribution p(X,Z|θ)given by (13.10) into (13.12),13.2. Hidden Markov Models 617\n",
      "and make use of the deﬁnitions of γandξ, we obtain\n",
      "Q(θ,θold)=K∑\n",
      "k=1γ(z1k)l nπk+N∑\n",
      "n=2K∑\n",
      "j=1K∑\n",
      "k=1ξ(zn−1,j,znk)l nAjk\n",
      "+N∑\n",
      "n=1K∑\n",
      "k=1γ(znk)l np(xn|φk). (13.17)\n",
      "The goal of the E step will be to evaluate the quantities γ(zn)andξ(zn−1,zn)efﬁ-\n",
      "ciently, and we shall discuss this in detail shortly.\n",
      "In the M step, we maximize Q(θ,θold)with respect to the parameters θ=\n",
      "{π,A,φ}in which we treat γ(zn)andξ(zn−1,zn)as constant. Maximization with\n",
      "respect to πandAis easily achieved using appropriate Lagrange multipliers with\n",
      "the results Exercise 13.5\n",
      "πk=γ(z1k)\n",
      "K∑\n",
      "j=1γ(z1j)(13.18)\n",
      "Ajk=N∑\n",
      "n=2ξ(zn−1,j,znk)\n",
      "K∑\n",
      "l=1N∑\n",
      "n=2ξ(zn−1,j,znl). (13.19)\n",
      "The EM algorithm must be initialized by choosing starting values for πandA, which\n",
      "should of course respect the summation constraints associated with their probabilis-\n",
      "tic interpretation. Note that any elements of πorAthat are set to zero initially will\n",
      "remain zero in subsequent EM updates. A typical initialization procedure would Exercise 13.6\n",
      "involve selecting random starting values for these parameters subject to the summa-\n",
      "tion and non-negativity constraints. Note that no particular modiﬁcation to the EMresults are required for the case of left-to-right models beyond choosing initial values\n",
      "for the elements A\n",
      "jkin which the appropriate elements are set to zero, because these\n",
      "will remain zero throughout.\n",
      "To maximize Q(θ,θold)with respect to φk, we notice that only the ﬁnal term\n",
      "in (13.17) depends on φk, and furthermore this term has exactly the same form as\n",
      "the data-dependent term in the corresponding function for a standard mixture dis-\n",
      "tribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a\n",
      "Gaussian mixture. Here the quantities γ(znk)are playing the role of the responsibil-\n",
      "ities. If the parameters φkare independent for the different components, then this\n",
      "term decouples into a sum of terms one for each value of k, each of which can be\n",
      "maximized independently. We are then simply maximizing the weighted log likeli-hood function for the emission density p(x|φ\n",
      "k)with weights γ(znk). Here we shall\n",
      "suppose that this maximization can be done efﬁciently. For instance, in the case of618 13. SEQUENTIAL DATA\n",
      "Gaussian emission densities we have p(x|φk)=N(x|µk,Σk), and maximization\n",
      "of the function Q(θ,θold)then gives\n",
      "µk=N∑\n",
      "n=1γ(znk)xn\n",
      "N∑\n",
      "n=1γ(znk)(13.20)\n",
      "Σk=N∑\n",
      "n=1γ(znk)(xn−µk)(xn−µk)T\n",
      "N∑\n",
      "n=1γ(znk). (13.21)\n",
      "For the case of discrete multinomial observed variables, the conditional distribution\n",
      "of the observations takes the form\n",
      "p(x|z)=D∏\n",
      "i=1K∏\n",
      "k=1µxizk\n",
      "ik(13.22)\n",
      "and the corresponding M-step equations are given by Exercise 13.8\n",
      "µik=N∑\n",
      "n=1γ(znk)xni\n",
      "N∑\n",
      "n=1γ(znk). (13.23)\n",
      "An analogous result holds for Bernoulli observed variables.\n",
      "The EM algorithm requires initial values for the parameters of the emission dis-\n",
      "tribution. One way to set these is ﬁrst to treat the data initially as i.i.d. and ﬁt the\n",
      "emission density by maximum likelihood, and then use the resulting values to ini-tialize the parameters for EM.\n",
      "13.2.2 The forward-backward algorithm\n",
      "Next we seek an efﬁcient procedure for evaluating the quantities γ(znk)and\n",
      "ξ(zn−1,j,znk), corresponding to the E step of the EM algorithm. The graph for the\n",
      "hidden Markov model, shown in Figure 13.5, is a tree, and so we know that the\n",
      "posterior distribution of the latent variables can be obtained efﬁciently using a two-stage message passing algorithm. In the particular context of the hidden Markov Section 8.4\n",
      "model, this is known as the forward-backward algorithm (Rabiner, 1989), or the\n",
      "Baum-Welch algorithm (Baum, 1972). There are in fact several variants of the basic\n",
      "algorithm, all of which lead to the exact marginals, according to the precise form of13.2. Hidden Markov Models 619\n",
      "the messages that are propagated along the chain (Jordan, 2007). We shall focus on\n",
      "the most widely used of these, known as the alpha-beta algorithm.\n",
      "As well as being of great practical importance in its own right, the forward-\n",
      "backward algorithm provides us with a nice illustration of many of the concepts\n",
      "introduced in earlier chapters. We shall therefore begin in this section with a ‘con-ventional’ derivation of the forward-backward equations, making use of the sum\n",
      "and product rules of probability, and exploiting conditional independence properties\n",
      "which we shall obtain from the corresponding graphical model using d-separation.Then in Section 13.2.3, we shall see how the forward-backward algorithm can be\n",
      "obtained very simply as a speciﬁc example of the sum-product algorithm introduced\n",
      "in Section 8.4.4.\n",
      "It is worth emphasizing that evaluation of the posterior distributions of the latent\n",
      "variables is independent of the form of the emission density p(x|z)or indeed of\n",
      "whether the observed variables are continuous or discrete. All we require is the\n",
      "values of the quantities p(x\n",
      "n|zn)for each value of znfor every n. Also, in this\n",
      "section and the next we shall omit the explicit dependence on the model parametersθ\n",
      "oldbecause these ﬁxed throughout.\n",
      "We therefore begin by writing down the following conditional independence\n",
      "properties (Jordan, 2007)\n",
      "p(X|zn)= p(x1,...,xn|zn)\n",
      "p(xn+1,...,xN|zn) (13.24)\n",
      "p(x1,...,xn−1|xn,zn)= p(x1,...,xn−1|zn) (13.25)\n",
      "p(x1,...,xn−1|zn−1,zn)= p(x1,...,xn−1|zn−1) (13.26)\n",
      "p(xn+1,...,xN|zn,zn+1)= p(xn+1,...,xN|zn+1) (13.27)\n",
      "p(xn+2,...,xN|zn+1,xn+1)= p(xn+2,...,xN|zn+1) (13.28)\n",
      "p(X|zn−1,zn)= p(x1,...,xn−1|zn−1)\n",
      "p(xn|zn)p(xn+1,...,xN|zn)(13.29)\n",
      "p(xN+1|X,zN+1)= p(xN+1|zN+1) (13.30)\n",
      "p(zN+1|zN,X)= p(zN+1|zN) (13.31)\n",
      "whereX={x1,...,xN}. These relations are most easily proved using d-separation.\n",
      "For instance in the ﬁrst of these results, we note that every path from any one of the\n",
      "nodesx1,...,xn−1to the node xnpasses through the node zn, which is observed.\n",
      "Because all such paths are head-to-tail, it follows that the conditional independence\n",
      "property must hold. The reader should take a few moments to verify each of these\n",
      "properties in turn, as an exercise in the application of d-separation. These relations\n",
      "can also be proved directly, though with signiﬁcantly greater effort, from the joint\n",
      "distribution for the hidden Markov model using the sum and product rules of proba-bility. Exercise 13.10\n",
      "Let us begin by evaluating γ(z\n",
      "nk). Recall that for a discrete multinomial ran-\n",
      "dom variable the expected value of one of its components is just the probability ofthat component having the value 1. Thus we are interested in ﬁnding the posterior\n",
      "distribution p(z\n",
      "n|x1,...,xN)ofzngiven the observed data set x1,...,xN. This620 13. SEQUENTIAL DATA\n",
      "represents a vector of length Kwhose entries correspond to the expected values of\n",
      "znk. Using Bayes’ theorem, we have\n",
      "γ(zn)=p(zn|X)=p(X|zn)p(zn)\n",
      "p(X). (13.32)\n",
      "Note that the denominator p(X)is implicitly conditioned on the parameters θold\n",
      "of the HMM and hence represents the likelihood function. Using the conditional\n",
      "independence property (13.24), together with the product rule of probability, we\n",
      "obtain\n",
      "γ(zn)=p(x1,...,xn,zn)p(xn+1,...,xN|zn)\n",
      "p(X)=α(zn)β(zn)\n",
      "p(X)(13.33)\n",
      "where we have deﬁned\n",
      "α(zn)≡p(x1,...,xn,zn) (13.34)\n",
      "β(zn)≡p(xn+1,...,xN|zn). (13.35)\n",
      "The quantity α(zn)represents the joint probability of observing all of the given\n",
      "data up to time nand the value of zn, whereas β(zn)represents the conditional\n",
      "probability of all future data from time n+1up toNgiven the value of zn. Again,\n",
      "α(zn)andβ(zn)each represent set of Knumbers, one for each of the possible\n",
      "settings of the 1-of- Kcoded binary vector zn. We shall use the notation α(znk)to\n",
      "denote the value of α(zn)whenznk=1, with an analogous interpretation of β(znk).\n",
      "We now derive recursion relations that allow α(zn)andβ(zn)to be evaluated\n",
      "efﬁciently. Again, we shall make use of conditional independence properties, inparticular (13.25) and (13.26), together with the sum and product rules, allowing us\n",
      "to express α(z\n",
      "n)in terms of α(zn−1)as follows\n",
      "α(zn)= p(x1,...,xn,zn)\n",
      "=p(x1,...,xn|zn)p(zn)\n",
      "=p(xn|zn)p(x1,...,xn−1|zn)p(zn)\n",
      "=p(xn|zn)p(x1,...,xn−1,zn)\n",
      "=p(xn|zn)∑\n",
      "zn−1p(x1,...,xn−1,zn−1,zn)\n",
      "=p(xn|zn)∑\n",
      "zn−1p(x1,...,xn−1,zn|zn−1)p(zn−1)\n",
      "=p(xn|zn)∑\n",
      "zn−1p(x1,...,xn−1|zn−1)p(zn|zn−1)p(zn−1)\n",
      "=p(xn|zn)∑\n",
      "zn−1p(x1,...,xn−1,zn−1)p(zn|zn−1)\n",
      "Making use of the deﬁnition (13.34) for α(zn), we then obtain\n",
      "α(zn)=p(xn|zn)∑\n",
      "zn−1α(zn−1)p(zn|zn−1). (13.36)13.2. Hidden Markov Models 621\n",
      "Figure 13.12 Illustration of the forward recursion (13.36) for\n",
      "evaluation of the αvariables. In this fragment\n",
      "of the lattice, we see that the quantity α(zn1)\n",
      "is obtained by taking the elements α(zn−1,j)of\n",
      "α(zn−1)at step n−1and summing them up with\n",
      "weights given by Aj1, corresponding to the val-\n",
      "ues of p(zn|zn−1), and then multiplying by the\n",
      "data contribution p(xn|zn1).k=1\n",
      "k=2\n",
      "k=3\n",
      "n−1 nα(zn−1,1)\n",
      "α(zn−1,2)\n",
      "α(zn−1,3)α(zn,1)\n",
      "A11\n",
      "A21\n",
      "A31p(xn|zn,1)\n",
      "It is worth taking a moment to study this recursion relation in some detail. Note\n",
      "that there are Kterms in the summation, and the right-hand side has to be evaluated\n",
      "for each of the Kvalues of znso each step of the αrecursion has computational\n",
      "cost that scaled like O(K2). The forward recursion equation for α(zn)is illustrated\n",
      "using a lattice diagram in Figure 13.12.\n",
      "In order to start this recursion, we need an initial condition that is given by\n",
      "α(z1)=p(x1,z1)=p(z1)p(x1|z1)=K∏\n",
      "k=1{πkp(x1|φk)}z1k(13.37)\n",
      "which tells us that α(z1k), fork=1,...,K , takes the value πkp(x1|φk). Starting\n",
      "at the ﬁrst node of the chain, we can then work along the chain and evaluate α(zn)\n",
      "for every latent node. Because each step of the recursion involves multiplying by a\n",
      "K×Kmatrix, the overall cost of evaluating these quantities for the whole chain is\n",
      "ofO(K2N).\n",
      "We can similarly ﬁnd a recursion relation for the quantities β(zn)by making\n",
      "use of the conditional independence properties (13.27) and (13.28) giving\n",
      "β(zn)= p(xn+1,...,xN|zn)\n",
      "=∑\n",
      "zn+1p(xn+1,...,xN,zn+1|zn)\n",
      "=∑\n",
      "zn+1p(xn+1,...,xN|zn,zn+1)p(zn+1|zn)\n",
      "=∑\n",
      "zn+1p(xn+1,...,xN|zn+1)p(zn+1|zn)\n",
      "=∑\n",
      "zn+1p(xn+2,...,xN|zn+1)p(xn+1|zn+1)p(zn+1|zn).622 13. SEQUENTIAL DATA\n",
      "Figure 13.13 Illustration of the backward recursion\n",
      "(13.38) for evaluation of the βvariables. In\n",
      "this fragment of the lattice, we see that the\n",
      "quantity β(zn1)is obtained by taking the\n",
      "components β(zn+1,k)ofβ(zn+1)at step\n",
      "n+1and summing them up with weights\n",
      "given by the products of A1k, correspond-\n",
      "ing to the values of p(zn+1|zn)and the cor-\n",
      "responding values of the emission density\n",
      "p(xn|zn+1,k).k=1\n",
      "k=2\n",
      "k=3\n",
      "nn +1β(zn,1) β(zn+1,1)\n",
      "β(zn+1,2)\n",
      "β(zn+1,3)A11\n",
      "A12\n",
      "A13p(xn|zn+1,1)\n",
      "p(xn|zn+1,2)\n",
      "p(xn|zn+1,3)\n",
      "Making use of the deﬁnition (13.35) for β(zn), we then obtain\n",
      "β(zn)=∑\n",
      "zn+1β(zn+1)p(xn+1|zn+1)p(zn+1|zn). (13.38)\n",
      "Note that in this case we have a backward message passing algorithm that evaluates\n",
      "β(zn)in terms of β(zn+1). At each step, we absorb the effect of observation xn+1\n",
      "through the emission probability p(xn+1|zn+1), multiply by the transition matrix\n",
      "p(zn+1|zn), and then marginalize out zn+1. This is illustrated in Figure 13.13.\n",
      "Again we need a starting condition for the recursion, namely a value for β(zN).\n",
      "This can be obtained by setting n=Nin (13.33) and replacing α(zN)with its\n",
      "deﬁnition (13.34) to give\n",
      "p(zN|X)=p(X,zN)β(zN)\n",
      "p(X)(13.39)\n",
      "which we see will be correct provided we take β(zN)=1 for all settings of zN.\n",
      "In the M step equations, the quantity p(X)will cancel out, as can be seen, for\n",
      "instance, in the M-step equation for µkgiven by (13.20), which takes the form\n",
      "µk=n∑\n",
      "n=1γ(znk)xn\n",
      "n∑\n",
      "n=1γ(znk)=n∑\n",
      "n=1α(znk)β(znk)xn\n",
      "n∑\n",
      "n=1α(znk)β(znk). (13.40)\n",
      "However, the quantity p(X)represents the likelihood function whose value we typ-\n",
      "ically wish to monitor during the EM optimization, and so it is useful to be able to\n",
      "evaluate it. If we sum both sides of (13.33) over zn, and use the fact that the left-hand\n",
      "side is a normalized distribution, we obtain\n",
      "p(X)=∑\n",
      "znα(zn)β(zn). (13.41)13.2. Hidden Markov Models 623\n",
      "Thus we can evaluate the likelihood function by computing this sum, for any conve-\n",
      "nient choice of n. For instance, if we only want to evaluate the likelihood function,\n",
      "then we can do this by running the αrecursion from the start to the end of the chain,\n",
      "and then use this result for n=N, making use of the fact that β(zN)is a vector of\n",
      "1s. In this case no βrecursion is required, and we simply have\n",
      "p(X)=∑\n",
      "zNα(zN). (13.42)\n",
      "Let us take a moment to interpret this result for p(X). Recall that to compute the\n",
      "likelihood we should take the joint distribution p(X,Z)and sum over all possible\n",
      "values of Z. Each such value represents a particular choice of hidden state for every\n",
      "time step, in other words every term in the summation is a path through the lattice\n",
      "diagram, and recall that there are exponentially many such paths. By expressingthe likelihood function in the form (13.42), we have reduced the computational cost\n",
      "from being exponential in the length of the chain to being linear by swapping the\n",
      "order of the summation and multiplications, so that at each time step nwe sum\n",
      "the contributions from all paths passing through each of the states z\n",
      "nkto give the\n",
      "intermediate quantities α(zn).\n",
      "Next we consider the evaluation of the quantities ξ(zn−1,zn), which correspond\n",
      "to the values of the conditional probabilities p(zn−1,zn|X)for each of the K×K\n",
      "settings for (zn−1,zn). Using the deﬁnition of ξ(zn−1,zn), and applying Bayes’\n",
      "theorem, we have\n",
      "ξ(zn−1,zn)=p(zn−1,zn|X)\n",
      "=p(X|zn−1,zn)p(zn−1,zn)\n",
      "p(X)\n",
      "=p(x1,...,xn−1|zn−1)p(xn|zn)p(xn+1,...,xN|zn)p(zn|zn−1)p(zn−1)\n",
      "p(X)\n",
      "=α(zn−1)p(xn|zn)p(zn|zn−1)β(zn)\n",
      "p(X)(13.43)\n",
      "where we have made use of the conditional independence property (13.29) together\n",
      "with the deﬁnitions of α(zn)andβ(zn)given by (13.34) and (13.35). Thus we can\n",
      "calculate the ξ(zn−1,zn)directly by using the results of the αandβrecursions.\n",
      "Let us summarize the steps required to train a hidden Markov model using\n",
      "the EM algorithm. We ﬁrst make an initial selection of the parameters θoldwhere\n",
      "θ≡(π,A,φ). TheAandπparameters are often initialized either uniformly or\n",
      "randomly from a uniform distribution (respecting their non-negativity and summa-\n",
      "tion constraints). Initialization of the parameters φwill depend on the form of the\n",
      "distribution. For instance in the case of Gaussians, the parameters µkmight be ini-\n",
      "tialized by applying the K-means algorithm to the data, and Σkmight be initialized\n",
      "to the covariance matrix of the corresponding Kmeans cluster. Then we run both\n",
      "the forward αrecursion and the backward βrecursion and use the results to evaluate\n",
      "γ(zn)andξ(zn−1,zn). At this stage, we can also evaluate the likelihood function.624 13. SEQUENTIAL DATA\n",
      "This completes the E step, and we use the results to ﬁnd a revised set of parameters\n",
      "θnewusing the M-step equations from Section 13.2.1. We then continue to alternate\n",
      "between E and M steps until some convergence criterion is satisﬁed, for instance\n",
      "when the change in the likelihood function is below some threshold.\n",
      "Note that in these recursion relations the observations enter through conditional\n",
      "distributions of the form p(xn|zn). The recursions are therefore independent of\n",
      "the type or dimensionality of the observed variables or the form of this conditional\n",
      "distribution, so long as its value can be computed for each of the Kpossible states\n",
      "ofzn. Since the observed variables {xn}are ﬁxed, the quantities p(xn|zn)can be\n",
      "pre-computed as functions of znat the start of the EM algorithm, and remain ﬁxed\n",
      "throughout.\n",
      "We have seen in earlier chapters that the maximum likelihood approach is most\n",
      "effective when the number of data points is large in relation to the number of parame-ters. Here we note that a hidden Markov model can be trained effectively, using max-\n",
      "imum likelihood, provided the training sequence is sufﬁciently long. Alternatively,\n",
      "we can make use of multiple shorter sequences, which requires a straightforwardmodiﬁcation of the hidden Markov model EM algorithm. In the case of left-to-right Exercise 13.12\n",
      "models, this is particularly important because, in a given observation sequence, a\n",
      "given state transition corresponding to a nondiagonal element of Awill seen at most\n",
      "once.\n",
      "Another quantity of interest is the predictive distribution, in which the observed\n",
      "data isX={x\n",
      "1,...,xN}and we wish to predict xN+1, which would be important\n",
      "for real-time applications such as ﬁnancial forecasting. Again we make use of the\n",
      "sum and product rules together with the conditional independence properties (13.29)\n",
      "and (13.31) giving\n",
      "p(xN+1|X)=∑\n",
      "zN+1p(xN+1,zN+1|X)\n",
      "=∑\n",
      "zN+1p(xN+1|zN+1)p(zN+1|X)\n",
      "=∑\n",
      "zN+1p(xN+1|zN+1)∑\n",
      "zNp(zN+1,zN|X)\n",
      "=∑\n",
      "zN+1p(xN+1|zN+1)∑\n",
      "zNp(zN+1|zN)p(zN|X)\n",
      "=∑\n",
      "zN+1p(xN+1|zN+1)∑\n",
      "zNp(zN+1|zN)p(zN,X)\n",
      "p(X)\n",
      "=1\n",
      "p(X)∑\n",
      "zN+1p(xN+1|zN+1)∑\n",
      "zNp(zN+1|zN)α(zN)(13.44)\n",
      "which can be evaluated by ﬁrst running a forward αrecursion and then computing\n",
      "the ﬁnal summations over zNandzN+1. The result of the ﬁrst summation over zN\n",
      "can be stored and used once the value of xN+1is observed in order to run the α\n",
      "recursion forward to the next step in order to predict the subsequent value xN+2.13.2. Hidden Markov Models 625\n",
      "Figure 13.14 A fragment of the fac-\n",
      "tor graph representation for the hidden\n",
      "Markov model.χψ n\n",
      "g1 gn−1 gnz1 zn−1 zn\n",
      "x1 xn−1 xn\n",
      "Note that in (13.44), the inﬂuence of all data from x1toxNis summarized in the K\n",
      "values of α(zN). Thus the predictive distribution can be carried forward indeﬁnitely\n",
      "using a ﬁxed amount of storage, as may be required for real-time applications.\n",
      "Here we have discussed the estimation of the parameters of an HMM using max-\n",
      "imum likelihood. This framework is easily extended to regularized maximum likeli-\n",
      "hood by introducing priors over the model parameters π,Aandφwhose values are\n",
      "then estimated by maximizing their posterior probability. This can again be done us-\n",
      "ing the EM algorithm in which the E step is the same as discussed above, and the M\n",
      "step involves adding the log of the prior distribution p(θ)to the function Q(θ,θold)\n",
      "before maximization and represents a straightforward application of the techniques\n",
      "developed at various points in this book. Furthermore, we can use variational meth-\n",
      "ods to give a fully Bayesian treatment of the HMM in which we marginalize over the Section 10.1\n",
      "parameter distributions (MacKay, 1997). As with maximum likelihood, this leads to\n",
      "a two-pass forward-backward recursion to compute posterior probabilities.\n",
      "13.2.3 The sum-product algorithm for the HMM\n",
      "The directed graph that represents the hidden Markov model, shown in Fig-\n",
      "ure 13.5, is a tree and so we can solve the problem of ﬁnding local marginals for the\n",
      "hidden variables using the sum-product algorithm. Not surprisingly, this turns out to Section 8.4.4\n",
      "be equivalent to the forward-backward algorithm considered in the previous section,\n",
      "and so the sum-product algorithm therefore provides us with a simple way to derive\n",
      "the alpha-beta recursion formulae.\n",
      "We begin by transforming the directed graph of Figure 13.5 into a factor graph,\n",
      "of which a representative fragment is shown in Figure 13.14. This form of the fac-\n",
      "tor graph shows all variables, both latent and observed, explicitly. However, for\n",
      "the purpose of solving the inference problem, we shall always be conditioning on\n",
      "the variables x1,...,xN, and so we can simplify the factor graph by absorbing the\n",
      "emission probabilities into the transition probability factors. This leads to the sim-\n",
      "pliﬁed factor graph representation in Figure 13.15, in which the factors are given\n",
      "by\n",
      "h(z1)= p(z1)p(x1|z1) (13.45)\n",
      "fn(zn−1,zn)= p(zn|zn−1)p(xn|zn). (13.46)626 13. SEQUENTIAL DATA\n",
      "Figure 13.15 A simpliﬁed form of fac-\n",
      "tor graph to describe the hidden Markov\n",
      "model.h fn\n",
      "z1 zn−1 zn\n",
      "To derive the alpha-beta algorithm, we denote the ﬁnal hidden variable zNas\n",
      "the root node, and ﬁrst pass messages from the leaf node hto the root. From the\n",
      "general results (8.66) and (8.69) for message propagation, we see that the messages\n",
      "which are propagated in the hidden Markov model take the form\n",
      "µzn−1→fn(zn−1)= µfn−1→zn−1(zn−1) (13.47)\n",
      "µfn→zn(zn)=∑\n",
      "zn−1fn(zn−1,zn)µzn−1→fn(zn−1) (13.48)\n",
      "These equations represent the propagation of messages forward along the chain and\n",
      "are equivalent to the alpha recursions derived in the previous section, as we shall\n",
      "now show. Note that because the variable nodes znhave only two neighbours, they\n",
      "perform no computation.\n",
      "We can eliminate µzn−1→fn(zn−1)from (13.48) using (13.47) to give a recur-\n",
      "sion for the f→zmessages of the form\n",
      "µfn→zn(zn)=∑\n",
      "zn−1fn(zn−1,zn)µfn−1→zn−1(zn−1). (13.49)\n",
      "If we now recall the deﬁnition (13.46), and if we deﬁne\n",
      "α(zn)=µfn→zn(zn) (13.50)\n",
      "then we obtain the alpha recursion given by (13.36). We also need to verify that\n",
      "the quantities α(zn)are themselves equivalent to those deﬁned previously. This\n",
      "is easily done by using the initial condition (8.71) and noting that α(z1)is given\n",
      "byh(z1)=p(z1)p(x1|z1)which is identical to (13.37). Because the initial αis\n",
      "the same, and because they are iteratively computed using the same equation, all\n",
      "subsequent αquantities must be the same.\n",
      "Next we consider the messages that are propagated from the root node back to\n",
      "the leaf node. These take the form\n",
      "µfn+1→fn(zn)=∑\n",
      "zn+1fn+1(zn,zn+1)µfn+2→fn+1(zn+1) (13.51)\n",
      "where, as before, we have eliminated the messages of the type z→fsince the\n",
      "variable nodes perform no computation. Using the deﬁnition (13.46) to substitute\n",
      "forfn+1(zn,zn+1), and deﬁning\n",
      "β(zn)=µfn+1→zn(zn) (13.52)13.2. Hidden Markov Models 627\n",
      "we obtain the beta recursion given by (13.38). Again, we can verify that the beta\n",
      "variables themselves are equivalent by noting that (8.70) implies that the initial mes-sage send by the root variable node is µ\n",
      "zN→fN(zN)=1 , which is identical to the\n",
      "initialization of β(zN)given in Section 13.2.2.\n",
      "The sum-product algorithm also speciﬁes how to evaluate the marginals once all\n",
      "the messages have been evaluated. In particular, the result (8.63) shows that the local\n",
      "marginal at the node znis given by the product of the incoming messages. Because\n",
      "we have conditioned on the variables X={x1,...,xN}, we are computing the\n",
      "joint distribution\n",
      "p(zn,X)=µfn→zn(zn)µfn+1→zn(zn)=α(zn)β(zn). (13.53)\n",
      "Dividing both sides by p(X), we then obtain\n",
      "γ(zn)=p(zn,X)\n",
      "p(X)=α(zn)β(zn)\n",
      "p(X)(13.54)\n",
      "in agreement with (13.33). The result (13.43) can similarly be derived from (8.72). Exercise 13.11\n",
      "13.2.4 Scaling factors\n",
      "There is an important issue that must be addressed before we can make use of the\n",
      "forward backward algorithm in practice. From the recursion relation (13.36), we note\n",
      "that at each step the new value α(zn)is obtained from the previous value α(zn−1)\n",
      "by multiplying by quantities p(zn|zn−1)andp(xn|zn). Because these probabilities\n",
      "are often signiﬁcantly less than unity, as we work our way forward along the chain,\n",
      "the values of α(zn)can go to zero exponentially quickly. For moderate lengths of\n",
      "chain (say 100 or so), the calculation of the α(zn)will soon exceed the dynamic\n",
      "range of the computer, even if double precision ﬂoating point is used.\n",
      "In the case of i.i.d. data, we implicitly circumvented this problem with the eval-\n",
      "uation of likelihood functions by taking logarithms. Unfortunately, this will not help\n",
      "here because we are forming sums of products of small numbers (we are in fact im-\n",
      "plicitly summing over all possible paths through the lattice diagram of Figure 13.7).\n",
      "We therefore work with re-scaled versions of α(zn)andβ(zn)whose values remain\n",
      "of order unity. As we shall see, the corresponding scaling factors cancel out whenwe use these re-scaled quantities in the EM algorithm.\n",
      "In (13.34), we deﬁned α(z\n",
      "n)=p(x1,...,xn,zn)representing the joint distri-\n",
      "bution of all the observations up to xnand the latent variable zn. Now we deﬁne a\n",
      "normalized version of αgiven by\n",
      "ˆα(zn)=p(zn|x1,...,xn)=α(zn)\n",
      "p(x1,...,xn)(13.55)\n",
      "which we expect to be well behaved numerically because it is a probability distribu-\n",
      "tion over Kvariables for any value of n. In order to relate the scaled and original al-\n",
      "pha variables, we introduce scaling factors deﬁned by conditional distributions overthe observed variables\n",
      "c\n",
      "n=p(xn|x1,...,xn−1). (13.56)628 13. SEQUENTIAL DATA\n",
      "From the product rule, we then have\n",
      "p(x1,...,xn)=n∏\n",
      "m=1cm (13.57)\n",
      "and so\n",
      "α(zn)=p(zn|x1,...,xn)p(x1,...,xn)=(n∏\n",
      "m=1cm)\n",
      "ˆα(zn). (13.58)\n",
      "We can then turn the recursion equation (13.36) for αinto one for ˆαgiven by\n",
      "cnˆα(zn)=p(xn|zn)∑\n",
      "zn−1ˆα(zn−1)p(zn|zn−1). (13.59)\n",
      "Note that at each stage of the forward message passing phase, used to evaluate ˆα(zn),\n",
      "we have to evaluate and store cn, which is easily done because it is the coefﬁcient\n",
      "that normalizes the right-hand side of (13.59) to give ˆα(zn).\n",
      "We can similarly deﬁne re-scaled variables ˆβ(zn)using\n",
      "β(zn)=(N∏\n",
      "m=n+1cm)\n",
      "ˆβ(zn) (13.60)\n",
      "which will again remain within machine precision because, from (13.35), the quan-\n",
      "titiesˆβ(zn)are simply the ratio of two conditional probabilities\n",
      "ˆβ(zn)=p(xn+1,...,xN|zn)\n",
      "p(xn+1,...,xN|x1,...,xn). (13.61)\n",
      "The recursion result (13.38) for βthen gives the following recursion for the re-scaled\n",
      "variables\n",
      "cn+1ˆβ(zn)=∑\n",
      "zn+1ˆβ(zn+1)p(xn+1|zn+1)p(zn+1|zn). (13.62)\n",
      "In applying this recursion relation, we make use of the scaling factors cnthat were\n",
      "previously computed in the αphase.\n",
      "From (13.57), we see that the likelihood function can be found using\n",
      "p(X)=N∏\n",
      "n=1cn. (13.63)\n",
      "Similarly, using (13.33) and (13.43), together with (13.63), we see that the required\n",
      "marginals are given by Exercise 13.15\n",
      "γ(zn)=ˆα(zn)ˆβ(zn) (13.64)\n",
      "ξ(zn−1,zn)= cnˆα(zn−1)p(xn|zn)p(zn|z−1)ˆβ(zn). (13.65)13.2. Hidden Markov Models 629\n",
      "Finally, we note that there is an alternative formulation of the forward-backward\n",
      "algorithm (Jordan, 2007) in which the backward pass is deﬁned by a recursion based\n",
      "the quantities γ(zn)=ˆα(zn)ˆβ(zn)instead of using ˆβ(zn). This α–γrecursion\n",
      "requires that the forward pass be completed ﬁrst so that all the quantities ˆα(zn)\n",
      "are available for the backward pass, whereas the forward and backward passes of\n",
      "theα–βalgorithm can be done independently. Although these two algorithms have\n",
      "comparable computational cost, the α–βversion is the most commonly encountered\n",
      "one in the case of hidden Markov models, whereas for linear dynamical systems a Section 13.3\n",
      "recursion analogous to the α–γform is more usual.\n",
      "13.2.5 The Viterbi algorithm\n",
      "In many applications of hidden Markov models, the latent variables have some\n",
      "meaningful interpretation, and so it is often of interest to ﬁnd the most probablesequence of hidden states for a given observation sequence. For instance in speech\n",
      "recognition, we might wish to ﬁnd the most probable phoneme sequence for a given\n",
      "series of acoustic observations. Because the graph for the hidden Markov model isa directed tree, this problem can be solved exactly using the max-sum algorithm.\n",
      "We recall from our discussion in Section 8.4.5 that the problem of ﬁnding the most\n",
      "probable sequence of latent states is not the same as that of ﬁnding the set of statesthat are individually the most probable. The latter problem can be solved by ﬁrst\n",
      "running the forward-backward (sum-product) algorithm to ﬁnd the latent variable\n",
      "marginals γ(z\n",
      "n)and then maximizing each of these individually (Duda et al. , 2001).\n",
      "However, the set of such states will not, in general, correspond to the most probable\n",
      "sequence of states. In fact, this set of states might even represent a sequence havingzero probability, if it so happens that two successive states, which in isolation are\n",
      "individually the most probable, are such that the transition matrix element connecting\n",
      "them is zero.\n",
      "In practice, we are usually interested in ﬁnding the most probable sequence of\n",
      "states, and this can be solved efﬁciently using the max-sum algorithm, which in the\n",
      "context of hidden Markov models is known as the Viterbi algorithm (Viterbi, 1967).\n",
      "Note that the max-sum algorithm works with log probabilities and so there is no\n",
      "need to use re-scaled variables as was done with the forward-backward algorithm.\n",
      "Figure 13.16 shows a fragment of the hidden Markov model expanded as latticediagram. As we have already noted, the number of possible paths through the lattice\n",
      "grows exponentially with the length of the chain. The Viterbi algorithm searches this\n",
      "space of paths efﬁciently to ﬁnd the most probable path with a computational costthat grows only linearly with the length of the chain.\n",
      "As with the sum-product algorithm, we ﬁrst represent the hidden Markov model\n",
      "as a factor graph, as shown in Figure 13.15. Again, we treat the variable node z\n",
      "N\n",
      "as the root, and pass messages to the root starting with the leaf nodes. Using the\n",
      "results (8.93) and (8.94), we see that the messages passed in the max-sum algorithmare given by\n",
      "µ\n",
      "zn→fn+1(zn)= µfn→zn(zn) (13.66)\n",
      "µfn+1→zn+1(zn+1) = max\n",
      "zn{\n",
      "lnfn+1(zn,zn+1)+µzn→fn+1(zn)}\n",
      ".(13.67)630 13. SEQUENTIAL DATA\n",
      "Figure 13.16 A fragment of the HMM lattice\n",
      "showing two possible paths. The Viterbi algorithm\n",
      "efﬁciently determines the most probable path from\n",
      "amongst the exponentially many possibilities. For\n",
      "any given path, the corresponding probability is\n",
      "given by the product of the elements of the tran-\n",
      "sition matrix Ajk, corresponding to the probabil-\n",
      "itiesp(zn+1|zn)for each segment of the path,\n",
      "along with the emission densities p(xn|k)asso-\n",
      "ciated with each node on the path.k=1\n",
      "k=2\n",
      "k=3\n",
      "n−2 n−1 nn +1\n",
      "If we eliminate µzn→fn+1(zn)between these two equations, and make use of (13.46),\n",
      "we obtain a recursion for the f→zmessages of the form\n",
      "ω(zn+1)=l n p(xn+1|zn+1) + max\n",
      "zn{lnp(x+1|zn)+ω(zn)} (13.68)\n",
      "where we have introduced the notation ω(zn)≡µfn→zn(zn).\n",
      "From (8.95) and (8.96), these messages are initialized using\n",
      "ω(z1)=l n p(z1)+l n p(x1|z1). (13.69)\n",
      "where we have used (13.45). Note that to keep the notation uncluttered, we omit\n",
      "the dependence on the model parameters θthat are held ﬁxed when ﬁnding the most\n",
      "probable sequence.\n",
      "The Viterbi algorithm can also be derived directly from the deﬁnition (13.6) of\n",
      "the joint distribution by taking the logarithm and then exchanging maximizations\n",
      "and summations. It is easily seen that the quantities ω(zn)have the probabilistic Exercise 13.16\n",
      "interpretation\n",
      "ω(zn)= m a x\n",
      "z1,...,zn−1p(x1,...,xn,z1,...,zn). (13.70)\n",
      "Once we have completed the ﬁnal maximization over zN, we will obtain the\n",
      "value of the joint distribution p(X,Z)corresponding to the most probable path. We\n",
      "also wish to ﬁnd the sequence of latent variable values that corresponds to this path.\n",
      "To do this, we simply make use of the back-tracking procedure discussed in Sec-\n",
      "tion 8.4.5. Speciﬁcally, we note that the maximization over znmust be performed\n",
      "for each of the Kpossible values of zn+1. Suppose we keep a record of the values\n",
      "ofznthat correspond to the maxima for each value of the Kvalues of zn+1. Let us\n",
      "denote this function by ψ(kn)where k∈{1,...,K }. Once we have passed mes-\n",
      "sages to the end of the chain and found the most probable state of zN, we can then\n",
      "use this function to backtrack along the chain by applying it recursively\n",
      "kmax\n",
      "n=ψ(kmax\n",
      "n+1). (13.71)13.2. Hidden Markov Models 631\n",
      "Intuitively, we can understand the Viterbi algorithm as follows. Naively, we\n",
      "could consider explicitly all of the exponentially many paths through the lattice,evaluate the probability for each, and then select the path having the highest proba-\n",
      "bility. However, we notice that we can make a dramatic saving in computational cost\n",
      "as follows. Suppose that for each path we evaluate its probability by summing upproducts of transition and emission probabilities as we work our way forward along\n",
      "each path through the lattice. Consider a particular time step nand a particular state\n",
      "kat that time step. There will be many possible paths converging on the correspond-\n",
      "ing node in the lattice diagram. However, we need only retain that particular path\n",
      "that so far has the highest probability. Because there are Kstates at time step n,w e\n",
      "need to keep track of Ksuch paths. At time step n+1, there will be K\n",
      "2possible\n",
      "paths to consider, comprising Kpossible paths leading out of each of the Kcurrent\n",
      "states, but again we need only retain Kof these corresponding to the best path for\n",
      "each state at time n+1. When we reach the ﬁnal time step Nwe will discover which\n",
      "state corresponds to the overall most probable path. Because there is a unique path\n",
      "coming into that state we can trace the path back to step N−1to see what state it\n",
      "occupied at that time, and so on back through the lattice to the state n=1.\n",
      "13.2.6 Extensions of the hidden Markov model\n",
      "The basic hidden Markov model, along with the standard training algorithm\n",
      "based on maximum likelihood, has been extended in numerous ways to meet therequirements of particular applications. Here we discuss a few of the more important\n",
      "examples.\n",
      "We see from the digits example in Figure 13.11 that hidden Markov models can\n",
      "be quite poor generative models for the data, because many of the synthetic digits\n",
      "look quite unrepresentative of the training data. If the goal is sequence classiﬁca-\n",
      "tion, there can be signiﬁcant beneﬁt in determining the parameters of hidden Markovmodels using discriminative rather than maximum likelihood techniques. Suppose\n",
      "we have a training set of Robservation sequences X\n",
      "r, where r=1,...,R , each of\n",
      "which is labelled according to its class m, where m=1,...,M . For each class, we\n",
      "have a separate hidden Markov model with its own parameters θm, and we treat the\n",
      "problem of determining the parameter values as a standard classiﬁcation problem in\n",
      "which we optimize the cross-entropy\n",
      "R∑\n",
      "r=1lnp(mr|Xr). (13.72)\n",
      "Using Bayes’ theorem this can be expressed in terms of the sequence probabilities\n",
      "associated with the hidden Markov models\n",
      "R∑\n",
      "r=1ln{\n",
      "p(Xr|θr)p(mr)∑M\n",
      "l=1p(Xr|θl)p(lr)}\n",
      "(13.73)\n",
      "where p(m)is the prior probability of class m. Optimization of this cost function\n",
      "is more complex than for maximum likelihood (Kapadia, 1998), and in particular632 13. SEQUENTIAL DATA\n",
      "Figure 13.17 Section of an autoregressive hidden\n",
      "Markov model, in which the distribution\n",
      "of the observation xndepends on a\n",
      "subset of the previous observations as\n",
      "well as on the hidden state zn. In this\n",
      "example, the distribution of xndepends\n",
      "on the two previous observations xn−1\n",
      "andxn−2.zn−1 zn zn+1\n",
      "xn−1 xn xn+1\n",
      "requires that every training sequence be evaluated under each of the models in or-\n",
      "der to compute the denominator in (13.73). Hidden Markov models, coupled with\n",
      "discriminative training methods, are widely used in speech recognition (Kapadia,\n",
      "1998).\n",
      "A signiﬁcant weakness of the hidden Markov model is the way in which it rep-\n",
      "resents the distribution of times for which the system remains in a given state. To see\n",
      "the problem, note that the probability that a sequence sampled from a given hidden\n",
      "Markov model will spend precisely Tsteps in state kand then make a transition to a\n",
      "different state is given by\n",
      "p(T)=(Akk)T(1−Akk)∝exp (−TlnAkk) (13.74)\n",
      "and so is an exponentially decaying function of T. For many applications, this will\n",
      "be a very unrealistic model of state duration. The problem can be resolved by mod-\n",
      "elling state duration directly in which the diagonal coefﬁcients Akkare all set to zero,\n",
      "and each state kis explicitly associated with a probability distribution p(T|k)of pos-\n",
      "sible duration times. From a generative point of view, when a state kis entered, a\n",
      "valueTrepresenting the number of time steps that the system will remain in state k\n",
      "is then drawn from p(T|k). The model then emits Tvalues of the observed variable\n",
      "xt, which are generally assumed to be independent so that the corresponding emis-\n",
      "sion density is simply∏T\n",
      "t=1p(xt|k). This approach requires some straightforward\n",
      "modiﬁcations to the EM optimization procedure (Rabiner, 1989).\n",
      "Another limitation of the standard HMM is that it is poor at capturing long-\n",
      "range correlations between the observed variables (i.e., between variables that are\n",
      "separated by many time steps) because these must be mediated via the ﬁrst-order\n",
      "Markov chain of hidden states. Longer-range effects could in principle be included\n",
      "by adding extra links to the graphical model of Figure 13.5. One way to address this\n",
      "is to generalize the HMM to give the autoregressive hidden Markov model (Ephraim\n",
      "et al. , 1989), an example of which is shown in Figure 13.17. For discrete observa-\n",
      "tions, this corresponds to expanded tables of conditional probabilities for the emis-\n",
      "sion distributions. In the case of a Gaussian emission density, we can use the linear-\n",
      "Gaussian framework in which the conditional distribution for xngiven the values\n",
      "of the previous observations, and the value of zn, is a Gaussian whose mean is a\n",
      "linear combination of the values of the conditioning variables. Clearly the number\n",
      "of additional links in the graph must be limited to avoid an excessive the number of\n",
      "free parameters. In the example shown in Figure 13.17, each observation depends on13.2. Hidden Markov Models 633\n",
      "Figure 13.18 Example of an input-output hidden\n",
      "Markov model. In this case, both the\n",
      "emission probabilities and the transition\n",
      "probabilities depend on the values of a\n",
      "sequence of observations u1,...,uN.\n",
      "zn−1 zn zn+1\n",
      "xn−1 xn xn+1un−1 un un+1\n",
      "the two preceding observed variables as well as on the hidden state. Although this\n",
      "graph looks messy, we can again appeal to d-separation to see that in fact it still has\n",
      "a simple probabilistic structure. In particular, if we imagine conditioning on znwe\n",
      "see that, as with the standard HMM, the values of zn−1andzn+1are independent,\n",
      "corresponding to the conditional independence property (13.5). This is easily veri-\n",
      "ﬁed by noting that every path from node zn−1to node zn+1passes through at least\n",
      "one observed node that is head-to-tail with respect to that path. As a consequence,\n",
      "we can again use a forward-backward recursion in the E step of the EM algorithm to\n",
      "determine the posterior distributions of the latent variables in a computational time\n",
      "that is linear in the length of the chain. Similarly, the M step involves only a minor\n",
      "modiﬁcation of the standard M-step equations. In the case of Gaussian emission\n",
      "densities this involves estimating the parameters using the standard linear regression\n",
      "equations, discussed in Chapter 3.\n",
      "We have seen that the autoregressive HMM appears as a natural extension of the\n",
      "standard HMM when viewed as a graphical model. In fact the probabilistic graphical\n",
      "modelling viewpoint motivates a plethora of different graphical structures based on\n",
      "the HMM. Another example is the input-output hidden Markov model (Bengio and\n",
      "Frasconi, 1995), in which we have a sequence of observed variables u1,...,uN,i n\n",
      "addition to the output variables x1,...,xN, whose values inﬂuence either the dis-\n",
      "tribution of latent variables or output variables, or both. An example is shown in\n",
      "Figure 13.18. This extends the HMM framework to the domain of supervised learn-\n",
      "ing for sequential data. It is again easy to show, through the use of the d-separation\n",
      "criterion, that the Markov property (13.5) for the chain of latent variables still holds.\n",
      "To verify this, simply note that there is only one path from node zn−1to node zn+1\n",
      "and this is head-to-tail with respect to the observed node zn. This conditional inde-\n",
      "pendence property again allows the formulation of a computationally efﬁcient learn-\n",
      "ing algorithm. In particular, we can determine the parameters θof the model by\n",
      "maximizing the likelihood function L(θ)=p(X|U,θ)whereUis a matrix whose\n",
      "rows are given by uT\n",
      "n. As a consequence of the conditional independence property\n",
      "(13.5) this likelihood function can be maximized efﬁciently using an EM algorithm\n",
      "in which the E step involves forward and backward recursions. Exercise 13.18\n",
      "Another variant of the HMM worthy of mention is the factorial hidden Markov\n",
      "model (Ghahramani and Jordan, 1997), in which there are multiple independent634 13. SEQUENTIAL DATA\n",
      "Figure 13.19 A factorial hidden Markov model com-\n",
      "prising two Markov chains of latent vari-\n",
      "ables. For continuous observed variables\n",
      "x, one possible choice of emission model\n",
      "is a linear-Gaussian density in which the\n",
      "mean of the Gaussian is a linear combi-\n",
      "nation of the states of the corresponding\n",
      "latent variables.z(1)\n",
      "n−1 z(1)\n",
      "nz(1)\n",
      "n+1z(2)\n",
      "n−1 z(2)\n",
      "nz(2)\n",
      "n+1\n",
      "xn−1 xn xn+1\n",
      "Markov chains of latent variables, and the distribution of the observed variable at\n",
      "a given time step is conditional on the states of all of the corresponding latent vari-\n",
      "ables at that same time step. Figure 13.19 shows the corresponding graphical model.\n",
      "The motivation for considering factorial HMM can be seen by noting that in order to\n",
      "represent, say, 10 bits of information at a given time step, a standard HMM would\n",
      "needK=210= 1024 latent states, whereas a factorial HMM could make use of 10\n",
      "binary latent chains. The primary disadvantage of factorial HMMs, however, lies in\n",
      "the additional complexity of training them. The M step for the factorial HMM model\n",
      "is straightforward. However, observation of the xvariables introduces dependencies\n",
      "between the latent chains, leading to difﬁculties with the E step. This can be seen\n",
      "by noting that in Figure 13.19, the variables z(1)\n",
      "nandz(2)\n",
      "nare connected by a path\n",
      "which is head-to-head at node xnand hence they are not d-separated. The exact E\n",
      "step for this model does notcorrespond to running forward and backward recursions\n",
      "along the MMarkov chains independently. This is conﬁrmed by noting that the key\n",
      "conditional independence property (13.5) is not satisﬁed for the individual Markov\n",
      "chains in the factorial HMM model, as is shown using d-separation in Figure 13.20.\n",
      "Now suppose that there are Mchains of hidden nodes and for simplicity suppose\n",
      "that all latent variables have the same number Kof states. Then one approach would\n",
      "be to note that there are KMcombinations of latent variables at a given time step\n",
      "Figure 13.20 Example of a path, highlighted in green,\n",
      "which is head-to-head at the observed\n",
      "nodes xn−1andxn+1, and head-to-tail\n",
      "at the unobserved nodes z(2)\n",
      "n−1,z(2)\n",
      "nand\n",
      "z(2)\n",
      "n+1. Thus the path is not blocked and\n",
      "so the conditional independence property\n",
      "(13.5) does not hold for the individual la-\n",
      "tent chains of the factorial HMM model.\n",
      "As a consequence, there is no efﬁcient\n",
      "exact E step for this model.z(1)\n",
      "n−1 z(1)\n",
      "nz(1)\n",
      "n+1z(2)\n",
      "n−1 z(2)\n",
      "nz(2)\n",
      "n+1\n",
      "xn−1 xn xn+113.3. Linear Dynamical Systems 635\n",
      "and so we can transform the model into an equivalent standard HMM having a single\n",
      "chain of latent variables each of which has KMlatent states. We can then run the\n",
      "standard forward-backward recursions in the E step. This has computational com-\n",
      "plexity O(NK2M)that is exponential in the number Mof latent chains and so will\n",
      "be intractable for anything other than small values of M. One solution would be\n",
      "to use sampling methods (discussed in Chapter 11). As an elegant deterministic al-\n",
      "ternative, Ghahramani and Jordan (1997) exploited variational inference techniques Section 10.1\n",
      "to obtain a tractable algorithm for approximate inference. This can be done usinga simple variational posterior distribution that is fully factorized with respect to the\n",
      "latent variables, or alternatively by using a more powerful approach in which the\n",
      "variational distribution is described by independent Markov chains corresponding to\n",
      "the chains of latent variables in the original model. In the latter case, the variational\n",
      "inference algorithms involves running independent forward and backward recursionsalong each chain, which is computationally efﬁcient and yet is also able to capture\n",
      "correlations between variables within the same chain.\n",
      "Clearly, there are many possible probabilistic structures that can be constructed\n",
      "according to the needs of particular applications. Graphical models provide a general\n",
      "technique for motivating, describing, and analysing such structures, and variational\n",
      "methods provide a powerful framework for performing inference in those models forwhich exact solution is intractable.\n",
      "13.3. Linear Dynamical Systems\n",
      "In order to motivate the concept of linear dynamical systems, let us consider thefollowing simple problem, which often arises in practical settings. Suppose we wish\n",
      "to measure the value of an unknown quantity zusing a noisy sensor that returns a\n",
      "observation xrepresenting the value of zplus zero-mean Gaussian noise. Given a\n",
      "single measurement, our best guess for zis to assume that z=x. However, we\n",
      "can improve our estimate for zby taking lots of measurements and averaging them,\n",
      "because the random noise terms will tend to cancel each other. Now let’s make thesituation more complicated by assuming that we wish to measure a quantity zthat\n",
      "is changing over time. We can take regular measurements of xso that at some point\n",
      "in time we have obtained x\n",
      "1,...,xNand we wish to ﬁnd the corresponding values\n",
      "z1,...,xN. If we simply average the measurements, the error due to random noise\n",
      "will be reduced, but unfortunately we will just obtain a single averaged estimate, inwhich we have averaged over the changing value of z, thereby introducing a new\n",
      "source of error.\n",
      "Intuitively, we could imagine doing a bit better as follows. To estimate the value\n",
      "ofz\n",
      "N, we take only the most recent few measurements, say xN−L,...,xNand just\n",
      "average these. If zis changing slowly, and the random noise level in the sensor is\n",
      "high, it would make sense to choose a relatively long window of observations toaverage. Conversely, if the signal is changing quickly, and the noise levels are small,\n",
      "we might be better just to use x\n",
      "Ndirectly as our estimate of zN. Perhaps we could\n",
      "do even better if we take a weighted average, in which more recent measurements636 13. SEQUENTIAL DATA\n",
      "make a greater contribution than less recent ones.\n",
      "Although this sort of intuitive argument seems plausible, it does not tell us how\n",
      "to form a weighted average, and any sort of hand-crafted weighing is hardly likely\n",
      "to be optimal. Fortunately, we can address problems such as this much more sys-\n",
      "tematically by deﬁning a probabilistic model that captures the time evolution andmeasurement processes and then applying the inference and learning methods devel-\n",
      "oped in earlier chapters. Here we shall focus on a widely used model known as a\n",
      "linear dynamical system .\n",
      "As we have seen, the HMM corresponds to the state space model shown in\n",
      "Figure 13.5 in which the latent variables are discrete but with arbitrary emission\n",
      "probability distributions. This graph of course describes a much broader class of\n",
      "probability distributions, all of which factorize according to (13.6). We now consider\n",
      "extensions to other distributions for the latent variables. In particular, we considercontinuous latent variables in which the summations of the sum-product algorithm\n",
      "become integrals. The general form of the inference algorithms will, however, be\n",
      "the same as for the hidden Markov model. It is interesting to note that, historically,hidden Markov models and linear dynamical systems were developed independently.\n",
      "Once they are both expressed as graphical models, however, the deep relationship\n",
      "between them immediately becomes apparent.\n",
      "One key requirement is that we retain an efﬁcient algorithm for inference which\n",
      "is linear in the length of the chain. This requires that, for instance, when we take\n",
      "a quantity\n",
      "ˆα(zn−1), representing the posterior probability of zngiven observations\n",
      "x1,...,xn, and multiply by the transition probability p(zn|zn−1)and the emission\n",
      "probability p(xn|zn)and then marginalize over zn−1, we obtain a distribution over\n",
      "znthat is of the same functional form as that over ˆα(zn−1). That is to say, the\n",
      "distribution must not become more complex at each stage, but must only change in\n",
      "its parameter values. Not surprisingly, the only distributions that have this property\n",
      "of being closed under multiplication are those belonging to the exponential family.\n",
      "Here we consider the most important example from a practical perspective,\n",
      "which is the Gaussian. In particular, we consider a linear-Gaussian state space modelso that the latent variables {z\n",
      "n}, as well as the observed variables {xn}, are multi-\n",
      "variate Gaussian distributions whose means are linear functions of the states of their\n",
      "parents in the graph. We have seen that a directed graph of linear-Gaussian unitsis equivalent to a joint Gaussian distribution over all of the variables. Furthermore,\n",
      "marginals such as\n",
      "ˆα(zn)are also Gaussian, so that the functional form of the mes-\n",
      "sages is preserved and we will obtain an efﬁcient inference algorithm. By contrast,suppose that the emission densities p(x\n",
      "n|zn)comprise a mixture of KGaussians\n",
      "each of which has a mean that is linear in zn. Then even if ˆα(z1)is Gaussian, the\n",
      "quantityˆα(z2)will be a mixture of KGaussians, ˆα(z3)will be a mixture of K2\n",
      "Gaussians, and so on, and exact inference will not be of practical value.\n",
      "We have seen that the hidden Markov model can be viewed as an extension of\n",
      "the mixture models of Chapter 9 to allow for sequential correlations in the data.In a similar way, we can view the linear dynamical system as a generalization of the\n",
      "continuous latent variable models of Chapter 12 such as probabilistic PCA and factor\n",
      "analysis. Each pair of nodes {z\n",
      "n,xn}represents a linear-Gaussian latent variable13.3. Linear Dynamical Systems 637\n",
      "model for that particular observation. However, the latent variables {zn}are no\n",
      "longer treated as independent but now form a Markov chain.\n",
      "Because the model is represented by a tree-structured directed graph, inference\n",
      "problems can be solved efﬁciently using the sum-product algorithm. The forward re-\n",
      "cursions, analogous to the αmessages of the hidden Markov model, are known as the\n",
      "Kalman ﬁlter equations (Kalman, 1960; Zarchan and Musoff, 2005), and the back-\n",
      "ward recursions, analogous to the βmessages, are known as the Kalman smoother\n",
      "equations, or the Rauch-Tung-Striebel (RTS) equations (Rauch et al. , 1965). The\n",
      "Kalman ﬁlter is widely used in many real-time tracking applications.\n",
      "Because the linear dynamical system is a linear-Gaussian model, the joint distri-\n",
      "bution over all variables, as well as all marginals and conditionals, will be Gaussian.\n",
      "It follows that the sequence of individually most probable latent variable values is\n",
      "the same as the most probable latent sequence. There is thus no need to consider the Exercise 13.19\n",
      "analogue of the Viterbi algorithm for the linear dynamical system.\n",
      "Because the model has linear-Gaussian conditional distributions, we can write\n",
      "the transition and emission distributions in the general form\n",
      "p(zn|zn−1)= N(zn|Azn−1,Γ) (13.75)\n",
      "p(xn|zn)= N(xn|Czn,Σ). (13.76)\n",
      "The initial latent variable also has a Gaussian distribution which we write as\n",
      "p(z1)=N(z1|µ0,V0). (13.77)\n",
      "Note that in order to simplify the notation, we have omitted additive constant terms\n",
      "from the means of the Gaussians. In fact, it is straightforward to include them ifdesired. Traditionally, these distributions are more commonly expressed in an equiv- Exercise 13.24\n",
      "alent form in terms of noisy linear equations given by\n",
      "z\n",
      "n=Azn−1+wn (13.78)\n",
      "xn=Czn+vn (13.79)\n",
      "z1=µ0+u (13.80)\n",
      "where the noise terms have the distributions\n",
      "w∼N (w|0,Γ) (13.81)\n",
      "v∼N (v|0,Σ) (13.82)\n",
      "u∼N (u|0,V0). (13.83)\n",
      "The parameters of the model, denoted by θ={A,Γ,C,Σ,µ0,V0}, can be\n",
      "determined using maximum likelihood through the EM algorithm. In the E step, weneed to solve the inference problem of determining the local posterior marginals for\n",
      "the latent variables, which can be solved efﬁciently using the sum-product algorithm,\n",
      "as we discuss in the next section.638 13. SEQUENTIAL DATA\n",
      "13.3.1 Inference in LDS\n",
      "We now turn to the problem of ﬁnding the marginal distributions for the latent\n",
      "variables conditional on the observation sequence. For given parameter settings, we\n",
      "also wish to make predictions of the next latent state znand of the next observation\n",
      "xnconditioned on the observed data x1,...,xn−1for use in real-time applications.\n",
      "These inference problems can be solved efﬁciently using the sum-product algorithm,\n",
      "which in the context of the linear dynamical system gives rise to the Kalman ﬁlter\n",
      "and Kalman smoother equations.\n",
      "It is worth emphasizing that because the linear dynamical system is a linear-\n",
      "Gaussian model, the joint distribution over all latent and observed variables is simplya Gaussian, and so in principle we could solve inference problems by using the\n",
      "standard results derived in previous chapters for the marginals and conditionals of a\n",
      "multivariate Gaussian. The role of the sum-product algorithm is to provide a moreefﬁcient way to perform such computations.\n",
      "Linear dynamical systems have the identical factorization, given by (13.6), to\n",
      "hidden Markov models, and are again described by the factor graphs in Figures 13.14and 13.15. Inference algorithms therefore take precisely the same form except that\n",
      "summations over latent variables are replaced by integrations. We begin by consid-\n",
      "ering the forward equations in which we treat z\n",
      "Nas the root node, and propagate\n",
      "messages from the leaf node h(z1)to the root. From (13.77), the initial message will\n",
      "be Gaussian, and because each of the factors is Gaussian, all subsequent messages\n",
      "will also be Gaussian. By convention, we shall propagate messages that are nor-malized marginal distributions corresponding to p(z\n",
      "n|x1,...,xn), which we denote\n",
      "by\n",
      "ˆα(zn)=N(zn|µn,Vn). (13.84)\n",
      "This is precisely analogous to the propagation of scaled variables ˆα(zn)given by\n",
      "(13.59) in the discrete case of the hidden Markov model, and so the recursion equa-\n",
      "tion now takes the form\n",
      "cnˆα(zn)=p(xn|zn)∫\n",
      "ˆα(zn−1)p(zn|zn−1)dzn−1. (13.85)\n",
      "Substituting for the conditionals p(zn|zn−1)andp(xn|zn), using (13.75) and (13.76),\n",
      "respectively, and making use of (13.84), we see that (13.85) becomes\n",
      "cnN(zn|µn,Vn)=N(xn|Czn,Σ)∫\n",
      "N(zn|Azn−1,Γ)N(zn−1|µn−1,Vn−1)dzn−1. (13.86)\n",
      "Here we are supposing that µn−1andVn−1are known, and by evaluating the inte-\n",
      "gral in (13.86), we wish to determine values for µnandVn. The integral is easily\n",
      "evaluated by making use of the result (2.115), from which it follows that\n",
      "∫\n",
      "N(zn|Azn−1,Γ)N(zn−1|µn−1,Vn−1)dzn−1\n",
      "=N(zn|Aµn−1,Pn−1) (13.87)13.3. Linear Dynamical Systems 639\n",
      "where we have deﬁned\n",
      "Pn−1=AVn−1AT+Γ. (13.88)\n",
      "We can now combine this result with the ﬁrst factor on the right-hand side of (13.86)\n",
      "by making use of (2.115) and (2.116) to give\n",
      "µn=Aµn−1+Kn(xn−CAµn−1) (13.89)\n",
      "Vn=(I−KnC)Pn−1 (13.90)\n",
      "cn=N(xn|CAµn−1,CPn−1CT+Σ). (13.91)\n",
      "Here we have made use of the matrix inverse identities (C.5) and (C.7) and also\n",
      "deﬁned the Kalman gain matrix\n",
      "Kn=Pn−1CT(\n",
      "CPn−1CT+Σ)−1. (13.92)\n",
      "Thus, given the values of µn−1andVn−1, together with the new observation xn,\n",
      "we can evaluate the Gaussian marginal for znhaving mean µnand covariance Vn,\n",
      "as well as the normalization coefﬁcient cn.\n",
      "The initial conditions for these recursion equations are obtained from\n",
      "c1ˆα(z1)=p(z1)p(x1|z1). (13.93)\n",
      "Because p(z1)is given by (13.77), and p(x1|z1)is given by (13.76), we can again\n",
      "make use of (2.115) to calculate c1and (2.116) to calculate µ1andV1giving\n",
      "µ1=µ0+K1(x1−Cµ0) (13.94)\n",
      "V1=(I−K1C)V0 (13.95)\n",
      "c1=N(x1|Cµ0,CV0CT+Σ) (13.96)\n",
      "where\n",
      "K1=V0CT(\n",
      "CV0CT+Σ)−1. (13.97)\n",
      "Similarly, the likelihood function for the linear dynamical system is given by (13.63)\n",
      "in which the factors cnare found using the Kalman ﬁltering equations.\n",
      "We can interpret the steps involved in going from the posterior marginal over\n",
      "zn−1to the posterior marginal over znas follows. In (13.89), we can view the\n",
      "quantity Aµn−1as the prediction of the mean over znobtained by simply taking the\n",
      "mean over zn−1and projecting it forward one step using the transition probability\n",
      "matrix A. This predicted mean would give a predicted observation for xngiven by\n",
      "CAz n−1obtained by applying the emission probability matrix Cto the predicted\n",
      "hidden state mean. We can view the update equation (13.89) for the mean of the\n",
      "hidden variable distribution as taking the predicted mean Aµn−1and then adding\n",
      "a correction that is proportional to the error xn−CAz n−1between the predicted\n",
      "observation and the actual observation. The coefﬁcient of this correction is given by\n",
      "the Kalman gain matrix. Thus we can view the Kalman ﬁlter as a process of makingsuccessive predictions and then correcting these predictions in the light of the new\n",
      "observations. This is illustrated graphically in Figure 13.21.640 13. SEQUENTIAL DATA\n",
      "zn−1 zn zn\n",
      "Figure 13.21 The linear dynamical system can be viewed as a sequence of steps in which increasing un-\n",
      "certainty in the state variable due to diffusion is compensated by the arrival of new data. In the left-hand plot,\n",
      "the blue curve shows the distribution p(zn−1|x1,...,xn−1), which incorporates all the data up to step n−1.\n",
      "The diffusion arising from the nonzero variance of the transition probability p(zn|zn−1)gives the distribution\n",
      "p(zn|x1,...,xn−1), shown in red in the centre plot. Note that this is broader and shifted relative to the blue curve\n",
      "(which is shown dashed in the centre plot for comparison). The next data observation xncontributes through the\n",
      "emission density p(xn|zn), which is shown as a function of znin green on the right-hand plot. Note that this is not\n",
      "a density with respect to znand so is not normalized to one. Inclusion of this new data point leads to a revised\n",
      "distribution p(zn|x1,...,xn)for the state density shown in blue. We see that observation of the data has shifted\n",
      "and narrowed the distribution compared to p(zn|x1,...,xn−1)(which is shown in dashed in the right-hand plot\n",
      "for comparison).\n",
      "If we consider a situation in which the measurement noise is small compared\n",
      "to the rate at which the latent variable is evolving, then we ﬁnd that the posterior\n",
      "distribution for zndepends only on the current measurement xn, in accordance with Exercise 13.27\n",
      "the intuition from our simple example at the start of the section. Similarly, if the\n",
      "latent variable is evolving slowly relative to the observation noise level, we ﬁnd that\n",
      "the posterior mean for znis obtained by averaging all of the measurements obtained\n",
      "up to that time. Exercise 13.28\n",
      "One of the most important applications of the Kalman ﬁlter is to tracking, and\n",
      "this is illustrated using a simple example of an object moving in two dimensions in\n",
      "Figure 13.22.\n",
      "So far, we have solved the inference problem of ﬁnding the posterior marginal\n",
      "for a node zngiven observations from x1up toxn. Next we turn to the problem of\n",
      "ﬁnding the marginal for a node zngiven all observations x1toxN. For temporal\n",
      "data, this corresponds to the inclusion of future as well as past observations. Al-\n",
      "though this cannot be used for real-time prediction, it plays a key role in learning the\n",
      "parameters of the model. By analogy with the hidden Markov model, this problem\n",
      "can be solved by propagating messages from node xNback to node x1and com-\n",
      "bining this information with that obtained during the forward message passing stage\n",
      "used to compute the ˆα(zn).\n",
      "In the LDS literature, it is usual to formulate this backward recursion in terms\n",
      "ofγ(zn)=ˆα(zn)ˆβ(zn)rather than in terms of ˆβ(zn). Because γ(zn)must also be\n",
      "Gaussian, we write it in the form\n",
      "γ(zn)=ˆα(zn)ˆβ(zn)=N(zn|ˆµn,ˆVn). (13.98)\n",
      "To derive the required recursion, we start from the backward recursion (13.62) for13.3. Linear Dynamical Systems 641\n",
      "Figure 13.22 An illustration of a linear dy-\n",
      "namical system being used to\n",
      "track a moving object. The blue\n",
      "points indicate the true positionsof the object in a two-dimensionalspace at successive time steps,the green points denote noisymeasurements of the positions,and the red crosses indicate themeans of the inferred posteriordistributions of the positions ob-tained by running the Kalman ﬁl-tering equations. The covari-ances of the inferred positionsare indicated by the red ellipses,which correspond to contourshaving one standard deviation.\n",
      "ˆβ(zn), which, for continuous latent variables, can be written in the form\n",
      "cn+1ˆβ(zn)=∫\n",
      "ˆβ(zn+1)p(xn+1|zn+1)p(zn+1|zn)dzn+1. (13.99)\n",
      "We now multiply both sides of (13.99) by ˆα(zn)and substitute for p(xn+1|zn+1)\n",
      "andp(zn+1|zn)using (13.75) and (13.76). Then we make use of (13.89), (13.90)\n",
      "and (13.91), together with (13.98), and after some manipulation we obtain Exercise 13.29\n",
      "ˆµn=µn+Jn(ˆµn+1−AµN)\n",
      "(13.100)\n",
      "ˆVn=Vn+Jn(\n",
      "ˆVn+1−Pn)\n",
      "JT\n",
      "n (13.101)\n",
      "where we have deﬁned\n",
      "Jn=VnAT(Pn)−1(13.102)\n",
      "and we have made use of AVn=PnJT\n",
      "n. Note that these recursions require that the\n",
      "forward pass be completed ﬁrst so that the quantities µnandVnwill be available\n",
      "for the backward pass.\n",
      "For the EM algorithm, we also require the pairwise posterior marginals, which\n",
      "can be obtained from (13.65) in the form\n",
      "ξ(zn−1,zn)=(cn)−1ˆα(zn−1)p(xn|zn)p(zn|z−1)ˆβ(zn)\n",
      "=N(zn−1|µn−1,Vn−1)N(zn|Azn−1,Γ)N(xn|Czn,Σ)N(zn|ˆµn,ˆVn)\n",
      "cnˆα(zn).\n",
      "(13.103)\n",
      "Substituting for ˆα(zn)using (13.84) and rearranging, we see that ξ(zn−1,zn)is a\n",
      "Gaussian with mean given with components γ(zn−1)andγ(zn), and a covariance\n",
      "between znandzn−1given by Exercise 13.31\n",
      "cov[zn,zn−1]=Jn−1ˆVn. (13.104)642 13. SEQUENTIAL DATA\n",
      "13.3.2 Learning in LDS\n",
      "So far, we have considered the inference problem for linear dynamical systems,\n",
      "assuming that the model parameters θ={A,Γ,C,Σ,µ0,V0}are known. Next, we\n",
      "consider the determination of these parameters using maximum likelihood (Ghahra-\n",
      "mani and Hinton, 1996b). Because the model has latent variables, this can be ad-dressed using the EM algorithm, which was discussed in general terms in Chapter 9.\n",
      "We can derive the EM algorithm for the linear dynamical system as follows. Let\n",
      "us denote the estimated parameter values at some particular cycle of the algorithm\n",
      "byθ\n",
      "old. For these parameter values, we can run the inference algorithm to determine\n",
      "the posterior distribution of the latent variables p(Z|X,θold), or more precisely those\n",
      "local posterior marginals that are required in the M step. In particular, we shall\n",
      "require the following expectations\n",
      "E[zn]=ˆµn (13.105)\n",
      "E[\n",
      "znzT\n",
      "n−1]\n",
      "=Jn−1ˆVn+ˆµnˆµT\n",
      "n−1 (13.106)\n",
      "E[\n",
      "znzT\n",
      "n]\n",
      "=ˆVn+ˆµnˆµT\n",
      "n (13.107)\n",
      "where we have used (13.104).\n",
      "Now we consider the complete-data log likelihood function, which is obtained\n",
      "by taking the logarithm of (13.6) and is therefore given by\n",
      "lnp(X,Z|θ)=l n p(z1|µ0,V0)+N∑\n",
      "n=2lnp(zn|zn−1,A,Γ)\n",
      "+N∑\n",
      "n=1lnp(xn|zn,C,Σ) (13.108)\n",
      "in which we have made the dependence on the parameters explicit. We now take the\n",
      "expectation of the complete-data log likelihood with respect to the posterior distri-\n",
      "bution p(Z|X,θold)which deﬁnes the function\n",
      "Q(θ,θold)= EZ|θold[lnp(X,Z|θ)]. (13.109)\n",
      "In the M step, this function is maximized with respect to the components of θ.\n",
      "Consider ﬁrst the parameters µ0andV0. If we substitute for p(z1|µ0,V0)in\n",
      "(13.108) using (13.77), and then take the expectation with respect to Z, we obtain\n",
      "Q(θ,θold)=−1\n",
      "2ln|V0|− EZ|θold[1\n",
      "2(z1−µ0)TV−1\n",
      "0(z1−µ0)]\n",
      "+c o n s t\n",
      "where all terms not dependent on µ0orV0have been absorbed into the additive\n",
      "constant. Maximization with respect to µ0andV0is easily performed by making\n",
      "use of the maximum likelihood solution for a Gaussian distribution discussed in\n",
      "Section 2.3.4, giving Exercise 13.3213.3. Linear Dynamical Systems 643\n",
      "µnew\n",
      "0= E[z1] (13.110)\n",
      "Vnew\n",
      "0= E[z1zT\n",
      "1]−E[z1]E[zT\n",
      "1]. (13.111)\n",
      "Similarly, to optimize AandΓ, we substitute for p(zn|zn−1,A,Γ)in (13.108)\n",
      "using (13.75) giving\n",
      "Q(θ,θold)=−N−1\n",
      "2ln|Γ|\n",
      "−EZ|θold[\n",
      "1\n",
      "2N∑\n",
      "n=2(zn−Azn−1)TΓ−1(zn−Azn−1)]\n",
      "+c o n s t (13.112)\n",
      "in which the constant comprises terms that are independent of AandΓ. Maximizing\n",
      "with respect to these parameters then gives Exercise 13.33\n",
      "Anew=(N∑\n",
      "n=2E[\n",
      "znzT\n",
      "n−1])(N∑\n",
      "n=2E[\n",
      "zn−1zT\n",
      "n−1])−1\n",
      "(13.113)\n",
      "Γnew=1\n",
      "N−1N∑\n",
      "n=2{\n",
      "E[\n",
      "znzT\n",
      "n]\n",
      "−AnewE[\n",
      "zn−1zT\n",
      "n]\n",
      "−E[\n",
      "znzT\n",
      "n−1]\n",
      "Anew+AnewE[\n",
      "zn−1zT\n",
      "n−1]\n",
      "(Anew)T}\n",
      ".(13.114)\n",
      "Note that Anewmust be evaluated ﬁrst, and the result can then be used to determine\n",
      "Γnew.\n",
      "Finally, in order to determine the new values of CandΣ, we substitute for\n",
      "p(xn|zn,C,Σ)in (13.108) using (13.76) giving\n",
      "Q(θ,θold)= −N\n",
      "2ln|Σ|\n",
      "−EZ|θold[\n",
      "1\n",
      "2N∑\n",
      "n=1(xn−Czn)TΣ−1(xn−Czn)]\n",
      "+c o n s t .\n",
      "Maximizing with respect to CandΣthen gives Exercise 13.34\n",
      "Cnew=(N∑\n",
      "n=1xnE[\n",
      "zT\n",
      "n])(N∑\n",
      "n=1E[\n",
      "znzT\n",
      "n])−1\n",
      "(13.115)\n",
      "Σnew=1\n",
      "NN∑\n",
      "n=1{\n",
      "xnxT\n",
      "n−CnewE[zn]xT\n",
      "n\n",
      "−xnE[\n",
      "zT\n",
      "n]\n",
      "Cnew+CnewE[\n",
      "znzT\n",
      "n]\n",
      "Cnew}\n",
      ". (13.116)644 13. SEQUENTIAL DATA\n",
      "We have approached parameter learning in the linear dynamical system using\n",
      "maximum likelihood. Inclusion of priors to give a MAP estimate is straightforward,and a fully Bayesian treatment can be found by applying the analytical approxima-\n",
      "tion techniques discussed in Chapter 10, though a detailed treatment is precluded\n",
      "here due to lack of space.\n",
      "13.3.3 Extensions of LDS\n",
      "As with the hidden Markov model, there is considerable interest in extending\n",
      "the basic linear dynamical system in order to increase its capabilities. Although the\n",
      "assumption of a linear-Gaussian model leads to efﬁcient algorithms for inference\n",
      "and learning, it also implies that the marginal distribution of the observed variablesis simply a Gaussian, which represents a signiﬁcant limitation. One simple extension\n",
      "of the linear dynamical system is to use a Gaussian mixture as the initial distribution\n",
      "forz\n",
      "1. If this mixture has Kcomponents, then the forward recursion equations\n",
      "(13.85) will lead to a mixture of KGaussians over each hidden variable zn, and so\n",
      "the model is again tractable.\n",
      "For many applications, the Gaussian emission density is a poor approximation.\n",
      "If instead we try to use a mixture of KGaussians as the emission density, then the\n",
      "posteriorˆα(z1)will also be a mixture of KGaussians. However, from (13.85) the\n",
      "posteriorˆα(z2)will comprise a mixture of K2Gaussians, and so on, with ˆα(zn)\n",
      "being given by a mixture of KnGaussians. Thus the number of components grows\n",
      "exponentially with the length of the chain, and so this model is impractical.\n",
      "More generally, introducing transition or emission models that depart from the\n",
      "linear-Gaussian (or other exponential family) model leads to an intractable infer-\n",
      "ence problem. We can make deterministic approximations such as assumed den-sity ﬁltering or expectation propagation, or we can make use of sampling methods, Chapter 10\n",
      "as discussed in Section 13.3.4. One widely used approach is to make a Gaussian\n",
      "approximation by linearizing around the mean of the predicted distribution, whichgives rise to the extended Kalman ﬁlter (Zarchan and Musoff, 2005).\n",
      "As with hidden Markov models, we can develop interesting extensions of the ba-\n",
      "sic linear dynamical system by expanding its graphical representation. For example,theswitching state space model (Ghahramani and Hinton, 1998) can be viewed as\n",
      "a combination of the hidden Markov model with a set of linear dynamical systems.\n",
      "The model has multiple Markov chains of continuous linear-Gaussian latent vari-ables, each of which is analogous to the latent chain of the linear dynamical system\n",
      "discussed earlier, together with a Markov chain of discrete variables of the form used\n",
      "in a hidden Markov model. The output at each time step is determined by stochas-tically choosing one of the continuous latent chains, using the state of the discrete\n",
      "latent variable as a switch, and then emitting an observation from the corresponding\n",
      "conditional output distribution. Exact inference in this model is intractable, but vari-\n",
      "ational methods lead to an efﬁcient inference scheme involving forward-backward\n",
      "recursions along each of the continuous and discrete Markov chains independently.Note that, if we consider multiple chains of discrete latent variables, and use one as\n",
      "the switch to select from the remainder, we obtain an analogous model having only\n",
      "discrete latent variables known as the switching hidden Markov model .13.3. Linear Dynamical Systems 645\n",
      "13.3.4 Particle ﬁlters\n",
      "For dynamical systems which do not have a linear-Gaussian, for example, if\n",
      "they use a non-Gaussian emission density, we can turn to sampling methods in order Chapter 11\n",
      "to ﬁnd a tractable inference algorithm. In particular, we can apply the sampling-\n",
      "importance-resampling formalism of Section 11.1.5 to obtain a sequential MonteCarlo algorithm known as the particle ﬁlter.\n",
      "Consider the class of distributions represented by the graphical model in Fig-\n",
      "ure 13.5, and suppose we are given the observed values X\n",
      "n=(x1,...,xn)and\n",
      "we wish to draw Lsamples from the posterior distribution p(zn|Xn). Using Bayes’\n",
      "theorem, we have\n",
      "E[f(zn)] =∫\n",
      "f(zn)p(zn|Xn)dzn\n",
      "=∫\n",
      "f(zn)p(zn|xn,Xn−1)dzn\n",
      "=∫\n",
      "f(zn)p(xn|zn)p(zn|Xn−1)dzn\n",
      "∫\n",
      "p(xn|zn)p(zn|Xn−1)dzn\n",
      "≃L∑\n",
      "l=1w(l)\n",
      "nf(z(l)\n",
      "n) (13.117)\n",
      "where {z(l)\n",
      "n}is a set of samples drawn from p(zn|Xn−1)and we have made use of\n",
      "the conditional independence property p(xn|zn,Xn−1)=p(xn|zn), which follows\n",
      "from the graph in Figure 13.5. The sampling weights {w(l)\n",
      "n}are deﬁned by\n",
      "w(l)\n",
      "n=p(xn|z(l)\n",
      "n)∑L\n",
      "m=1p(xn|z(m)\n",
      "n)(13.118)\n",
      "where the same samples are used in the numerator as in the denominator. Thus the\n",
      "posterior distribution p(zn|xn)is represented by the set of samples {z(l)\n",
      "n}together\n",
      "with the corresponding weights {w(l)\n",
      "n}. Note that these weights satisfy 0⩽w(l)\n",
      "n1\n",
      "and∑\n",
      "lw(l)\n",
      "n=1.\n",
      "Because we wish to ﬁnd a sequential sampling scheme, we shall suppose that\n",
      "a set of samples and weights have been obtained at time step n, and that we have\n",
      "subsequently observed the value of xn+1, and we wish to ﬁnd the weights and sam-\n",
      "ples at time step n+1. We ﬁrst sample from the distribution p(zn+1|Xn). This is646 13. SEQUENTIAL DATA\n",
      "straightforward since, again using Bayes’ theorem\n",
      "p(zn+1|Xn)=∫\n",
      "p(zn+1|zn,Xn)p(zn|Xn)dzn\n",
      "=∫\n",
      "p(zn+1|zn)p(zn|Xn)dzn\n",
      "=∫\n",
      "p(zn+1|zn)p(zn|xn,Xn−1)dzn\n",
      "=∫\n",
      "p(zn+1|zn)p(xn|zn)p(zn|Xn−1)dzn\n",
      "∫\n",
      "p(xn|zn)p(zn|Xn−1)dzn\n",
      "=∑\n",
      "lw(l)\n",
      "np(zn+1|z(l)\n",
      "n) (13.119)\n",
      "where we have made use of the conditional independence properties\n",
      "p(zn+1|zn,Xn)= p(zn+1|zn) (13.120)\n",
      "p(xn|zn,Xn−1)= p(xn|zn) (13.121)\n",
      "which follow from the application of the d-separation criterion to the graph in Fig-\n",
      "ure 13.5. The distribution given by (13.119) is a mixture distribution, and samplescan be drawn by choosing a component lwith probability given by the mixing coef-\n",
      "ﬁcients w\n",
      "(l)and then drawing a sample from the corresponding component.\n",
      "In summary, we can view each step of the particle ﬁlter algorithm as comprising\n",
      "two stages. At time step n, we have a sample representation of the posterior dis-\n",
      "tribution p(zn|Xn)expressed as samples {z(l)\n",
      "n}with corresponding weights {w(l)\n",
      "n}.\n",
      "This can be viewed as a mixture representation of the form (13.119). To obtain the\n",
      "corresponding representation for the next time step, we ﬁrst draw Lsamples from\n",
      "the mixture distribution (13.119), and then for each sample we use the new obser-\n",
      "vationxn+1to evaluate the corresponding weights w(l)\n",
      "n+1∝p(xn+1|z(l)\n",
      "n+1). This is\n",
      "illustrated, for the case of a single variable z, in Figure 13.23.\n",
      "The particle ﬁltering, or sequential Monte Carlo, approach has appeared in the\n",
      "literature under various names including the bootstrap ﬁlter (Gordon et al. , 1993),\n",
      "survival of the ﬁttest (Kanazawa et al. , 1995), and the condensation algorithm (Isard\n",
      "and Blake, 1998).\n",
      "Exercises\n",
      "13.1 (⋆)www Use the technique of d-separation, discussed in Section 8.2, to verify\n",
      "that the Markov model shown in Figure 13.3 having Nnodes in total satisﬁes the\n",
      "conditional independence properties (13.3) for n=2,...,N . Similarly, show that\n",
      "a model described by the graph in Figure 13.4 in which there are Nnodes in totalExercises 647\n",
      "p(zn|Xn)\n",
      "p(zn+1|Xn)\n",
      "p(xn+1|zn+1)\n",
      "p(zn+1|Xn+1)z\n",
      "Figure 13.23 Schematic illustration of the operation of the particle ﬁlter for a one-dimensional latent\n",
      "space. At time step n, the posterior p(zn|xn)is represented as a mixture distribution,\n",
      "shown schematically as circles whose sizes are proportional to the weights w(l)\n",
      "n.As e to f\n",
      "Lsamples is then drawn from this distribution and the new weights w(l)\n",
      "n+1evaluated using\n",
      "p(xn+1|z(l)\n",
      "n+1).\n",
      "satisﬁes the conditional independence properties\n",
      "p(xn|x1,...,xn−1)=p(xn|xn−1,xn−2) (13.122)\n",
      "forn=3,...,N .\n",
      "13.2 (⋆⋆)Consider the joint probability distribution (13.2) corresponding to the directed\n",
      "graph of Figure 13.3. Using the sum and product rules of probability, verify that\n",
      "this joint distribution satisﬁes the conditional independence property (13.3) for n=\n",
      "2,...,N . Similarly, show that the second-order Markov model described by the\n",
      "joint distribution (13.4) satisﬁes the conditional independence property\n",
      "p(xn|x1,...,xn−1)=p(xn|xn−1,xn−2) (13.123)\n",
      "forn=3,...,N .\n",
      "13.3 (⋆)By using d-separation, show that the distribution p(x1,...,xN)of the observed\n",
      "data for the state space model represented by the directed graph in Figure 13.5 does\n",
      "not satisfy any conditional independence properties and hence does not exhibit the\n",
      "Markov property at any ﬁnite order.\n",
      "13.4 (⋆⋆)www Consider a hidden Markov model in which the emission densities are\n",
      "represented by a parametric model p(x|z,w), such as a linear regression model or\n",
      "a neural network, in which wis a vector of adaptive parameters. Describe how the\n",
      "parameters wcan be learned from data using maximum likelihood.648 13. SEQUENTIAL DATA\n",
      "13.5 (⋆⋆)Verify the M-step equations (13.18) and (13.19) for the initial state probabili-\n",
      "ties and transition probability parameters of the hidden Markov model by maximiza-tion of the expected complete-data log likelihood function (13.17), using appropriate\n",
      "Lagrange multipliers to enforce the summation constraints on the components of π\n",
      "andA.\n",
      "13.6 (⋆)Show that if any elements of the parameters πorAfor a hidden Markov\n",
      "model are initially set to zero, then those elements will remain zero in all subsequent\n",
      "updates of the EM algorithm.\n",
      "13.7 (⋆)Consider a hidden Markov model with Gaussian emission densities. Show that\n",
      "maximization of the function Q(θ,θ\n",
      "old)with respect to the mean and covariance\n",
      "parameters of the Gaussians gives rise to the M-step equations (13.20) and (13.21).\n",
      "13.8 (⋆⋆)www For a hidden Markov model having discrete observations governed by\n",
      "a multinomial distribution, show that the conditional distribution of the observations\n",
      "given the hidden variables is given by (13.22) and the corresponding M step equa-tions are given by (13.23). Write down the analogous equations for the conditional\n",
      "distribution and the M step equations for the case of a hidden Markov with multiple\n",
      "binary output variables each of which is governed by a Bernoulli conditional dis-tribution. Hint: refer to Sections 2.1 and 2.2 for a discussion of the corresponding\n",
      "maximum likelihood solutions for i.i.d. data if required.\n",
      "13.9 (⋆⋆)\n",
      "www Use the d-separation criterion to verify that the conditional indepen-\n",
      "dence properties (13.24)–(13.31) are satisﬁed by the joint distribution for the hiddenMarkov model deﬁned by (13.6).\n",
      "13.10 (⋆⋆⋆ )By applying the sum and product rules of probability, verify that the condi-\n",
      "tional independence properties (13.24)–(13.31) are satisﬁed by the joint distribution\n",
      "for the hidden Markov model deﬁned by (13.6).\n",
      "13.11 (⋆⋆)Starting from the expression (8.72) for the marginal distribution over the vari-\n",
      "ables of a factor in a factor graph, together with the results for the messages in the\n",
      "sum-product algorithm obtained in Section 13.2.3, derive the result (13.43) for the\n",
      "joint posterior distribution over two successive latent variables in a hidden Markovmodel.\n",
      "13.12 (⋆⋆)Suppose we wish to train a hidden Markov model by maximum likelihood\n",
      "using data that comprises Rindependent sequences of observations, which we de-\n",
      "note by X\n",
      "(r)where r=1,...,R . Show that in the E step of the EM algorithm,\n",
      "we simply evaluate posterior probabilities for the latent variables by running the α\n",
      "andβrecursions independently for each of the sequences. Also show that in the\n",
      "M step, the initial probability and transition probability parameters are re-estimatedExercises 649\n",
      "using modiﬁed forms of (13.18 ) and (13.19) given by\n",
      "πk=R∑\n",
      "r=1γ(z(r)\n",
      "1k)\n",
      "R∑\n",
      "r=1K∑\n",
      "j=1γ(z(r)\n",
      "1j)(13.124)\n",
      "Ajk=R∑\n",
      "r=1N∑\n",
      "n=2ξ(z(r)\n",
      "n−1,j,z(r)\n",
      "n,k)\n",
      "R∑\n",
      "r=1K∑\n",
      "l=1N∑\n",
      "n=2ξ(z(r)\n",
      "n−1,j,z(r)\n",
      "n,l)(13.125)\n",
      "where, for notational convenience, we have assumed that the sequences are of the\n",
      "same length (the generalization to sequences of different lengths is straightforward).\n",
      "Similarly, show that the M-step equation for re-estimation of the means of Gaussianemission models is given by\n",
      "µ\n",
      "k=R∑\n",
      "r=1N∑\n",
      "n=1γ(z(r)\n",
      "nk)x(r)\n",
      "n\n",
      "R∑\n",
      "r=1N∑\n",
      "n=1γ(z(r)\n",
      "nk). (13.126)\n",
      "Note that the M-step equations for other emission model parameters and distributions\n",
      "take an analogous form.\n",
      "13.13 (⋆⋆)www Use the deﬁnition (8.64) of the messages passed from a factor node\n",
      "to a variable node in a factor graph, together with the expression (13.6) for the jointdistribution in a hidden Markov model, to show that the deﬁnition (13.50) of the\n",
      "alpha message is the same as the deﬁnition (13.34).\n",
      "13.14 (⋆⋆)Use the deﬁnition (8.67) of the messages passed from a factor node to a\n",
      "variable node in a factor graph, together with the expression (13.6) for the joint\n",
      "distribution in a hidden Markov model, to show that the deﬁnition (13.52) of the\n",
      "beta message is the same as the deﬁnition (13.35).\n",
      "13.15 (⋆⋆)Use the expressions (13.33) and (13.43) for the marginals in a hidden Markov\n",
      "model to derive the corresponding results (13.64) and (13.65) expressed in terms of\n",
      "re-scaled variables.\n",
      "13.16 (⋆⋆⋆ )In this exercise, we derive the forward message passing equation for the\n",
      "Viterbi algorithm directly from the expression (13.6) for the joint distribution. Thisinvolves maximizing over all of the hidden variables z\n",
      "1,...,zN. By taking the log-\n",
      "arithm and then exchanging maximizations and summations, derive the recursion650 13. SEQUENTIAL DATA\n",
      "(13.68) where the quantities ω(zn)are deﬁned by (13.70). Show that the initial\n",
      "condition for this recursion is given by (13.69).\n",
      "13.17 (⋆)www Show that the directed graph for the input-output hidden Markov model,\n",
      "given in Figure 13.18, can be expressed as a tree-structured factor graph of the form\n",
      "shown in Figure 13.15 and write down expressions for the initial factor h(z1)and\n",
      "for the general factor fn(zn−1,zn)where 2⩽n⩽N.\n",
      "13.18 (⋆⋆⋆ )Using the result of Exercise 13.17, derive the recursion equations, includ-\n",
      "ing the initial conditions, for the forward-backward algorithm for the input-outputhidden Markov model shown in Figure 13.18.\n",
      "13.19 (⋆)\n",
      "www The Kalman ﬁlter and smoother equations allow the posterior distribu-\n",
      "tions over individual latent variables, conditioned on all of the observed variables,\n",
      "to be found efﬁciently for linear dynamical systems. Show that the sequence oflatent variable values obtained by maximizing each of these posterior distributions\n",
      "individually is the same as the most probable sequence of latent values. To do this,\n",
      "simply note that the joint distribution of all latent and observed variables in a lineardynamical system is Gaussian, and hence all conditionals and marginals will also be\n",
      "Gaussian, and then make use of the result (2.98).\n",
      "13.20 (⋆⋆)\n",
      "www Use the result (2.115) to prove (13.87).\n",
      "13.21 (⋆⋆)Use the results (2.115) and (2.116), together with the matrix identities (C.5)\n",
      "and (C.7), to derive the results (13.89), (13.90), and (13.91), where the Kalman gain\n",
      "matrixKnis deﬁned by (13.92).\n",
      "13.22 (⋆⋆)www Using (13.93), together with the deﬁnitions (13.76) and (13.77) and\n",
      "the result (2.115), derive (13.96).\n",
      "13.23 (⋆⋆)Using (13.93), together with the deﬁnitions (13.76) and (13.77) and the result\n",
      "(2.116), derive (13.94), (13.95) and (13.97).\n",
      "13.24 (⋆⋆)www Consider a generalization of (13.75) and (13.76) in which we include\n",
      "constant terms aandcin the Gaussian means, so that\n",
      "p(zn|zn−1)=N(zn|Azn−1+a,Γ) (13.127)\n",
      "p(xn|zn)=N(xn|Czn+c,Σ). (13.128)\n",
      "Show that this extension can be re-case in the framework discussed in this chapter by\n",
      "deﬁning a state vector zwith an additional component ﬁxed at unity, and then aug-\n",
      "menting the matrices AandCusing extra columns corresponding to the parameters\n",
      "aandc.\n",
      "13.25 (⋆⋆)In this exercise, we show that when the Kalman ﬁlter equations are applied\n",
      "to independent observations, they reduce to the results given in Section 2.3 for the\n",
      "maximum likelihood solution for a single Gaussian distribution. Consider the prob-lem of ﬁnding the mean µof a single Gaussian random variable x, in which we are\n",
      "given a set of independent observations {x\n",
      "1,...,x N}. To model this we can useExercises 651\n",
      "a linear dynamical system governed by (13.75) and (13.76), with latent variables\n",
      "{z1,...,z N}in which Cbecomes the identity matrix and where the transition prob-\n",
      "ability A=0because the observations are independent. Let the parameters m0\n",
      "andV0of the initial state be denoted by µ0andσ2\n",
      "0, respectively, and suppose that\n",
      "Σbecomes σ2. Write down the corresponding Kalman ﬁlter equations starting from\n",
      "the general results (13.89) and (13.90), together with (13.94) and (13.95). Show that\n",
      "these are equivalent to the results (2.141) and (2.142) obtained directly by consider-\n",
      "ing independent data.\n",
      "13.26 (⋆⋆⋆ )Consider a special case of the linear dynamical system of Section 13.3 that is\n",
      "equivalent to probabilistic PCA, so that the transition matrix A=0, the covariance\n",
      "Γ=I, and the noise covariance Σ=σ2I. By making use of the matrix inversion\n",
      "identity (C.7) show that, if the emission density matrix Cis denoted W, then the\n",
      "posterior distribution over the hidden states deﬁned by (13.89) and (13.90) reduces\n",
      "to the result (12.42) for probabilistic PCA.\n",
      "13.27 (⋆)www Consider a linear dynamical system of the form discussed in Sec-\n",
      "tion 13.3 in which the amplitude of the observation noise goes to zero, so that Σ=0.\n",
      "Show that the posterior distribution for znhas mean xnand zero variance. This\n",
      "accords with our intuition that if there is no noise, we should just use the current\n",
      "observation xnto estimate the state variable znand ignore all previous observations.\n",
      "13.28 (⋆⋆⋆ )Consider a special case of the linear dynamical system of Section 13.3 in\n",
      "which the state variable znis constrained to be equal to the previous state variable,\n",
      "which corresponds to A=IandΓ=0. For simplicity, assume also that V0→∞\n",
      "so that the initial conditions for zare unimportant, and the predictions are determined\n",
      "purely by the data. Use proof by induction to show that the posterior mean for state\n",
      "znis determined by the average of x1,...,xn. This corresponds to the intuitive\n",
      "result that if the state variable is constant, our best estimate is obtained by averaging\n",
      "the observations.\n",
      "13.29 (⋆⋆⋆ )Starting from the backwards recursion equation (13.99), derive the RTS\n",
      "smoothing equations (13.100) and (13.101) for the Gaussian linear dynamical sys-\n",
      "tem.\n",
      "13.30 (⋆⋆)Starting from the result (13.65) for the pairwise posterior marginal in a state\n",
      "space model, derive the speciﬁc form (13.103) for the case of the Gaussian linear\n",
      "dynamical system.\n",
      "13.31 (⋆⋆)Starting from the result (13.103) and by substituting for ˆα(zn)using (13.84),\n",
      "verify the result (13.104) for the covariance between znandzn−1.\n",
      "13.32 (⋆⋆)www Verify the results (13.110) and (13.111) for the M-step equations for\n",
      "µ0andV0in the linear dynamical system.\n",
      "13.33 (⋆⋆)Verify the results (13.113) and (13.114) for the M-step equations for AandΓ\n",
      "in the linear dynamical system.652 13. SEQUENTIAL DATA\n",
      "13.34 (⋆⋆)Verify the results (13.115) and (13.116) for the M-step equations for CandΣ\n",
      "in the linear dynamical system.14\n",
      "Combining\n",
      "Models\n",
      "In earlier chapters, we have explored a range of different models for solving classiﬁ-\n",
      "cation and regression problems. It is often found that improved performance can beobtained by combining multiple models together in some way, instead of just using\n",
      "a single model in isolation. For instance, we might train Ldifferent models and then\n",
      "make predictions using the average of the predictions made by each model. Suchcombinations of models are sometimes called committees . In Section 14.2, we dis-\n",
      "cuss ways to apply the committee concept in practice, and we also give some insight\n",
      "into why it can sometimes be an effective procedure.\n",
      "One important variant of the committee method, known as boosting , involves\n",
      "training multiple models in sequence in which the error function used to train a par-\n",
      "ticular model depends on the performance of the previous models. This can producesubstantial improvements in performance compared to the use of a single model and\n",
      "is discussed in Section 14.3.\n",
      "Instead of averaging the predictions of a set of models, an alternative form of\n",
      "653654 14. COMBINING MODELS\n",
      "model combination is to select one of the models to make the prediction, in which\n",
      "the choice of model is a function of the input variables. Thus different models be-come responsible for making predictions in different regions of input space. One\n",
      "widely used framework of this kind is known as a decision tree in which the selec-\n",
      "tion process can be described as a sequence of binary selections corresponding tothe traversal of a tree structure and is discussed in Section 14.4. In this case, the\n",
      "individual models are generally chosen to be very simple, and the overall ﬂexibility\n",
      "of the model arises from the input-dependent selection process. Decision trees canbe applied to both classiﬁcation and regression problems.\n",
      "One limitation of decision trees is that the division of input space is based on\n",
      "hard splits in which only one model is responsible for making predictions for any\n",
      "given value of the input variables. The decision process can be softened by moving\n",
      "to a probabilistic framework for combining models, as discussed in Section 14.5. Forexample, if we have a set of Kmodels for a conditional distribution p(t|x,k)where\n",
      "xis the input variable, tis the target variable, and k=1,...,K indexes the model,\n",
      "then we can form a probabilistic mixture of the form\n",
      "p(t|x)=\n",
      "K∑\n",
      "k=1πk(x)p(t|x,k) (14.1)\n",
      "in which πk(x)=p(k|x)represent the input-dependent mixing coefﬁcients. Such\n",
      "models can be viewed as mixture distributions in which the component densities, as\n",
      "well as the mixing coefﬁcients, are conditioned on the input variables and are known\n",
      "asmixtures of experts . They are closely related to the mixture density network model\n",
      "discussed in Section 5.6.\n",
      "14.1. Bayesian Model Averaging\n",
      "It is important to distinguish between model combination methods and Bayesian\n",
      "model averaging, as the two are often confused. To understand the difference, con-\n",
      "sider the example of density estimation using a mixture of Gaussians in which several Section 9.2\n",
      "Gaussian components are combined probabilistically. The model contains a binary\n",
      "latent variable zthat indicates which component of the mixture is responsible for\n",
      "generating the corresponding data point. Thus the model is speciﬁed in terms of a\n",
      "joint distribution\n",
      "p(x,z) (14.2)\n",
      "and the corresponding density over the observed variable xis obtained by marginal-\n",
      "izing over the latent variable\n",
      "p(x)=∑\n",
      "zp(x,z). (14.3)14.2. Committees 655\n",
      "In the case of our Gaussian mixture example, this leads to a distribution of the form\n",
      "p(x)=K∑\n",
      "k=1πkN(x|µk,Σk) (14.4)\n",
      "with the usual interpretation of the symbols. This is an example of model combi-\n",
      "nation. For independent, identically distributed data, we can use (14.3) to write themarginal probability of a data set X={x\n",
      "1,...,xN}in the form\n",
      "p(X)=N∏\n",
      "n=1p(xn)=N∏\n",
      "n=1[∑\n",
      "znp(xn,zn)]\n",
      ". (14.5)\n",
      "Thus we see that each observed data point xnhas a corresponding latent variable zn.\n",
      "Now suppose we have several different models indexed by h=1,...,H with\n",
      "prior probabilities p(h). For instance one model might be a mixture of Gaussians and\n",
      "another model might be a mixture of Cauchy distributions. The marginal distributionover the data set is given by\n",
      "p(X)=H∑\n",
      "h=1p(X|h)p(h). (14.6)\n",
      "This is an example of Bayesian model averaging. The interpretation of this summa-\n",
      "tion over his that just one model is responsible for generating the whole data set,\n",
      "and the probability distribution over hsimply reﬂects our uncertainty as to which\n",
      "model that is. As the size of the data set increases, this uncertainty reduces, and\n",
      "the posterior probabilities p(h|X)become increasingly focussed on just one of the\n",
      "models.\n",
      "This highlights the key difference between Bayesian model averaging and model\n",
      "combination, because in Bayesian model averaging the whole data set is generated\n",
      "by a single model. By contrast, when we combine multiple models, as in (14.5), wesee that different data points within the data set can potentially be generated from\n",
      "different values of the latent variable zand hence by different components.\n",
      "Although we have considered the marginal probability p(X), the same consid-\n",
      "erations apply for the predictive density p(x|X)or for conditional distributions such\n",
      "asp(t|x,X,T). Exercise 14.1\n",
      "14.2. Committees\n",
      "The simplest way to construct a committee is to average the predictions of a set of\n",
      "individual models. Such a procedure can be motivated from a frequentist perspective\n",
      "by considering the trade-off between bias and variance, which decomposes the er- Section 3.2\n",
      "ror due to a model into the bias component that arises from differences between themodel and the true function to be predicted, and the variance component that repre-\n",
      "sents the sensitivity of the model to the individual data points. Recall from Figure 3.5656 14. COMBINING MODELS\n",
      "that when we trained multiple polynomials using the sinusoidal data, and then aver-\n",
      "aged the resulting functions, the contribution arising from the variance term tended tocancel, leading to improved predictions. When we averaged a set of low-bias mod-\n",
      "els (corresponding to higher order polynomials), we obtained accurate predictions\n",
      "for the underlying sinusoidal function from which the data were generated.\n",
      "In practice, of course, we have only a single data set, and so we have to ﬁnd\n",
      "a way to introduce variability between the different models within the committee.\n",
      "One approach is to use bootstrap data sets, discussed in Section 1.2.3. Consider a\n",
      "regression problem in which we are trying to predict the value of a single continuous\n",
      "variable, and suppose we generate Mbootstrap data sets and then use each to train\n",
      "a separate copy y\n",
      "m(x)of a predictive model where m=1,...,M . The committee\n",
      "prediction is given by\n",
      "yCOM(x)=1\n",
      "MM∑\n",
      "m=1ym(x). (14.7)\n",
      "This procedure is known as bootstrap aggregation or bagging (Breiman, 1996).\n",
      "Suppose the true regression function that we are trying to predict is given by\n",
      "h(x), so that the output of each of the models can be written as the true value plus\n",
      "an error in the form\n",
      "ym(x)=h(x)+ϵm(x). (14.8)\n",
      "The average sum-of-squares error then takes the form\n",
      "Ex[\n",
      "{ym(x)−h(x)}2]\n",
      "=Ex[\n",
      "ϵm(x)2]\n",
      "(14.9)\n",
      "where Ex[·]denotes a frequentist expectation with respect to the distribution of the\n",
      "input vector x. The average error made by the models acting individually is therefore\n",
      "EAV=1\n",
      "MM∑\n",
      "m=1Ex[\n",
      "ϵm(x)2]\n",
      ". (14.10)\n",
      "Similarly, the expected error from the committee (14.7) is given by\n",
      "ECOM = Ex⎡\n",
      "⎣{\n",
      "1\n",
      "MM∑\n",
      "m=1ym(x)−h(x)}2⎤\n",
      "⎦\n",
      "= Ex⎡\n",
      "⎣{\n",
      "1\n",
      "MM∑\n",
      "m=1ϵm(x)}2⎤\n",
      "⎦ (14.11)\n",
      "If we assume that the errors have zero mean and are uncorrelated, so that\n",
      "Ex[ϵm(x) ]=0 (14.12)\n",
      "Ex[ϵm(x)ϵl(x) ]=0 ,m ̸=l (14.13)14.3. Boosting 657\n",
      "then we obtain Exercise 14.2\n",
      "ECOM=1\n",
      "MEAV. (14.14)\n",
      "This apparently dramatic result suggests that the average error of a model can be\n",
      "reduced by a factor of Msimply by averaging Mversions of the model. Unfortu-\n",
      "nately, it depends on the key assumption that the errors due to the individual models\n",
      "are uncorrelated. In practice, the errors are typically highly correlated, and the reduc-tion in overall error is generally small. It can, however, be shown that the expected\n",
      "committee error will not exceed the expected error of the constituent models, so that\n",
      "E\n",
      "COM⩽EAV. In order to achieve more signiﬁcant improvements, we turn to a more Exercise 14.3\n",
      "sophisticated technique for building committees, known as boosting.\n",
      "14.3. Boosting\n",
      "Boosting is a powerful technique for combining multiple ‘base’ classiﬁers to producea form of committee whose performance can be signiﬁcantly better than that of anyof the base classiﬁers. Here we describe the most widely used form of boosting\n",
      "algorithm called AdaBoost , short for ‘adaptive boosting’, developed by Freund and\n",
      "Schapire (1996). Boosting can give good results even if the base classiﬁers have a\n",
      "performance that is only slightly better than random, and hence sometimes the base\n",
      "classiﬁers are known as weak learners . Originally designed for solving classiﬁcation\n",
      "problems, boosting can also be extended to regression (Friedman, 2001).\n",
      "The principal difference between boosting and the committee methods such as\n",
      "bagging discussed above, is that the base classiﬁers are trained in sequence, andeach base classiﬁer is trained using a weighted form of the data set in which the\n",
      "weighting coefﬁcient associated with each data point depends on the performance\n",
      "of the previous classiﬁers. In particular, points that are misclassiﬁed by one of thebase classiﬁers are given greater weight when used to train the next classiﬁer in\n",
      "the sequence. Once all the classiﬁers have been trained, their predictions are then\n",
      "combined through a weighted majority voting scheme, as illustrated schematicallyin Figure 14.1.\n",
      "Consider a two-class classiﬁcation problem, in which the training data comprises\n",
      "input vectors x\n",
      "1,...,xNalong with corresponding binary target variables t1,...,t N\n",
      "where tn∈{ −1,1}. Each data point is given an associated weighting parameter\n",
      "wn, which is initially set 1/Nfor all data points. We shall suppose that we have\n",
      "a procedure available for training a base classiﬁer using weighted data to give a\n",
      "function y(x)∈{ −1,1}. At each stage of the algorithm, AdaBoost trains a new\n",
      "classiﬁer using a data set in which the weighting coefﬁcients are adjusted accordingto the performance of the previously trained classiﬁer so as to give greater weight\n",
      "to the misclassiﬁed data points. Finally, when the desired number of base classiﬁers\n",
      "have been trained, they are combined to form a committee using coefﬁcients thatgive different weight to different base classiﬁers. The precise form of the AdaBoost\n",
      "algorithm is given below.658 14. COMBINING MODELS\n",
      "Figure 14.1 Schematic illustration of the\n",
      "boosting framework. Each\n",
      "base classiﬁer ym(x)is trained\n",
      "on a weighted form of the train-\n",
      "ing set (blue arrows) in which\n",
      "the weights w(m)\n",
      "ndepend on\n",
      "the performance of the pre-\n",
      "vious base classiﬁer ym−1(x)\n",
      "(green arrows). Once all base\n",
      "classiﬁers have been trained,\n",
      "they are combined to give\n",
      "the ﬁnal classiﬁer YM(x)(red\n",
      "arrows).{w(1)\n",
      "n}{ w(2)\n",
      "n}{ w(M)\n",
      "n}\n",
      "y1(x) y2(x) yM(x)\n",
      "YM(x)=s i g n(M∑\n",
      "mαmym(x))\n",
      "AdaBoost\n",
      "1. Initialize the data weighting coefﬁcients {wn}by setting w(1)\n",
      "n=1/Nfor\n",
      "n=1,...,N .\n",
      "2. For m=1,...,M :\n",
      "(a) Fit a classiﬁer ym(x)to the training data by minimizing the weighted\n",
      "error function\n",
      "Jm=N∑\n",
      "n=1w(m)\n",
      "nI(ym(xn)̸=tn) (14.15)\n",
      "where I(ym(xn)̸=tn)is the indicator function and equals 1when\n",
      "ym(xn)̸=tnand0otherwise.\n",
      "(b) Evaluate the quantities\n",
      "ϵm=N∑\n",
      "n=1w(m)\n",
      "nI(ym(xn)̸=tn)\n",
      "N∑\n",
      "n=1w(m)\n",
      "n(14.16)\n",
      "and then use these to evaluate\n",
      "αm=l n{1−ϵm\n",
      "ϵm}\n",
      ". (14.17)\n",
      "(c) Update the data weighting coefﬁcients\n",
      "w(m+1)\n",
      "n =w(m)\n",
      "nexp{αmI(ym(xn)̸=tn)} (14.18)14.3. Boosting 659\n",
      "3. Make predictions using the ﬁnal model, which is given by\n",
      "YM(x) = sign(M∑\n",
      "m=1αmym(x))\n",
      ". (14.19)\n",
      "We see that the ﬁrst base classiﬁer y1(x)is trained using weighting coefﬁ-\n",
      "cients w(1)\n",
      "nthat are all equal, which therefore corresponds to the usual procedure\n",
      "for training a single classiﬁer. From (14.18), we see that in subsequent iterations\n",
      "the weighting coefﬁcients w(m)\n",
      "nare increased for data points that are misclassiﬁed\n",
      "and decreased for data points that are correctly classiﬁed. Successive classiﬁers are\n",
      "therefore forced to place greater emphasis on points that have been misclassiﬁed by\n",
      "previous classiﬁers, and data points that continue to be misclassiﬁed by successive\n",
      "classiﬁers receive ever greater weight. The quantities ϵmrepresent weighted mea-\n",
      "sures of the error rates of each of the base classiﬁers on the data set. We therefore\n",
      "see that the weighting coefﬁcients αmdeﬁned by (14.17) give greater weight to the\n",
      "more accurate classiﬁers when computing the overall output given by (14.19).\n",
      "The AdaBoost algorithm is illustrated in Figure 14.2, using a subset of 30data\n",
      "points taken from the toy classiﬁcation data set shown in Figure A.7. Here each base\n",
      "learners consists of a threshold on one of the input variables. This simple classiﬁercorresponds to a form of decision tree known as a ‘decision stumps’, i.e., a deci- Section 14.4\n",
      "sion tree with a single node. Thus each base learner classiﬁes an input according to\n",
      "whether one of the input features exceeds some threshold and therefore simply parti-tions the space into two regions separated by a linear decision surface that is parallel\n",
      "to one of the axes.\n",
      "14.3.1 Minimizing exponential error\n",
      "Boosting was originally motivated using statistical learning theory, leading to\n",
      "upper bounds on the generalization error. However, these bounds turn out to be too\n",
      "loose to have practical value, and the actual performance of boosting is much better\n",
      "than the bounds alone would suggest. Friedman et al. (2000) gave a different and\n",
      "very simple interpretation of boosting in terms of the sequential minimization of an\n",
      "exponential error function.\n",
      "Consider the exponential error function deﬁned by\n",
      "E=N∑\n",
      "n=1exp{−tnfm(xn)} (14.20)\n",
      "where fm(x)is a classiﬁer deﬁned in terms of a linear combination of base classiﬁers\n",
      "yl(x)of the form\n",
      "fm(x)=1\n",
      "2m∑\n",
      "l=1αlyl(x) (14.21)\n",
      "andtn∈{ −1,1}are the training set target values. Our goal is to minimize Ewith\n",
      "respect to both the weighting coefﬁcients αland the parameters of the base classiﬁers\n",
      "yl(x).660 14. COMBINING MODELS\n",
      "m=1\n",
      "−1 0 1 2−202m=2\n",
      "−1 0 1 2−202m=3\n",
      "−1 0 1 2−202\n",
      "m=6\n",
      "−1 0 1 2−202m=1 0\n",
      "−1 0 1 2−202m= 150\n",
      "−1 0 1 2−202\n",
      "Figure 14.2 Illustration of boosting in which the base learners consist of simple thresholds applied to one or\n",
      "other of the axes. Each ﬁgure shows the number mof base learners trained so far, along with the decision\n",
      "boundary of the most recent base learner (dashed black line) and the combined decision boundary of the en-\n",
      "semble (solid green line). Each data point is depicted by a circle whose radius indicates the weight assigned to\n",
      "that data point when training the most recently added base learner. Thus, for instance, we see that points that\n",
      "are misclassiﬁed by the m=1base learner are given greater weight when training the m=2base learner.\n",
      "Instead of doing a global error function minimization, however, we shall sup-\n",
      "pose that the base classiﬁers y1(x),...,y m−1(x)are ﬁxed, as are their coefﬁcients\n",
      "α1,...,α m−1, and so we are minimizing only with respect to αmandym(x). Sep-\n",
      "arating off the contribution from base classiﬁer ym(x), we can then write the error\n",
      "function in the form\n",
      "E=N∑\n",
      "n=1exp{\n",
      "−tnfm−1(xn)−1\n",
      "2tnαmym(xn)}\n",
      "=N∑\n",
      "n=1w(m)\n",
      "nexp{\n",
      "−1\n",
      "2tnαmym(xn)}\n",
      "(14.22)\n",
      "where the coefﬁcients w(m)\n",
      "n=e x p {−tnfm−1(xn)}can be viewed as constants\n",
      "because we are optimizing only αmandym(x). If we denote by Tmthe set of\n",
      "data points that are correctly classiﬁed by ym(x), and if we denote the remaining\n",
      "misclassiﬁed points by Mm, then we can in turn rewrite the error function in the14.3. Boosting 661\n",
      "form\n",
      "E=e−αm/2∑\n",
      "n∈Tmw(m)\n",
      "n+eαm/2∑\n",
      "n∈M mw(m)\n",
      "n\n",
      "=(eαm/2−e−αm/2)N∑\n",
      "n=1w(m)\n",
      "nI(ym(xn)̸=tn)+e−αm/2N∑\n",
      "n=1w(m)\n",
      "n.\n",
      "(14.23)\n",
      "When we minimize this with respect to ym(x), we see that the second term is con-\n",
      "stant, and so this is equivalent to minimizing (14.15) because the overall multiplica-tive factor in front of the summation does not affect the location of the minimum.\n",
      "Similarly, minimizing with respect to α\n",
      "m, we obtain (14.17) in which ϵmis deﬁned\n",
      "by (14.16). Exercise 14.6\n",
      "From (14.22) we see that, having found αmandym(x), the weights on the data\n",
      "points are updated using\n",
      "w(m+1)\n",
      "n =w(m)\n",
      "nexp{\n",
      "−1\n",
      "2tnαmym(xn)}\n",
      ". (14.24)\n",
      "Making use of the fact that\n",
      "tnym(xn)=1−2I(ym(xn)̸=tn) (14.25)\n",
      "we see that the weights w(m)\n",
      "nare updated at the next iteration using\n",
      "w(m+1)\n",
      "n =w(m)\n",
      "nexp(−αm/2) exp {αmI(ym(xn)̸=tn)}. (14.26)\n",
      "Because the term exp(−αm/2)is independent of n, we see that it weights all data\n",
      "points by the same factor and so can be discarded. Thus we obtain (14.18).\n",
      "Finally, once all the base classiﬁers are trained, new data points are classiﬁed by\n",
      "evaluating the sign of the combined function deﬁned according to (14.21). Becausethe factor of 1/2does not affect the sign it can be omitted, giving (14.19).\n",
      "14.3.2 Error functions for boosting\n",
      "The exponential error function that is minimized by the AdaBoost algorithm\n",
      "differs from those considered in previous chapters. To gain some insight into the\n",
      "nature of the exponential error function, we ﬁrst consider the expected error given\n",
      "by\n",
      "Ex,t[exp{−ty(x)}]=∑\n",
      "t∫\n",
      "exp{−ty(x)}p(t|x)p(x)dx. (14.27)\n",
      "If we perform a variational minimization with respect to all possible functions y(x),\n",
      "we obtain Exercise 14.7\n",
      "y(x)=1\n",
      "2ln{p(t=1|x)\n",
      "p(t=−1|x)}\n",
      "(14.28)662 14. COMBINING MODELS\n",
      "Figure 14.3 Plot of the exponential (green) and\n",
      "rescaled cross-entropy (red) error\n",
      "functions along with the hinge er-\n",
      "ror (blue) used in support vector\n",
      "machines, and the misclassiﬁcation\n",
      "error (black). Note that for large\n",
      "negative values of z=ty(x), the\n",
      "cross-entropy gives a linearly in-\n",
      "creasing penalty, whereas the expo-\n",
      "nential loss gives an exponentially in-\n",
      "creasing penalty.\n",
      "−2 −1 012zE(z)\n",
      "which is half the log-odds. Thus the AdaBoost algorithm is seeking the best approx-\n",
      "imation to the log odds ratio, within the space of functions represented by the linear\n",
      "combination of base classiﬁers, subject to the constrained minimization resulting\n",
      "from the sequential optimization strategy. This result motivates the use of the sign\n",
      "function in (14.19) to arrive at the ﬁnal classiﬁcation decision.\n",
      "We have already seen that the minimizer y(x)of the cross-entropy error (4.90)\n",
      "for two-class classiﬁcation is given by the posterior class probability. In the case\n",
      "of a target variable t∈{ −1,1}, we have seen that the error function is given by Section 7.1.2\n",
      "ln(1 + exp( −yt)). This is compared with the exponential error function in Fig-\n",
      "ure 14.3, where we have divided the cross-entropy error by a constant factor ln(2)\n",
      "so that it passes through the point (0,1)for ease of comparison. We see that both\n",
      "can be seen as continuous approximations to the ideal misclassiﬁcation error func-\n",
      "tion. An advantage of the exponential error is that its sequential minimization leads\n",
      "to the simple AdaBoost scheme. One drawback, however, is that it penalizes large\n",
      "negative values of ty(x)much more strongly than cross-entropy. In particular, we\n",
      "see that for large negative values of ty, the cross-entropy grows linearly with |ty|,\n",
      "whereas the exponential error function grows exponentially with |ty|. Thus the ex-\n",
      "ponential error function will be much less robust to outliers or misclassiﬁed data\n",
      "points. Another important difference between cross-entropy and the exponential er-\n",
      "ror function is that the latter cannot be interpreted as the log likelihood function of\n",
      "any well-deﬁned probabilistic model. Furthermore, the exponential error does not Exercise 14.8\n",
      "generalize to classiﬁcation problems having K>2classes, again in contrast to the\n",
      "cross-entropy for a probabilistic model, which is easily generalized to give (4.108). Section 4.3.4\n",
      "The interpretation of boosting as the sequential optimization of an additive model\n",
      "under an exponential error (Friedman et al. , 2000) opens the door to a wide range\n",
      "of boosting-like algorithms, including multiclass extensions, by altering the choice\n",
      "of error function. It also motivates the extension to regression problems (Friedman,\n",
      "2001). If we consider a sum-of-squares error function for regression, then sequential\n",
      "minimization of an additive model of the form (14.21) simply involves ﬁtting each\n",
      "new base classiﬁer to the residual errors tn−fm−1(xn)from the previous model. As Exercise 14.9\n",
      "we have noted, however, the sum-of-squares error is not robust to outliers, and this14.4. Tree-based Models 663\n",
      "Figure 14.4 Comparison of the squared error\n",
      "(green) with the absolute error (red)\n",
      "showing how the latter places much\n",
      "less emphasis on large errors and\n",
      "hence is more robust to outliers and\n",
      "mislabelled data points.\n",
      "0 zE(z)\n",
      "−11\n",
      "can be addressed by basing the boosting algorithm on the absolute deviation |y−t|\n",
      "instead. These two error functions are compared in Figure 14.4.\n",
      "14.4. Tree-based Models\n",
      "There are various simple, but widely used, models that work by partitioning the\n",
      "input space into cuboid regions, whose edges are aligned with the axes, and then\n",
      "assigning a simple model (for example, a constant) to each region. They can be\n",
      "viewed as a model combination method in which only one model is responsible\n",
      "for making predictions at any given point in input space. The process of selecting\n",
      "a speciﬁc model, given a new input x, can be described by a sequential decision\n",
      "making process corresponding to the traversal of a binary tree (one that splits into\n",
      "two branches at each node). Here we focus on a particular tree-based framework\n",
      "called classiﬁcation and regression trees ,o r CART (Breiman et al. , 1984), although\n",
      "there are many other variants going by such names as ID3 and C4.5 (Quinlan, 1986;\n",
      "Quinlan, 1993).\n",
      "Figure 14.5 shows an illustration of a recursive binary partitioning of the input\n",
      "space, along with the corresponding tree structure. In this example, the ﬁrst step\n",
      "Figure 14.5 Illustration of a two-dimensional in-\n",
      "put space that has been partitioned\n",
      "into ﬁve regions using axis-aligned\n",
      "boundaries.\n",
      "AB\n",
      "CDE\n",
      "θ1 θ4θ2θ3\n",
      "x1x2664 14. COMBINING MODELS\n",
      "Figure 14.6 Binary tree corresponding to the par-\n",
      "titioning of input space shown in Fig-\n",
      "ure 14.5.x1>θ1\n",
      "x2>θ3\n",
      "x1⩽θ4x2⩽θ2\n",
      "ABCDE\n",
      "divides the whole of the input space into two regions according to whether x1⩽θ1\n",
      "orx1>θ1where θ1is a parameter of the model. This creates two subregions, each\n",
      "of which can then be subdivided independently. For instance, the region x1⩽θ1\n",
      "is further subdivided according to whether x2⩽θ2orx2>θ2, giving rise to the\n",
      "regions denoted A and B. The recursive subdivision can be described by the traversal\n",
      "of the binary tree shown in Figure 14.6. For any new input x, we determine which\n",
      "region it falls into by starting at the top of the tree at the root node and following\n",
      "a path down to a speciﬁc leaf node according to the decision criteria at each node.\n",
      "Note that such decision trees are not probabilistic graphical models.\n",
      "Within each region, there is a separate model to predict the target variable. For\n",
      "instance, in regression we might simply predict a constant over each region, or in\n",
      "classiﬁcation we might assign each region to a speciﬁc class. A key property of tree-\n",
      "based models, which makes them popular in ﬁelds such as medical diagnosis, for\n",
      "example, is that they are readily interpretable by humans because they correspond\n",
      "to a sequence of binary decisions applied to the individual input variables. For in-\n",
      "stance, to predict a patient’s disease, we might ﬁrst ask “is their temperature greater\n",
      "than some threshold?”. If the answer is yes, then we might next ask “is their blood\n",
      "pressure less than some threshold?”. Each leaf of the tree is then associated with a\n",
      "speciﬁc diagnosis.\n",
      "In order to learn such a model from a training set, we have to determine the\n",
      "structure of the tree, including which input variable is chosen at each node to form\n",
      "the split criterion as well as the value of the threshold parameter θifor the split. We\n",
      "also have to determine the values of the predictive variable within each region.\n",
      "Consider ﬁrst a regression problem in which the goal is to predict a single target\n",
      "variable tfrom a D-dimensional vector x=(x1,...,x D)Tof input variables. The\n",
      "training data consists of input vectors {x1,...,xN}along with the corresponding\n",
      "continuous labels {t1,...,t N}. If the partitioning of the input space is given, and we\n",
      "minimize the sum-of-squares error function, then the optimal value of the predictive\n",
      "variable within any given region is just given by the average of the values of tnfor\n",
      "those data points that fall in that region. Exercise 14.10\n",
      "Now consider how to determine the structure of the decision tree. Even for a\n",
      "ﬁxed number of nodes in the tree, the problem of determining the optimal structure\n",
      "(including choice of input variable for each split as well as the corresponding thresh-14.4. Tree-based Models 665\n",
      "olds) to minimize the sum-of-squares error is usually computationally infeasible due\n",
      "to the combinatorially large number of possible solutions. Instead, a greedy opti-mization is generally done by starting with a single root node, corresponding to the\n",
      "whole input space, and then growing the tree by adding nodes one at a time. At each\n",
      "step there will be some number of candidate regions in input space that can be split,corresponding to the addition of a pair of leaf nodes to the existing tree. For each\n",
      "of these, there is a choice of which of the Dinput variables to split, as well as the\n",
      "value of the threshold. The joint optimization of the choice of region to split, and thechoice of input variable and threshold, can be done efﬁciently by exhaustive search\n",
      "noting that, for a given choice of split variable and threshold, the optimal choice of\n",
      "predictive variable is given by the local average of the data, as noted earlier. This\n",
      "is repeated for all possible choices of variable to be split, and the one that gives the\n",
      "smallest residual sum-of-squares error is retained.\n",
      "Given a greedy strategy for growing the tree, there remains the issue of when\n",
      "to stop adding nodes. A simple approach would be to stop when the reduction in\n",
      "residual error falls below some threshold. However, it is found empirically that oftennone of the available splits produces a signiﬁcant reduction in error, and yet after\n",
      "several more splits a substantial error reduction is found. For this reason, it is com-\n",
      "mon practice to grow a large tree, using a stopping criterion based on the numberof data points associated with the leaf nodes, and then prune back the resulting tree.\n",
      "The pruning is based on a criterion that balances residual error against a measure of\n",
      "model complexity. If we denote the starting tree for pruning by T\n",
      "0, then we deﬁne\n",
      "T⊂T0to be a subtree of T0if it can be obtained by pruning nodes from T0(in\n",
      "other words, by collapsing internal nodes by combining the corresponding regions).\n",
      "Suppose the leaf nodes are indexed by τ=1,...,|T|, with leaf node τrepresenting\n",
      "a region Rτof input space having Nτdata points, and |T|denoting the total number\n",
      "of leaf nodes. The optimal prediction for region Rτis then given by\n",
      "yτ=1\n",
      "Nτ∑\n",
      "xn∈Rτtn (14.29)\n",
      "and the corresponding contribution to the residual sum-of-squares is then\n",
      "Qτ(T)=∑\n",
      "xn∈Rτ{tn−yτ}2. (14.30)\n",
      "The pruning criterion is then given by\n",
      "C(T)=|T|∑\n",
      "τ=1Qτ(T)+λ|T| (14.31)\n",
      "The regularization parameter λdetermines the trade-off between the overall residual\n",
      "sum-of-squares error and the complexity of the model as measured by the number\n",
      "|T|of leaf nodes, and its value is chosen by cross-validation.\n",
      "For classiﬁcation problems, the process of growing and pruning the tree is sim-\n",
      "ilar, except that the sum-of-squares error is replaced by a more appropriate measure666 14. COMBINING MODELS\n",
      "of performance. If we deﬁne pτkto be the proportion of data points in region Rτ\n",
      "assigned to class k, where k=1,...,K , then two commonly used choices are the\n",
      "cross-entropy\n",
      "Qτ(T)=K∑\n",
      "k=1pτklnpτk (14.32)\n",
      "and the Gini index\n",
      "Qτ(T)=K∑\n",
      "k=1pτk(1−pτk). (14.33)\n",
      "These both vanish for pτk=0andpτk=1and have a maximum at pτk=0.5. They\n",
      "encourage the formation of regions in which a high proportion of the data points are\n",
      "assigned to one class. The cross entropy and the Gini index are better measures than\n",
      "the misclassiﬁcation rate for growing the tree because they are more sensitive to thenode probabilities. Also, unlike misclassiﬁcation rate, they are differentiable and Exercise 14.11\n",
      "hence better suited to gradient based optimization methods. For subsequent pruning\n",
      "of the tree, the misclassiﬁcation rate is generally used.\n",
      "The human interpretability of a tree model such as CART is often seen as its\n",
      "major strength. However, in practice it is found that the particular tree structure that\n",
      "is learned is very sensitive to the details of the data set, so that a small change to thetraining data can result in a very different set of splits (Hastie et al. , 2001).\n",
      "There are other problems with tree-based methods of the kind considered in\n",
      "this section. One is that the splits are aligned with the axes of the feature space,\n",
      "which may be very suboptimal. For instance, to separate two classes whose optimal\n",
      "decision boundary runs at 45 degrees to the axes would need a large number ofaxis-parallel splits of the input space as compared to a single non-axis-aligned split.\n",
      "Furthermore, the splits in a decision tree are hard, so that each region of input space\n",
      "is associated with one, and only one, leaf node model. The last issue is particularlyproblematic in regression where we are typically aiming to model smooth functions,\n",
      "and yet the tree model produces piecewise-constant predictions with discontinuities\n",
      "at the split boundaries.\n",
      "14.5. Conditional Mixture Models\n",
      "We have seen that standard decision trees are restricted by hard, axis-aligned splits of\n",
      "the input space. These constraints can be relaxed, at the expense of interpretability,\n",
      "by allowing soft, probabilistic splits that can be functions of all of the input variables,\n",
      "not just one of them at a time. If we also give the leaf models a probabilistic inter-\n",
      "pretation, we arrive at a fully probabilistic tree-based model called the hierarchical\n",
      "mixture of experts , which we consider in Section 14.5.3.\n",
      "An alternative way to motivate the hierarchical mixture of experts model is to\n",
      "start with a standard probabilistic mixtures of unconditional density models such asGaussians and replace the component densities with conditional distributions. Here Chapter 9\n",
      "we consider mixtures of linear regression models (Section 14.5.1) and mixtures of14.5. Conditional Mixture Models 667\n",
      "logistic regression models (Section 14.5.2). In the simplest case, the mixing coefﬁ-\n",
      "cients are independent of the input variables. If we make a further generalization toallow the mixing coefﬁcients also to depend on the inputs then we obtain a mixture\n",
      "of experts model. Finally, if we allow each component in the mixture model to be\n",
      "itself a mixture of experts model, then we obtain a hierarchical mixture of experts.\n",
      "14.5.1 Mixtures of linear regression models\n",
      "One of the many advantages of giving a probabilistic interpretation to the lin-\n",
      "ear regression model is that it can then be used as a component in more complex\n",
      "probabilistic models. This can be done, for instance, by viewing the conditional\n",
      "distribution representing the linear regression model as a node in a directed prob-abilistic graph. Here we consider a simple example corresponding to a mixture of\n",
      "linear regression models, which represents a straightforward extension of the Gaus-\n",
      "sian mixture model discussed in Section 9.2 to the case of conditional Gaussiandistributions.\n",
      "We therefore consider Klinear regression models, each governed by its own\n",
      "weight parameter w\n",
      "k. In many applications, it will be appropriate to use a common\n",
      "noise variance, governed by a precision parameter β, for all Kcomponents, and this\n",
      "is the case we consider here. We will once again restrict attention to a single targetvariable t, though the extension to multiple outputs is straightforward. If we denote Exercise 14.12\n",
      "the mixing coefﬁcients by π\n",
      "k, then the mixture distribution can be written\n",
      "p(t|θ)=K∑\n",
      "k=1πkN(t|wT\n",
      "kφ,β−1) (14.34)\n",
      "where θdenotes the set of all adaptive parameters in the model, namely W={wk},\n",
      "π={πk}, andβ. The log likelihood function for this model, given a data set of\n",
      "observations {φn,tn}, then takes the form\n",
      "lnp(t|θ)=N∑\n",
      "n=1ln(K∑\n",
      "k=1πkN(tn|wT\n",
      "kφn,β−1))\n",
      "(14.35)\n",
      "where t=(t1,...,t N)Tdenotes the vector of target variables.\n",
      "In order to maximize this likelihood function, we can once again appeal to the\n",
      "EM algorithm, which will turn out to be a simple extension of the EM algorithm for\n",
      "unconditional Gaussian mixtures of Section 9.2. We can therefore build on our expe-rience with the unconditional mixture and introduce a set Z={z\n",
      "n}of binary latent\n",
      "variables where znk∈{0,1}in which, for each data point n, all of the elements\n",
      "k=1,...,K are zero except for a single value of 1indicating which component\n",
      "of the mixture was responsible for generating that data point. The joint distribution\n",
      "over latent and observed variables can be represented by the graphical model shown\n",
      "in Figure 14.7.\n",
      "The complete-data log likelihood function then takes the form Exercise 14.13\n",
      "lnp(t,Z|θ)=N∑\n",
      "n=1K∑\n",
      "k=1znkln{\n",
      "πkN(tn|wT\n",
      "kφn,β−1)}\n",
      ". (14.36)668 14. COMBINING MODELS\n",
      "Figure 14.7 Probabilistic directed graph representing a mixture of\n",
      "linear regression models, deﬁned by (14.35).zn\n",
      "tnφn\n",
      "N Wβπ\n",
      "The EM algorithm begins by ﬁrst choosing an initial value θoldfor the model param-\n",
      "eters. In the E step, these parameter values are then used to evaluate the posterior\n",
      "probabilities, or responsibilities, of each component kfor every data point ngiven\n",
      "by\n",
      "γnk=E[znk]=p(k|φn,θold)=πkN(tn|wT\n",
      "kφn,β−1)∑\n",
      "jπjN(tn|wT\n",
      "jφn,β−1). (14.37)\n",
      "The responsibilities are then used to determine the expectation, with respect to the\n",
      "posterior distribution p(Z|t,θold), of the complete-data log likelihood, which takes\n",
      "the form\n",
      "Q(θ,θold)= EZ[lnp(t,Z|θ)] =N∑\n",
      "n=1K∑\n",
      "k=1γnk{\n",
      "lnπk+l nN(tn|wT\n",
      "kφn,β−1)}\n",
      ".\n",
      "In the M step, we maximize the function Q(θ,θold)with respect to θ, keeping the\n",
      "γnkﬁxed. For the optimization with respect to the mixing coefﬁcients πkwe need\n",
      "to take account of the constraint∑\n",
      "kπk=1, which can be done with the aid of a\n",
      "Lagrange multiplier, leading to an M-step re-estimation equation for πkin the form Exercise 14.14\n",
      "πk=1\n",
      "NN∑\n",
      "n=1γnk. (14.38)\n",
      "Note that this has exactly the same form as the corresponding result for a simple\n",
      "mixture of unconditional Gaussians given by (9.22).\n",
      "Next consider the maximization with respect to the parameter vector wkof the\n",
      "kthlinear regression model. Substituting for the Gaussian distribution, we see that\n",
      "the function Q(θ,θold), as a function of the parameter vector wk, takes the form\n",
      "Q(θ,θold)=N∑\n",
      "n=1γnk{\n",
      "−β\n",
      "2(\n",
      "tn−wT\n",
      "kφn)2}\n",
      "+c o n s t (14.39)\n",
      "where the constant term includes the contributions from other weight vectors wjfor\n",
      "j̸=k. Note that the quantity we are maximizing is similar to the (negative of the)\n",
      "standard sum-of-squares error (3.12) for a single linear regression model, but with\n",
      "the inclusion of the responsibilities γnk. This represents a weighted least squares14.5. Conditional Mixture Models 669\n",
      "problem, in which the term corresponding to the nthdata point carries a weighting\n",
      "coefﬁcient given by βγnk, which could be interpreted as an effective precision for\n",
      "each data point. We see that each component linear regression model in the mixture,\n",
      "governed by its own parameter vector wk, is ﬁtted separately to the whole data set in\n",
      "the M step, but with each data point nweighted by the responsibility γnkthat model\n",
      "ktakes for that data point. Setting the derivative of (14.39) with respect to wkequal\n",
      "to zero gives\n",
      "0=N∑\n",
      "n=1γnk(\n",
      "tn−wT\n",
      "kφn)\n",
      "φn (14.40)\n",
      "which we can write in matrix notation as\n",
      "0=ΦTRk(t−Φwk) (14.41)\n",
      "whereRk= diag( γnk)is a diagonal matrix of size N×N. Solving for wk,w e\n",
      "obtain\n",
      "wk=(\n",
      "ΦTRkΦ)−1ΦTRkt. (14.42)\n",
      "This represents a set of modiﬁed normal equations corresponding to the weighted\n",
      "least squares problem, of the same form as (4.99) found in the context of logisticregression. Note that after each E step, the matrix R\n",
      "kwill change and so we will\n",
      "have to solve the normal equations afresh in the subsequent M step.\n",
      "Finally, we maximize Q(θ,θold)with respect to β. Keeping only terms that\n",
      "depend on β, the function Q(θ,θold)can be written\n",
      "Q(θ,θold)=N∑\n",
      "n=1K∑\n",
      "k=1γnk{1\n",
      "2lnβ−β\n",
      "2(\n",
      "tn−wT\n",
      "kφn)2}\n",
      ". (14.43)\n",
      "Setting the derivative with respect to βequal to zero, and rearranging, we obtain the\n",
      "M-step equation for βin the form\n",
      "1\n",
      "β=1\n",
      "NN∑\n",
      "n=1K∑\n",
      "k=1γnk(\n",
      "tn−wT\n",
      "kφn)2. (14.44)\n",
      "In Figure 14.8, we illustrate this EM algorithm using the simple example of\n",
      "ﬁtting a mixture of two straight lines to a data set having one input variable xand\n",
      "one target variable t. The predictive density (14.34) is plotted in Figure 14.9 using\n",
      "the converged parameter values obtained from the EM algorithm, corresponding to\n",
      "the right-hand plot in Figure 14.8. Also shown in this ﬁgure is the result of ﬁtting\n",
      "a single linear regression model, which gives a unimodal predictive density. We see\n",
      "that the mixture model gives a much better representation of the data distribution,\n",
      "and this is reﬂected in the higher likelihood value. However, the mixture modelalso assigns signiﬁcant probability mass to regions where there is no data because its\n",
      "predictive distribution is bimodal for all values of x. This problem can be resolved by\n",
      "extending the model to allow the mixture coefﬁcients themselves to be functions ofx, leading to models such as the mixture density networks discussed in Section 5.6,\n",
      "and hierarchical mixture of experts discussed in Section 14.5.3.670 14. COMBINING MODELS\n",
      "−1 −0.5 0 0.5 1−1.5−1−0.500.511.5\n",
      "−1 −0.5 0 0.5 1−1.5−1−0.500.511.5\n",
      "−1 −0.5 0 0.5 1−1.5−1−0.500.511.5\n",
      "−1 −0.5 0 0.5 100.20.40.60.81\n",
      "−1 −0.5 0 0.5 100.20.40.60.81\n",
      "−1 −0.5 0 0.5 100.20.40.60.81\n",
      "Figure 14.8 Example of a synthetic data set, shown by the green points, having one input variable xand one\n",
      "target variable t, together with a mixture of two linear regression models whose mean functions y(x,wk), where\n",
      "k∈{1,2}, are shown by the blue and red lines. The upper three plots show the initial conﬁguration (left), the\n",
      "result of running 30 iterations of EM (centre), and the result after 50 iterations of EM (right). Here βwas initialized\n",
      "to the reciprocal of the true variance of the set of target values. The lower three plots show the corresponding\n",
      "responsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the\n",
      "posterior probability of the blue line for that data point (and similarly for the red segment).\n",
      "14.5.2 Mixtures of logistic models\n",
      "Because the logistic regression model deﬁnes a conditional distribution for the\n",
      "target variable, given the input vector, it is straightforward to use it as the component\n",
      "distribution in a mixture model, thereby giving rise to a richer family of conditional\n",
      "distributions compared to a single logistic regression model. This example involves\n",
      "a straightforward combination of ideas encountered in earlier sections of the book\n",
      "and will help consolidate these for the reader.\n",
      "The conditional distribution of the target variable, for a probabilistic mixture of\n",
      "Klogistic regression models, is given by\n",
      "p(t|φ,θ)=K∑\n",
      "k=1πkyt\n",
      "k[1−yk]1−t(14.45)\n",
      "where φis the feature vector, yk=σ(\n",
      "wT\n",
      "kφ)\n",
      "is the output of component k, andθ\n",
      "denotes the adjustable parameters namely {πk}and{wk}.\n",
      "Now suppose we are given a data set {φn,tn}. The corresponding likelihood14.5. Conditional Mixture Models 671\n",
      "Figure 14.9 The left plot shows the predictive conditional density corresponding to the converged solution in\n",
      "Figure 14.8. This gives a log likelihood value of −3.0. A vertical slice through one of these plots at a particular\n",
      "value of xrepresents the corresponding conditional distribution p(t|x), which we see is bimodal. The plot on the\n",
      "right shows the predictive density for a single linear regression model ﬁtted to the same data set using maximum\n",
      "likelihood. This model has a smaller log likelihood of −27.6.\n",
      "function is then given by\n",
      "p(t|θ)=N∏\n",
      "n=1(K∑\n",
      "k=1πkytn\n",
      "nk[1−ynk]1−tn)\n",
      "(14.46)\n",
      "where ynk=σ(wT\n",
      "kφn)and t=(t1,...,t N)T. We can maximize this likelihood\n",
      "function iteratively by making use of the EM algorithm. This involves introducing\n",
      "latent variables znkthat correspond to a 1-of- Kcoded binary indicator variable for\n",
      "each data point n. The complete-data likelihood function is then given by\n",
      "p(t,Z|θ)=N∏\n",
      "n=1K∏\n",
      "k=1{\n",
      "πkytn\n",
      "nk[1−ynk]1−tn}znk(14.47)\n",
      "whereZis the matrix of latent variables with elements znk. We initialize the EM\n",
      "algorithm by choosing an initial value θoldfor the model parameters. In the E step,\n",
      "we then use these parameter values to evaluate the posterior probabilities of the com-ponents kfor each data point n, which are given by\n",
      "γ\n",
      "nk=E[znk]=p(k|φn,θold)=πkytn\n",
      "nk[1−ynk]1−tn\n",
      "∑\n",
      "jπjytn\n",
      "nj[1−ynj]1−tn. (14.48)\n",
      "These responsibilities are then used to ﬁnd the expected complete-data log likelihood\n",
      "as a function of θ, given by\n",
      "Q(θ,θold)= EZ[lnp(t,Z|θ)]\n",
      "=N∑\n",
      "n=1K∑\n",
      "k=1γnk{lnπk+tnlnynk+( 1−tn)l n( 1−ynk)}.(14.49)\n",
      "672 14. COMBINING MODELS\n",
      "The M step involves maximization of this function with respect to θ, keeping θold,\n",
      "and hence γnk, ﬁxed. Maximization with respect to πkcan be done in the usual way,\n",
      "with a Lagrange multiplier to enforce the summation constraint∑\n",
      "kπk=1, giving\n",
      "the familiar result\n",
      "πk=1\n",
      "NN∑\n",
      "n=1γnk. (14.50)\n",
      "To determine the {wk}, we note that the Q(θ,θold)function comprises a sum\n",
      "over terms indexed by keach of which depends only on one of the vectors wk,s o\n",
      "that the different vectors are decoupled in the M step of the EM algorithm. In other\n",
      "words, the different components interact only via the responsibilities, which are ﬁxedduring the M step. Note that the M step does not have a closed-form solution and\n",
      "must be solved iteratively using, for instance, the iterative reweighted least squares\n",
      "(IRLS) algorithm. The gradient and the Hessian for the vector w\n",
      "kare given by Section 4.3.3\n",
      "∇kQ=N∑\n",
      "n=1γnk(tn−ynk)φn (14.51)\n",
      "Hk=−∇k∇kQ=N∑\n",
      "n=1γnkynk(1−ynk)φnφT\n",
      "n (14.52)\n",
      "where ∇kdenotes the gradient with respect to wk. For ﬁxed γnk, these are indepen-\n",
      "dent of {wj}forj̸=kand so we can solve for each wkseparately using the IRLS\n",
      "algorithm. Thus the M-step equations for component kcorrespond simply to ﬁtting Section 4.3.3\n",
      "a single logistic regression model to a weighted data set in which data point ncarries\n",
      "a weight γnk. Figure 14.10 shows an example of the mixture of logistic regression\n",
      "models applied to a simple classiﬁcation problem. The extension of this model to amixture of softmax models for more than two classes is straightforward. Exercise 14.16\n",
      "14.5.3 Mixtures of experts\n",
      "In Section 14.5.1, we considered a mixture of linear regression models, and in\n",
      "Section 14.5.2 we discussed the analogous mixture of linear classiﬁers. Although\n",
      "these simple mixtures extend the ﬂexibility of linear models to include more com-\n",
      "plex (e.g., multimodal) predictive distributions, they are still very limited. We canfurther increase the capability of such models by allowing the mixing coefﬁcients\n",
      "themselves to be functions of the input variable, so that\n",
      "p(t|x)=K∑\n",
      "k=1πk(x)pk(t|x). (14.53)\n",
      "This is known as a mixture of experts model (Jacobs et al. , 1991) in which the mix-\n",
      "ing coefﬁcients πk(x)are known as gating functions and the individual component\n",
      "densities pk(t|x)are called experts . The notion behind the terminology is that differ-\n",
      "ent components can model the distribution in different regions of input space (they14.5. Conditional Mixture Models 673\n",
      "Figure 14.10 Illustration of a mixture of logistic regression models. The left plot shows data points drawn\n",
      "from two classes denoted red and blue, in which the background colour (which varies from pure red to pure blue)\n",
      "denotes the true probability of the class label. The centre plot shows the result of ﬁtting a single logistic regressionmodel using maximum likelihood, in which the background colour denotes the corresponding probability of theclass label. Because the colour is a near-uniform purple, we see that the model assigns a probability of around0.5to each of the classes over most of input space. The right plot shows the result of ﬁtting a mixture of two\n",
      "logistic regression models, which now gives much higher probability to the correct labels for many of the points\n",
      "in the blue class.\n",
      "are ‘experts’ at making predictions in their own regions), and the gating functions\n",
      "determine which components are dominant in which region.\n",
      "The gating functions πk(x)must satisfy the usual constraints for mixing co-\n",
      "efﬁcients, namely 0⩽πk(x)⩽1and∑\n",
      "kπk(x)=1 . They can therefore be\n",
      "represented, for example, by linear softmax models of the form (4.104) and (4.105).\n",
      "If the experts are also linear (regression or classiﬁcation) models, then the wholemodel can be ﬁtted efﬁciently using the EM algorithm, with iterative reweighted\n",
      "least squares being employed in the M step (Jordan and Jacobs, 1994).\n",
      "Such a model still has signiﬁcant limitations due to the use of linear models\n",
      "for the gating and expert functions. A much more ﬂexible model is obtained byusing a multilevel gating function to give the hierarchical mixture of experts ,o r\n",
      "HME model (Jordan and Jacobs, 1994). To understand the structure of this model,\n",
      "imagine a mixture distribution in which each component in the mixture is itself amixture distribution. For simple unconditional mixtures, this hierarchical mixture is\n",
      "trivially equivalent to a single ﬂat mixture distribution. However, when the mixing Exercise 14.17\n",
      "coefﬁcients are input dependent, this hierarchical model becomes nontrivial. The\n",
      "HME model can also be viewed as a probabilistic version of decision trees discussed\n",
      "in Section 14.4 and can again be trained efﬁciently by maximum likelihood using an\n",
      "EM algorithm with IRLS in the M step. A Bayesian treatment of the HME has been Section 4.3.3\n",
      "given by Bishop and Svens ´en (2003) based on variational inference.\n",
      "We shall not discuss the HME in detail here. However, it is worth pointing out\n",
      "the close connection with the mixture density network discussed in Section 5.6. The\n",
      "principal advantage of the mixtures of experts model is that it can be optimized byEM in which the M step for each mixture component and gating model involves\n",
      "a convex optimization (although the overall optimization is nonconvex). By con-\n",
      "trast, the advantage of the mixture density network approach is that the component674 14. COMBINING MODELS\n",
      "densities and the mixing coefﬁcients share the hidden units of the neural network.\n",
      "Furthermore, in the mixture density network, the splits of the input space are furtherrelaxed compared to the hierarchical mixture of experts in that they are not only soft,\n",
      "and not constrained to be axis aligned, but they can also be nonlinear.\n",
      "Exercises\n",
      "14.1 (⋆⋆)www Consider a set models of the form p(t|x,zh,θh,h)in which xis the\n",
      "input vector, tis the target vector, hindexes the different models, zhis a latent vari-\n",
      "able for model h, andθhis the set of parameters for model h. Suppose the models\n",
      "have prior probabilities p(h)and that we are given a training set X={x1,...,xN}\n",
      "andT={t1,...,tN}. Write down the formulae needed to evaluate the predic-\n",
      "tive distribution p(t|x,X,T)in which the latent variables and the model index are\n",
      "marginalized out. Use these formulae to highlight the difference between Bayesian\n",
      "averaging of different models and the use of latent variables within a single model.\n",
      "14.2 (⋆)The expected sum-of-squares error EAVfor a simple committee model can\n",
      "be deﬁned by (14.10), and the expected error of the committee itself is given by\n",
      "(14.11). Assuming that the individual errors satisfy (14.12) and (14.13), derive the\n",
      "result (14.14).\n",
      "14.3 (⋆)www By making use of Jensen’s inequality (1.115), for the special case of\n",
      "the convex function f(x)=x2, show that the average expected sum-of-squares\n",
      "errorEAVof the members of a simple committee model, given by (14.10), and the\n",
      "expected error ECOM of the committee itself, given by (14.11), satisfy\n",
      "ECOM⩽EAV. (14.54)\n",
      "14.4 (⋆⋆)By making use of Jensen’s in equality (1.115), show that the result (14.54)\n",
      "derived in the previous exercise hods for any error function E(y), not just sum-of-\n",
      "squares, provided it is a convex function of y.\n",
      "14.5 (⋆⋆)www Consider a committee in which we allow unequal weighting of the\n",
      "constituent models, so that\n",
      "yCOM(x)=M∑\n",
      "m=1αmym(x). (14.55)\n",
      "In order to ensure that the predictions yCOM(x)remain within sensible limits, sup-\n",
      "pose that we require that they be bounded at each value of xby the minimum and\n",
      "maximum values given by any of the members of the committee, so that\n",
      "ymin(x)⩽yCOM(x)⩽ymax(x). (14.56)\n",
      "Show that a necessary and sufﬁcient condition for this constraint is that the coefﬁ-\n",
      "cients αmsatisfy\n",
      "αm⩾0,M∑\n",
      "m=1αm=1. (14.57)Exercises 675\n",
      "14.6 (⋆)www By differentiating the error function (14.23) with respect to αm, show\n",
      "that the parameters αmin the AdaBoost algorithm are updated using (14.17) in\n",
      "which ϵmis deﬁned by (14.16).\n",
      "14.7 (⋆)By making a variational minimization of the expected exponential error function\n",
      "given by (14.27) with respect to all possible functions y(x), show that the minimizing\n",
      "function is given by (14.28).\n",
      "14.8 (⋆)Show that the exponential error function (14.20), which is minimized by the\n",
      "AdaBoost algorithm, does not correspond to the log likelihood of any well-behaved\n",
      "probabilistic model. This can be done by showing that the corresponding conditionaldistribution p(t|x)cannot be correctly normalized.\n",
      "14.9 (⋆)\n",
      "www Show that the sequential minimization of the sum-of-squares error func-\n",
      "tion for an additive model of the form (14.21) in the style of boosting simply involvesﬁtting each new base classiﬁer to the residual errors t\n",
      "n−fm−1(xn)from the previous\n",
      "model.\n",
      "14.10 (⋆)Verify that if we minimize the sum-of-squares error between a set of training\n",
      "values {tn}and a single predictive value t, then the optimal solution for tis given\n",
      "by the mean of the {tn}.\n",
      "14.11 (⋆⋆)Consider a data set comprising 400data points from class C1and400data\n",
      "points from class C2. Suppose that a tree model A splits these into (300,100) at\n",
      "the ﬁrst leaf node and (100,300) at the second leaf node, where (n, m)denotes that\n",
      "npoints are assigned to C1andmpoints are assigned to C2. Similarly, suppose\n",
      "that a second tree model B splits them into (200,400) and(200,0). Evaluate the\n",
      "misclassiﬁcation rates for the two trees and hence show that they are equal. Similarly,\n",
      "evaluate the cross-entropy (14.32) and Gini index (14.33) for the two trees and show\n",
      "that they are both lower for tree B than for tree A.\n",
      "14.12 (⋆⋆)Extend the results of Section 14.5.1 for a mixture of linear regression models\n",
      "to the case of multiple target values described by a vector t. To do this, make use of\n",
      "the results of Section 3.1.5.\n",
      "14.13 (⋆)www Verify that the complete-data log likelihood function for the mixture of\n",
      "linear regression models is given by (14.36).\n",
      "14.14 (⋆)Use the technique of Lagrange multipliers (Appendix E) to show that the M-step\n",
      "re-estimation equation for the mixing coefﬁcients in the mixture of linear regressionmodels trained by maximum likelihood EM is given by (14.38).\n",
      "14.15 (⋆)\n",
      "www We have already noted that if we use a squared loss function in a regres-\n",
      "sion problem, the corresponding optimal prediction of the target variable for a newinput vector is given by the conditional mean of the predictive distribution. Show\n",
      "that the conditional mean for the mixture of linear regression models discussed in\n",
      "Section 14.5.1 is given by a linear combination of the means of each component dis-tribution. Note that if the conditional distribution of the target data is multimodal,\n",
      "the conditional mean can give poor predictions.676 14. COMBINING MODELS\n",
      "14.16 (⋆⋆⋆ )Extend the logistic regression mixture model of Section 14.5.2 to a mixture\n",
      "of softmax classiﬁers representing C⩾2classes. Write down the EM algorithm for\n",
      "determining the parameters of this model through maximum likelihood.\n",
      "14.17 (⋆⋆)www Consider a mixture model for a conditional distribution p(t|x)of the\n",
      "form\n",
      "p(t|x)=K∑\n",
      "k=1πkψk(t|x) (14.58)\n",
      "in which each mixture component ψk(t|x)is itself a mixture model. Show that this\n",
      "two-level hierarchical mixture is equivalent to a conventional single-level mixturemodel. Now suppose that the mixing coefﬁcients in both levels of such a hierar-\n",
      "chical model are arbitrary functions of x. Again, show that this hierarchical model\n",
      "is again equivalent to a single-level model with x-dependent mixing coefﬁcients.\n",
      "Finally, consider the case in which the mixing coefﬁcients at both levels of the hi-\n",
      "erarchical mixture are constrained to be linear classiﬁcation (logistic or softmax)\n",
      "models. Show that the hierarchical mixture cannot in general be represented by asingle-level mixture having linear classiﬁcation models for the mixing coefﬁcients.\n",
      "Hint: to do this it is sufﬁcient to construct a single counter-example, so consider a\n",
      "mixture of two components in which one of those components is itself a mixture oftwo components, with mixing coefﬁcients given by linear-logistic models. Show that\n",
      "this cannot be represented by a single-level mixture of 3 components having mixing\n",
      "coefﬁcients determined by a linear-softmax model.Appendix A. Data Sets\n",
      "In this appendix, we give a brief introduction to the data sets used to illustrate some\n",
      "of the algorithms described in this book. Detailed information on ﬁle formats for\n",
      "these data sets, as well as the data ﬁles themselves, can be obtained from the book\n",
      "web site:\n",
      "http://research.microsoft.com/ ∼cmbishop/PRML\n",
      "Handwritten Digits\n",
      "The digits data used in this book is taken from the MNIST data set (LeCun et al. ,\n",
      "1998), which itself was constructed by modifying a subset of the much larger data\n",
      "set produced by NIST (the National Institute of Standards and Technology). It com-\n",
      "prises a training set of 60,000examples and a test set of 10,000examples. Some\n",
      "of the data was collected from Census Bureau employees and the rest was collected\n",
      "from high-school children, and care was taken to ensure that the test examples werewritten by different individuals to the training examples.\n",
      "The original NIST data had binary (black or white) pixels. To create MNIST,\n",
      "these images were size normalized to ﬁt in a 20×20pixel box while preserving their\n",
      "aspect ratio. As a consequence of the anti-aliasing used to change the resolution of\n",
      "the images, the resulting MNIST digits are grey scale. These images were then\n",
      "centred in a 28×28box. Examples of the MNIST digits are shown in Figure A.1.\n",
      "Error rates for classifying the digits range from 12% for a simple linear classi-\n",
      "ﬁer, through 0.56% for a carefully designed support vector machine, to 0.4% for a\n",
      "convolutional neural network (LeCun et al. , 1998).\n",
      "677678 A. DATA SETS\n",
      "Figure A.1 One hundred examples of the\n",
      "MNIST digits chosen at ran-\n",
      "dom from the training set.\n",
      "Oil Flow\n",
      "This is a synthetic data set that arose out of a project aimed at measuring nonin-\n",
      "vasively the proportions of oil, water, and gas in North Sea oil transfer pipelines\n",
      "(Bishop and James, 1993). It is based on the principle of dual-energy gamma densit-\n",
      "ometry . The ideas is that if a narrow beam of gamma rays is passed through the pipe,\n",
      "the attenuation in the intensity of the beam provides information about the density of\n",
      "material along its path. Thus, for instance, the beam will be attenuated more stronglyby oil than by gas.\n",
      "A single attenuation measurement alone is not sufﬁcient because there are two\n",
      "degrees of freedom corresponding to the fraction of oil and the fraction of water (thefraction of gas is redundant because the three fractions must add to one). To address\n",
      "this, two gamma beams of different energies (in other words different frequencies or\n",
      "wavelengths) are passed through the pipe along the same path, and the attenuation ofeach is measured. Because the absorbtion properties of different materials vary dif-\n",
      "ferently as a function of energy, measurement of the attenuations at the two energies\n",
      "provides two independent pieces of information. Given the known absorbtion prop-erties of oil, water, and gas at the two energies, it is then a simple matter to calculate\n",
      "the average fractions of oil and water (and hence of gas) measured along the path of\n",
      "the gamma beams.\n",
      "There is a further complication, however, associated with the motion of the ma-\n",
      "terials along the pipe. If the ﬂow velocity is small, then the oil ﬂoats on top of the\n",
      "water with the gas sitting above the oil. This is known as a laminar orstratiﬁedA. DATA SETS 679\n",
      "Figure A.2 The three geometrical conﬁgurations of the oil,\n",
      "water, and gas phases used to generate the oil-\n",
      "ﬂow data set. For each conﬁguration, the pro-\n",
      "portions of the three phases can vary.\n",
      "MixGasWaterOil\n",
      "HomogeneousStratiﬁed Annular\n",
      "ﬂow conﬁguration and is illustrated in Figure A.2. As the ﬂow velocity is increased,\n",
      "more complex geometrical conﬁgurations of the oil, water, and gas can arise. For the\n",
      "purposes of this data set, two speciﬁc idealizations are considered. In the annular\n",
      "conﬁguration the oil, water, and gas form concentric cylinders with the water around\n",
      "the outside and the gas in the centre, whereas in the homogeneous conﬁguration the\n",
      "oil, water and gas are assumed to be intimately mixed as might occur at high ﬂow\n",
      "velocities under turbulent conditions. These conﬁgurations are also illustrated in\n",
      "Figure A.2.\n",
      "We have seen that a single dual-energy beam gives the oil and water fractions\n",
      "measured along the path length, whereas we are interested in the volume fractions of\n",
      "oil and water. This can be addressed by using multiple dual-energy gamma densit-\n",
      "ometers whose beams pass through different regions of the pipe. For this particular\n",
      "data set, there are six such beams, and their spatial arrangement is shown in Fig-\n",
      "ure A.3. A single observation is therefore represented by a 12-dimensional vector\n",
      "comprising the fractions of oil and water measured along the paths of each of the\n",
      "beams. We are, however, interested in obtaining the overall volume fractions of the\n",
      "three phases in the pipe. This is much like the classical problem of tomographic re-\n",
      "construction, used in medical imaging for example, in which a two-dimensional dis-\n",
      "Figure A.3 Cross section of the pipe showing the arrangement of the\n",
      "six beam lines, each of which comprises a single dual-\n",
      "energy gamma densitometer. Note that the vertical beams\n",
      "are asymmetrically arranged relative to the central axis\n",
      "(shown by the dotted line).680 A. DATA SETS\n",
      "tribution is to be reconstructed from an number of one-dimensional averages. Here\n",
      "there are far fewer line measurements than in a typical tomography application. Onthe other hand the range of geometrical conﬁgurations is much more limited, and so\n",
      "the conﬁguration, as well as the phase fractions, can be predicted with reasonable\n",
      "accuracy from the densitometer data.\n",
      "For safety reasons, the intensity of the gamma beams is kept relatively weak and\n",
      "so to obtain an accurate measurement of the attenuation, the measured beam intensity\n",
      "is integrated over a speciﬁc time interval. For a ﬁnite integration time, there arerandom ﬂuctuations in the measured intensity due to the fact that the gamma beams\n",
      "comprise discrete packets of energy called photons. In practice, the integration time\n",
      "is chosen as a compromise between reducing the noise level (which requires a long\n",
      "integration time) and detecting temporal variations in the ﬂow (which requires a short\n",
      "integration time). The oil ﬂow data set is generated using realistic known values forthe absorption properties of oil, water, and gas at the two gamma energies used, and\n",
      "with a speciﬁc choice of integration time ( 10seconds) chosen as characteristic of a\n",
      "typical practical setup.\n",
      "Each point in the data set is generated independently using the following steps:\n",
      "1. Choose one of the three phase conﬁgurations at random with equal probability.2. Choose three random numbers f\n",
      "1,f2andf3from the uniform distribution over\n",
      "(0,1)and deﬁne\n",
      "foil=f1\n",
      "f1+f2+f3,f water=f2\n",
      "f1+f2+f3. (A.1)\n",
      "This treats the three phases on an equal footing and ensures that the volume\n",
      "fractions add to one.\n",
      "3. For each of the six beam lines, calculate the effective path lengths through oil\n",
      "and water for the given phase conﬁguration.\n",
      "4. Perturb the path lengths using the Poisson distribution based on the known\n",
      "beam intensities and integration time to allow for the effect of photon statistics.\n",
      "Each point in the data set comprises the 12path length measurements, together\n",
      "with the fractions of oil and water and a binary label describing the phase conﬁgu-ration. The data set is divided into training, validation, and test sets, each of which\n",
      "comprises 1,000independent data points. Details of the data format are available\n",
      "from the book web site.\n",
      "In Bishop and James (1993), statistical machine learning techniques were used\n",
      "to predict the volume fractions and also the geometrical conﬁguration of the phases\n",
      "shown in Figure A.2, from the 12-dimensional vector of measurements. The 12-\n",
      "dimensional observation vectors can also be used to test data visualization algo-\n",
      "rithms.\n",
      "This data set has a rich and interesting structure, as follows. For any given\n",
      "conﬁguration there are two degrees of freedom corresponding to the fractions ofA. DATA SETS 681\n",
      "oil and water, and so for inﬁnite integration time the data will locally live on a two-\n",
      "dimensional manifold. For a ﬁnite integration time, the individual data points will beperturbed away from the manifold by the photon noise. In the homogeneous phase\n",
      "conﬁguration, the path lengths in oil and water are linearly related to the fractions of\n",
      "oil and water, and so the data points lie close to a linear manifold. For the annularconﬁguration, the relationship between phase fraction and path length is nonlinear\n",
      "and so the manifold will be nonlinear. In the case of the laminar conﬁguration the\n",
      "situation is even more complex because small variations in the phase fractions cancause one of the horizontal phase boundaries to move across one of the horizontal\n",
      "beam lines leading to a discontinuous jump in the 12-dimensional observation space.\n",
      "In this way, the two-dimensional nonlinear manifold for the laminar conﬁguration is\n",
      "broken into six distinct segments. Note also that some of the manifolds for different\n",
      "phase conﬁgurations meet at speciﬁc points, for example if the pipe is ﬁlled entirelywith oil, it corresponds to speciﬁc instances of the laminar, annular, and homoge-\n",
      "neous conﬁgurations.\n",
      "Old Faithful\n",
      "Old Faithful, shown in Figure A.4, is a hydrothermal geyser in Yellowstone National\n",
      "Park in the state of Wyoming, U.S.A., and is a popular tourist attraction. Its name\n",
      "stems from the supposed regularity of its eruptions.\n",
      "The data set comprises 272observations, each of which represents a single erup-\n",
      "tion and contains two variables corresponding to the duration in minutes of the erup-\n",
      "tion, and the time until the next eruption, also in minutes. Figure A.5 shows a plot of\n",
      "the time to the next eruption versus the duration of the eruptions. It can be seen thatthe time to the next eruption varies considerably, although knowledge of the duration\n",
      "of the current eruption allows it to be predicted more accurately. Note that there exist\n",
      "several other data sets relating to the eruptions of Old Faithful.\n",
      "Figure A.4 The Old Faithful geyser\n",
      "in Y ellowstone National\n",
      "Park. c⃝Bruce T. Gourley\n",
      "www.brucegourley.com.\n",
      "682 A. DATA SETS\n",
      "Figure A.5 Plot of the time to the next eruption\n",
      "in minutes (vertical axis) versus the\n",
      "duration of the eruption in minutes\n",
      "(horizontal axis) for the Old Faithful\n",
      "data set.\n",
      "1 2 3 4 5 6405060708090100\n",
      "Synthetic Data\n",
      "Throughout the book, we use two simple synthetic data sets to illustrate many of the\n",
      "algorithms. The ﬁrst of these is a regression problem, based on the sinusoidal func-\n",
      "tion, shown in Figure A.6. The input values {xn}are generated uniformly in range\n",
      "(0,1), and the corresponding target values {tn}are obtained by ﬁrst computing the\n",
      "corresponding values of the function sin(2πx), and then adding random noise with\n",
      "a Gaussian distribution having standard deviation 0.3. Various forms of this data set,\n",
      "having different numbers of data points, are used in the book.\n",
      "The second data set is a classiﬁcation problem having two classes, with equal\n",
      "prior probabilities, and is shown in Figure A.7. The blue class is generated from a\n",
      "single Gaussian while the red class comes from a mixture of two Gaussians. Be-\n",
      "cause we know the class priors and the class-conditional densities, it is straightfor-\n",
      "ward to evaluate and plot the true posterior probabilities as well as the minimum\n",
      "misclassiﬁcation-rate decision boundary, as shown in Figure A.7.A. DATA SETS 683\n",
      "xt\n",
      "0 1−101\n",
      "xt\n",
      "0 1−101\n",
      "Figure A.6 The left-hand plot shows the synthetic regression data set along with the underlying sinusoidal\n",
      "function from which the data points were generated. The right-hand plot shows the true conditional distribution\n",
      "p(t|x)from which the labels are generated, in which the green curve denotes the mean, and the shaded region\n",
      "spans one standard deviation on each side of the mean.\n",
      "−2 0 2−202\n",
      "Figure A.7 The left plot shows the synthetic classiﬁcation data set with data from the two classes shown in\n",
      "red and blue. On the right is a plot of the true posterior probabilities, shown on a colour scale going from pure\n",
      "red denoting probability of the red class is 1to pure blue denoting probability of the red class is 0. Because\n",
      "these probabilities are known, the optimal decision boundary for minimizing the misclassiﬁcation rate (which\n",
      "corresponds to the contour along which the posterior probabilities for each class equal 0.5) can be evaluated\n",
      "and is shown by the green curve. This decision boundary is also plotted on the left-hand ﬁgure.Appendix B. Probability Distributions\n",
      "In this appendix, we summarize the main properties of some of the most widely used\n",
      "probability distributions, and for each distribution we list some key statistics such as\n",
      "the expectation E[x], the variance (or covariance), the mode, and the entropy H[x].\n",
      "All of these distributions are members of the exponential family and are widely used\n",
      "as building blocks for more sophisticated probabilistic models.\n",
      "Bernoulli\n",
      "This is the distribution for a single binary variable x∈{0,1}representing, for\n",
      "example, the result of ﬂipping a coin. It is governed by a single continuous parameter\n",
      "µ∈[0,1]that represents the probability of x=1.\n",
      "Bern(x|µ)= µx(1−µ)1−x(B.1)\n",
      "E[x]= µ (B.2)\n",
      "var[x]= µ(1−µ) (B.3)\n",
      "mode[ x]={\n",
      "1ifµ⩾0.5,\n",
      "0otherwise(B.4)\n",
      "H[x]= −µlnµ−(1−µ)l n ( 1−µ). (B.5)\n",
      "The Bernoulli is a special case of the binomial distribution for the case of a single\n",
      "observation. Its conjugate prior for µis the beta distribution.\n",
      "685686 B. PROBABILITY DISTRIBUTIONS\n",
      "Beta\n",
      "This is a distribution over a continuous variable µ∈[0,1], which is often used to\n",
      "represent the probability for some binary event. It is governed by two parameters a\n",
      "andbthat are constrained by a>0andb>0to ensure that the distribution can be\n",
      "normalized.\n",
      "Beta(µ|a, b)=Γ(a+b)\n",
      "Γ(a)Γ(b)µa−1(1−µ)b−1(B.6)\n",
      "E[µ]=a\n",
      "a+b(B.7)\n",
      "var[µ]=ab\n",
      "(a+b)2(a+b+1 )(B.8)\n",
      "mode[ µ]=a−1\n",
      "a+b−2. (B.9)\n",
      "The beta is the conjugate prior for the Bernoulli distribution, for which aandbcan\n",
      "be interpreted as the effective prior number of observations of x=1 andx=0,\n",
      "respectively. Its density is ﬁnite if a⩾1andb⩾1, otherwise there is a singularity\n",
      "atµ=0and/or µ=1.F o ra=b=1, it reduces to a uniform distribution. The beta\n",
      "distribution is a special case of the K-state Dirichlet distribution for K=2.\n",
      "Binomial\n",
      "The binomial distribution gives the probability of observing moccurrences of x=1\n",
      "in a set of Nsamples from a Bernoulli distribution, where the probability of observ-\n",
      "ingx=1isµ∈[0,1].\n",
      "Bin(m|N,µ)=(N\n",
      "m)\n",
      "µm(1−µ)N−m(B.10)\n",
      "E[m]= Nµ (B.11)\n",
      "var[m]= Nµ(1−µ) (B.12)\n",
      "mode[ m]= ⌊(N+1 )µ⌋ (B.13)\n",
      "where ⌊(N+1 )µ⌋denotes the largest integer that is less than or equal to (N+1 )µ,\n",
      "and the quantity(N\n",
      "m)\n",
      "=N!\n",
      "m!(N−m)!(B.14)\n",
      "denotes the number of ways of choosing mobjects out of a total of Nidentical\n",
      "objects. Here m!, pronounced ‘factorial m’, denotes the product m×(m−1)×\n",
      "...,×2×1. The particular case of the binomial distribution for N=1is known as\n",
      "the Bernoulli distribution, and for large Nthe binomial distribution is approximately\n",
      "Gaussian. The conjugate prior for µis the beta distribution.B. PROBABILITY DISTRIBUTIONS 687\n",
      "Dirichlet\n",
      "The Dirichlet is a multivariate distribution over Krandom variables 0⩽µk⩽1,\n",
      "where k=1,...,K , subject to the constraints\n",
      "0⩽µk⩽1,K∑\n",
      "k=1µk=1. (B.15)\n",
      "Denoting µ=(µ1,...,µ K)Tandα=(α1,...,α K)T,w eh a v e\n",
      "Dir(µ|α)= C(α)K∏\n",
      "k=1µαk−1\n",
      "k(B.16)\n",
      "E[µk]=αk\n",
      "ˆα(B.17)\n",
      "var[µk]=αk(ˆα−αk)\n",
      "ˆα2(ˆα+1 )(B.18)\n",
      "cov[µjµk]= −αjαk\n",
      "ˆα2(ˆα+1 )(B.19)\n",
      "mode[ µk]=αk−1\n",
      "ˆα−K(B.20)\n",
      "E[lnµk]= ψ(αk)−ψ(ˆα) (B.21)\n",
      "H[µ]= −K∑\n",
      "k=1(αk−1){ψ(αk)−ψ(ˆα)}−lnC(α) (B.22)\n",
      "where\n",
      "C(α)=Γ(ˆα)\n",
      "Γ(α1)···Γ(αK)(B.23)\n",
      "and\n",
      "ˆα=K∑\n",
      "k=1αk. (B.24)\n",
      "Here\n",
      "ψ(a)≡d\n",
      "daln Γ(a) (B.25)\n",
      "is known as the digamma function (Abramowitz and Stegun, 1965). The parameters\n",
      "αkare subject to the constraint αk>0in order to ensure that the distribution can be\n",
      "normalized.\n",
      "The Dirichlet forms the conjugate prior for the multinomial distribution and rep-\n",
      "resents a generalization of the beta distribution. In this case, the parameters αkcan\n",
      "be interpreted as effective numbers of observations of the corresponding values of\n",
      "theK-dimensional binary observation vector x. As with the beta distribution, the\n",
      "Dirichlet has ﬁnite density everywhere provided αk⩾1for all k.688 B. PROBABILITY DISTRIBUTIONS\n",
      "Gamma\n",
      "The Gamma is a probability distribution over a positive random variable τ>0\n",
      "governed by parameters aandbthat are subject to the constraints a>0andb>0\n",
      "to ensure that the distribution can be normalized.\n",
      "Gam( τ|a, b)=1\n",
      "Γ(a)baτa−1e−bτ(B.26)\n",
      "E[τ]=a\n",
      "b(B.27)\n",
      "var[τ]=a\n",
      "b2(B.28)\n",
      "mode[ τ]=a−1\n",
      "bforα⩾1 (B.29)\n",
      "E[lnτ]= ψ(a)−lnb (B.30)\n",
      "H[τ]=l n Γ ( a)−(a−1)ψ(a)−lnb+a (B.31)\n",
      "where ψ(·)is the digamma function deﬁned by (B.25). The gamma distribution is\n",
      "the conjugate prior for the precision (inverse variance) of a univariate Gaussian. For\n",
      "a⩾1the density is everywhere ﬁnite, and the special case of a=1is known as the\n",
      "exponential distribution.\n",
      "Gaussian\n",
      "The Gaussian is the most widely used distribution for continuous variables. It is also\n",
      "known as the normal distribution. In the case of a single variable x∈(−∞,∞)it is\n",
      "governed by two parameters, the mean µ∈(−∞,∞)and the variance σ2>0.\n",
      "N(x|µ, σ2)=1\n",
      "(2πσ2)1/2exp{\n",
      "−1\n",
      "2σ2(x−µ)2}\n",
      "(B.32)\n",
      "E[x]= µ (B.33)\n",
      "var[x]= σ2(B.34)\n",
      "mode[ x]= µ (B.35)\n",
      "H[x]=1\n",
      "2lnσ2+1\n",
      "2(1 + ln(2 π)). (B.36)\n",
      "The inverse of the variance τ=1/σ2is called the precision, and the square root\n",
      "of the variance σis called the standard deviation. The conjugate prior for µis the\n",
      "Gaussian, and the conjugate prior for τis the gamma distribution. If both µandτ\n",
      "are unknown, their joint conjugate prior is the Gaussian-gamma distribution.\n",
      "For a D-dimensional vector x, the Gaussian is governed by a D-dimensional\n",
      "mean vector µand a D×Dcovariance matrix Σthat must be symmetric andB. PROBABILITY DISTRIBUTIONS 689\n",
      "positive-deﬁnite.\n",
      "N(x|µ,Σ)=1\n",
      "(2π)D/21\n",
      "|Σ|1/2exp{\n",
      "−1\n",
      "2(x−µ)TΣ−1(x−µ)}\n",
      "(B.37)\n",
      "E[x]= µ (B.38)\n",
      "cov[x]= Σ (B.39)\n",
      "mode[x]= µ (B.40)\n",
      "H[x]=1\n",
      "2ln|Σ|+D\n",
      "2(1 + ln(2 π)). (B.41)\n",
      "The inverse of the covariance matrix Λ=Σ−1is the precision matrix, which is also\n",
      "symmetric and positive deﬁnite. Averages of random variables tend to a Gaussian, bythe central limit theorem, and the sum of two Gaussian variables is again Gaussian.\n",
      "The Gaussian is the distribution that maximizes the entropy for a given variance\n",
      "(or covariance). Any linear transformation of a Gaussian random variable is againGaussian. The marginal distribution of a multivariate Gaussian with respect to a\n",
      "subset of the variables is itself Gaussian, and similarly the conditional distribution is\n",
      "also Gaussian. The conjugate prior for µis the Gaussian, the conjugate prior for Λ\n",
      "is the Wishart, and the conjugate prior for (µ,Λ)is the Gaussian-Wishart.\n",
      "If we have a marginal Gaussian distribution for xand a conditional Gaussian\n",
      "distribution for ygivenxin the form\n",
      "p(x)= N(x|µ,Λ\n",
      "−1) (B.42)\n",
      "p(y|x)= N(y|Ax+b,L−1) (B.43)\n",
      "then the marginal distribution of y, and the conditional distribution of xgiveny, are\n",
      "given by\n",
      "p(y)= N(y|Aµ+b,L−1+AΛ−1AT) (B.44)\n",
      "p(x|y)= N(x|Σ{ATL(y−b)+Λµ},Σ) (B.45)\n",
      "where\n",
      "Σ=(Λ+ATLA)−1. (B.46)\n",
      "If we have a joint Gaussian distribution N(x|µ,Σ)withΛ≡Σ−1and we\n",
      "deﬁne the following partitions\n",
      "x=(\n",
      "xa\n",
      "xb)\n",
      ",µ=(\n",
      "µa\n",
      "µb)\n",
      "(B.47)\n",
      "Σ=(\n",
      "ΣaaΣab\n",
      "ΣbaΣbb)\n",
      ",Λ=(\n",
      "ΛaaΛab\n",
      "ΛbaΛbb)\n",
      "(B.48)\n",
      "then the conditional distribution p(xa|xb)is given by\n",
      "p(xa|xb)= N(x|µa|b,Λ−1\n",
      "aa) (B.49)\n",
      "µa|b=µa−Λ−1\n",
      "aaΛab(xb−µb) (B.50)690 B. PROBABILITY DISTRIBUTIONS\n",
      "and the marginal distribution p(xa)is given by\n",
      "p(xa)=N(xa|µa,Σaa). (B.51)\n",
      "Gaussian-Gamma\n",
      "This is the conjugate prior distribution for a univariate Gaussian N(x|µ, λ−1)in\n",
      "which the mean µand the precision λare both unknown and is also called the\n",
      "normal-gamma distribution. It comprises the product of a Gaussian distribution for\n",
      "µ, whose precision is proportional to λ, and a gamma distribution over λ.\n",
      "p(µ, λ|µ0,β,a ,b )=N(\n",
      "µ|µo,(βλ)−1)\n",
      "Gam( λ|a, b). (B.52)\n",
      "Gaussian-Wishart\n",
      "This is the conjugate prior distribution for a multivariate Gaussian N(x|µ,Λ)in\n",
      "which both the mean µand the precision Λare unknown, and is also called the\n",
      "normal-Wishart distribution. It comprises the product of a Gaussian distribution for\n",
      "µ, whose precision is proportional to Λ, and a Wishart distribution over Λ.\n",
      "p(µ,Λ|µ0,β,W,ν)=N(\n",
      "µ|µ0,(βΛ)−1)\n",
      "W(Λ|W,ν). (B.53)\n",
      "For the particular case of a scalar x, this is equivalent to the Gaussian-gamma distri-\n",
      "bution.\n",
      "Multinomial\n",
      "If we generalize the Bernoulli distribution to an K-dimensional binary variable x\n",
      "with components xk∈{0,1}such that∑\n",
      "kxk=1, then we obtain the following\n",
      "discrete distribution\n",
      "p(x)=K∏\n",
      "k=1µxk\n",
      "k(B.54)\n",
      "E[xk]= µk (B.55)\n",
      "var[xk]= µk(1−µk) (B.56)\n",
      "cov[xjxk]= Ijkµk (B.57)\n",
      "H[x]= −M∑\n",
      "k=1µklnµk (B.58)B. PROBABILITY DISTRIBUTIONS 691\n",
      "where Ijkis thej,kelement of the identity matrix. Because p(xk=1 )= µk, the\n",
      "parameters must satisfy 0⩽µk⩽1and∑\n",
      "kµk=1.\n",
      "The multinomial distribution is a multivariate generalization of the binomial and\n",
      "gives the distribution over counts mkfor aK-state discrete variable to be in state k\n",
      "given a total number of observations N.\n",
      "Mult( m1,m2,...,m K|µ,N)=(N\n",
      "m1m2...m M)M∏\n",
      "k=1µm k\n",
      "k(B.59)\n",
      "E[mk]= Nµk (B.60)\n",
      "var[mk]= Nµk(1−µk) (B.61)\n",
      "cov[mjmk]= −Nµjµk (B.62)\n",
      "where µ=(µ1,...,µ K)T, and the quantity\n",
      "(N\n",
      "m1m2...m K)\n",
      "=N!\n",
      "m1!...m K!(B.63)\n",
      "gives the number of ways of taking Nidentical objects and assigning mkof them to\n",
      "binkfork=1,...,K . The value of µkgives the probability of the random variable\n",
      "taking state k, and so these parameters are subject to the constraints 0⩽µk⩽1\n",
      "and∑\n",
      "kµk=1. The conjugate prior distribution for the parameters {µk}is the\n",
      "Dirichlet.\n",
      "Normal\n",
      "The normal distribution is simply another name for the Gaussian. In this book, weuse the term Gaussian throughout, although we retain the conventional use of the\n",
      "symbol Nto denote this distribution. For consistency, we shall refer to the normal-\n",
      "gamma distribution as the Gaussian-gamma distribution, and similarly the normal-Wishart is called the Gaussian-Wishart.\n",
      "Student’s t\n",
      "This distribution was published by William Gosset in 1908, but his employer, Gui-\n",
      "ness Breweries, required him to publish under a pseudonym, so he chose ‘Student’.In the univariate form, Student’s t-distribution is obtained by placing a conjugate\n",
      "gamma prior over the precision of a univariate Gaussian distribution and then inte-\n",
      "grating out the precision variable. It can therefore be viewed as an inﬁnite mixture692 B. PROBABILITY DISTRIBUTIONS\n",
      "of Gaussians having the same mean but different variances.\n",
      "St(x|µ, λ, ν )=Γ(ν/2+1/2)\n",
      "Γ(ν/2)(λ\n",
      "πν)1/2[\n",
      "1+λ(x−µ)2\n",
      "ν]−ν/2−1/2\n",
      "(B.64)\n",
      "E[x]= µforν>1 (B.65)\n",
      "var[x]=1\n",
      "λν\n",
      "ν−2forν>2 (B.66)\n",
      "mode[ x]= µ. (B.67)\n",
      "Hereν>0is called the number of degrees of freedom of the distribution. The\n",
      "particular case of ν=1is called the Cauchy distribution.\n",
      "For aD-dimensional variable x, Student’s t-distribution corresponds to marginal-\n",
      "izing the precision matrix of a multivariate Gaussian with respect to a conjugate\n",
      "Wishart prior and takes the form\n",
      "St(x|µ,Λ,ν)=Γ(ν/2+D/2)\n",
      "Γ(ν/2)|Λ|1/2\n",
      "(νπ)D/2[\n",
      "1+∆2\n",
      "ν]−ν/2−D/2\n",
      "(B.68)\n",
      "E[x]= µforν>1 (B.69)\n",
      "cov[x]=ν\n",
      "ν−2Λ−1forν>2 (B.70)\n",
      "mode[x]= µ (B.71)\n",
      "where ∆2is the squared Mahalanobis distance deﬁned by\n",
      "∆2=(x−µ)TΛ(x−µ). (B.72)\n",
      "In the limit ν→∞ , the t-distribution reduces to a Gaussian with mean µand pre-\n",
      "cisionΛ. Student’s t-distribution provides a generalization of the Gaussian whose\n",
      "maximum likelihood parameter values are robust to outliers.\n",
      "Uniform\n",
      "This is a simple distribution for a continuous variable xdeﬁned over a ﬁnite interval\n",
      "x∈[a, b]where b>a .\n",
      "U(x|a, b)=1\n",
      "b−a(B.73)\n",
      "E[x]=(b+a)\n",
      "2(B.74)\n",
      "var[x]=(b−a)2\n",
      "12(B.75)\n",
      "H[x]=l n ( b−a). (B.76)\n",
      "Ifxhas distribution U(x|0,1), thena+(b−a)xwill have distribution U(x|a, b).B. PROBABILITY DISTRIBUTIONS 693\n",
      "Von Mises\n",
      "The von Mises distribution, also known as the circular normal or the circular Gaus-\n",
      "sian, is a univariate Gaussian-like periodic distribution for a variable θ∈[0,2π).\n",
      "p(θ|θ0,m)=1\n",
      "2πI0(m)exp{mcos(θ−θ0)} (B.77)\n",
      "where I0(m)is the zeroth-order Bessel function of the ﬁrst kind. The distribution\n",
      "has period 2πso that p(θ+2π)=p(θ)for all θ. Care must be taken in interpret-\n",
      "ing this distribution because simple expectations will be dependent on the (arbitrary)\n",
      "choice of origin for the variable θ. The parameter θ0is analogous to the mean of a\n",
      "univariate Gaussian, and the parameter m>0, known as the concentration param-\n",
      "eter, is analogous to the precision (inverse variance). For large m, the von Mises\n",
      "distribution is approximately a Gaussian centred on θ0.\n",
      "Wishart\n",
      "The Wishart distribution is the conjugate prior for the precision matrix of a multi-\n",
      "variate Gaussian.\n",
      "W(Λ|W,ν)=B(W,ν)|Λ|(ν−D−1)/2exp(\n",
      "−1\n",
      "2Tr(W−1Λ))\n",
      "(B.78)\n",
      "where\n",
      "B(W,ν)≡|W|−ν/2(\n",
      "2νD/2πD(D−1)/4D∏\n",
      "i=1Γ(ν+1−i\n",
      "2))−1\n",
      "(B.79)\n",
      "E[Λ]= νW (B.80)\n",
      "E[ln|Λ|]=D∑\n",
      "i=1ψ(ν+1−i\n",
      "2)\n",
      "+Dln 2 + ln |W| (B.81)\n",
      "H[Λ]= −lnB(W,ν)−(ν−D−1)\n",
      "2E[ln|Λ|]+νD\n",
      "2(B.82)\n",
      "whereWis aD×Dsymmetric, positive deﬁnite matrix, and ψ(·)is the digamma\n",
      "function deﬁned by (B.25). The parameter νis called the number of degrees of\n",
      "freedom of the distribution and is restricted to ν>D −1to ensure that the Gamma\n",
      "function in the normalization factor is well-deﬁned. In one dimension, the Wishart\n",
      "reduces to the gamma distribution Gam( λ|a, b)given by (B.26) with parameters\n",
      "a=ν/2andb=1/2W.Appendix C. Properties of Matrices\n",
      "In this appendix, we gather together some useful properties and identities involving\n",
      "matrices and determinants. This is not intended to be an introductory tutorial, and\n",
      "it is assumed that the reader is already familiar with basic linear algebra. For some\n",
      "results, we indicate how to prove them, whereas in more complex cases we leave\n",
      "the interested reader to refer to standard textbooks on the subject. In all cases, we\n",
      "assume that inverses exist and that matrix dimensions are such that the formulaeare correctly deﬁned. A comprehensive discussion of linear algebra can be found in\n",
      "Golub and Van Loan (1996), and an extensive collection of matrix properties is given\n",
      "by L ¨utkepohl (1996). Matrix derivatives are discussed in Magnus and Neudecker\n",
      "(1999).\n",
      "Basic Matrix Identities\n",
      "A matrix Ahas elements Aijwhere iindexes the rows, and jindexes the columns.\n",
      "We use INto denote the N×Nidentity matrix (also called the unit matrix), and\n",
      "where there is no ambiguity over dimensionality we simply use I. The transpose\n",
      "matrixAThas elements (AT)ij=Aji. From the deﬁnition of transpose, we have\n",
      "(AB)T=BTAT(C.1)\n",
      "which can be veriﬁed by writing out the indices. The inverse of A, denoted A−1,\n",
      "satisﬁes\n",
      "AA−1=A−1A=I. (C.2)\n",
      "Because ABB−1A−1=I,w eh a v e\n",
      "(AB)−1=B−1A−1. (C.3)\n",
      "Also we have (\n",
      "AT)−1=(\n",
      "A−1)T(C.4)\n",
      "695696 C. PROPERTIES OF MATRICES\n",
      "which is easily proven by taking the transpose of (C.2) and applying (C.1).\n",
      "A useful identity involving matrix inverses is the following\n",
      "(P−1+BTR−1B)−1BTR−1=PBT(BPBT+R)−1. (C.5)\n",
      "which is easily veriﬁed by right multiplying both sides by (BPBT+R). Suppose\n",
      "thatPhas dimensionality N×NwhileRhas dimensionality M×M, so that Bis\n",
      "M×N. Then if M≪N, it will be much cheaper to evaluate the right-hand side of\n",
      "(C.5) than the left-hand side. A special case that sometimes arises is\n",
      "(I+AB)−1A=A(I+BA)−1. (C.6)\n",
      "Another useful identity involving inverses is the following:\n",
      "(A+BD−1C)−1=A−1−A−1B(D+CA−1B)−1CA−1(C.7)\n",
      "which is known as the Woodbury identity and which can be veriﬁed by multiplying\n",
      "both sides by (A+BD−1C). This is useful, for instance, when Ais large and\n",
      "diagonal, and hence easy to invert, while Bhas many rows but few columns (and\n",
      "conversely for C) so that the right-hand side is much cheaper to evaluate than the\n",
      "left-hand side.\n",
      "A set of vectors {a1,...,aN}is said to be linearly independent if the relation∑\n",
      "nαnan=0 holds only if all αn=0. This implies that none of the vectors\n",
      "can be expressed as a linear combination of the remainder. The rank of a matrix is\n",
      "the maximum number of linearly independent rows (or equivalently the maximum\n",
      "number of linearly independent columns).\n",
      "Traces and Determinants\n",
      "Trace and determinant apply to square matrices. The trace Tr (A)of a matrix A\n",
      "is deﬁned as the sum of the elements on the leading diagonal. By writing out the\n",
      "indices, we see that\n",
      "Tr(AB)=Tr(BA). (C.8)\n",
      "By applying this formula multiple times to the product of three matrices, we see that\n",
      "Tr(ABC )=Tr(CAB )=Tr(BCA ) (C.9)\n",
      "which is known as the cyclic property of the trace operator and which clearly extends\n",
      "to the product of any number of matrices. The determinant |A|of anN×Nmatrix\n",
      "Ais deﬁned by\n",
      "|A|=∑\n",
      "(±1)A1i1A2i2···ANi N (C.10)\n",
      "in which the sum is taken over all products consisting of precisely one element from\n",
      "each row and one element from each column, with a coefﬁcient +1or−1accordingC. PROPERTIES OF MATRICES 697\n",
      "to whether the permutation i1i2...iNis even or odd, respectively. Note that |I|=1.\n",
      "Thus, for a 2×2matrix, the determinant takes the form\n",
      "|A|=⏐⏐⏐⏐a11a12\n",
      "a21a22⏐⏐⏐⏐\n",
      "=a11a22−a12a21. (C.11)\n",
      "The determinant of a product of two matrices is given by\n",
      "|AB|=|A||B| (C.12)\n",
      "as can be shown from (C.10). Also, the determinant of an inverse matrix is given by\n",
      "⏐⏐A−1⏐⏐\n",
      "=1\n",
      "|A|(C.13)\n",
      "which can be shown by taking the determinant of (C.2) and applying (C.12).\n",
      "IfAandBare matrices of size N×M, then\n",
      "⏐⏐IN+ABT⏐⏐=⏐⏐\n",
      "IM+ATB⏐⏐. (C.14)\n",
      "A useful special case is ⏐⏐IN+abT⏐⏐=1+aTb (C.15)\n",
      "whereaandbareN-dimensional column vectors.\n",
      "Matrix Derivatives\n",
      "Sometimes we need to consider derivatives of vectors and matrices with respect to\n",
      "scalars. The derivative of a vector awith respect to a scalar xis itself a vector whose\n",
      "components are given by(∂a\n",
      "∂x)\n",
      "i=∂ai\n",
      "∂x(C.16)\n",
      "with an analogous deﬁnition for the derivative of a matrix. Derivatives with respect\n",
      "to vectors and matrices can also be deﬁned, for instance\n",
      "(∂x\n",
      "∂a)\n",
      "i=∂x\n",
      "∂ai(C.17)\n",
      "and similarly(∂a\n",
      "∂b)\n",
      "ij=∂ai\n",
      "∂bj. (C.18)\n",
      "The following is easily proven by writing out the components\n",
      "∂\n",
      "∂x(\n",
      "xTa)\n",
      "=∂\n",
      "∂x(\n",
      "aTx)\n",
      "=a. (C.19)698 C. PROPERTIES OF MATRICES\n",
      "Similarly\n",
      "∂\n",
      "∂x(AB)=∂A\n",
      "∂xB+A∂B\n",
      "∂x. (C.20)\n",
      "The derivative of the inverse of a matrix can be expressed as\n",
      "∂\n",
      "∂x(\n",
      "A−1)\n",
      "=−A−1∂A\n",
      "∂xA−1(C.21)\n",
      "as can be shown by differentiating the equation A−1A=Iusing (C.20) and then\n",
      "right multiplying by A−1. Also\n",
      "∂\n",
      "∂xln|A|=Tr(\n",
      "A−1∂A\n",
      "∂x)\n",
      "(C.22)\n",
      "which we shall prove later. If we choose xto be one of the elements of A,w eh a v e\n",
      "∂\n",
      "∂AijTr(AB)=Bji (C.23)\n",
      "as can be seen by writing out the matrices using index notation. We can write this\n",
      "result more compactly in the form\n",
      "∂\n",
      "∂ATr(AB)=BT. (C.24)\n",
      "With this notation, we have the following properties\n",
      "∂\n",
      "∂ATr(\n",
      "ATB)\n",
      "=B (C.25)\n",
      "∂\n",
      "∂ATr(A)= I (C.26)\n",
      "∂\n",
      "∂ATr(ABAT)= A(B+BT) (C.27)\n",
      "which can again be proven by writing out the matrix indices. We also have\n",
      "∂\n",
      "∂Aln|A|=(\n",
      "A−1)T(C.28)\n",
      "which follows from (C.22) and (C.26).\n",
      "Eigenvector Equation\n",
      "For a square matrix Aof size M×M, the eigenvector equation is deﬁned by\n",
      "Aui=λiui (C.29)C. PROPERTIES OF MATRICES 699\n",
      "fori=1,...,M , where uiis an eigenvector andλiis the corresponding eigenvalue .\n",
      "This can be viewed as a set of Msimultaneous homogeneous linear equations, and\n",
      "the condition for a solution is that\n",
      "|A−λiI|=0 (C.30)\n",
      "which is known as the characteristic equation . Because this is a polynomial of order\n",
      "Minλi, it must have Msolutions (though these need not all be distinct). The rank\n",
      "ofAis equal to the number of nonzero eigenvalues.\n",
      "Of particular interest are symmetric matrices, which arise as covariance ma-\n",
      "trices, kernel matrices, and Hessians. Symmetric matrices have the property that\n",
      "Aij=Aji, or equivalently AT=A. The inverse of a symmetric matrix is also sym-\n",
      "metric, as can be seen by taking the transpose of A−1A=Iand using AA−1=I\n",
      "together with the symmetry of I.\n",
      "In general, the eigenvalues of a matrix are complex numbers, but for symmetric\n",
      "matrices the eigenvalues λiare real. This can be seen by ﬁrst left multiplying (C.29)\n",
      "by(u⋆\n",
      "i)T, where ⋆denotes the complex conjugate, to give\n",
      "(u⋆\n",
      "i)TAui=λi(u⋆\n",
      "i)Tui. (C.31)\n",
      "Next we take the complex conjugate of (C.29) and left multiply by uT\n",
      "ito give\n",
      "uT\n",
      "iAu⋆\n",
      "i=λ⋆\n",
      "iuT\n",
      "iu⋆i. (C.32)\n",
      "where we have used A⋆=Abecause we consider only real matrices A. Taking\n",
      "the transpose of the second of these equations, and using AT=A, we see that the\n",
      "left-hand sides of the two equations are equal, and hence that λ⋆\n",
      "i=λiand so λi\n",
      "must be real.\n",
      "The eigenvectors uiof a real symmetric matrix can be chosen to be orthonormal\n",
      "(i.e., orthogonal and of unit length) so that\n",
      "uT\n",
      "iuj=Iij (C.33)\n",
      "where Iijare the elements of the identity matrix I. To show this, we ﬁrst left multiply\n",
      "(C.29) by uT\n",
      "jto give\n",
      "uT\n",
      "jAui=λiuT\n",
      "jui (C.34)\n",
      "and hence, by exchange of indices, we have\n",
      "uT\n",
      "iAuj=λjuT\n",
      "iuj. (C.35)\n",
      "We now take the transpose of the second equation and make use of the symmetry\n",
      "property AT=A, and then subtract the two equations to give\n",
      "(λi−λj)uT\n",
      "iuj=0. (C.36)\n",
      "Hence, for λi̸=λj,w eh a v e uT\n",
      "iuj=0, and hence uiandujare orthogonal. If the\n",
      "two eigenvalues are equal, then any linear combination αui+βujis also an eigen-\n",
      "vector with the same eigenvalue, so we can select one linear combination arbitrarily,700 C. PROPERTIES OF MATRICES\n",
      "and then choose the second to be orthogonal to the ﬁrst (it can be shown that the de-\n",
      "generate eigenvectors are never linearly dependent). Hence the eigenvectors can bechosen to be orthogonal, and by normalizing can be set to unit length. Because there\n",
      "areMeigenvalues, the corresponding Morthogonal eigenvectors form a complete\n",
      "set and so any M-dimensional vector can be expressed as a linear combination of\n",
      "the eigenvectors.\n",
      "We can take the eigenvectors u\n",
      "ito be the columns of an M×Mmatrix U,\n",
      "which from orthonormality satisﬁes\n",
      "UTU=I. (C.37)\n",
      "Such a matrix is said to be orthogonal . Interestingly, the rows of this matrix are also\n",
      "orthogonal, so that UUT=I. To show this, note that (C.37) implies UTUU−1=\n",
      "U−1=UTand soUU−1=UUT=I. Using (C.12), it also follows that |U|=1.\n",
      "The eigenvector equation (C.29) can be expressed in terms of Uin the form\n",
      "AU=UΛ (C.38)\n",
      "whereΛis anM×Mdiagonal matrix whose diagonal elements are given by the\n",
      "eigenvalues λi.\n",
      "If we consider a column vector xthat is transformed by an orthogonal matrix U\n",
      "to give a new vector\n",
      "˜x=Ux (C.39)\n",
      "then the length of the vector is preserved because\n",
      "˜xT˜x=xTUTUx=xTx (C.40)\n",
      "and similarly the angle between any two such vectors is preserved because\n",
      "˜xT˜y=xTUTUy=xTy. (C.41)\n",
      "Thus, multiplication by Ucan be interpreted as a rigid rotation of the coordinate\n",
      "system.\n",
      "From (C.38), it follows that\n",
      "UTAU=Λ (C.42)\n",
      "and because Λis a diagonal matrix, we say that the matrix Aisdiagonalized by the\n",
      "matrixU. If we left multiply by Uand right multiply by UT, we obtain\n",
      "A=UΛUT(C.43)\n",
      "Taking the inverse of this equation, and using (C.3) together with U−1=UT,w e\n",
      "have\n",
      "A−1=UΛ−1UT. (C.44)C. PROPERTIES OF MATRICES 701\n",
      "These last two equations can also be written in the form\n",
      "A=M∑\n",
      "i=1λiuiuT\n",
      "i (C.45)\n",
      "A−1=M∑\n",
      "i=11\n",
      "λiuiuT\n",
      "i. (C.46)\n",
      "If we take the determinant of (C.43), and use (C.12), we obtain\n",
      "|A|=M∏\n",
      "i=1λi. (C.47)\n",
      "Similarly, taking the trace of (C.43), and using the cyclic property (C.8) of the trace\n",
      "operator together with UTU=I,w eh a v e\n",
      "Tr(A)=M∑\n",
      "i=1λi. (C.48)\n",
      "We leave it as an exercise for the reader to verify (C.22) by making use of the results\n",
      "(C.33), (C.45), (C.46), and (C.47).\n",
      "A matrix Ais said to be positive deﬁnite , denoted by A≻0,i fwTAw>0for\n",
      "all values of the vector w. Equivalently, a positive deﬁnite matrix has λi>0for all\n",
      "of its eigenvalues (as can be seen by setting wto each of the eigenvectors in turn,\n",
      "and by noting that an arbitrary vector can be expanded as a linear combination of the\n",
      "eigenvectors). Note that positive deﬁnite is not the same as all the elements beingpositive. For example, the matrix\n",
      "(\n",
      "1234)\n",
      "(C.49)\n",
      "has eigenvalues λ1≃5.37andλ2≃−0.37. A matrix is said to be positive semidef-\n",
      "inite ifwTAw⩾0holds for all values of w, which is denoted A⪰0, and is\n",
      "equivalent to λi⩾0.Appendix D. Calculus of Variations\n",
      "We can think of a function y(x)as being an operator that, for any input value x,\n",
      "returns an output value y. In the same way, we can deﬁne a functional F[y]to be\n",
      "an operator that takes a function y(x)and returns an output value F. An example of\n",
      "a functional is the length of a curve drawn in a two-dimensional plane in which the\n",
      "path of the curve is deﬁned in terms of a function. In the context of machine learning,\n",
      "a widely used functional is the entropy H[x]for a continuous variable xbecause, for\n",
      "any choice of probability density function p(x), it returns a scalar value representing\n",
      "the entropy of xunder that density. Thus the entropy of p(x)could equally well have\n",
      "been written as H[p].\n",
      "A common problem in conventional calculus is to ﬁnd a value of xthat max-\n",
      "imizes (or minimizes) a function y(x). Similarly, in the calculus of variations we\n",
      "seek a function y(x)that maximizes (or minimizes) a functional F[y]. That is, of all\n",
      "possible functions y(x), we wish to ﬁnd the particular function for which the func-\n",
      "tional F[y]is a maximum (or minimum). The calculus of variations can be used, for\n",
      "instance, to show that the shortest path between two points is a straight line or thatthe maximum entropy distribution is a Gaussian.\n",
      "If we weren’t familiar with the rules of ordinary calculus, we could evaluate a\n",
      "conventional derivative dy/dxby making a small change ϵto the variable xand\n",
      "then expanding in powers of ϵ, so that\n",
      "y\n",
      "(x+ϵ)=y(x)+dy\n",
      "dxϵ+O(ϵ2) (D.1)\n",
      "and ﬁnally taking the limit ϵ→0. Similarly, for a function of several variables\n",
      "y(x1,...,x D), the corresponding partial derivatives are deﬁned by\n",
      "y(x1+ϵ1,...,x D+ϵD)=y(x1,...,x D)+D∑\n",
      "i=1∂y\n",
      "∂xiϵi+O(ϵ2). (D.2)\n",
      "The analogous deﬁnition of a functional derivative arises when we consider how\n",
      "much a functional F[y]changes when we make a small change ϵη(x)to the function\n",
      "703704 D. CALCULUS OF V ARIATIONS\n",
      "Figure D.1 A functional derivative can be deﬁned by\n",
      "considering how the value of a functional\n",
      "F[y]changes when the function y(x)is\n",
      "changed to y(x)+ϵη(x)where η(x)is an\n",
      "arbitrary function of x.\n",
      "y(x)\n",
      "y(x)+ϵη(x)\n",
      "x\n",
      "y(x), where η(x)is an arbitrary function of x, as illustrated in Figure D.1. We denote\n",
      "the functional derivative of E[f]with respect to f(x)byδF/δf (x), and deﬁne it by\n",
      "the following relation:\n",
      "F[y(x)+ϵη(x)] =F[y(x)] +ϵ∫δF\n",
      "δy(x)η(x)dx+O(ϵ2). (D.3)\n",
      "This can be seen as a natural extension of (D.2) in which F[y]now depends on a\n",
      "continuous set of variables, namely the values of yat all points x. Requiring that the\n",
      "functional be stationary with respect to small variations in the function y(x)gives\n",
      "∫δE\n",
      "δy(x)η(x)dx=0. (D.4)\n",
      "Because this must hold for an arbitrary choice of η(x), it follows that the functional\n",
      "derivative must vanish. To see this, imagine choosing a perturbation η(x)that is zero\n",
      "everywhere except in the neighbourhood of a point ˆx, in which case the functional\n",
      "derivative must be zero at x=ˆx. However, because this must be true for every\n",
      "choice ofˆx, the functional derivative must vanish for all values of x.\n",
      "Consider a functional that is deﬁned by an integral over a function G(y,y′,x)\n",
      "that depends on both y(x)and its derivative y′(x)as well as having a direct depen-\n",
      "dence on x\n",
      "F[y]=∫\n",
      "G(y(x),y′(x),x)dx (D.5)\n",
      "where the value of y(x)is assumed to be ﬁxed at the boundary of the region of\n",
      "integration (which might be at inﬁnity). If we now consider variations in the function\n",
      "y(x), we obtain\n",
      "F[y(x)+ϵη(x)] =F[y(x)] +ϵ∫{∂G\n",
      "∂yη(x)+∂G\n",
      "∂y′η′(x)}\n",
      "dx+O(ϵ2).(D.6)\n",
      "We now have to cast this in the form (D.3). To do so, we integrate the second term by\n",
      "parts and make use of the fact that η(x)must vanish at the boundary of the integral\n",
      "(because y(x)is ﬁxed at the boundary). This gives\n",
      "F[y(x)+ϵη(x)] =F[y(x)] +ϵ∫{∂G\n",
      "∂y−d\n",
      "dx(∂G\n",
      "∂y′)}\n",
      "η(x)dx+O(ϵ2)(D.7)D. CALCULUS OF V ARIATIONS 705\n",
      "from which we can read off the functional derivative by comparison with (D.3).\n",
      "Requiring that the functional derivative vanishes then gives\n",
      "∂G\n",
      "∂y−d\n",
      "dx(∂G\n",
      "∂y′)\n",
      "=0 (D.8)\n",
      "which are known as the Euler-Lagrange equations. For example, if\n",
      "G=y(x)2+(y′(x))2(D.9)\n",
      "then the Euler-Lagrange equations take the form\n",
      "y(x)−d2y\n",
      "dx2=0. (D.10)\n",
      "This second order differential equation can be solved for y(x)by making use of the\n",
      "boundary conditions on y(x).\n",
      "Often, we consider functionals deﬁned by integrals whose integrands take the\n",
      "formG(y,x)and that do not depend on the derivatives of y(x). In this case, station-\n",
      "arity simply requires that ∂G/∂y (x)=0 for all values of x.\n",
      "If we are optimizing a functional with respect to a probability distribution, then\n",
      "we need to maintain the normalization constraint on the probabilities. This is often\n",
      "most conveniently done using a Lagrange multiplier, which then allows an uncon- Appendix E\n",
      "strained optimization to be performed.\n",
      "The extension of the above results to a multidimensional variable xis straight-\n",
      "forward. For a more comprehensive discussion of the calculus of variations, seeSagan (1969).Appendix E. Lagrange Multipliers\n",
      "Lagrange multipliers , also sometimes called undetermined multipliers , are used to\n",
      "ﬁnd the stationary points of a function of several variables subject to one or more\n",
      "constraints.\n",
      "Consider the problem of ﬁnding the maximum of a function f(x1,x2)subject to\n",
      "a constraint relating x1andx2, which we write in the form\n",
      "g(x1,x2)=0. (E.1)\n",
      "One approach would be to solve the constraint equation (E.1) and thus express x2as\n",
      "a function of x1in the form x2=h(x1). This can then be substituted into f(x1,x2)\n",
      "to give a function of x1alone of the form f(x1,h(x1)). The maximum with respect\n",
      "tox1could then be found by differentiation in the usual way, to give the stationary\n",
      "valuex⋆\n",
      "1, with the corresponding value of x2given by x⋆\n",
      "2=h(x⋆\n",
      "1).\n",
      "One problem with this approach is that it may be difﬁcult to ﬁnd an analytic\n",
      "solution of the constraint equation that allows x2to be expressed as an explicit func-\n",
      "tion of x1. Also, this approach treats x1andx2differently and so spoils the natural\n",
      "symmetry between these variables.\n",
      "A more elegant, and often simpler, approach is based on the introduction of a\n",
      "parameter λcalled a Lagrange multiplier. We shall motivate this technique from\n",
      "a geometrical perspective. Consider a D-dimensional variable xwith components\n",
      "x1,...,x D. The constraint equation g(x)=0 then represents a (D−1)-dimensional\n",
      "surface in x-space as indicated in Figure E.1.\n",
      "We ﬁrst note that at any point on the constraint surface the gradient ∇g(x)of\n",
      "the constraint function will be orthogonal to the surface. To see this, consider a point\n",
      "xthat lies on the constraint surface, and consider a nearby point x+ϵthat also lies\n",
      "on the surface. If we make a Taylor expansion around x,w eh a v e\n",
      "g(x+ϵ)≃g(x)+ϵT∇g(x). (E.2)\n",
      "Because both xandx+ϵlie on the constraint surface, we have g(x)=g(x+ϵ)and\n",
      "hence ϵT∇g(x)≃0. In the limit ∥ϵ∥→0we have ϵT∇g(x)=0 , and because ϵis\n",
      "707708 E. LAGRANGE MULTIPLIERS\n",
      "Figure E.1 A geometrical picture of the technique of La-\n",
      "grange multipliers in which we seek to maximize a\n",
      "function f(x), subject to the constraint g(x)=0 .\n",
      "IfxisDdimensional, the constraint g(x)=0 cor-\n",
      "responds to a subspace of dimensionality D−1,\n",
      "indicated by the red curve. The problem can\n",
      "be solved by optimizing the Lagrangian function\n",
      "L(x,λ)=f(x)+λg(x).∇f(x)\n",
      "∇g(x)xA\n",
      "g(x)=0\n",
      "then parallel to the constraint surface g(x)=0 , we see that the vector ∇gis normal\n",
      "to the surface.\n",
      "Next we seek a point x⋆on the constraint surface such that f(x)is maximized.\n",
      "Such a point must have the property that the vector ∇f(x)is also orthogonal to the\n",
      "constraint surface, as illustrated in Figure E.1, because otherwise we could increase\n",
      "the value of f(x)by moving a short distance along the constraint surface. Thus ∇f\n",
      "and∇gare parallel (or anti-parallel) vectors, and so there must exist a parameter λ\n",
      "such that\n",
      "∇f+λ∇g=0 (E.3)\n",
      "where λ̸=0is known as a Lagrange multiplier . Note that λcan have either sign.\n",
      "At this point, it is convenient to introduce the Lagrangian function deﬁned by\n",
      "L(x,λ)≡f(x)+λg(x). (E.4)\n",
      "The constrained stationarity condition (E.3) is obtained by setting ∇xL=0. Fur-\n",
      "thermore, the condition ∂L/∂λ =0leads to the constraint equation g(x)=0 .\n",
      "Thus to ﬁnd the maximum of a function f(x)subject to the constraint g(x)=0 ,\n",
      "we deﬁne the Lagrangian function given by (E.4) and we then ﬁnd the stationary\n",
      "point of L(x,λ)with respect to both xandλ. For a D-dimensional vector x, this\n",
      "givesD+1equations that determine both the stationary point x⋆and the value of λ.\n",
      "If we are only interested in x⋆, then we can eliminate λfrom the stationarity equa-\n",
      "tions without needing to ﬁnd its value (hence the term ‘undetermined multiplier’).\n",
      "As a simple example, suppose we wish to ﬁnd the stationary point of the function\n",
      "f(x1,x2)=1−x2\n",
      "1−x2\n",
      "2subject to the constraint g(x1,x2)=x1+x2−1=0 ,a s\n",
      "illustrated in Figure E.2. The corresponding Lagrangian function is given by\n",
      "L(x,λ)=1−x2\n",
      "1−x2\n",
      "2+λ(x1+x2−1). (E.5)\n",
      "The conditions for this Lagrangian to be stationary with respect to x1,x2, andλgive\n",
      "the following coupled equations:\n",
      "−2x1+λ=0 (E.6)\n",
      "−2x2+λ=0 (E.7)\n",
      "x1+x2−1=0 . (E.8)E. LAGRANGE MULTIPLIERS 709\n",
      "Figure E.2 A simple example of the use of Lagrange multipli-\n",
      "ers in which the aim is to maximize f(x1,x2)=\n",
      "1−x2\n",
      "1−x2\n",
      "2subject to the constraint g(x1,x2)=0\n",
      "where g(x1,x2)=x1+x2−1. The circles show\n",
      "contours of the function f(x1,x2), and the diagonal\n",
      "line shows the constraint surface g(x1,x2)=0 .\n",
      "g(x1,x2)=0x1x2\n",
      "(x⋆\n",
      "1,x⋆\n",
      "2)\n",
      "Solution of these equations then gives the stationary point as (x⋆\n",
      "1,x⋆\n",
      "2)=(1\n",
      "2,1\n",
      "2), and\n",
      "the corresponding value for the Lagrange multiplier is λ=1.\n",
      "So far, we have considered the problem of maximizing a function subject to an\n",
      "equality constraint of the form g(x)=0 . We now consider the problem of maxi-\n",
      "mizing f(x)subject to an inequality constraint of the form g(x)⩾0, as illustrated\n",
      "in Figure E.3.\n",
      "There are now two kinds of solution possible, according to whether the con-\n",
      "strained stationary point lies in the region where g(x)>0, in which case the con-\n",
      "straint is inactive , or whether it lies on the boundary g(x)=0 , in which case the\n",
      "constraint is said to be active . In the former case, the function g(x)plays no role\n",
      "and so the stationary condition is simply ∇f(x)=0 . This again corresponds to\n",
      "a stationary point of the Lagrange function (E.4) but this time with λ=0. The\n",
      "latter case, where the solution lies on the boundary, is analogous to the equality con-\n",
      "straint discussed previously and corresponds to a stationary point of the Lagrange\n",
      "function (E.4) with λ̸=0. Now, however, the sign of the Lagrange multiplier is\n",
      "crucial, because the function f(x)will only be at a maximum if its gradient is ori-\n",
      "ented away from the region g(x)>0, as illustrated in Figure E.3. We therefore have\n",
      "∇f(x)=−λ∇g(x)for some value of λ>0.\n",
      "For either of these two cases, the product λg(x)=0 . Thus the solution to the\n",
      "Figure E.3 Illustration of the problem of maximizing\n",
      "f(x)subject to the inequality constraint\n",
      "g(x)⩾0.∇f(x)\n",
      "∇g(x)xA\n",
      "xB\n",
      "g(x)=0g(x)>0710 E. LAGRANGE MULTIPLIERS\n",
      "problem of maximizing f(x)subject to g(x)⩾0is obtained by optimizing the\n",
      "Lagrange function (E.4) with respect to xandλsubject to the conditions\n",
      "g(x)⩾0 (E.9)\n",
      "λ⩾0 (E.10)\n",
      "λg(x)=0 (E.11)\n",
      "These are known as the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939; Kuhn\n",
      "and Tucker, 1951).\n",
      "Note that if we wish to minimize (rather than maximize) the function f(x)sub-\n",
      "ject to an inequality constraint g(x)⩾0, then we minimize the Lagrangian function\n",
      "L(x,λ)=f(x)−λg(x)with respect to x, again subject to λ⩾0.\n",
      "Finally, it is straightforward to extend the technique of Lagrange multipliers to\n",
      "the case of multiple equality and inequality constraints. Suppose we wish to maxi-\n",
      "mizef(x)subject to gj(x)=0 forj=1,...,J , andhk(x)⩾0fork=1,...,K .\n",
      "We then introduce Lagrange multipliers {λj}and{µk}, and then optimize the La-\n",
      "grangian function given by\n",
      "L(x,{λj},{µk})=f(x)+J∑\n",
      "j=1λjgj(x)+K∑\n",
      "k=1µkhk(x) (E.12)\n",
      "subject to µk⩾0andµkhk(x)=0 fork=1,...,K . Extensions to constrained\n",
      "functional derivatives are similarly straightforward. For a more detailed discussion Appendix D\n",
      "of the technique of Lagrange multipliers, see Nocedal and Wright (1999).REFERENCES 711\n",
      "References\n",
      "Abramowitz, M. and I. A. Stegun (1965). Handbook\n",
      "of Mathematical Functions . Dover.\n",
      "Adler, S. L. (1981). Over-relaxation method for the\n",
      "Monte Carlo evaluation of the partition func-tion for multiquadratic actions. Physical Review\n",
      "D23, 2901–2904.\n",
      "Ahn, J. H. and J. H. Oh (2003). A constrained EM\n",
      "algorithm for principal component analysis. Neu-\n",
      "ral Computation 15(1), 57–65.\n",
      "Aizerman, M. A., E. M. Braverman, and L. I. Rozo-\n",
      "noer (1964). The probability problem of pattern\n",
      "recognition learning and the method of potential\n",
      "functions. Automation and Remote Control 25,\n",
      "1175–1190.\n",
      "Akaike, H. (1974). A new look at statistical model\n",
      "identiﬁcation. IEEE Transactions on Automatic\n",
      "Control 19, 716–723.\n",
      "Ali, S. M. and S. D. Silvey (1966). A general class\n",
      "of coefﬁcients of divergence of one distributionfrom another. Journal of the Royal Statistical So-\n",
      "ciety, B 28(1), 131–142.\n",
      "Allwein, E. L., R. E. Schapire, and Y . Singer (2000).\n",
      "Reducing multiclass to binary: a unifying ap-\n",
      "proach for margin classiﬁers. Journal of Machine\n",
      "Learning Research 1, 113–141.\n",
      "Amari, S. (1985). Differential-Geometrical Methods\n",
      "in Statistics . Springer.Amari, S., A. Cichocki, and H. H. Yang (1996). A\n",
      "new learning algorithm for blind signal separa-\n",
      "tion. In D. S. Touretzky, M. C. Mozer, and M. E.\n",
      "Hasselmo (Eds.), Advances in Neural Informa-\n",
      "tion Processing Systems , V olume 8, pp. 757–763.\n",
      "MIT Press.\n",
      "Amari, S. I. (1998). Natural gradient works efﬁ-\n",
      "ciently in learning. Neural Computation 10,\n",
      "251–276.\n",
      "Anderson, J. A. and E. Rosenfeld (Eds.) (1988).\n",
      "Neurocomputing: F oundations of Research . MIT\n",
      "Press.\n",
      "Anderson, T. W. (1963). Asymptotic theory for prin-\n",
      "cipal component analysis. Annals of Mathemati-\n",
      "cal Statistics 34, 122–148.\n",
      "Andrieu, C., N. de Freitas, A. Doucet, and M. I. Jor-\n",
      "dan (2003). An introduction to MCMC for ma-chine learning. Machine Learning 50, 5–43.\n",
      "Anthony, M. and N. Biggs (1992). An Introduction\n",
      "to Computational Learning Theory . Cambridge\n",
      "University Press.\n",
      "Attias, H. (1999a). Independent factor analysis. Neu-\n",
      "ral Computation 11(4), 803–851.\n",
      "Attias, H. (1999b). Inferring parameters and struc-\n",
      "ture of latent variable models by variational\n",
      "Bayes. In K. B. Laskey and H. Prade (Eds.),712 REFERENCES\n",
      "Uncertainty in Artiﬁcial Intelligence: Proceed-\n",
      "ings of the Fifth Conference , pp. 21–30. Morgan\n",
      "Kaufmann.\n",
      "Bach, F. R. and M. I. Jordan (2002). Kernel inde-\n",
      "pendent component analysis. Journal of Machine\n",
      "Learning Research 3, 1–48.\n",
      "Bakir, G. H., J. Weston, and B. Sch ¨olkopf (2004).\n",
      "Learning to ﬁnd pre-images. In S. Thrun, L. K.\n",
      "Saul, and B. Sch ¨olkopf (Eds.), Advances in Neu-\n",
      "ral Information Processing Systems , V olume 16,\n",
      "pp. 449–456. MIT Press.\n",
      "Baldi, P. and S. Brunak (2001). Bioinformatics: The\n",
      "Machine Learning Approach (Second ed.). MIT\n",
      "Press.\n",
      "Baldi, P. and K. Hornik (1989). Neural networks\n",
      "and principal component analysis: learning fromexamples without local minima. Neural Net-\n",
      "works 2(1), 53–58.\n",
      "Barber, D. and C. M. Bishop (1997). Bayesian\n",
      "model comparison by Monte Carlo chaining. In\n",
      "M. Mozer, M. Jordan, and T. Petsche (Eds.), Ad-\n",
      "vances in Neural Information Processing Sys-\n",
      "tems, V olume 9, pp. 333–339. MIT Press.\n",
      "Barber, D. and C. M. Bishop (1998a). Ensemble\n",
      "learning for multi-layer networks. In M. I. Jor-\n",
      "dan, K. J. Kearns, and S. A. Solla (Eds.), Ad-\n",
      "vances in Neural Information Processing Sys-tems, V olume 10, pp. 395–401.\n",
      "Barber, D. and C. M. Bishop (1998b). Ensemble\n",
      "learning in Bayesian neural networks. In C. M.Bishop (Ed.), Generalization in Neural Networks\n",
      "and Machine Learning , pp. 215–237. Springer.\n",
      "Bartholomew, D. J. (1987). Latent V ariable Models\n",
      "and Factor Analysis . Charles Grifﬁn.\n",
      "Basilevsky, A. (1994). Statistical Factor Analysis\n",
      "and Related Methods: Theory and Applications .\n",
      "Wiley.\n",
      "Bather, J. (2000). Decision Theory: An Introduction\n",
      "to Dynamic Programming and Sequential Deci-\n",
      "sions . Wiley.\n",
      "Baudat, G. and F. Anouar (2000). Generalized dis-\n",
      "criminant analysis using a kernel approach. Neu-\n",
      "ral Computation 12(10), 2385–2404.Baum, L. E. (1972). An inequality and associated\n",
      "maximization technique in statistical estimationof probabilistic functions of Markov processes.\n",
      "Inequalities 3, 1–8.\n",
      "Becker, S. and Y . Le Cun (1989). Improving the con-\n",
      "vergence of back-propagation learning with sec-\n",
      "ond order methods. In D. Touretzky, G. E. Hin-\n",
      "ton, and T. J. Sejnowski (Eds.), Proceedings of\n",
      "the 1988 Connectionist Models Summer School ,\n",
      "pp. 29–37. Morgan Kaufmann.\n",
      "Bell, A. J. and T. J. Sejnowski (1995). An infor-\n",
      "mation maximization approach to blind separa-\n",
      "tion and blind deconvolution. Neural Computa-\n",
      "tion 7(6), 1129–1159.\n",
      "Bellman, R. (1961). Adaptive Control Processes: A\n",
      "Guided Tour . Princeton University Press.\n",
      "Bengio, Y . and P. Frasconi (1995). An input output\n",
      "HMM architecture. In G. Tesauro, D. S. Touret-\n",
      "zky, and T. K. Leen (Eds.), Advances in Neural\n",
      "Information Processing Systems\n",
      ", V olume 7, pp.\n",
      "427–434. MIT Press.\n",
      "Bennett, K. P. (1992). Robust linear programming\n",
      "discrimination of two linearly separable sets. Op-\n",
      "timization Methods and Software 1, 23–34.\n",
      "Berger, J. O. (1985). Statistical Decision Theory and\n",
      "Bayesian Analysis (Second ed.). Springer.\n",
      "Bernardo, J. M. and A. F. M. Smith (1994). Bayesian\n",
      "Theory . Wiley.\n",
      "Berrou, C., A. Glavieux, and P. Thitimajshima\n",
      "(1993). Near Shannon limit error-correcting cod-ing and decoding: Turbo-codes (1). In Proceed-\n",
      "ings ICC’93 , pp. 1064–1070.\n",
      "Besag, J. (1974). On spatio-temporal models and\n",
      "Markov ﬁelds. In Transactions of the 7th Prague\n",
      "Conference on Information Theory, Statistical\n",
      "Decision Functions and Random Processes , pp.\n",
      "47–75. Academia.\n",
      "Besag, J. (1986). On the statistical analysis of dirty\n",
      "pictures. Journal of the Royal Statistical Soci-\n",
      "ety B-48 , 259–302.\n",
      "Besag, J., P. J. Green, D. Hidgon, and K. Megersen\n",
      "(1995). Bayesian computation and stochastic\n",
      "systems. Statistical Science 10(1), 3–66.REFERENCES 713\n",
      "Bishop, C. M. (1991). A fast procedure for retraining\n",
      "the multilayer perceptron. International Journal\n",
      "of Neural Systems 2(3), 229–236.\n",
      "Bishop, C. M. (1992). Exact calculation of the Hes-\n",
      "sian matrix for the multilayer perceptron. Neural\n",
      "Computation 4(4), 494–501.\n",
      "Bishop, C. M. (1993). Curvature-driven smoothing:\n",
      "a learning algorithm for feedforward networks.\n",
      "IEEE Transactions on Neural Networks 4(5),\n",
      "882–884.\n",
      "Bishop, C. M. (1994). Novelty detection and neu-\n",
      "ral network validation. IEE Proceedings: Vision,\n",
      "Image and Signal Processing 141(4), 217–222.\n",
      "Special issue on applications of neural networks.\n",
      "Bishop, C. M. (1995a). Neural Networks for Pattern\n",
      "Recognition . Oxford University Press.\n",
      "Bishop, C. M. (1995b). Training with noise is equiv-\n",
      "alent to Tikhonov regularization. Neural Compu-\n",
      "tation 7(1), 108–116.\n",
      "Bishop, C. M. (1999a). Bayesian PCA. In M. S.\n",
      "Kearns, S. A. Solla, and D. A. Cohn (Eds.), Ad-\n",
      "vances in Neural Information Processing Sys-\n",
      "tems, V olume 11, pp. 382–388. MIT Press.\n",
      "Bishop, C. M. (1999b). Variational principal\n",
      "components. In Proceedings Ninth Interna-\n",
      "tional Conference on Artiﬁcial Neural Networks,ICANN’99 , V olume 1, pp. 509–514. IEE.\n",
      "Bishop, C. M. and G. D. James (1993). Analysis of\n",
      "multiphase ﬂows using dual-energy gamma den-\n",
      "sitometry and neural networks. Nuclear Instru-\n",
      "ments and Methods in Physics Research A327 ,\n",
      "580–593.\n",
      "Bishop, C. M. and I. T. Nabney (1996). Modelling\n",
      "conditional probability distributions for periodic\n",
      "variables. Neural Computation 8(5), 1123–1133.\n",
      "Bishop, C. M. and I. T. Nabney (2008). Pattern\n",
      "Recognition and Machine Learning: A MatlabCompanion . Springer. In preparation.\n",
      "Bishop, C. M., D. Spiegelhalter, and J. Winn\n",
      "(2003). VIBES: A variational inference enginefor Bayesian networks. In S. Becker, S. Thrun,\n",
      "and K. Obermeyer (Eds.), Advances in NeuralInformation Processing Systems , V olume 15, pp.\n",
      "793–800. MIT Press.\n",
      "Bishop, C. M. and M. Svens ´en (2003). Bayesian hi-\n",
      "erarchical mixtures of experts. In U. Kjaerulff\n",
      "and C. Meek (Eds.), Proceedings Nineteenth\n",
      "Conference on Uncertainty in Artiﬁcial Intelli-\n",
      "gence , pp. 57–64. Morgan Kaufmann.\n",
      "Bishop, C. M., M. Svens ´en, and G. E. Hinton\n",
      "(2004). Distinguishing text from graphics in on-\n",
      "line handwritten ink. In F. Kimura and H. Fu-\n",
      "jisawa (Eds.), Proceedings Ninth International\n",
      "Workshop on Frontiers in Handwriting Recogni-\n",
      "tion, IWFHR-9 , Tokyo, Japan, pp. 142–147.\n",
      "Bishop, C. M., M. Svens ´en, and C. K. I. Williams\n",
      "(1996). EM optimization of latent variable den-\n",
      "sity models. In D. S. Touretzky, M. C. Mozer,\n",
      "and M. E. Hasselmo (Eds.), Advances in Neural\n",
      "Information Processing Systems , V olume 8, pp.\n",
      "465–471. MIT Press.\n",
      "Bishop, C. M., M. Svens ´en, and C. K. I. Williams\n",
      "(1997a). GTM: a principled alternative to the\n",
      "Self-Organizing Map. In M. C. Mozer, M. I. Jor-\n",
      "dan, and T. Petche (Eds.), Advances in Neural\n",
      "Information Processing Systems , V olume 9, pp.\n",
      "354–360. MIT Press.\n",
      "Bishop, C. M., M. Svens ´en, and C. K. I. Williams\n",
      "(1997b). Magniﬁcation factors for the GTM al-\n",
      "gorithm. In Proceedings IEE Fifth International\n",
      "Conference on Artiﬁcial Neural Networks, Cam-\n",
      "bridge, U.K. , pp. 64–69. Institute of Electrical\n",
      "Engineers.\n",
      "Bishop, C. M., M. Svens ´en, and C. K. I. Williams\n",
      "(1998a). Developments of the Generative To-\n",
      "pographic Mapping. Neurocomputing 21, 203–\n",
      "224.\n",
      "Bishop, C. M., M. Svens ´en, and C. K. I. Williams\n",
      "(1998b). GTM: the Generative TopographicMapping. Neural Computation 10(1), 215–234.\n",
      "Bishop, C. M. and M. E. Tipping (1998). A hier-\n",
      "archical latent variable model for data visualiza-tion. IEEE Transactions on Pattern Analysis and\n",
      "Machine Intelligence 20(3), 281–293.714 REFERENCES\n",
      "Bishop, C. M. and J. Winn (2000). Non-linear\n",
      "Bayesian image modelling. In Proceedings Sixth\n",
      "European Conference on Computer Vision,\n",
      "Dublin , V olume 1, pp. 3–17. Springer.\n",
      "Blei, D. M., M. I. Jordan, and A. Y . Ng (2003). Hi-\n",
      "erarchical Bayesian models for applications in\n",
      "information retrieval. In J. M. B. et al. (Ed.),\n",
      "Bayesian Statistics, 7 , pp. 25–43. Oxford Uni-\n",
      "versity Press.\n",
      "Block, H. D. (1962). The perceptron: a model\n",
      "for brain functioning. Reviews of Modern\n",
      "Physics 34(1), 123–135. Reprinted in Anderson\n",
      "and Rosenfeld (1988).\n",
      "Blum, J. A. (1965). Multidimensional stochastic ap-\n",
      "proximation methods. Annals of Mathematical\n",
      "Statistics 25, 737–744.\n",
      "Bodlaender, H. (1993). A tourist guide through\n",
      "treewidth. Acta Cybernetica 11, 1–21.\n",
      "Boser, B. E., I. M. Guyon, and V . N. Vapnik (1992).\n",
      "A training algorithm for optimal margin classi-\n",
      "ﬁers. In D. Haussler (Ed.), Proceedings Fifth An-\n",
      "nual Workshop on Computational Learning The-ory (COLT) , pp. 144–152. ACM.\n",
      "Bourlard, H. and Y . Kamp (1988). Auto-association\n",
      "by multilayer perceptrons and singular value de-\n",
      "composition. Biological Cybernetics 59, 291–\n",
      "294.\n",
      "Box, G. E. P., G. M. Jenkins, and G. C. Reinsel\n",
      "(1994). Time Series Analysis . Prentice Hall.\n",
      "Box, G. E. P. and G. C. Tao (1973). Bayesian Infer-\n",
      "ence in Statistical Analysis . Wiley.\n",
      "Boyd, S. and L. Vandenberghe (2004). Convex Opti-\n",
      "mization . Cambridge University Press.\n",
      "Boyen, X. and D. Koller (1998). Tractable inference\n",
      "for complex stochastic processes. In G. F. Cooper\n",
      "and S. Moral (Eds.), Proceedings 14th Annual\n",
      "Conference on Uncertainty in Artiﬁcial Intelli-gence (UAI) , pp. 33–42. Morgan Kaufmann.\n",
      "Boykov, Y ., O. Veksler, and R. Zabih (2001). Fast\n",
      "approximate energy minimization via graph cuts.IEEE Transactions on Pattern Analysis and Ma-\n",
      "chine Intelligence 23(11), 1222–1239.Breiman, L. (1996). Bagging predictors. Machine\n",
      "Learning 26, 123–140.\n",
      "Breiman, L., J. H. Friedman, R. A. Olshen, and\n",
      "P. J. Stone (1984). Classiﬁcation and Regression\n",
      "Trees . Wadsworth.\n",
      "Brooks, S. P. (1998). Markov chain Monte\n",
      "Carlo method and its application. The Statisti-\n",
      "cian 47(1), 69–100.\n",
      "Broomhead, D. S. and D. Lowe (1988). Multivari-\n",
      "able functional interpolation and adaptive net-\n",
      "works. Complex Systems 2, 321–355.\n",
      "Buntine, W. and A. Weigend (1991). Bayesian back-\n",
      "propagation. Complex Systems 5, 603–643.\n",
      "Buntine, W. L. and A. S. Weigend (1993). Com-\n",
      "puting second derivatives in feed-forward net-\n",
      "works: a review. IEEE Transactions on Neural\n",
      "Networks 5(3), 480–488.\n",
      "Burges, C. J. C. (1998). A tutorial on support vec-\n",
      "tor machines for pattern recognition. Knowledge\n",
      "Discovery and Data Mining 2(2), 121–167.\n",
      "Cardoso, J.-F. (1998). Blind signal separation: statis-\n",
      "tical principles. Proceedings of the IEEE 9(10),\n",
      "2009–2025.\n",
      "Casella, G. and R. L. Berger (2002). Statistical In-\n",
      "ference (Second ed.). Duxbury.\n",
      "Castillo, E., J. M. Guti ´errez, and A. S. Hadi (1997).\n",
      "Expert Systems and Probabilistic Network Mod-\n",
      "els. Springer.\n",
      "Chan, K., T. Lee, and T. J. Sejnowski (2003). Vari-\n",
      "ational Bayesian learning of ICA with missing\n",
      "data. Neural Computation 15(8), 1991–2011.\n",
      "Chen, A. M., H. Lu, and R. Hecht-Nielsen (1993).\n",
      "On the geometry of feedforward neural network\n",
      "error surfaces. Neural Computation 5(6), 910–\n",
      "927.\n",
      "Chen, M. H., Q. M. Shao, and J. G. Ibrahim (Eds.)\n",
      "(2001). Monte Carlo Methods for Bayesian Com-\n",
      "putation . Springer.\n",
      "Chen, S., C. F. N. Cowan, and P. M. Grant (1991).\n",
      "Orthogonal least squares learning algorithm forradial basis function networks. IEEE Transac-\n",
      "tions on Neural Networks 2(2), 302–309.REFERENCES 715\n",
      "Choudrey, R. A. and S. J. Roberts (2003). Variational\n",
      "mixture of Bayesian independent component an-alyzers. Neural Computation 15(1), 213–252.\n",
      "Clifford, P. (1990). Markov random ﬁelds in statis-\n",
      "tics. In G. R. Grimmett and D. J. A. Welsh (Eds.),\n",
      "Disorder in Physical Systems. A V olume in Hon-\n",
      "our of John M. Hammersley , pp. 19–32. Oxford\n",
      "University Press.\n",
      "Collins, M., S. Dasgupta, and R. E. Schapire (2002).\n",
      "A generalization of principal component analy-\n",
      "sis to the exponential family. In T. G. Dietterich,\n",
      "S. Becker, and Z. Ghahramani (Eds.), Advances\n",
      "in Neural Information Processing Systems , V ol-\n",
      "ume 14, pp. 617–624. MIT Press.\n",
      "Comon, P., C. Jutten, and J. Herault (1991). Blind\n",
      "source separation, 2: problems statement. Signal\n",
      "Processing 24(1), 11–20.\n",
      "Corduneanu, A. and C. M. Bishop (2001). Vari-\n",
      "ational Bayesian model selection for mixturedistributions. In T. Richardson and T. Jaakkola\n",
      "(Eds.), Proceedings Eighth International Confer-\n",
      "ence on Artiﬁcial Intelligence and Statistics , pp.\n",
      "27–34. Morgan Kaufmann.\n",
      "Cormen, T. H., C. E. Leiserson, R. L. Rivest, and\n",
      "C. Stein (2001). Introduction to Algorithms (Sec-\n",
      "ond ed.). MIT Press.\n",
      "Cortes, C. and V . N. Vapnik (1995). Support vector\n",
      "networks. Machine Learning 20, 273–297.\n",
      "Cotter, N. E. (1990). The Stone-Weierstrass theo-\n",
      "rem and its application to neural networks. IEEE\n",
      "Transactions on Neural Networks 1(4), 290–295.\n",
      "Cover, T. and P. Hart (1967). Nearest neighbor pat-\n",
      "tern classiﬁcation. IEEE Transactions on Infor-\n",
      "mation Theory IT-11 , 21–27.\n",
      "Cover, T. M. and J. A. Thomas (1991). Elements of\n",
      "Information Theory . Wiley.\n",
      "Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J.\n",
      "Spiegelhalter (1999). Probabilistic Networks and\n",
      "Expert Systems . Springer.\n",
      "Cox, R. T. (1946). Probability, frequency and\n",
      "reasonable expectation. American Journal of\n",
      "Physics 14(1), 1–13.Cox, T. F. and M. A. A. Cox (2000). Multidimen-\n",
      "sional Scaling (Second ed.). Chapman and Hall.\n",
      "Cressie, N. (1993). Statistics for Spatial Data . Wiley.\n",
      "Cristianini, N. and J. Shawe-Taylor (2000). Support\n",
      "vector machines and other kernel-based learning\n",
      "methods . Cambridge University Press.\n",
      "Csat´o, L. and M. Opper (2002). Sparse on-line Gaus-\n",
      "sian processes. Neural Computation 14(3), 641–\n",
      "668.\n",
      "Csisz `ar, I. and G. Tusn `ady (1984). Information ge-\n",
      "ometry and alternating minimization procedures.\n",
      "Statistics and Decisions 1(1), 205–237.\n",
      "Cybenko, G. (1989). Approximation by superposi-\n",
      "tions of a sigmoidal function. Mathematics of\n",
      "Control, Signals and Systems 2, 304–314.\n",
      "Dawid, A. P. (1979). Conditional independence in\n",
      "statistical theory (with discussion). Journal of the\n",
      "Royal Statistical Society, Series B 4, 1–31.\n",
      "Dawid, A. P. (1980). Conditional independence for\n",
      "statistical operations. Annals of Statistics 8, 598–\n",
      "617.\n",
      "deFinetti, B. (1970). Theory of Probability . Wiley\n",
      "and Sons.\n",
      "Dempster, A. P., N. M. Laird, and D. B. Rubin\n",
      "(1977). Maximum likelihood from incompletedata via the EM algorithm. Journal of the Royal\n",
      "Statistical Society, B 39(1), 1–38.\n",
      "Denison, D. G. T., C. C. Holmes, B. K. Mallick,\n",
      "and A. F. M. Smith (2002). Bayesian Methods for\n",
      "Nonlinear Classiﬁcation and Regression . Wiley.\n",
      "Diaconis, P. and L. Saloff-Coste (1998). What do we\n",
      "know about the Metropolis algorithm? Journal\n",
      "of Computer and System Sciences 57, 20–36.\n",
      "Dietterich, T. G. and G. Bakiri (1995). Solving\n",
      "multiclass learning problems via error-correctingoutput codes. Journal of Artiﬁcial Intelligence\n",
      "Research 2, 263–286.\n",
      "Duane, S., A. D. Kennedy, B. J. Pendleton, and\n",
      "D. Roweth (1987). Hybrid Monte Carlo. Physics\n",
      "Letters B 195(2), 216–222.\n",
      "Duda, R. O. and P. E. Hart (1973). Pattern Classiﬁ-\n",
      "cation and Scene Analysis . Wiley.716 REFERENCES\n",
      "Duda, R. O., P. E. Hart, and D. G. Stork (2001). Pat-\n",
      "tern Classiﬁcation (Second ed.). Wiley.\n",
      "Durbin, R., S. Eddy, A. Krogh, and G. Mitchi-\n",
      "son (1998). Biological Sequence Analysis . Cam-\n",
      "bridge University Press.\n",
      "Dybowski, R. and S. Roberts (2005). An anthology\n",
      "of probabilistic models for medical informatics.\n",
      "In D. Husmeier, R. Dybowski, and S. Roberts\n",
      "(Eds.), Probabilistic Modeling in Bioinformatics\n",
      "and Medical Informatics , pp. 297–349. Springer.\n",
      "Efron, B. (1979). Bootstrap methods: another look\n",
      "at the jackknife. Annals of Statistics 7, 1–26.\n",
      "Elkan, C. (2003). Using the triangle inequality to ac-\n",
      "celerate k-means. In Proceedings of the Twelfth\n",
      "International Conference on Machine Learning ,\n",
      "pp. 147–153. AAAI.\n",
      "Elliott, R. J., L. Aggoun, and J. B. Moore (1995).\n",
      "Hidden Markov Models: Estimation and Con-\n",
      "trol. Springer.\n",
      "Ephraim, Y ., D. Malah, and B. H. Juang (1989).\n",
      "On the application of hidden Markov models forenhancing noisy speech. IEEE Transactions on\n",
      "Acoustics, Speech and Signal Processing 37(12),\n",
      "1846–1856.\n",
      "Erwin, E., K. Obermayer, and K. Schulten (1992).\n",
      "Self-organizing maps: ordering, convergence\n",
      "properties and energy functions. Biological Cy-\n",
      "bernetics 67, 47–55.\n",
      "Everitt, B. S. (1984). An Introduction to Latent V ari-\n",
      "able Models . Chapman and Hall.\n",
      "Faul, A. C. and M. E. Tipping (2002). Analysis of\n",
      "sparse Bayesian learning. In T. G. Dietterich,\n",
      "S. Becker, and Z. Ghahramani (Eds.), Advances\n",
      "in Neural Information Processing Systems , V ol-\n",
      "ume 14, pp. 383–389. MIT Press.\n",
      "Feller, W. (1966). An Introduction to Probability\n",
      "Theory and its Applications (Second ed.), V ol-\n",
      "ume 2. Wiley.\n",
      "Feynman, R. P., R. B. Leighton, and M. Sands\n",
      "(1964). The Feynman Lectures of Physics , V ol-\n",
      "ume Two. Addison-Wesley. Chapter 19.Fletcher, R. (1987). Practical Methods of Optimiza-\n",
      "tion(Second ed.). Wiley.\n",
      "Forsyth, D. A. and J. Ponce (2003). Computer Vi-\n",
      "sion: A Modern Approach . Prentice Hall.\n",
      "Freund, Y . and R. E. Schapire (1996). Experiments\n",
      "with a new boosting algorithm. In L. Saitta (Ed.),\n",
      "Thirteenth International Conference on Machine\n",
      "Learning , pp. 148–156. Morgan Kaufmann.\n",
      "Frey, B. J. (1998). Graphical Models for Ma-\n",
      "chine Learning and Digital Communication .\n",
      "MIT Press.\n",
      "Frey, B. J. and D. J. C. MacKay (1998). A revolu-\n",
      "tion: Belief propagation in graphs with cycles. In\n",
      "M. I. Jordan, M. J. Kearns, and S. A. Solla (Eds.),Advances in Neural Information Processing Sys-\n",
      "tems, V olume 10. MIT Press.\n",
      "Friedman, J. H. (2001). Greedy function approxi-\n",
      "mation: a gradient boosting machine. Annals of\n",
      "Statistics 29(5), 1189–1232.\n",
      "Friedman, J. H., T. Hastie, and R. Tibshirani (2000).\n",
      "Additive logistic regression: a statistical view of\n",
      "boosting. Annals of Statistics 28, 337–407.\n",
      "Friedman, N. and D. Koller (2003). Being Bayesian\n",
      "about network structure: A Bayesian approach\n",
      "to structure discovery in Bayesian networks. Ma-\n",
      "chine Learning 50, 95–126.\n",
      "Frydenberg, M. (1990). The chain graph Markov\n",
      "property. Scandinavian Journal of Statistics 17,\n",
      "333–353.\n",
      "Fukunaga, K. (1990). Introduction to Statistical Pat-\n",
      "tern Recognition (Second ed.). Academic Press.\n",
      "Funahashi, K. (1989). On the approximate realiza-\n",
      "tion of continuous mappings by neural networks.\n",
      "Neural Networks 2(3), 183–192.\n",
      "Fung, R. and K. C. Chang (1990). Weighting and\n",
      "integrating evidence for stochastic simulation in\n",
      "Bayesian networks. In P. P. Bonissone, M. Hen-rion, L. N. Kanal, and J. F. Lemmer (Eds.), Un-\n",
      "certainty in Artiﬁcial Intelligence , V olume 5, pp.\n",
      "208–219. Elsevier.\n",
      "Gallager, R. G. (1963). Low-Density Parity-Check\n",
      "Codes . MIT Press.REFERENCES 717\n",
      "Gamerman, D. (1997). Markov Chain Monte Carlo:\n",
      "Stochastic Simulation for Bayesian Inference .\n",
      "Chapman and Hall.\n",
      "Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Ru-\n",
      "bin (2004). Bayesian Data Analysis (Second ed.).\n",
      "Chapman and Hall.\n",
      "Geman, S. and D. Geman (1984). Stochastic re-\n",
      "laxation, Gibbs distributions, and the Bayesian\n",
      "restoration of images. IEEE Transactions on Pat-\n",
      "tern Analysis and Machine Intelligence 6(1),\n",
      "721–741.\n",
      "Ghahramani, Z. and M. J. Beal (2000). Variational\n",
      "inference for Bayesian mixtures of factor ana-\n",
      "lyzers. In S. A. Solla, T. K. Leen, and K. R.\n",
      "M¨uller (Eds.), Advances in Neural Information\n",
      "Processing Systems , V olume 12, pp. 449–455.\n",
      "MIT Press.\n",
      "Ghahramani, Z. and G. E. Hinton (1996a). The\n",
      "EM algorithm for mixtures of factor analyzers.\n",
      "Technical Report CRG-TR-96-1, University of\n",
      "Toronto.\n",
      "Ghahramani, Z. and G. E. Hinton (1996b). Param-\n",
      "eter estimation for linear dynamical systems.Technical Report CRG-TR-96-2, University of\n",
      "Toronto.\n",
      "Ghahramani, Z. and G. E. Hinton (1998). Variational\n",
      "learning for switching state-space models. Neu-\n",
      "ral Computation 12(4), 963–996.\n",
      "Ghahramani, Z. and M. I. Jordan (1994). Super-\n",
      "vised learning from incomplete data via an EMappproach. In J. D. Cowan, G. T. Tesauro, and\n",
      "J. Alspector (Eds.), Advances in Neural Informa-\n",
      "tion Processing Systems , V olume 6, pp. 120–127.\n",
      "Morgan Kaufmann.\n",
      "Ghahramani, Z. and M. I. Jordan (1997). Factorial\n",
      "hidden Markov models. Machine Learning 29,\n",
      "245–275.\n",
      "Gibbs, M. N. (1997). Bayesian Gaussian processes\n",
      "for regression and classiﬁcation . Phd thesis, Uni-\n",
      "versity of Cambridge.\n",
      "Gibbs, M. N. and D. J. C. MacKay (2000). Varia-\n",
      "tional Gaussian process classiﬁers. IEEE Trans-\n",
      "actions on Neural Networks 11, 1458–1464.Gilks, W. R. (1992). Derivative-free adaptive\n",
      "rejection sampling for Gibbs sampling. InJ. Bernardo, J. Berger, A. P. Dawid, and A. F. M.\n",
      "Smith (Eds.), Bayesian Statistics , V olume 4. Ox-\n",
      "ford University Press.\n",
      "Gilks, W. R., N. G. Best, and K. K. C. Tan (1995).\n",
      "Adaptive rejection Metropolis sampling. Applied\n",
      "Statistics 44, 455–472.\n",
      "Gilks, W. R., S. Richardson, and D. J. Spiegelhal-\n",
      "ter (Eds.) (1996). Markov Chain Monte Carlo in\n",
      "Practice . Chapman and Hall.\n",
      "Gilks, W. R. and P. Wild (1992). Adaptive rejection\n",
      "sampling for Gibbs sampling. Applied Statis-\n",
      "tics 41, 337–348.\n",
      "Gill, P. E., W. Murray, and M. H. Wright (1981).\n",
      "Practical Optimization . Academic Press.\n",
      "Goldberg, P. W., C. K. I. Williams, and C. M.\n",
      "Bishop (1998). Regression with input-dependent\n",
      "noise: A Gaussian process treatment. In Ad-\n",
      "vances in Neural Information Processing Sys-\n",
      "tems, V olume 10, pp. 493–499. MIT Press.\n",
      "Golub, G. H. and C. F. Van Loan (1996).\n",
      "Matrix\n",
      "Computations (Third ed.). John Hopkins Univer-\n",
      "sity Press.\n",
      "Good, I. (1950). Probability and the Weighing of Ev-\n",
      "idence . Hafners.\n",
      "Gordon, N. J., D. J. Salmond, and A. F. M. Smith\n",
      "(1993). Novel approach to nonlinear/non-\n",
      "Gaussian Bayesian state estimation. IEE\n",
      "Proceedings-F 140(2), 107–113.\n",
      "Graepel, T. (2003). Solving noisy linear operator\n",
      "equations by Gaussian processes: Application\n",
      "to ordinary and partial differential equations. InProceedings of the Twentieth International Con-\n",
      "ference on Machine Learning , pp. 234–241.\n",
      "Greig, D., B. Porteous, and A. Seheult (1989). Ex-\n",
      "act maximum a-posteriori estimation for binary\n",
      "images. Journal of the Royal Statistical Society,\n",
      "Series B 51(2), 271–279.\n",
      "Gull, S. F. (1989). Developments in maximum en-\n",
      "tropy data analysis. In J. Skilling (Ed.), Maxi-\n",
      "mum Entropy and Bayesian Methods , pp. 53–71.\n",
      "Kluwer.718 REFERENCES\n",
      "Hassibi, B. and D. G. Stork (1993). Second order\n",
      "derivatives for network pruning: optimal brainsurgeon. In S. J. Hanson, J. D. Cowan, and\n",
      "C. L. Giles (Eds.), Advances in Neural Informa-\n",
      "tion Processing Systems , V olume 5, pp. 164–171.\n",
      "Morgan Kaufmann.\n",
      "Hastie, T. and W. Stuetzle (1989). Principal curves.\n",
      "Journal of the American Statistical Associa-tion 84(106), 502–516.\n",
      "Hastie, T., R. Tibshirani, and J. Friedman (2001).\n",
      "The Elements of Statistical Learning . Springer.\n",
      "Hastings, W. K. (1970). Monte Carlo sampling\n",
      "methods using Markov chains and their applica-\n",
      "tions. Biometrika 57, 97–109.\n",
      "Hathaway, R. J. (1986). Another interpretation of the\n",
      "EM algorithm for mixture distributions. Statistics\n",
      "and Probability Letters 4, 53–56.\n",
      "Haussler, D. (1999). Convolution kernels on discrete\n",
      "structures. Technical Report UCSC-CRL-99-10,University of California, Santa Cruz, Computer\n",
      "Science Department.\n",
      "Henrion, M. (1988). Propagation of uncertainty by\n",
      "logic sampling in Bayes’ networks. In J. F. Lem-mer and L. N. Kanal (Eds.), Uncertainty in Arti-\n",
      "ﬁcial Intelligence , V olume 2, pp. 149–164. North\n",
      "Holland.\n",
      "Herbrich, R. (2002). Learning Kernel Classiﬁers .\n",
      "MIT Press.\n",
      "Hertz, J., A. Krogh, and R. G. Palmer (1991). In-\n",
      "troduction to the Theory of Neural Computation .\n",
      "Addison Wesley.\n",
      "Hinton, G. E., P. Dayan, and M. Revow (1997).\n",
      "Modelling the manifolds of images of handwrit-ten digits. IEEE Transactions on Neural Net-\n",
      "works 8(1), 65–74.\n",
      "Hinton, G. E. and D. van Camp (1993). Keeping\n",
      "neural networks simple by minimizing the de-scription length of the weights. In Proceedings of\n",
      "the Sixth Annual Conference on Computational\n",
      "Learning Theory , pp. 5–13. ACM.\n",
      "Hinton, G. E., M. Welling, Y . W. Teh, and S. Osin-\n",
      "dero (2001). A new view of ICA. In Proceedingsof the International Conference on Independent\n",
      "Component Analysis and Blind Signal Separa-tion, V olume 3.\n",
      "Hodgson, M. E. (1998). Reducing computational re-\n",
      "quirements of the minimum-distance classiﬁer.\n",
      "Remote Sensing of Environments 25, 117–128.\n",
      "Hoerl, A. E. and R. Kennard (1970). Ridge regres-\n",
      "sion: biased estimation for nonorthogonal prob-\n",
      "lems. Technometrics 12, 55–67.\n",
      "Hofmann, T. (2000). Learning the similarity of doc-\n",
      "uments: an information-geometric approach to\n",
      "document retrieval and classiﬁcation. In S. A.Solla, T. K. Leen, and K. R. M ¨uller (Eds.), Ad-\n",
      "vances in Neural Information Processing Sys-\n",
      "tems, V olume 12, pp. 914–920. MIT Press.\n",
      "Hojen-Sorensen, P. A., O. Winther, and L. K. Hansen\n",
      "(2002). Mean ﬁeld approaches to independent\n",
      "component analysis. Neural Computation 14(4),\n",
      "889–918.\n",
      "Hornik, K. (1991). Approximation capabilities of\n",
      "multilayer feedforward networks. Neural Net-\n",
      "works 4(2), 251–257.\n",
      "Hornik, K., M. Stinchcombe, and H. White (1989).\n",
      "Multilayer feedforward networks are universalapproximators. Neural Networks 2\n",
      "(5), 359–366.\n",
      "Hotelling, H. (1933). Analysis of a complex of statis-\n",
      "tical variables into principal components. Jour-\n",
      "nal of Educational Psychology 24, 417–441.\n",
      "Hotelling, H. (1936). Relations between two sets of\n",
      "variables. Biometrika 28, 321–377.\n",
      "Hyv¨arinen, A. and E. Oja (1997). A fast ﬁxed-point\n",
      "algorithm for independent component analysis.\n",
      "Neural Computation 9(7), 1483–1492.\n",
      "Isard, M. and A. Blake (1998). CONDENSATION\n",
      "– conditional density propagation for visualtracking. International Journal of Computer Vi-\n",
      "sion 29(1), 5–18.\n",
      "Ito, Y . (1991). Representation of functions by su-\n",
      "perpositions of a step or sigmoid function andtheir applications to neural network theory. Neu-\n",
      "ral Networks 4(3), 385–394.REFERENCES 719\n",
      "Jaakkola, T. and M. I. Jordan (2000). Bayesian\n",
      "parameter estimation via variational methods.Statistics and Computing 10, 25–37.\n",
      "Jaakkola, T. S. (2001). Tutorial on variational ap-\n",
      "proximation methods. In M. Opper and D. Saad\n",
      "(Eds.), Advances in Mean Field Methods , pp.\n",
      "129–159. MIT Press.\n",
      "Jaakkola, T. S. and D. Haussler (1999). Exploiting\n",
      "generative models in discriminative classiﬁers. In\n",
      "M. S. Kearns, S. A. Solla, and D. A. Cohn (Eds.),\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, V olume 11. MIT Press.\n",
      "Jacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E.\n",
      "Hinton (1991). Adaptive mixtures of local ex-\n",
      "perts. Neural Computation 3(1), 79–87.\n",
      "Jaynes, E. T. (2003). Probability Theory: The Logic\n",
      "of Science . Cambridge University Press.\n",
      "Jebara, T. (2004). Machine Learning: Discrimina-\n",
      "tive and Generative . Kluwer.\n",
      "Jeffries, H. (1946). An invariant form for the prior\n",
      "probability in estimation problems. Pro. Roy.\n",
      "Soc. AA 186, 453–461.\n",
      "Jelinek, F. (1997). Statistical Methods for Speech\n",
      "Recognition . MIT Press.\n",
      "Jensen, C., A. Kong, and U. Kjaerulff (1995). Block-\n",
      "ing gibbs sampling in very large probabilistic\n",
      "expert systems. International Journal of Human\n",
      "Computer Studies. Special Issue on Real-World\n",
      "Applications of Uncertain Reasoning. 42, 647–\n",
      "666.\n",
      "Jensen, F. V . (1996). An Introduction to Bayesian\n",
      "Networks . UCL Press.\n",
      "Jerrum, M. and A. Sinclair (1996). The Markov\n",
      "chain Monte Carlo method: an approach to ap-proximate counting and integration. In D. S.\n",
      "Hochbaum (Ed.), Approximation Algorithms for\n",
      "NP-Hard Problems . PWS Publishing.\n",
      "Jolliffe, I. T. (2002). Principal Component Analysis\n",
      "(Second ed.). Springer.\n",
      "Jordan, M. I. (1999). Learning in Graphical Models .\n",
      "MIT Press.Jordan, M. I. (2007). An Introduction to Probabilis-\n",
      "tic Graphical Models . In preparation.\n",
      "Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and\n",
      "L. K. Saul (1999). An introduction to variational\n",
      "methods for graphical models. In M. I. Jordan\n",
      "(Ed.), Learning in Graphical Models , pp. 105–\n",
      "162. MIT Press.\n",
      "Jordan, M. I. and R. A. Jacobs (1994). Hierarchical\n",
      "mixtures of experts and the EM algorithm. Neu-\n",
      "ral Computation 6(2), 181–214.\n",
      "Jutten, C. and J. Herault (1991). Blind separation of\n",
      "sources, 1: An adaptive algorithm based on neu-romimetic architecture. Signal Processing 24(1),\n",
      "1–10.\n",
      "Kalman, R. E. (1960). A new approach to linear ﬁl-\n",
      "tering and prediction problems. Transactions of\n",
      "the American Society for Mechanical Engineer-\n",
      "ing, Series D, Journal of Basic Engineering 82,\n",
      "35–45.\n",
      "Kambhatla, N. and T. K. Leen (1997). Dimension\n",
      "reduction by local principal component analysis.\n",
      "Neural Computation 9(7), 1493–1516.\n",
      "Kanazawa, K., D. Koller, and S. Russel (1995).\n",
      "Stochastic simulation algorithms for dynamicprobabilistic networks. In Uncertainty in Artiﬁ-\n",
      "cial Intelligence , V olume 11. Morgan Kaufmann.\n",
      "Kapadia, S. (1998). Discriminative Training of Hid-\n",
      "den Markov Models . Phd thesis, University of\n",
      "Cambridge, U.K.\n",
      "Kapur, J. (1989). Maximum entropy methods in sci-\n",
      "ence and engineering . Wiley.\n",
      "Karush, W. (1939). Minima of functions of several\n",
      "variables with inequalities as side constraints.\n",
      "Master’s thesis, Department of Mathematics,University of Chicago.\n",
      "Kass, R. E. and A. E. Raftery (1995). Bayes fac-\n",
      "tors. Journal of the American Statistical Associ-\n",
      "ation 90, 377–395.\n",
      "Kearns, M. J. and U. V . Vazirani (1994). An Intro-\n",
      "duction to Computational Learning Theory .M I T\n",
      "Press.720 REFERENCES\n",
      "Kindermann, R. and J. L. Snell (1980). Markov Ran-\n",
      "dom Fields and Their Applications . American\n",
      "Mathematical Society.\n",
      "Kittler, J. and J. F ¨oglein (1984). Contextual classiﬁ-\n",
      "cation of multispectral pixel data. Image and Vi-\n",
      "sion Computing 2, 13–29.\n",
      "Kohonen, T. (1982). Self-organized formation of\n",
      "topologically correct feature maps. Biological\n",
      "Cybernetics 43, 59–69.\n",
      "Kohonen, T. (1995). Self-Organizing Maps .\n",
      "Springer.\n",
      "Kolmogorov, V . and R. Zabih (2004). What en-\n",
      "ergy functions can be minimized via graph cuts?IEEE Transactions on Pattern Analysis and Ma-\n",
      "chine Intelligence 26(2), 147–159.\n",
      "Kreinovich, V . Y . (1991). Arbitrary nonlinearity is\n",
      "sufﬁcient to represent all functions by neural net-works: a theorem. Neural Networks 4(3), 381–\n",
      "383.\n",
      "Krogh, A., M. Brown, I. S. Mian, K. Sj ¨olander, and\n",
      "D. Haussler (1994). Hidden Markov models incomputational biology: Applications to protein\n",
      "modelling. Journal of Molecular Biology 235,\n",
      "1501–1531.\n",
      "Kschischnang, F. R., B. J. Frey, and H. A. Loeliger\n",
      "(2001). Factor graphs and the sum-product algo-\n",
      "rithm. IEEE Transactions on Information The-\n",
      "ory47(2), 498–519.\n",
      "Kuhn, H. W. and A. W. Tucker (1951). Nonlinear\n",
      "programming. In Proceedings of the 2nd Berke-\n",
      "ley Symposium on Mathematical Statistics and\n",
      "Probabilities , pp. 481–492. University of Cali-\n",
      "fornia Press.\n",
      "Kullback, S. and R. A. Leibler (1951). On infor-\n",
      "mation and sufﬁciency. Annals of Mathematical\n",
      "Statistics 22(1), 79–86.\n",
      "K˙urkov ´a, V . and P. C. Kainen (1994). Functionally\n",
      "equivalent feed-forward neural networks. Neural\n",
      "Computation 6(3), 543–558.\n",
      "Kuss, M. and C. Rasmussen (2006). Assessing ap-\n",
      "proximations for Gaussian process classiﬁcation.InAdvances in Neural Information Processing\n",
      "Systems , Number 18. MIT Press. in press.\n",
      "Lasserre, J., C. M. Bishop, and T. Minka (2006).\n",
      "Principled hybrids of generative and discrimina-tive models. In Proceedings 2006 IEEE Confer-\n",
      "ence on Computer Vision and Pattern Recogni-\n",
      "tion, New York .\n",
      "Lauritzen, S. and N. Wermuth (1989). Graphical\n",
      "models for association between variables, someof which are qualitative some quantitative. An-\n",
      "nals of Statistics 17, 31–57.\n",
      "Lauritzen, S. L. (1992). Propagation of probabilities,\n",
      "means and variances in mixed graphical associa-\n",
      "tion models. Journal of the American Statistical\n",
      "Association 87, 1098–1108.\n",
      "Lauritzen, S. L. (1996).\n",
      "Graphical Models . Oxford\n",
      "University Press.\n",
      "Lauritzen, S. L. and D. J. Spiegelhalter (1988). Lo-\n",
      "cal computations with probabailities on graphical\n",
      "structures and their application to expert systems.Journal of the Royal Statistical Society 50, 157–\n",
      "224.\n",
      "Lawley, D. N. (1953). A modiﬁed method of esti-\n",
      "mation in factor analysis and some large sam-\n",
      "ple results. In Uppsala Symposium on Psycho-\n",
      "logical Factor Analysis , Number 3 in Nordisk\n",
      "Psykologi Monograph Series, pp. 35–42. Upp-\n",
      "sala: Almqvist and Wiksell.\n",
      "Lawrence, N. D., A. I. T. Rowstron, C. M. Bishop,\n",
      "and M. J. Taylor (2002). Optimising synchro-nisation times for mobile devices. In T. G. Di-\n",
      "etterich, S. Becker, and Z. Ghahramani (Eds.),\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, V olume 14, pp. 1401–1408. MIT Press.\n",
      "Lazarsfeld, P. F. and N. W. Henry (1968). Latent\n",
      "Structure Analysis . Houghton Mifﬂin.\n",
      "Le Cun, Y ., B. Boser, J. S. Denker, D. Henderson,\n",
      "R. E. Howard, W. Hubbard, and L. D. Jackel\n",
      "(1989). Backpropagation applied to handwrittenzip code recognition. Neural Computation 1(4),\n",
      "541–551.\n",
      "Le Cun, Y ., J. S. Denker, and S. A. Solla (1990).\n",
      "Optimal brain damage. In D. S. Touretzky (Ed.),REFERENCES 721\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, V olume 2, pp. 598–605. Morgan Kauf-\n",
      "mann.\n",
      "LeCun, Y ., L. Bottou, Y . Bengio, and P. Haffner\n",
      "(1998). Gradient-based learning applied to doc-\n",
      "ument recognition. Proceedings of the IEEE 86,\n",
      "2278–2324.\n",
      "Lee, Y ., Y . Lin, and G. Wahba (2001). Multicategory\n",
      "support vector machines. Technical Report 1040,\n",
      "Department of Statistics, University of Madison,Wisconsin.\n",
      "Leen, T. K. (1995). From data distributions to regu-\n",
      "larization in invariant learning. Neural Computa-\n",
      "tion 7, 974–981.\n",
      "Lindley, D. V . (1982). Scoring rules and the in-\n",
      "evitability of probability. International Statisti-\n",
      "cal Review 50, 1–26.\n",
      "Liu, J. S. (Ed.) (2001). Monte Carlo Strategies in\n",
      "Scientiﬁc Computing . Springer.\n",
      "Lloyd, S. P. (1982). Least squares quantization in\n",
      "PCM. IEEE Transactions on Information The-\n",
      "ory28(2), 129–137.\n",
      "L¨utkepohl, H. (1996). Handbook of Matrices . Wiley.\n",
      "MacKay, D. J. C. (1992a). Bayesian interpolation.\n",
      "Neural Computation 4(3), 415–447.\n",
      "MacKay, D. J. C. (1992b). The evidence framework\n",
      "applied to classiﬁcation networks. Neural Com-\n",
      "putation 4(5), 720–736.\n",
      "MacKay, D. J. C. (1992c). A practical Bayesian\n",
      "framework for back-propagation networks. Neu-\n",
      "ral Computation 4(3), 448–472.\n",
      "MacKay, D. J. C. (1994). Bayesian methods for\n",
      "backprop networks. In E. Domany, J. L. van\n",
      "Hemmen, and K. Schulten (Eds.), Models of\n",
      "Neural Networks, III , Chapter 6, pp. 211–254.\n",
      "Springer.\n",
      "MacKay, D. J. C. (1995). Bayesian neural networks\n",
      "and density networks. Nuclear Instruments and\n",
      "Methods in Physics Research, A 354(1), 73–80.\n",
      "MacKay, D. J. C. (1997). Ensemble learning for hid-\n",
      "den Markov models. Unpublished manuscript,Department of Physics, University of Cam-\n",
      "bridge.\n",
      "MacKay, D. J. C. (1998). Introduction to Gaus-\n",
      "sian processes. In C. M. Bishop (Ed.), Neural\n",
      "Networks and Machine Learning , pp. 133–166.\n",
      "Springer.\n",
      "MacKay, D. J. C. (1999). Comparison of approx-\n",
      "imate methods for handling hyperparameters.\n",
      "Neural Computation 11(5), 1035–1068.\n",
      "MacKay, D. J. C. (2003). Information Theory, Infer-\n",
      "ence and Learning Algorithms . Cambridge Uni-\n",
      "versity Press.\n",
      "MacKay, D. J. C. and M. N. Gibbs (1999). Den-\n",
      "sity networks. In J. W. Kay and D. M. Tittering-ton (Eds.), Statistics and Neural Networks: Ad-\n",
      "vances at the Interface , Chapter 5, pp. 129–145.\n",
      "Oxford University Press.\n",
      "MacKay, D. J. C. and R. M. Neal (1999). Good error-\n",
      "correcting codes based on very sparse matrices.\n",
      "IEEE Transactions on Information Theory\n",
      "45,\n",
      "399–431.\n",
      "MacQueen, J. (1967). Some methods for classiﬁca-\n",
      "tion and analysis of multivariate observations. In\n",
      "L. M. LeCam and J. Neyman (Eds.), Proceed-\n",
      "ings of the Fifth Berkeley Symposium on Mathe-matical Statistics and Probability , V olume I, pp.\n",
      "281–297. University of California Press.\n",
      "Magnus, J. R. and H. Neudecker (1999). Matrix Dif-\n",
      "ferential Calculus with Applications in Statisticsand Econometrics . Wiley.\n",
      "Mallat, S. (1999). A Wavelet Tour of Signal Process-\n",
      "ing(Second ed.). Academic Press.\n",
      "Manning, C. D. and H. Sch ¨utze (1999). F oundations\n",
      "of Statistical Natural Language Processing . MIT\n",
      "Press.\n",
      "Mardia, K. V . and P. E. Jupp (2000). Directional\n",
      "Statistics . Wiley.\n",
      "Maybeck, P. S. (1982). Stochastic models, estima-\n",
      "tion and control . Academic Press.\n",
      "McAllester, D. A. (2003). PAC-Bayesian stochastic\n",
      "model selection. Machine Learning 51(1), 5–21.722 REFERENCES\n",
      "McCullagh, P. and J. A. Nelder (1989). Generalized\n",
      "Linear Models (Second ed.). Chapman and Hall.\n",
      "McCulloch, W. S. and W. Pitts (1943). A logical\n",
      "calculus of the ideas immanent in nervous ac-\n",
      "tivity. Bulletin of Mathematical Biophysics 5,\n",
      "115–133. Reprinted in Anderson and Rosenfeld\n",
      "(1988).\n",
      "McEliece, R. J., D. J. C. MacKay, and J. F. Cheng\n",
      "(1998). Turbo decoding as an instance of Pearl’s\n",
      "‘Belief Ppropagation’ algorithm. IEEE Journal\n",
      "on Selected Areas in Communications 16, 140–\n",
      "152.\n",
      "McLachlan, G. J. and K. E. Basford (1988). Mixture\n",
      "Models: Inference and Applications to Cluster-\n",
      "ing. Marcel Dekker.\n",
      "McLachlan, G. J. and T. Krishnan (1997). The EM\n",
      "Algorithm and its Extensions . Wiley.\n",
      "McLachlan, G. J. and D. Peel (2000). Finite Mixture\n",
      "Models . Wiley.\n",
      "Meng, X. L. and D. B. Rubin (1993). Maximum like-\n",
      "lihood estimation via the ECM algorithm: a gen-\n",
      "eral framework. Biometrika 80, 267–278.\n",
      "Metropolis, N., A. W. Rosenbluth, M. N. Rosen-\n",
      "bluth, A. H. Teller, and E. Teller (1953). Equa-tion of state calculations by fast computing\n",
      "machines. Journal of Chemical Physics 21(6),\n",
      "1087–1092.\n",
      "Metropolis, N. and S. Ulam (1949). The Monte\n",
      "Carlo method. Journal of the American Statisti-\n",
      "cal Association 44(247), 335–341.\n",
      "Mika, S., G. R ¨atsch, J. Weston, and B. Sch ¨olkopf\n",
      "(1999). Fisher discriminant analysis with ker-\n",
      "nels. In Y . H. Hu, J. Larsen, E. Wilson, andS. Douglas (Eds.), Neural Networks for Signal\n",
      "Processing IX , pp. 41–48. IEEE.\n",
      "Minka, T. (2001a). Expectation propagation for ap-\n",
      "proximate Bayesian inference. In J. Breese and\n",
      "D. Koller (Eds.), Proceedings of the Seventeenth\n",
      "Conference on Uncertainty in Artiﬁcial Intelli-\n",
      "gence , pp. 362–369. Morgan Kaufmann.\n",
      "Minka, T. (2001b). A family of approximate al-\n",
      "gorithms for Bayesian inference . Ph. D. thesis,\n",
      "MIT.Minka, T. (2004). Power EP. Technical Report\n",
      "MSR-TR-2004-149, Microsoft Research Cam-bridge.\n",
      "Minka, T. (2005). Divergence measures and mes-\n",
      "sage passing. Technical Report MSR-TR-2005-\n",
      "173, Microsoft Research Cambridge.\n",
      "Minka, T. P. (2001c). Automatic choice of dimen-\n",
      "sionality for PCA. In T. K. Leen, T. G. Diet-\n",
      "terich, and V . Tresp (Eds.), Advances in Neural\n",
      "Information Processing Systems , V olume 13, pp.\n",
      "598–604. MIT Press.\n",
      "Minsky, M. L. and S. A. Papert (1969). Perceptrons .\n",
      "MIT Press. Expanded edition 1990.\n",
      "Miskin, J. W. and D. J. C. MacKay (2001). Ensem-\n",
      "ble learning for blind source separation. In S. J.\n",
      "Roberts and R. M. Everson (Eds.), Independent\n",
      "Component Analysis: Principles and Practice .\n",
      "Cambridge University Press.\n",
      "Møller, M. (1993). Efﬁcient Training of Feed-\n",
      "Forward Neural Networks. Ph. D. thesis, Aarhus\n",
      "University, Denmark.\n",
      "Moody, J. and C. J. Darken (1989). Fast learning in\n",
      "networks of locally-tuned processing units. Neu-\n",
      "ral Computation 1(2), 281–294.\n",
      "Moore, A. W. (2000). The anchors hierarch: us-\n",
      "ing the triangle inequality to survive high dimen-\n",
      "sional data. In Proceedings of the Twelfth Con-\n",
      "ference on Uncertainty in Artiﬁcial Intelligence ,\n",
      "pp. 397–405.\n",
      "M¨uller, K. R., S. Mika, G. R ¨atsch, K. Tsuda, and\n",
      "B. Sch ¨olkopf (2001). An introduction to kernel-\n",
      "based learning algorithms. IEEE Transactions on\n",
      "Neural Networks 12(2), 181–202.\n",
      "M¨uller, P. and F. A. Quintana (2004). Nonparametric\n",
      "Bayesian data analysis. Statistical Science 19(1),\n",
      "95–110.\n",
      "Nabney, I. T. (2002). Netlab: Algorithms for Pattern\n",
      "Recognition . Springer.\n",
      "Nadaraya, ´E. A. (1964). On estimating regression.\n",
      "Theory of Probability and its Applications 9(1),\n",
      "141–142.REFERENCES 723\n",
      "Nag, R., K. Wong, and F. Fallside (1986). Script\n",
      "recognition using hidden markov models. InICASSP86 , pp. 2071–2074. IEEE.\n",
      "Neal, R. M. (1993). Probabilistic inference using\n",
      "Markov chain Monte Carlo methods. Technical\n",
      "Report CRG-TR-93-1, Department of ComputerScience, University of Toronto, Canada.\n",
      "Neal, R. M. (1996). Bayesian Learning for Neural\n",
      "Networks . Springer. Lecture Notes in Statistics\n",
      "118.\n",
      "Neal, R. M. (1997). Monte Carlo implementation of\n",
      "Gaussian process models for Bayesian regressionand classiﬁcation. Technical Report 9702, De-\n",
      "partment of Computer Statistics, University of\n",
      "Toronto.\n",
      "Neal, R. M. (1999). Suppressing random walks in\n",
      "Markov chain Monte Carlo using ordered over-\n",
      "relaxation. In M. I. Jordan (Ed.), Learning in\n",
      "Graphical Models , pp. 205–228. MIT Press.\n",
      "Neal, R. M. (2000). Markov chain sampling for\n",
      "Dirichlet process mixture models. Journal of\n",
      "Computational and Graphical Statistics 9, 249–\n",
      "265.\n",
      "Neal, R. M. (2003). Slice sampling. Annals of Statis-\n",
      "tics 31, 705–767.\n",
      "Neal, R. M. and G. E. Hinton (1999). A new view of\n",
      "the EM algorithm that justiﬁes incremental and\n",
      "other variants. In M. I. Jordan (Ed.), Learning in\n",
      "Graphical Models , pp. 355–368. MIT Press.\n",
      "Nelder, J. A. and R. W. M. Wedderburn (1972). Gen-\n",
      "eralized linear models. Journal of the Royal Sta-\n",
      "tistical Society, A 135, 370–384.\n",
      "Nilsson, N. J. (1965). Learning Machines . McGraw-\n",
      "Hill. Reprinted as The Mathematical F ounda-\n",
      "tions of Learning Machines , Morgan Kaufmann,\n",
      "(1990).\n",
      "Nocedal, J. and S. J. Wright (1999). Numerical Op-\n",
      "timization . Springer.\n",
      "Nowlan, S. J. and G. E. Hinton (1992). Simplifying\n",
      "neural networks by soft weight sharing. Neural\n",
      "Computation 4(4), 473–493.Ogden, R. T. (1997). Essential Wavelets for Statisti-\n",
      "cal Applications and Data Analysis . Birkh ¨auser.\n",
      "Opper, M. and O. Winther (1999). A Bayesian ap-\n",
      "proach to on-line learning. In D. Saad (Ed.), On-\n",
      "Line Learning in Neural Networks , pp. 363–378.\n",
      "Cambridge University Press.\n",
      "Opper, M. and O. Winther (2000a). Gaussian\n",
      "processes and SVM: mean ﬁeld theory and\n",
      "leave-one-out. In A. J. Smola, P. L. Bartlett,\n",
      "B. Sch ¨olkopf, and D. Shuurmans (Eds.), Ad-\n",
      "vances in Large Margin Classiﬁers , pp. 311–326.\n",
      "MIT Press.\n",
      "Opper, M. and O. Winther (2000b). Gaussian\n",
      "processes for classiﬁcation. Neural Computa-\n",
      "tion 12(11), 2655–2684.\n",
      "Osuna, E., R. Freund, and F. Girosi (1996). Support\n",
      "vector machines: training and applications. A.I.\n",
      "Memo AIM-1602, MIT.\n",
      "Papoulis, A. (1984). Probability, Random V ariables,\n",
      "and Stochastic Processes (Second ed.). McGraw-\n",
      "Hill.\n",
      "Parisi, G. (1988). Statistical Field Theory . Addison-\n",
      "Wesley.\n",
      "Pearl, J. (1988). Probabilistic Reasoning in Intelli-\n",
      "gent Systems . Morgan Kaufmann.\n",
      "Pearlmutter, B. A. (1994). Fast exact multiplication\n",
      "by the Hessian. Neural Computation 6(1), 147–\n",
      "160.\n",
      "Pearlmutter, B. A. and L. C. Parra (1997). Maximum\n",
      "likelihood source separation: a context-sensitivegeneralization of ICA. In M. C. Mozer, M. I. Jor-\n",
      "dan, and T. Petsche (Eds.), Advances in Neural\n",
      "Information Processing Systems , V olume 9, pp.\n",
      "613–619. MIT Press.\n",
      "Pearson, K. (1901). On lines and planes of closest ﬁt\n",
      "to systems of points in space. The London, Edin-\n",
      "burgh and Dublin Philosophical Magazine and\n",
      "Journal of Science, Sixth Series 2, 559–572.\n",
      "Platt, J. C. (1999). Fast training of support vector\n",
      "machines using sequential minimal optimization.\n",
      "In B. Sch ¨olkopf, C. J. C. Burges, and A. J. Smola\n",
      "(Eds.), Advances in Kernel Methods – Support\n",
      "V ector Learning , pp. 185–208. MIT Press.724 REFERENCES\n",
      "Platt, J. C. (2000). Probabilities for SV machines.\n",
      "In A. J. Smola, P. L. Bartlett, B. Sch ¨olkopf, and\n",
      "D. Shuurmans (Eds.), Advances in Large Margin\n",
      "Classiﬁers , pp. 61–73. MIT Press.\n",
      "Platt, J. C., N. Cristianini, and J. Shawe-Taylor\n",
      "(2000). Large margin DAGs for multiclass clas-\n",
      "siﬁcation. In S. A. Solla, T. K. Leen, and K. R.M¨uller (Eds.), Advances in Neural Information\n",
      "Processing Systems , V olume 12, pp. 547–553.\n",
      "MIT Press.\n",
      "Poggio, T. and F. Girosi (1990). Networks for ap-\n",
      "proximation and learning. Proceedings of the\n",
      "IEEE 78(9), 1481–1497.\n",
      "Powell, M. J. D. (1987). Radial basis functions for\n",
      "multivariable interpolation: a review. In J. C.\n",
      "Mason and M. G. Cox (Eds.), Algorithms for\n",
      "Approximation , pp. 143–167. Oxford University\n",
      "Press.\n",
      "Press, W. H., S. A. Teukolsky, W. T. Vetterling, and\n",
      "B. P. Flannery (1992). Numerical Recipes in C:\n",
      "The Art of Scientiﬁc Computing (Second ed.).\n",
      "Cambridge University Press.\n",
      "Qazaz, C. S., C. K. I. Williams, and C. M. Bishop\n",
      "(1997). An upper bound on the Bayesian error\n",
      "bars for generalized linear regression. In S. W.\n",
      "Ellacott, J. C. Mason, and I. J. Anderson (Eds.),Mathematics of Neural Networks: Models, Algo-\n",
      "rithms and Applications , pp. 295–299. Kluwer.\n",
      "Quinlan, J. R. (1986). Induction of decision trees.\n",
      "Machine Learning 1(1), 81–106.\n",
      "Quinlan, J. R. (1993). C4.5: Programs for Machine\n",
      "Learning . Morgan Kaufmann.\n",
      "Rabiner, L. and B. H. Juang (1993). Fundamentals\n",
      "of Speech Recognition . Prentice Hall.\n",
      "Rabiner, L. R. (1989). A tutorial on hidden Markov\n",
      "models and selected applications in speechrecognition. Proceedings of the IEEE 77(2),\n",
      "257–285.\n",
      "Ramasubramanian, V . and K. K. Paliwal (1990). A\n",
      "generalized optimization of the k-dtree for fast\n",
      "nearest-neighbour search. In Proceedings F ourth\n",
      "IEEE Region 10 International Conference (TEN-\n",
      "CON’89) , pp. 565–568.Ramsey, F. (1931). Truth and probability. In\n",
      "R. Braithwaite (Ed.), The F oundations of Math-\n",
      "ematics and other Logical Essays . Humanities\n",
      "Press.\n",
      "Rao, C. R. and S. K. Mitra (1971). Generalized In-\n",
      "verse of Matrices and Its Applications . Wiley.\n",
      "Rasmussen, C. E. (1996). Evaluation of Gaussian\n",
      "Processes and Other Methods for Non-Linear\n",
      "Regression . Ph. D. thesis, University of Toronto.\n",
      "Rasmussen, C. E. and J. Qui ˜nonero-Candela (2005).\n",
      "Healing the relevance vector machine by aug-\n",
      "mentation. In L. D. Raedt and S. Wrobel (Eds.),\n",
      "Proceedings of the 22nd International Confer-ence on Machine Learning , pp. 689–696.\n",
      "Rasmussen, C. E. and C. K. I. Williams (2006).\n",
      "Gaussian Processes for Machine Learning . MIT\n",
      "Press.\n",
      "Rauch, H. E., F. Tung, and C. T. Striebel (1965).\n",
      "Maximum likelihood estimates of linear dynam-\n",
      "ical systems. AIAA Journal 3, 1445–1450.\n",
      "Ricotti, L. P., S. Ragazzini, and G. Martinelli (1988).\n",
      "Learning of word stress in a sub-optimal second\n",
      "order backpropagation neural network. In Pro-\n",
      "ceedings of the IEEE International Conferenceon Neural Networks , V olume 1, pp. 355–361.\n",
      "IEEE.\n",
      "Ripley, B. D. (1996). Pattern Recognition and Neu-\n",
      "ral Networks . Cambridge University Press.\n",
      "Robbins, H. and S. Monro (1951). A stochastic\n",
      "approximation method. Annals of Mathematical\n",
      "Statistics 22, 400–407.\n",
      "Robert, C. P. and G. Casella (1999). Monte Carlo\n",
      "Statistical Methods . Springer.\n",
      "Rockafellar, R. (1972). Convex Analysis . Princeton\n",
      "University Press.\n",
      "Rosenblatt, F. (1962). Principles of Neurodynam-\n",
      "ics: Perceptrons and the Theory of Brain Mech-\n",
      "anisms . Spartan.\n",
      "Roth, V . and V . Steinhage (2000). Nonlinear discrim-\n",
      "inant analysis using kernel functions. In S. A.REFERENCES 725\n",
      "Solla, T. K. Leen, and K. R. M ¨uller (Eds.), Ad-\n",
      "vances in Neural Information Processing Sys-tems, V olume 12. MIT Press.\n",
      "Roweis, S. (1998). EM algorithms for PCA and\n",
      "SPCA. In M. I. Jordan, M. J. Kearns, and S. A.\n",
      "Solla (Eds.), Advances in Neural Information\n",
      "Processing Systems , V olume 10, pp. 626–632.\n",
      "MIT Press.\n",
      "Roweis, S. and Z. Ghahramani (1999). A unifying\n",
      "review of linear Gaussian models. Neural Com-\n",
      "putation 11(2), 305–345.\n",
      "Roweis, S. and L. Saul (2000, December). Nonlinear\n",
      "dimensionality reduction by locally linear em-\n",
      "bedding. Science 290, 2323–2326.\n",
      "Rubin, D. B. (1983). Iteratively reweighted least\n",
      "squares. In Encyclopedia of Statistical Sciences ,\n",
      "V olume 4, pp. 272–275. Wiley.\n",
      "Rubin, D. B. and D. T. Thayer (1982). EM al-\n",
      "gorithms for ML factor analysis. Psychome-\n",
      "trika 47(1), 69–76.\n",
      "Rumelhart, D. E., G. E. Hinton, and R. J. Williams\n",
      "(1986). Learning internal representations by er-ror propagation. In D. E. Rumelhart, J. L. Mc-\n",
      "Clelland, and the PDP Research Group (Eds.),\n",
      "Parallel Distributed Processing: Explorationsin the Microstructure of Cognition , V olume 1:\n",
      "Foundations, pp. 318–362. MIT Press. Reprinted\n",
      "in Anderson and Rosenfeld (1988).\n",
      "Rumelhart, D. E., J. L. McClelland, and the PDP Re-\n",
      "search Group (Eds.) (1986). Parallel Distributed\n",
      "Processing: Explorations in the Microstruc-\n",
      "ture of Cognition , V olume 1: Foundations. MIT\n",
      "Press.\n",
      "Sagan, H. (1969). Introduction to the Calculus of\n",
      "V ariations . Dover.\n",
      "Savage, L. J. (1961). The subjective basis of sta-\n",
      "tistical practice. Technical report, Department of\n",
      "Statistics, University of Michigan, Ann Arbor.\n",
      "Sch¨olkopf, B., J. Platt, J. Shawe-Taylor, A. Smola,\n",
      "and R. C. Williamson (2001). Estimating the sup-port of a high-dimensional distribution. Neural\n",
      "Computation 13(7), 1433–1471.Sch¨olkopf, B., A. Smola, and K.-R. M ¨uller (1998).\n",
      "Nonlinear component analysis as a kerneleigenvalue problem. Neural Computation 10(5),\n",
      "1299–1319.\n",
      "Sch¨olkopf, B., A. Smola, R. C. Williamson, and P. L.\n",
      "Bartlett (2000). New support vector algorithms.Neural Computation 12(5), 1207–1245.\n",
      "Sch¨olkopf, B. and A. J. Smola (2002). Learning with\n",
      "Kernels . MIT Press.\n",
      "Schwarz, G. (1978). Estimating the dimension of a\n",
      "model. Annals of Statistics 6, 461–464.\n",
      "Schwarz, H. R. (1988). Finite element methods . Aca-\n",
      "demic Press.\n",
      "Seeger, M. (2003). Bayesian Gaussian Process Mod-\n",
      "els: PAC-Bayesian Generalization Error Bounds\n",
      "and Sparse Approximations . Ph. D. thesis, Uni-\n",
      "versity of Edinburg.\n",
      "Seeger, M., C. K. I. Williams, and N. Lawrence\n",
      "(2003). Fast forward selection to speed up sparse\n",
      "Gaussian processes. In C. M. Bishop and B. Frey\n",
      "(Eds.), Proceedings Ninth International Work-\n",
      "shop on Artiﬁcial Intelligence and Statistics, Key\n",
      "West, Florida .\n",
      "Shachter, R. D. and M. Peot (1990). Simulation ap-\n",
      "proaches to general probabilistic inference on be-lief networks. In P. P. Bonissone, M. Henrion,\n",
      "L. N. Kanal, and J. F. Lemmer (Eds.), Uncer-\n",
      "tainty in Artiﬁcial Intelligence , V olume 5. Else-\n",
      "vier.\n",
      "Shannon, C. E. (1948). A mathematical theory of\n",
      "communication. The Bell System Technical Jour-\n",
      "nal27(3), 379–423 and 623–656.\n",
      "Shawe-Taylor, J. and N. Cristianini (2004). Kernel\n",
      "Methods for Pattern Analysis . Cambridge Uni-\n",
      "versity Press.\n",
      "Sietsma, J. and R. J. F. Dow (1991). Creating artiﬁ-\n",
      "cial neural networks that generalize. Neural Net-\n",
      "works 4(1), 67–79.\n",
      "Simard, P., Y . Le Cun, and J. Denker (1993). Efﬁ-\n",
      "cient pattern recognition using a new transforma-\n",
      "tion distance. In S. J. Hanson, J. D. Cowan, and726 REFERENCES\n",
      "C. L. Giles (Eds.), Advances in Neural Informa-\n",
      "tion Processing Systems , V olume 5, pp. 50–58.\n",
      "Morgan Kaufmann.\n",
      "Simard, P., B. Victorri, Y . Le Cun, and J. Denker\n",
      "(1992). Tangent prop – a formalism for specify-\n",
      "ing selected invariances in an adaptive network.\n",
      "In J. E. Moody, S. J. Hanson, and R. P. Lippmann(Eds.), Advances in Neural Information Process-\n",
      "ing Systems , V olume 4, pp. 895–903. Morgan\n",
      "Kaufmann.\n",
      "Simard, P. Y ., D. Steinkraus, and J. Platt (2003).\n",
      "Best practice for convolutional neural networks\n",
      "applied to visual document analysis. In Pro-\n",
      "ceedings International Conference on Document\n",
      "Analysis and Recognition (ICDAR) , pp. 958–\n",
      "962. IEEE Computer Society.\n",
      "Sirovich, L. (1987). Turbulence and the dynamics\n",
      "of coherent structures. Quarterly Applied Math-\n",
      "ematics 45(3), 561–590.\n",
      "Smola, A. J. and P. Bartlett (2001). Sparse greedy\n",
      "Gaussian process regression. In T. K. Leen, T. G.\n",
      "Dietterich, and V . Tresp (Eds.), Advances in Neu-\n",
      "ral Information Processing Systems , V olume 13,\n",
      "pp. 619–625. MIT Press.\n",
      "Spiegelhalter, D. and S. Lauritzen (1990). Sequential\n",
      "updating of conditional probabilities on directed\n",
      "graphical structures. Networks 20, 579–605.\n",
      "Stinchecombe, M. and H. White (1989). Universal\n",
      "approximation using feed-forward networks with\n",
      "non-sigmoid hidden layer activation functions. In\n",
      "International Joint Conference on Neural Net-\n",
      "works , V olume 1, pp. 613–618. IEEE.\n",
      "Stone, J. V . (2004). Independent Component Analy-\n",
      "sis: A Tutorial Introduction . MIT Press.\n",
      "Sung, K. K. and T. Poggio (1994). Example-based\n",
      "learning for view-based human face detection.\n",
      "A.I. Memo 1521, MIT.\n",
      "Sutton, R. S. and A. G. Barto (1998). Reinforcement\n",
      "Learning: An Introduction . MIT Press.\n",
      "Svens ´en, M. and C. M. Bishop (2004). Ro-\n",
      "bust Bayesian mixture modelling. Neurocomput-\n",
      "ing 64, 235–252.Tarassenko, L. (1995). Novelty detection for the\n",
      "identiﬁcation of masses in mamograms. In Pro-\n",
      "ceedings F ourth IEE International Conference\n",
      "on Artiﬁcial Neural Networks , V olume 4, pp.\n",
      "442–447. IEE.\n",
      "Tax, D. and R. Duin (1999). Data domain descrip-\n",
      "tion by support vectors. In M. Verleysen (Ed.),Proceedings European Symposium on Artiﬁcial\n",
      "Neural Networks, ESANN , pp. 251–256. D. Facto\n",
      "Press.\n",
      "Teh, Y . W., M. I. Jordan, M. J. Beal, and D. M. Blei\n",
      "(2006). Hierarchical Dirichlet processes. Journal\n",
      "of the Americal Statistical Association . to appear.\n",
      "Tenenbaum, J. B., V . de Silva, and J. C. Langford\n",
      "(2000, December). A global framework for non-linear dimensionality reduction. Science 290,\n",
      "2319–2323.\n",
      "Tesauro, G. (1994). TD-Gammon, a self-teaching\n",
      "backgammon program, achieves master-level\n",
      "play. Neural Computation 6(2), 215–219.\n",
      "Thiesson, B., D. M. Chickering, D. Heckerman, and\n",
      "C. Meek (2004). ARMA time-series modelling\n",
      "with graphical models. In M. Chickering and\n",
      "J. Halpern (Eds.), Proceedings of the Twentieth\n",
      "Conference on Uncertainty in Artiﬁcial Intelli-\n",
      "gence, Banff, Canada , pp. 552–560. AUAI Press.\n",
      "Tibshirani, R. (1996). Regression shrinkage and se-\n",
      "lection via the lasso. Journal of the Royal Statis-\n",
      "tical Society, B 58, 267–288.\n",
      "Tierney, L. (1994). Markov chains for exploring pos-\n",
      "terior distributions. Annals of Statistics 22(4),\n",
      "1701–1762.\n",
      "Tikhonov, A. N. and V . Y . Arsenin (1977). Solutions\n",
      "of Ill-Posed Problems . V . H. Winston.\n",
      "Tino, P. and I. T. Nabney (2002). Hierarchical\n",
      "GTM: constructing localized non-linear projec-\n",
      "tion manifolds in a principled way. IEEE Trans-\n",
      "actions on Pattern Analysis and Machine Intelli-gence 24(5), 639–656.\n",
      "Tino, P., I. T. Nabney, and Y . Sun (2001). Us-\n",
      "ing directional curvatures to visualize folding\n",
      "patterns of the GTM projection manifolds. InREFERENCES 727\n",
      "G. Dorffner, H. Bischof, and K. Hornik (Eds.),\n",
      "Artiﬁcial Neural Networks – ICANN 2001 , pp.\n",
      "421–428. Springer.\n",
      "Tipping, M. E. (1999). Probabilistic visualisation of\n",
      "high-dimensional binary data. In M. S. Kearns,S. A. Solla, and D. A. Cohn (Eds.), Advances\n",
      "in Neural Information Processing Systems , V ol-\n",
      "ume 11, pp. 592–598. MIT Press.\n",
      "Tipping, M. E. (2001). Sparse Bayesian learning and\n",
      "the relevance vector machine. Journal of Ma-\n",
      "chine Learning Research 1, 211–244.\n",
      "Tipping, M. E. and C. M. Bishop (1997). Probabilis-\n",
      "tic principal component analysis. Technical Re-port NCRG/97/010, Neural Computing Research\n",
      "Group, Aston University.\n",
      "Tipping, M. E. and C. M. Bishop (1999a). Mixtures\n",
      "of probabilistic principal component analyzers.\n",
      "Neural Computation 11(2), 443–482.\n",
      "Tipping, M. E. and C. M. Bishop (1999b). Prob-\n",
      "abilistic principal component analysis. Journal\n",
      "of the Royal Statistical Society, Series B 21(3),\n",
      "611–622.\n",
      "Tipping, M. E. and A. Faul (2003). Fast marginal\n",
      "likelihood maximization for sparse Bayesian\n",
      "models. In C. M. Bishop and B. Frey (Eds.),Proceedings Ninth International Workshop on\n",
      "Artiﬁcial Intelligence and Statistics, Key West,\n",
      "Florida .\n",
      "Tong, S. and D. Koller (2000). Restricted Bayes op-\n",
      "timal classiﬁers. In Proceedings 17th National\n",
      "Conference on Artiﬁcial Intelligence , pp. 658–\n",
      "664. AAAI.\n",
      "Tresp, V . (2001). Scaling kernel-based systems to\n",
      "large data sets. Data Mining and Knowledge Dis-\n",
      "covery 5(3), 197–211.\n",
      "Uhlenbeck, G. E. and L. S. Ornstein (1930). On the\n",
      "theory of Brownian motion. Phys. Rev. 36, 823–\n",
      "841.\n",
      "Valiant, L. G. (1984). A theory of the learnable.\n",
      "Communications of the Association for Comput-\n",
      "ing Machinery 27, 1134–1142.Vapnik, V . N. (1982). Estimation of dependences\n",
      "based on empirical data . Springer.\n",
      "Vapnik, V . N. (1995). The nature of statistical learn-\n",
      "ing theory . Springer.\n",
      "Vapnik, V . N. (1998). Statistical learning theory .W i -\n",
      "ley.\n",
      "Veropoulos, K., C. Campbell, and N. Cristianini\n",
      "(1999). Controlling the sensitivity of supportvector machines. In Proceedings of the Interna-\n",
      "tional Joint Conference on Artiﬁcial Intelligence\n",
      "(IJCAI99), Workshop ML3 , pp. 55–60.\n",
      "Vidakovic, B. (1999). Statistical Modelling by\n",
      "Wavelets . Wiley.\n",
      "Viola, P. and M. Jones (2004). Robust real-time face\n",
      "detection. International Journal of Computer Vi-\n",
      "sion 57(2), 137–154.\n",
      "Viterbi, A. J. (1967). Error bounds for convolu-\n",
      "tional codes and an asymptotically optimum de-\n",
      "coding algorithm. IEEE Transactions on Infor-\n",
      "mation Theory IT-13 , 260–267.\n",
      "Viterbi, A. J. and J. K. Omura (1979). Principles of\n",
      "Digital Communication and Coding . McGraw-\n",
      "Hill.\n",
      "Wahba, G. (1975). A comparison of GCV and GML\n",
      "for choosing the smoothing parameter in the gen-\n",
      "eralized spline smoothing problem. Numerical\n",
      "Mathematics 24, 383–393.\n",
      "Wainwright, M. J., T. S. Jaakkola, and A. S. Willsky\n",
      "(2005). A new class of upper bounds on the log\n",
      "partition function. IEEE Transactions on Infor-\n",
      "mation Theory 51, 2313–2335.\n",
      "Walker, A. M. (1969). On the asymptotic behaviour\n",
      "of posterior distributions. Journal of the Royal\n",
      "Statistical Society, B 31(1), 80–88.\n",
      "Walker, S. G., P. Damien, P. W. Laud, and A. F. M.\n",
      "Smith (1999). Bayesian nonparametric inference\n",
      "for random distributions and related functions(with discussion). Journal of the Royal Statisti-\n",
      "cal Society, B 61(3), 485–527.\n",
      "Watson, G. S. (1964). Smooth regression analysis.\n",
      "Sankhy ¯a: The Indian Journal of Statistics. Series\n",
      "A26, 359–372.728 REFERENCES\n",
      "Webb, A. R. (1994). Functional approximation by\n",
      "feed-forward networks: a least-squares approachto generalisation. IEEE Transactions on Neural\n",
      "Networks 5(3), 363–371.\n",
      "Weisstein, E. W. (1999). CRC Concise Encyclopedia\n",
      "of Mathematics . Chapman and Hall, and CRC.\n",
      "Weston, J. and C. Watkins (1999). Multi-class sup-\n",
      "port vector machines. In M. Verlysen (Ed.), Pro-\n",
      "ceedings ESANN’99, Brussels . D-Facto Publica-\n",
      "tions.\n",
      "Whittaker, J. (1990). Graphical Models in Applied\n",
      "Multivariate Statistics . Wiley.\n",
      "Widrow, B. and M. E. Hoff (1960). Adaptive\n",
      "switching circuits. In IRE WESCON Convention\n",
      "Record , V olume 4, pp. 96–104. Reprinted in An-\n",
      "derson and Rosenfeld (1988).\n",
      "Widrow, B. and M. A. Lehr (1990). 30 years of adap-\n",
      "tive neural networks: perceptron, madeline, and\n",
      "backpropagation. Proceedings of the IEEE 78(9),\n",
      "1415–1442.\n",
      "Wiegerinck, W. and T. Heskes (2003). Fractional\n",
      "belief propagation. In S. Becker, S. Thrun, andK. Obermayer (Eds.), Advances in Neural Infor-\n",
      "mation Processing Systems , V olume 15, pp. 455–\n",
      "462. MIT Press.\n",
      "Williams, C. K. I. (1998). Computation with inﬁ-\n",
      "nite neural networks. Neural Computation 10(5),\n",
      "1203–1216.\n",
      "Williams, C. K. I. (1999). Prediction with Gaussian\n",
      "processes: from linear regression to linear pre-diction and beyond. In M. I. Jordan (Ed.), Learn-\n",
      "ing in Graphical Models , pp. 599–621. MIT\n",
      "Press.\n",
      "Williams, C. K. I. and D. Barber (1998). Bayesian\n",
      "classiﬁcation with Gaussian processes. IEEE\n",
      "Transactions on Pattern Analysis and Machine\n",
      "Intelligence 20, 1342–1351.\n",
      "Williams, C. K. I. and M. Seeger (2001). Using the\n",
      "Nystrom method to speed up kernel machines. In\n",
      "T. K. Leen, T. G. Dietterich, and V . Tresp (Eds.),Advances in Neural Information Processing Sys-\n",
      "tems, V olume 13, pp. 682–688. MIT Press.Williams, O., A. Blake, and R. Cipolla (2005).\n",
      "Sparse Bayesian learning for efﬁcient visualtracking. IEEE Transactions on Pattern Analysis\n",
      "and Machine Intelligence 27(8), 1292–1304.\n",
      "Williams, P. M. (1996). Using neural networks to\n",
      "model conditional multivariate densities. Neural\n",
      "Computation 8(4), 843–854.\n",
      "Winn, J. and C. M. Bishop (2005). Variational mes-\n",
      "sage passing. Journal of Machine Learning Re-\n",
      "search 6, 661–694.\n",
      "Zarchan, P. and H. Musoff (2005). Fundamentals of\n",
      "Kalman Filtering: A Practical Approach (Sec-\n",
      "ond ed.). AIAA.INDEX 729\n",
      "Index\n",
      "Page numbers in bold indicate the primary source of information for the corresponding topic.\n",
      "1-of-Kcoding scheme, 424\n",
      "acceptance criterion, 538, 541, 544\n",
      "activation function, 180, 213, 227\n",
      "active constraint, 328, 709\n",
      "AdaBoost, 657, 658\n",
      "adaline, 196adaptive rejection sampling, 530ADF, seeassumed density ﬁltering\n",
      "AIC, seeAkaike information criterion\n",
      "Akaike information criterion, 33, 217\n",
      "αfamily of divergences, 469\n",
      "αrecursion, 620\n",
      "ancestral sampling, 365, 525, 613\n",
      "annular ﬂow, 679AR model, seeautoregressive model\n",
      "arc, 360ARD, seeautomatic relevance determination\n",
      "ARMA, seeautoregressive moving average\n",
      "assumed density ﬁltering, 510autoassociative networks, 592automatic relevance determination, 259, 312, 349,\n",
      "485, 582\n",
      "autoregressive hidden Markov model, 632autoregressive model, 609autoregressive moving average, 304\n",
      "back-tracking, 415, 630backgammon, 3\n",
      "backpropagation, 241bagging, 656basis function, 138, 172, 204, 227\n",
      "batch training, 240Baum-Welch algorithm, 618\n",
      "Bayes’ theorem, 15\n",
      "Bayes, Thomas, 21Bayesian analysis, vii, 9, 21\n",
      "hierarchical, 372model averaging, 654\n",
      "Bayesian information criterion, 33, 216\n",
      "Bayesian model comparison, 161, 473, 483\n",
      "Bayesian network, 360Bayesian probability, 21belief propagation, 403Bernoulli distribution, 69, 113, 685\n",
      "mixture model, 444\n",
      "Bernoulli, Jacob, 69\n",
      "beta distribution, 71, 686\n",
      "beta recursion, 621between-class covariance, 189bias, 27, 149\n",
      "bias parameter, 138, 181, 227, 346\n",
      "bias-variance trade-off, 147BIC, seeBayesian information criterion\n",
      "binary entropy, 495binomial distribution, 70, 686730 INDEX\n",
      "biological sequence, 610\n",
      "bipartite graph, 401bits, 49\n",
      "blind source separation, 591\n",
      "blocked path, 374, 378, 384\n",
      "Boltzmann distribution, 387Boltzmann, Ludwig Eduard, 53Boolean logic, 21boosting, 657bootstrap, 23, 656\n",
      "bootstrap ﬁlter, 646box constraints, 333, 342\n",
      "Box-Muller method, 527\n",
      "C4.5, 663\n",
      "calculus of variations, 462canonical correlation analysis, 565\n",
      "canonical link function, 212\n",
      "CART, seeclassiﬁcation and regression trees\n",
      "Cauchy distribution, 527, 529, 692\n",
      "causality, 366CCA, seecanonical correlation analysis\n",
      "central differences, 246central limit theorem, 78chain graph, 393chaining, 555Chapman-Kolmogorov equations, 397child node, 361Cholesky decomposition, 528chunking, 335circular normal, seevon Mises distribution\n",
      "classical probability, 21\n",
      "classiﬁcation, 3\n",
      "classiﬁcation and regression trees, 663clique, 385clustering, 3clutter problem, 511co-parents, 383, 492\n",
      "code-book vectors, 429combining models, 45, 653\n",
      "committee, 655complete data set, 440completing the square, 86computational learning theory, 326, 344concave function, 56concentration parameter, 108, 693\n",
      "condensation algorithm, 646\n",
      "conditional entropy, 55\n",
      "conditional expectation, 20\n",
      "conditional independence, 46, 372, 383\n",
      "conditional mixture model, seemixture model\n",
      "conditional probability, 14\n",
      "conjugate prior, 68, 98, 117, 490\n",
      "convex duality, 494\n",
      "convex function, 55, 493\n",
      "convolutional neural network, 267\n",
      "correlation matrix, 567\n",
      "cost function, 41\n",
      "covariance, 20\n",
      "between-class, 189\n",
      "within-class, 189\n",
      "covariance matrix\n",
      "diagonal, 84\n",
      "isotropic, 84\n",
      "partitioned, 85, 307\n",
      "positive deﬁnite, 308\n",
      "Cox’s axioms, 21\n",
      "credit assignment, 3\n",
      "cross-entropy error function, 206, 209, 235, 631,\n",
      "666\n",
      "cross-validation, 32, 161\n",
      "cumulative distribution function, 18\n",
      "curse of dimensionality, 33,3 6\n",
      "curve ﬁtting, 4\n",
      "D map, seedependency map\n",
      "d-separation, 373, 378, 443\n",
      "DAG, seedirected acyclic graph\n",
      "DAGSVM, 339\n",
      "data augmentation, 537\n",
      "data compression, 429\n",
      "decision boundary, 39, 179\n",
      "decision region,\n",
      "39, 179\n",
      "decision surface, seedecision boundary\n",
      "decision theory, 38\n",
      "decision tree, 654, 663, 673\n",
      "decomposition methods, 335\n",
      "degrees of freedom, 559\n",
      "degrees-of-freedom parameter, 102, 693\n",
      "density estimation, 3, 67INDEX 731\n",
      "density network, 597\n",
      "dependency map, 392descendant node, 376\n",
      "design matrix, 142, 347\n",
      "differential entropy, 53digamma function, 687directed acyclic graph, 362directed cycle, 362directed factorization, 381Dirichlet distribution, 76, 687\n",
      "Dirichlet, Lejeune, 77discriminant function, 43, 180, 181\n",
      "discriminative model, 43, 203\n",
      "distortion measure, 424distributive law of multiplication, 396DNA, 610document retrieval, 299dual representation, 293, 329\n",
      "dual-energy gamma densitometry, 678\n",
      "dynamic programming, 411dynamical system, 548\n",
      "E step, seeexpectation step\n",
      "early stopping, 259ECM, seeexpectation conditional maximization\n",
      "edge, 360effective number of observations, 72, 101\n",
      "effective number of parameters, 9, 170, 281\n",
      "elliptical K-means, 444\n",
      "EM, seeexpectation maximization\n",
      "emission probability, 611empirical Bayes, seeevidence approximation\n",
      "energy function, 387\n",
      "entropy, 49\n",
      "conditional, 55differential, 53relative, 55\n",
      "EP, seeexpectation propagation\n",
      "ϵ-tube, 341\n",
      "ϵ-insensitive error function, 340\n",
      "equality constraint, 709equivalent kernel, 159, 301\n",
      "erf function, 211error backpropagation, seebackpropagation\n",
      "error function, 5,2 3error-correcting output codes, 339\n",
      "Euler, Leonhard, 465Euler-Lagrange equations, 705\n",
      "evidence approximation, 165, 347, 581\n",
      "evidence function, 161expectation, 19expectation conditional maximization, 454expectation maximization, 113, 423, 440\n",
      "Gaussian mixture, 435generalized, 454sampling methods, 536\n",
      "expectation propagation, 315, 468, 505\n",
      "expectation step, 437explaining away, 378exploitation, 3exploration, 3exponential distribution, 526, 688\n",
      "exponential family, 68, 113, 202, 490\n",
      "extensive variables, 490\n",
      "face detection, 2\n",
      "face tracking, 355factor analysis, 583\n",
      "mixture model, 595\n",
      "factor graph, 360, 399, 625\n",
      "factor loading, 584factorial hidden Markov model, 633factorized distribution, 464, 476\n",
      "feature extraction, 2feature map, 268feature space, 292, 586\n",
      "Fisher information matrix, 298\n",
      "Fisher kernel, 298\n",
      "Fisher’s linear discriminant, 186ﬂooding schedule, 417forward kinematics, 272forward problem, 272forward propagation, 228, 243\n",
      "forward-backward algorithm, 618fractional belief propagation, 517frequentist probability, 21fuel system, 376function interpolation, 299functional, 462, 703\n",
      "derivative, 463732 INDEX\n",
      "gamma densitometry, 678\n",
      "gamma distribution, 529, 688\n",
      "gamma function, 71gating function, 672Gauss, Carl Friedrich, 79\n",
      "Gaussian, 24, 78, 688\n",
      "conditional, 85,9 3\n",
      "marginal, 88,9 3\n",
      "maximum likelihood, 93\n",
      "mixture, 110, 270, 273, 430\n",
      "sequential estimation, 94sufﬁcient statistics, 93wrapped, 110\n",
      "Gaussian kernel, 296\n",
      "Gaussian process, 160, 303\n",
      "Gaussian random ﬁeld, 305Gaussian-gamma distribution, 101, 690\n",
      "Gaussian-Wishart distribution, 102, 475, 478, 690\n",
      "GEM, seeexpectation maximization, generalized\n",
      "generalization, 2generalized linear model, 180, 213\n",
      "generalized maximum likelihood, seeevidence ap-\n",
      "proximation\n",
      "generative model, 43, 196, 297, 365, 572, 631\n",
      "generative topographic mapping, 597\n",
      "directional curvature, 599magniﬁcation factor, 599\n",
      "geodesic distance, 596Gibbs sampling, 542\n",
      "blocking, 546\n",
      "Gibbs, Josiah Willard, 543Gini index, 666global minimum, 237\n",
      "gradient descent, 240\n",
      "Gram matrix, 293graph-cut algorithm, 390graphical model, 359\n",
      "bipartite, 401\n",
      "directed, 360factorization, 362, 384fully connected, 361\n",
      "inference, 393\n",
      "tree, 398treewidth, 417triangulated, 416undirected, 360\n",
      "Green’s function, 299GTM, seegenerative topographic mapping\n",
      "Hamilton, William Rowan, 549\n",
      "Hamiltonian dynamics, 548Hamiltonian function, 549Hammersley-Clifford theorem, 387handwriting recognition, 1, 610, 614handwritten digit, 565, 614, 677\n",
      "head-to-head path, 376head-to-tail path, 375Heaviside step function, 206Hellinger distance, 470Hessian matrix, 167, 215, 217, 238, 249\n",
      "diagonal approximation, 250exact evaluation, 253\n",
      "fast multiplication, 254\n",
      "ﬁnite differences, 252inverse, 252outer product approximation, 251\n",
      "heteroscedastic, 273, 311\n",
      "hidden Markov model, 297, 610\n",
      "autoregressive, 632factorial, 633forward-backward algorithm, 618input-output, 633left-to-right, 613maximum likelihood, 615scaling factor, 627sum-product algorithm, 625switching, 644\n",
      "variational inference, 625\n",
      "hidden unit, 227hidden variable, 84, 364, 430, 559\n",
      "hierarchical Bayesian model, 372hierarchical mixture of experts, 673hinge error function, 337Hinton diagram, 584histogram density estimation, 120HME, seehierarchical mixture of experts\n",
      "hold-out set, 11homogeneous ﬂow, 679homogeneous kernel, 292homogeneous Markov chain, 540, 608INDEX 733\n",
      "Hooke’s law, 580\n",
      "hybrid Monte Carlo, 548hyperparameter, 71, 280, 311, 346, 372, 502\n",
      "hyperprior, 372\n",
      "I map, seeindependence map\n",
      "i.i.d., seeindependent identically distributed\n",
      "ICA, seeindependent component analysis\n",
      "ICM, seeiterated conditional modes\n",
      "ID3, 663identiﬁability, 435image de-noising, 387importance sampling, 525, 532\n",
      "importance weights, 533improper prior, 118, 259, 472\n",
      "imputation step, 537imputation-posterior algorithm, 537\n",
      "inactive constraint, 328, 709\n",
      "incomplete data set, 440independence map, 392independent component analysis, 591independent factor analysis, 592independent identically distributed, 26, 379\n",
      "independent variables, 17independent, identically distributed, 605induced factorization, 485inequality constraint, 709inference, 38, 42\n",
      "information criterion, 33information geometry, 298information theory, 48input-output hidden Markov model, 633\n",
      "intensive variables, 490\n",
      "intrinsic dimensionality, 559invariance, 261inverse gamma distribution, 101inverse kinematics, 272inverse problem, 272inverse Wishart distribution, 102IP algorithm, seeimputation-posterior algorithm\n",
      "IRLS, seeiterative reweighted least squares\n",
      "Ising model, 389isomap, 596isometric feature map, 596iterated conditional modes, 389, 415iterative reweighted least squares, 207, 210, 316,\n",
      "354, 672\n",
      "Jacobian matrix, 247, 264\n",
      "Jensen’s inequality, 56\n",
      "join tree, 416\n",
      "junction tree algorithm, 392, 416\n",
      "Knearest neighbours, 125\n",
      "K-means clustering algorithm, 424, 443\n",
      "K-medoids algorithm, 428\n",
      "Kalman ﬁlter, 304, 637\n",
      "extended, 644\n",
      "Kalman gain matrix, 639Kalman smoother, 637\n",
      "Karhunen-Lo `eve transform, 561\n",
      "Karush-Kuhn-Tucker conditions, 330, 333, 342,\n",
      "710\n",
      "kernel density estimator, 122, 326\n",
      "kernel function, 123, 292, 294\n",
      "Fisher, 298\n",
      "Gaussian, 296\n",
      "homogeneous, 292\n",
      "nonvectorial inputs, 297\n",
      "stationary, 292\n",
      "kernel PCA, 586\n",
      "kernel regression, 300, 302\n",
      "kernel substitution, 292\n",
      "kernel trick, 292\n",
      "kinetic energy, 549KKT, seeKarush-Kuhn-Tucker conditions\n",
      "KL divergence, seeKullback-Leibler divergence\n",
      "kriging, seeGaussian process\n",
      "Kullback-Leibler divergence, 55, 451, 468, 505\n",
      "Lagrange multiplier, 707\n",
      "Lagrange, Joseph-Louis, 329\n",
      "Lagrangian, 328, 332, 341, 708\n",
      "laminar ﬂow, 678\n",
      "Laplace approximation, 213, 217, 278, 315, 354\n",
      "Laplace, Pierre-Simon, 24\n",
      "large margin, seemargin\n",
      "lasso, 145latent class analysis, 444\n",
      "latent trait model, 597\n",
      "latent variable, 84, 364, 430, 559734 INDEX\n",
      "lattice diagram, 414, 611, 621, 629\n",
      "LDS, seelinear dynamical system\n",
      "leapfrog discretization, 551\n",
      "learning, 2\n",
      "learning rate parameter, 240least-mean-squares algorithm, 144\n",
      "leave-one-out, 33\n",
      "likelihood function, 22likelihood weighted sampling, 534\n",
      "linear discriminant, 181\n",
      "Fisher, 186\n",
      "linear dynamical system, 84, 635\n",
      "inference, 638\n",
      "linear independence, 696\n",
      "linear regression, 138\n",
      "EM, 448mixture model, 667\n",
      "variational, 486\n",
      "linear smoother, 159linear-Gaussian model, 87, 370\n",
      "linearly separable, 179\n",
      "link, 360link function, 180, 213\n",
      "Liouville’s Theorem, 550\n",
      "LLE, seelocally linear embedding\n",
      "LMS algorithm, seeleast-mean-squares algorithm\n",
      "local minimum, 237\n",
      "local receptive ﬁeld, 268\n",
      "locally linear embedding, 596\n",
      "location parameter, 118log odds, 197\n",
      "logic sampling, 525\n",
      "logistic regression, 205, 336\n",
      "Bayesian, 217, 498\n",
      "mixture model, 670\n",
      "multiclass, 209\n",
      "logistic sigmoid, 114, 139, 197, 205, 220, 227, 495\n",
      "logit function, 197\n",
      "loopy belief propagation, 417loss function, 41\n",
      "loss matrix, 41\n",
      "lossless data compression, 429lossy data compression, 429\n",
      "lower bound, 484\n",
      "M step, seemaximization stepmachine learning, vii\n",
      "macrostate, 51Mahalanobis distance, 80manifold, 38, 590, 595, 681\n",
      "MAP, seemaximum posterior\n",
      "margin, 326, 327, 502\n",
      "error, 334soft, 332\n",
      "marginal likelihood, 162, 165\n",
      "marginal probability, 14\n",
      "Markov blanket, 382, 384, 545\n",
      "Markov boundary, seeMarkov blanket\n",
      "Markov chain, 397, 539\n",
      "ﬁrst order, 607\n",
      "homogeneous, 540, 608\n",
      "second order, 608\n",
      "Markov chain Monte Carlo, 537\n",
      "Markov model, 607\n",
      "homogeneous, 612\n",
      "Markov network, seeMarkov random ﬁeld\n",
      "Markov random ﬁeld, 84, 360, 383\n",
      "max-sum algorithm, 411, 629\n",
      "maximal clique, 385\n",
      "maximal spanning tree, 416maximization step, 437maximum likelihood, 9, 23, 26, 116\n",
      "Gaussian mixture, 432\n",
      "singularities, 480type 2, see\n",
      "evidence approximation\n",
      "maximum margin, seemargin\n",
      "maximum posterior, 30, 441\n",
      "MCMC, seeMarkov chain Monte Carlo\n",
      "MDN, seemixture density network\n",
      "MDS, seemultidimensional scaling\n",
      "mean, 24\n",
      "mean ﬁeld theory, 465\n",
      "mean value theorem, 52measure theory, 19memory-based methods, 292\n",
      "message passing, 396\n",
      "pending message, 417schedule, 417variational, 491\n",
      "Metropolis algorithm, 538\n",
      "Metropolis-Hastings algorithm, 541INDEX 735\n",
      "microstate, 51\n",
      "minimum risk, 44Minkowski loss, 48\n",
      "missing at random, 441, 579\n",
      "missing data, 579mixing coefﬁcient, 111mixture component, 111mixture density network, 272, 673\n",
      "mixture distribution, seemixture model\n",
      "mixture model, 162, 423\n",
      "conditional, 273, 666\n",
      "linear regression, 667logistic regression, 670symmetries, 483\n",
      "mixture of experts, 672mixture of Gaussians, 110, 270, 273, 430\n",
      "MLP, seemultilayer perceptron\n",
      "MNIST data, 677\n",
      "model comparison, 6, 32, 161, 473, 483\n",
      "model evidence, 161model selection, 162moment matching, 506, 510\n",
      "momentum variable, 548Monte Carlo EM algorithm, 536Monte Carlo sampling, 24, 523\n",
      "Moore-Penrose pseudo-inverse, seepseudo-inverse\n",
      "moralization, 391, 401\n",
      "MRF, seeMarkov random ﬁeld\n",
      "multidimensional scaling, 596multilayer perceptron, 226, 229\n",
      "multimodality, 272multinomial distribution, 76, 114, 690\n",
      "multiplicity, 51\n",
      "mutual information, 55, 57\n",
      "Nadaraya-Watson, seekernel regression\n",
      "naive Bayes model, 46, 380\n",
      "nats, 50natural language modelling, 610natural parameters, 113nearest-neighbour methods, 124neural network, 225\n",
      "convolutional, 267regularization, 256relation to Gaussian process, 319Newton-Raphson, 207, 317\n",
      "node, 360\n",
      "noiseless coding theorem, 50nonidentiﬁability, 585\n",
      "noninformative prior, 23, 117\n",
      "nonparametric methods, 68, 120\n",
      "normal distribution, seeGaussian\n",
      "normal equations, 142normal-gamma distribution, 101, 691\n",
      "normal-Wishart distribution, 102, 475, 478, 691\n",
      "normalized exponential, seesoftmax function\n",
      "novelty detection, 44\n",
      "ν-SVM, 334\n",
      "object recognition, 366\n",
      "observed variable, 364Occam factor, 217\n",
      "oil ﬂow data, 34, 560, 568, 678\n",
      "Old Faithful data, 110, 479, 484, 681\n",
      "on-line learning, seesequential learning\n",
      "one-versus-one classiﬁer, 183, 339\n",
      "one-versus-the-rest classiﬁer, 182, 338\n",
      "ordered over-relaxation, 545\n",
      "Ornstein-Uhlenbeck process, 305orthogonal least squares, 301\n",
      "outlier, 44, 185, 212\n",
      "outliers, 103\n",
      "over-ﬁtting, 6\n",
      ", 147, 434, 464\n",
      "over-relaxation, 544\n",
      "PAC learning, seeprobably approximately correct\n",
      "PAC-Bayesian framework, 345\n",
      "parameter shrinkage, 144\n",
      "parent node, 361particle ﬁlter, 645\n",
      "partition function, 386, 554\n",
      "Parzen estimator, seekernel density estimator\n",
      "Parzen window, 123\n",
      "pattern recognition, vii\n",
      "PCA, seeprincipal component analysis\n",
      "pending message, 417\n",
      "perceptron, 192\n",
      "convergence theorem, 194\n",
      "hardware, 196\n",
      "perceptron criterion, 193perfect map, 392736 INDEX\n",
      "periodic variable, 105\n",
      "phase space, 549photon noise, 680plate, 363\n",
      "polynomial curve ﬁtting, 4, 362\n",
      "polytree, 399position variable, 548positive deﬁnite covariance, 81positive deﬁnite matrix, 701\n",
      "positive semideﬁnite covariance, 81\n",
      "positive semideﬁnite matrix, 701posterior probability, 17posterior step, 537\n",
      "potential energy, 549\n",
      "potential function, 386power EP, 517power method, 563\n",
      "precision matrix, 85\n",
      "precision parameter, 24predictive distribution, 30, 156\n",
      "preprocessing, 2principal component analysis, 561, 572, 593\n",
      "Bayesian, 580\n",
      "EM algorithm, 577Gibbs sampling, 583mixture distribution, 595\n",
      "physical analogy, 580\n",
      "principal curve, 595principal subspace, 561principal surface, 596prior, 17\n",
      "conjugate, 68, 98, 117, 490\n",
      "consistent, 257improper, 118, 259, 472\n",
      "noninformative, 23, 117\n",
      "probabilistic graphical model, seegraphical model\n",
      "probabilistic PCA, 570probability, 12\n",
      "Bayesian, 21\n",
      "classical, 21\n",
      "density, 17frequentist, 21mass function, 19prior, 45\n",
      "product rule, 13, 14, 359sum rule, 13, 14, 359\n",
      "theory, 12\n",
      "probably approximately correct, 344\n",
      "probit function, 211, 219\n",
      "probit regression, 210product rule of probability, 13, 14, 359\n",
      "proposal distribution, 528, 532, 538\n",
      "protected conjugate gradients, 335\n",
      "protein sequence, 610\n",
      "pseudo-inverse, 142, 185\n",
      "pseudo-random numbers, 526\n",
      "quadratic discriminant, 199\n",
      "quality parameter, 351\n",
      "radial basis function, 292, 299\n",
      "Rauch-Tung-Striebel equations, 637\n",
      "regression, 3regression function, 47,9 5\n",
      "regularization, 10\n",
      "Tikhonov, 267\n",
      "regularized least squares, 144\n",
      "reinforcement learning, 3\n",
      "reject option, 42,4 5\n",
      "rejection sampling, 528\n",
      "relative entropy, 55relevance vector, 348\n",
      "relevance vector machine, 161, 345\n",
      "responsibility, 112, 432, 477\n",
      "ridge regression, 10\n",
      "RMS error, seeroot-mean-square error\n",
      "Robbins-Monro algorithm, 95robot arm, 272\n",
      "robustness, 103, 185\n",
      "root node, 399root-mean-square error, 6\n",
      "Rosenblatt, Frank, 193\n",
      "rotation invariance, 573, 585\n",
      "RTS equations, seeRauch-Tung-Striebel equations\n",
      "running intersection property, 416RVM, seerelevance vector machine\n",
      "sample mean, 27\n",
      "sample variance, 27\n",
      "sampling-importance-resampling, 534\n",
      "scale invariance, 119, 261INDEX 737\n",
      "scale parameter, 119\n",
      "scaling factor, 627Schwarz criterion, seeBayesian information crite-\n",
      "rion\n",
      "self-organizing map, 598sequential data, 605sequential estimation, 94sequential gradient descent, 144, 240sequential learning, 73, 143\n",
      "sequential minimal optimization, 335serial message passing schedule, 417\n",
      "Shannon, Claude, 55\n",
      "shared parameters, 368shrinkage, 10Shur complement, 87sigmoid, seelogistic sigmoid\n",
      "simplex, 76single-class support vector machine, 339singular value decomposition, 143sinusoidal data, 682SIR, seesampling-importance-resampling\n",
      "skip-layer connection, 229slack variable, 331slice sampling, 546SMO, seesequential minimal optimization\n",
      "smoother matrix, 159smoothing parameter, 122soft margin, 332\n",
      "soft weight sharing, 269\n",
      "softmax function, 115, 198, 236, 274, 356, 497\n",
      "SOM, seeself-organizing map\n",
      "sparsity, 145, 347, 349, 582\n",
      "sparsity parameter, 351spectrogram, 606speech recognition, 605, 610\n",
      "sphereing, 568spline functions, 139standard deviation, 24standardizing, 425, 567\n",
      "state space model, 609\n",
      "switching, 644\n",
      "stationary kernel, 292statistical bias, seebias\n",
      "statistical independence, seeindependent variablesstatistical learning theory, seecomputational learn-\n",
      "ing theory, 326, 344\n",
      "steepest descent, 240\n",
      "Stirling’s approximation, 51\n",
      "stochastic, 5\n",
      "stochastic EM, 536\n",
      "stochastic gradient descent, 144, 240\n",
      "stochastic process, 305\n",
      "stratiﬁed ﬂow, 678\n",
      "Student’s t-distribution, 102, 483, 691\n",
      "subsampling, 268\n",
      "sufﬁcient statistics, 69, 75, 116\n",
      "sum rule of probability, 13, 14, 359\n",
      "sum-of-squares error, 5, 29, 184, 232, 662\n",
      "sum-product algorithm, 399, 402\n",
      "for hidden Markov model, 625\n",
      "supervised learning, 3\n",
      "support vector, 330\n",
      "support vector machine, 225\n",
      "for regression, 339\n",
      "multiclass, 338\n",
      "survival of the ﬁttest, 646\n",
      "SVD, seesingular value decomposition\n",
      "SVM, seesupport vector machine\n",
      "switching hidden Markov model, 644switching state space model, 644\n",
      "synthetic data sets, 682\n",
      "tail-to-tail path, 374\n",
      "tangent distance, 265\n",
      "tangent propagation, 262, 263\n",
      "tapped delay line, 609\n",
      "target vector, 2\n",
      "test set, 2, 32\n",
      "threshold parameter, 181\n",
      "tied parameters, 368\n",
      "Tikhonov regularization, 267\n",
      "time warping, 615\n",
      "tomography, 679\n",
      "training, 2\n",
      "training set, 2\n",
      "transition probability,\n",
      "540, 610\n",
      "translation invariance, 118, 261\n",
      "tree-reweighted message passing, 517\n",
      "treewidth, 417738 INDEX\n",
      "trellis diagram, seelattice diagram\n",
      "triangulated graph, 416type 2 maximum likelihood, seeevidence approxi-\n",
      "mation\n",
      "undetermined multiplier, seeLagrange multiplier\n",
      "undirected graph, seeMarkov random ﬁeld\n",
      "uniform distribution, 692\n",
      "uniform sampling, 534uniquenesses, 584\n",
      "unobserved variable, seelatent variable\n",
      "unsupervised learning, 3utility function, 41\n",
      "validation set, 11, 32\n",
      "Vapnik-Chervonenkis dimension, 344\n",
      "variance, 20, 24, 149\n",
      "variational inference, 315, 462, 635\n",
      "for Gaussian mixture, 474\n",
      "for hidden Markov model, 625\n",
      "local, 493\n",
      "VC dimension, seeVapnik-Chervonenkis dimen-\n",
      "sion\n",
      "vector quantization, 429vertex, seenode\n",
      "visualization, 3\n",
      "Viterbi algorithm, 415, 629\n",
      "von Mises distribution, 108, 693\n",
      "wavelets, 139\n",
      "weak learner, 657weight decay, 10, 144, 257\n",
      "weight parameter, 227\n",
      "weight sharing, 268\n",
      "soft, 269\n",
      "weight vector, 181\n",
      "weight-space symmetry, 231, 281\n",
      "weighted least squares, 668\n",
      "well-determined parameters, 170whitening, 299, 568\n",
      "Wishart distribution, 102, 693\n",
      "within-class covariance, 189Woodbury identity, 696\n",
      "wrapped distribution, 110\n",
      "Yellowstone National Park, 110, 681\n"
     ]
    }
   ],
   "source": [
    "print(question_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the extracted text to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TextSplitter\n",
    "\n",
    "splitter_question_gen = TextSplitter(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
